import torch
import torch.nn as nn

class trunk_module(nn.Module):
    def  __init__(self, model):
        super(trunk_module, self).__init__()
        self.device = torch.device('cuda')
        self.arg0_1 = getattr(model.template_embedder.pairformer.blocks, "0").transition_pair.layer_norm.weight
        self.arg1_1 = getattr(model.template_embedder.pairformer.blocks, "0").transition_pair.layer_norm.bias
        self.arg2_1 = getattr(model.template_embedder.pairformer.blocks, "0").transition_pair.linear_no_bias_ab.weight
        self.arg3_1 = getattr(model.template_embedder.pairformer.blocks, "0").transition_pair.linear_out.weight
        self.arg4_1 = getattr(model.template_embedder.pairformer.blocks, "0").triangle_multiplication.layernorm_z_in.weight
        self.arg5_1 = getattr(model.template_embedder.pairformer.blocks, "0").triangle_multiplication.layernorm_z_in.bias
        self.arg6_1 = getattr(model.template_embedder.pairformer.blocks, "0").triangle_multiplication.linear_z_out.weight
        self.arg7_1 = getattr(model.template_embedder.pairformer.blocks, "0").triangle_multiplication.merged_linear_p.weight
        self.arg8_1 = getattr(model.template_embedder.pairformer.blocks, "0").triangle_multiplication.merged_linear_g.weight
        self.arg9_1 = getattr(model.template_embedder.pairformer.blocks, "0").triangle_attention.out_scalers
        self.arg10_1 = getattr(model.template_embedder.pairformer.blocks, "0").triangle_attention.pair2b.weight
        self.arg11_1 = getattr(model.template_embedder.pairformer.blocks, "0").triangle_attention.pair2qkvg1.weight
        self.arg12_1 = getattr(model.template_embedder.pairformer.blocks, "0").triangle_attention.pair2qkvg2.weight
        self.arg13_1 = getattr(model.template_embedder.pairformer.blocks, "0").triangle_attention.linear_out.weight
        self.arg14_1 = getattr(model.template_embedder.pairformer.blocks, "1").transition_pair.layer_norm.weight
        self.arg15_1 = getattr(model.template_embedder.pairformer.blocks, "1").transition_pair.layer_norm.bias
        self.arg16_1 = getattr(model.template_embedder.pairformer.blocks, "1").transition_pair.linear_no_bias_ab.weight
        self.arg17_1 = getattr(model.template_embedder.pairformer.blocks, "1").transition_pair.linear_out.weight
        self.arg18_1 = getattr(model.template_embedder.pairformer.blocks, "1").triangle_multiplication.layernorm_z_in.weight
        self.arg19_1 = getattr(model.template_embedder.pairformer.blocks, "1").triangle_multiplication.layernorm_z_in.bias
        self.arg20_1 = getattr(model.template_embedder.pairformer.blocks, "1").triangle_multiplication.linear_z_out.weight
        self.arg21_1 = getattr(model.template_embedder.pairformer.blocks, "1").triangle_multiplication.merged_linear_p.weight
        self.arg22_1 = getattr(model.template_embedder.pairformer.blocks, "1").triangle_multiplication.merged_linear_g.weight
        self.arg23_1 = getattr(model.template_embedder.pairformer.blocks, "1").triangle_attention.out_scalers
        self.arg24_1 = getattr(model.template_embedder.pairformer.blocks, "1").triangle_attention.pair2b.weight
        self.arg25_1 = getattr(model.template_embedder.pairformer.blocks, "1").triangle_attention.pair2qkvg1.weight
        self.arg26_1 = getattr(model.template_embedder.pairformer.blocks, "1").triangle_attention.pair2qkvg2.weight
        self.arg27_1 = getattr(model.template_embedder.pairformer.blocks, "1").triangle_attention.linear_out.weight
        self.arg28_1 = getattr(model.template_embedder.proj_in, "0").weight
        self.arg29_1 = getattr(model.template_embedder.proj_in, "0").bias
        self.arg30_1 = getattr(model.template_embedder.proj_in, "1").weight
        self.arg31_1 = model.template_embedder.template_layernorm.weight
        self.arg32_1 = model.template_embedder.template_layernorm.bias
        self.arg33_1 = getattr(model.template_embedder.proj_out, "1").weight
        self.arg34_1 = model.msa_module.linear_s2m.weight
        self.arg35_1 = getattr(model.msa_module.outer_product_mean, "0").weight_ab
        self.arg36_1 = getattr(model.msa_module.outer_product_mean, "0").ln_out.weight
        self.arg37_1 = getattr(model.msa_module.outer_product_mean, "0").ln_out.bias
        self.arg38_1 = getattr(model.msa_module.outer_product_mean, "0").linear_out.weight
        self.arg39_1 = getattr(model.msa_module.outer_product_mean, "0").linear_out.bias
        self.arg40_1 = getattr(model.msa_module.outer_product_mean, "1").weight_ab
        self.arg41_1 = getattr(model.msa_module.outer_product_mean, "1").ln_out.weight
        self.arg42_1 = getattr(model.msa_module.outer_product_mean, "1").ln_out.bias
        self.arg43_1 = getattr(model.msa_module.outer_product_mean, "1").linear_out.weight
        self.arg44_1 = getattr(model.msa_module.outer_product_mean, "1").linear_out.bias
        self.arg45_1 = getattr(model.msa_module.outer_product_mean, "2").weight_ab
        self.arg46_1 = getattr(model.msa_module.outer_product_mean, "2").ln_out.weight
        self.arg47_1 = getattr(model.msa_module.outer_product_mean, "2").ln_out.bias
        self.arg48_1 = getattr(model.msa_module.outer_product_mean, "2").linear_out.weight
        self.arg49_1 = getattr(model.msa_module.outer_product_mean, "2").linear_out.bias
        self.arg50_1 = getattr(model.msa_module.outer_product_mean, "3").weight_ab
        self.arg51_1 = getattr(model.msa_module.outer_product_mean, "3").ln_out.weight
        self.arg52_1 = getattr(model.msa_module.outer_product_mean, "3").ln_out.bias
        self.arg53_1 = getattr(model.msa_module.outer_product_mean, "3").linear_out.weight
        self.arg54_1 = getattr(model.msa_module.outer_product_mean, "3").linear_out.bias
        self.arg55_1 = getattr(model.msa_module.msa_pair_weighted_averaging, "0").layernorm_msa.weight
        self.arg56_1 = getattr(model.msa_module.msa_pair_weighted_averaging, "0").layernorm_msa.bias
        self.arg57_1 = getattr(model.msa_module.msa_pair_weighted_averaging, "0").linear_msa2vg.weight
        self.arg58_1 = getattr(model.msa_module.msa_pair_weighted_averaging, "0").layernorm_pair.weight
        self.arg59_1 = getattr(model.msa_module.msa_pair_weighted_averaging, "0").layernorm_pair.bias
        self.arg60_1 = getattr(model.msa_module.msa_pair_weighted_averaging, "0").linear_pair.weight
        self.arg61_1 = getattr(model.msa_module.msa_pair_weighted_averaging, "0").linear_out_no_bias.weight
        self.arg62_1 = getattr(model.msa_module.msa_pair_weighted_averaging, "1").layernorm_msa.weight
        self.arg63_1 = getattr(model.msa_module.msa_pair_weighted_averaging, "1").layernorm_msa.bias
        self.arg64_1 = getattr(model.msa_module.msa_pair_weighted_averaging, "1").linear_msa2vg.weight
        self.arg65_1 = getattr(model.msa_module.msa_pair_weighted_averaging, "1").layernorm_pair.weight
        self.arg66_1 = getattr(model.msa_module.msa_pair_weighted_averaging, "1").layernorm_pair.bias
        self.arg67_1 = getattr(model.msa_module.msa_pair_weighted_averaging, "1").linear_pair.weight
        self.arg68_1 = getattr(model.msa_module.msa_pair_weighted_averaging, "1").linear_out_no_bias.weight
        self.arg69_1 = getattr(model.msa_module.msa_pair_weighted_averaging, "2").layernorm_msa.weight
        self.arg70_1 = getattr(model.msa_module.msa_pair_weighted_averaging, "2").layernorm_msa.bias
        self.arg71_1 = getattr(model.msa_module.msa_pair_weighted_averaging, "2").linear_msa2vg.weight
        self.arg72_1 = getattr(model.msa_module.msa_pair_weighted_averaging, "2").layernorm_pair.weight
        self.arg73_1 = getattr(model.msa_module.msa_pair_weighted_averaging, "2").layernorm_pair.bias
        self.arg74_1 = getattr(model.msa_module.msa_pair_weighted_averaging, "2").linear_pair.weight
        self.arg75_1 = getattr(model.msa_module.msa_pair_weighted_averaging, "2").linear_out_no_bias.weight
        self.arg76_1 = getattr(model.msa_module.msa_transition, "0").layer_norm.weight
        self.arg77_1 = getattr(model.msa_module.msa_transition, "0").layer_norm.bias
        self.arg78_1 = getattr(model.msa_module.msa_transition, "0").linear_no_bias_ab.weight
        self.arg79_1 = getattr(model.msa_module.msa_transition, "0").linear_out.weight
        self.arg80_1 = getattr(model.msa_module.msa_transition, "1").layer_norm.weight
        self.arg81_1 = getattr(model.msa_module.msa_transition, "1").layer_norm.bias
        self.arg82_1 = getattr(model.msa_module.msa_transition, "1").linear_no_bias_ab.weight
        self.arg83_1 = getattr(model.msa_module.msa_transition, "1").linear_out.weight
        self.arg84_1 = getattr(model.msa_module.msa_transition, "2").layer_norm.weight
        self.arg85_1 = getattr(model.msa_module.msa_transition, "2").layer_norm.bias
        self.arg86_1 = getattr(model.msa_module.msa_transition, "2").linear_no_bias_ab.weight
        self.arg87_1 = getattr(model.msa_module.msa_transition, "2").linear_out.weight
        self.arg88_1 = getattr(model.msa_module.pair_transition, "0").layer_norm.weight
        self.arg89_1 = getattr(model.msa_module.pair_transition, "0").layer_norm.bias
        self.arg90_1 = getattr(model.msa_module.pair_transition, "0").linear_no_bias_ab.weight
        self.arg91_1 = getattr(model.msa_module.pair_transition, "0").linear_out.weight
        self.arg92_1 = getattr(model.msa_module.pair_transition, "1").layer_norm.weight
        self.arg93_1 = getattr(model.msa_module.pair_transition, "1").layer_norm.bias
        self.arg94_1 = getattr(model.msa_module.pair_transition, "1").linear_no_bias_ab.weight
        self.arg95_1 = getattr(model.msa_module.pair_transition, "1").linear_out.weight
        self.arg96_1 = getattr(model.msa_module.pair_transition, "2").layer_norm.weight
        self.arg97_1 = getattr(model.msa_module.pair_transition, "2").layer_norm.bias
        self.arg98_1 = getattr(model.msa_module.pair_transition, "2").linear_no_bias_ab.weight
        self.arg99_1 = getattr(model.msa_module.pair_transition, "2").linear_out.weight
        self.arg100_1 = getattr(model.msa_module.pair_transition, "3").layer_norm.weight
        self.arg101_1 = getattr(model.msa_module.pair_transition, "3").layer_norm.bias
        self.arg102_1 = getattr(model.msa_module.pair_transition, "3").linear_no_bias_ab.weight
        self.arg103_1 = getattr(model.msa_module.pair_transition, "3").linear_out.weight
        self.arg104_1 = getattr(model.msa_module.triangular_multiplication, "0").layernorm_z_in.weight
        self.arg105_1 = getattr(model.msa_module.triangular_multiplication, "0").layernorm_z_in.bias
        self.arg106_1 = getattr(model.msa_module.triangular_multiplication, "0").linear_z_out.weight
        self.arg107_1 = getattr(model.msa_module.triangular_multiplication, "0").merged_linear_p.weight
        self.arg108_1 = getattr(model.msa_module.triangular_multiplication, "0").merged_linear_g.weight
        self.arg109_1 = getattr(model.msa_module.triangular_multiplication, "1").layernorm_z_in.weight
        self.arg110_1 = getattr(model.msa_module.triangular_multiplication, "1").layernorm_z_in.bias
        self.arg111_1 = getattr(model.msa_module.triangular_multiplication, "1").linear_z_out.weight
        self.arg112_1 = getattr(model.msa_module.triangular_multiplication, "1").merged_linear_p.weight
        self.arg113_1 = getattr(model.msa_module.triangular_multiplication, "1").merged_linear_g.weight
        self.arg114_1 = getattr(model.msa_module.triangular_multiplication, "2").layernorm_z_in.weight
        self.arg115_1 = getattr(model.msa_module.triangular_multiplication, "2").layernorm_z_in.bias
        self.arg116_1 = getattr(model.msa_module.triangular_multiplication, "2").linear_z_out.weight
        self.arg117_1 = getattr(model.msa_module.triangular_multiplication, "2").merged_linear_p.weight
        self.arg118_1 = getattr(model.msa_module.triangular_multiplication, "2").merged_linear_g.weight
        self.arg119_1 = getattr(model.msa_module.triangular_multiplication, "3").layernorm_z_in.weight
        self.arg120_1 = getattr(model.msa_module.triangular_multiplication, "3").layernorm_z_in.bias
        self.arg121_1 = getattr(model.msa_module.triangular_multiplication, "3").linear_z_out.weight
        self.arg122_1 = getattr(model.msa_module.triangular_multiplication, "3").merged_linear_p.weight
        self.arg123_1 = getattr(model.msa_module.triangular_multiplication, "3").merged_linear_g.weight
        self.arg124_1 = getattr(model.msa_module.triangular_attention, "0").out_scalers
        self.arg125_1 = getattr(model.msa_module.triangular_attention, "0").pair2b.weight
        self.arg126_1 = getattr(model.msa_module.triangular_attention, "0").pair2qkvg1.weight
        self.arg127_1 = getattr(model.msa_module.triangular_attention, "0").pair2qkvg2.weight
        self.arg128_1 = getattr(model.msa_module.triangular_attention, "0").linear_out.weight
        self.arg129_1 = getattr(model.msa_module.triangular_attention, "1").out_scalers
        self.arg130_1 = getattr(model.msa_module.triangular_attention, "1").pair2b.weight
        self.arg131_1 = getattr(model.msa_module.triangular_attention, "1").pair2qkvg1.weight
        self.arg132_1 = getattr(model.msa_module.triangular_attention, "1").pair2qkvg2.weight
        self.arg133_1 = getattr(model.msa_module.triangular_attention, "1").linear_out.weight
        self.arg134_1 = getattr(model.msa_module.triangular_attention, "2").out_scalers
        self.arg135_1 = getattr(model.msa_module.triangular_attention, "2").pair2b.weight
        self.arg136_1 = getattr(model.msa_module.triangular_attention, "2").pair2qkvg1.weight
        self.arg137_1 = getattr(model.msa_module.triangular_attention, "2").pair2qkvg2.weight
        self.arg138_1 = getattr(model.msa_module.triangular_attention, "2").linear_out.weight
        self.arg139_1 = getattr(model.msa_module.triangular_attention, "3").out_scalers
        self.arg140_1 = getattr(model.msa_module.triangular_attention, "3").pair2b.weight
        self.arg141_1 = getattr(model.msa_module.triangular_attention, "3").pair2qkvg1.weight
        self.arg142_1 = getattr(model.msa_module.triangular_attention, "3").pair2qkvg2.weight
        self.arg143_1 = getattr(model.msa_module.triangular_attention, "3").linear_out.weight
        self.arg144_1 = getattr(model.pairformer_stack.blocks, "0").transition_pair.layer_norm.weight
        self.arg145_1 = getattr(model.pairformer_stack.blocks, "0").transition_pair.layer_norm.bias
        self.arg146_1 = getattr(model.pairformer_stack.blocks, "0").transition_pair.linear_no_bias_ab.weight
        self.arg147_1 = getattr(model.pairformer_stack.blocks, "0").transition_pair.linear_out.weight
        self.arg148_1 = getattr(model.pairformer_stack.blocks, "0").triangle_multiplication.layernorm_z_in.weight
        self.arg149_1 = getattr(model.pairformer_stack.blocks, "0").triangle_multiplication.layernorm_z_in.bias
        self.arg150_1 = getattr(model.pairformer_stack.blocks, "0").triangle_multiplication.linear_z_out.weight
        self.arg151_1 = getattr(model.pairformer_stack.blocks, "0").triangle_multiplication.merged_linear_p.weight
        self.arg152_1 = getattr(model.pairformer_stack.blocks, "0").triangle_multiplication.merged_linear_g.weight
        self.arg153_1 = getattr(model.pairformer_stack.blocks, "0").triangle_attention.out_scalers
        self.arg154_1 = getattr(model.pairformer_stack.blocks, "0").triangle_attention.pair2b.weight
        self.arg155_1 = getattr(model.pairformer_stack.blocks, "0").triangle_attention.pair2qkvg1.weight
        self.arg156_1 = getattr(model.pairformer_stack.blocks, "0").triangle_attention.pair2qkvg2.weight
        self.arg157_1 = getattr(model.pairformer_stack.blocks, "0").triangle_attention.linear_out.weight
        self.arg158_1 = getattr(model.pairformer_stack.blocks, "0").transition_single.layer_norm.weight
        self.arg159_1 = getattr(model.pairformer_stack.blocks, "0").transition_single.layer_norm.bias
        self.arg160_1 = getattr(model.pairformer_stack.blocks, "0").transition_single.linear_no_bias_ab.weight
        self.arg161_1 = getattr(model.pairformer_stack.blocks, "0").transition_single.linear_out.weight
        self.arg162_1 = getattr(model.pairformer_stack.blocks, "0").attention_pair_bias.single_layer_norm.weight
        self.arg163_1 = getattr(model.pairformer_stack.blocks, "0").attention_pair_bias.single_layer_norm.bias
        self.arg164_1 = getattr(model.pairformer_stack.blocks, "0").attention_pair_bias.pair_layer_norm.weight
        self.arg165_1 = getattr(model.pairformer_stack.blocks, "0").attention_pair_bias.pair_layer_norm.bias
        self.arg166_1 = getattr(model.pairformer_stack.blocks, "0").attention_pair_bias.pair_linear.weight
        self.arg167_1 = getattr(model.pairformer_stack.blocks, "0").attention_pair_bias.attention.query_bias
        self.arg168_1 = getattr(model.pairformer_stack.blocks, "0").attention_pair_bias.attention.input2qkvg.weight
        self.arg169_1 = getattr(model.pairformer_stack.blocks, "0").attention_pair_bias.attention.output_proj.weight
        self.arg170_1 = getattr(model.pairformer_stack.blocks, "1").transition_pair.layer_norm.weight
        self.arg171_1 = getattr(model.pairformer_stack.blocks, "1").transition_pair.layer_norm.bias
        self.arg172_1 = getattr(model.pairformer_stack.blocks, "1").transition_pair.linear_no_bias_ab.weight
        self.arg173_1 = getattr(model.pairformer_stack.blocks, "1").transition_pair.linear_out.weight
        self.arg174_1 = getattr(model.pairformer_stack.blocks, "1").triangle_multiplication.layernorm_z_in.weight
        self.arg175_1 = getattr(model.pairformer_stack.blocks, "1").triangle_multiplication.layernorm_z_in.bias
        self.arg176_1 = getattr(model.pairformer_stack.blocks, "1").triangle_multiplication.linear_z_out.weight
        self.arg177_1 = getattr(model.pairformer_stack.blocks, "1").triangle_multiplication.merged_linear_p.weight
        self.arg178_1 = getattr(model.pairformer_stack.blocks, "1").triangle_multiplication.merged_linear_g.weight
        self.arg179_1 = getattr(model.pairformer_stack.blocks, "1").triangle_attention.out_scalers
        self.arg180_1 = getattr(model.pairformer_stack.blocks, "1").triangle_attention.pair2b.weight
        self.arg181_1 = getattr(model.pairformer_stack.blocks, "1").triangle_attention.pair2qkvg1.weight
        self.arg182_1 = getattr(model.pairformer_stack.blocks, "1").triangle_attention.pair2qkvg2.weight
        self.arg183_1 = getattr(model.pairformer_stack.blocks, "1").triangle_attention.linear_out.weight
        self.arg184_1 = getattr(model.pairformer_stack.blocks, "1").transition_single.layer_norm.weight
        self.arg185_1 = getattr(model.pairformer_stack.blocks, "1").transition_single.layer_norm.bias
        self.arg186_1 = getattr(model.pairformer_stack.blocks, "1").transition_single.linear_no_bias_ab.weight
        self.arg187_1 = getattr(model.pairformer_stack.blocks, "1").transition_single.linear_out.weight
        self.arg188_1 = getattr(model.pairformer_stack.blocks, "1").attention_pair_bias.single_layer_norm.weight
        self.arg189_1 = getattr(model.pairformer_stack.blocks, "1").attention_pair_bias.single_layer_norm.bias
        self.arg190_1 = getattr(model.pairformer_stack.blocks, "1").attention_pair_bias.pair_layer_norm.weight
        self.arg191_1 = getattr(model.pairformer_stack.blocks, "1").attention_pair_bias.pair_layer_norm.bias
        self.arg192_1 = getattr(model.pairformer_stack.blocks, "1").attention_pair_bias.pair_linear.weight
        self.arg193_1 = getattr(model.pairformer_stack.blocks, "1").attention_pair_bias.attention.query_bias
        self.arg194_1 = getattr(model.pairformer_stack.blocks, "1").attention_pair_bias.attention.input2qkvg.weight
        self.arg195_1 = getattr(model.pairformer_stack.blocks, "1").attention_pair_bias.attention.output_proj.weight
        self.arg196_1 = getattr(model.pairformer_stack.blocks, "2").transition_pair.layer_norm.weight
        self.arg197_1 = getattr(model.pairformer_stack.blocks, "2").transition_pair.layer_norm.bias
        self.arg198_1 = getattr(model.pairformer_stack.blocks, "2").transition_pair.linear_no_bias_ab.weight
        self.arg199_1 = getattr(model.pairformer_stack.blocks, "2").transition_pair.linear_out.weight
        self.arg200_1 = getattr(model.pairformer_stack.blocks, "2").triangle_multiplication.layernorm_z_in.weight
        self.arg201_1 = getattr(model.pairformer_stack.blocks, "2").triangle_multiplication.layernorm_z_in.bias
        self.arg202_1 = getattr(model.pairformer_stack.blocks, "2").triangle_multiplication.linear_z_out.weight
        self.arg203_1 = getattr(model.pairformer_stack.blocks, "2").triangle_multiplication.merged_linear_p.weight
        self.arg204_1 = getattr(model.pairformer_stack.blocks, "2").triangle_multiplication.merged_linear_g.weight
        self.arg205_1 = getattr(model.pairformer_stack.blocks, "2").triangle_attention.out_scalers
        self.arg206_1 = getattr(model.pairformer_stack.blocks, "2").triangle_attention.pair2b.weight
        self.arg207_1 = getattr(model.pairformer_stack.blocks, "2").triangle_attention.pair2qkvg1.weight
        self.arg208_1 = getattr(model.pairformer_stack.blocks, "2").triangle_attention.pair2qkvg2.weight
        self.arg209_1 = getattr(model.pairformer_stack.blocks, "2").triangle_attention.linear_out.weight
        self.arg210_1 = getattr(model.pairformer_stack.blocks, "2").transition_single.layer_norm.weight
        self.arg211_1 = getattr(model.pairformer_stack.blocks, "2").transition_single.layer_norm.bias
        self.arg212_1 = getattr(model.pairformer_stack.blocks, "2").transition_single.linear_no_bias_ab.weight
        self.arg213_1 = getattr(model.pairformer_stack.blocks, "2").transition_single.linear_out.weight
        self.arg214_1 = getattr(model.pairformer_stack.blocks, "2").attention_pair_bias.single_layer_norm.weight
        self.arg215_1 = getattr(model.pairformer_stack.blocks, "2").attention_pair_bias.single_layer_norm.bias
        self.arg216_1 = getattr(model.pairformer_stack.blocks, "2").attention_pair_bias.pair_layer_norm.weight
        self.arg217_1 = getattr(model.pairformer_stack.blocks, "2").attention_pair_bias.pair_layer_norm.bias
        self.arg218_1 = getattr(model.pairformer_stack.blocks, "2").attention_pair_bias.pair_linear.weight
        self.arg219_1 = getattr(model.pairformer_stack.blocks, "2").attention_pair_bias.attention.query_bias
        self.arg220_1 = getattr(model.pairformer_stack.blocks, "2").attention_pair_bias.attention.input2qkvg.weight
        self.arg221_1 = getattr(model.pairformer_stack.blocks, "2").attention_pair_bias.attention.output_proj.weight
        self.arg222_1 = getattr(model.pairformer_stack.blocks, "3").transition_pair.layer_norm.weight
        self.arg223_1 = getattr(model.pairformer_stack.blocks, "3").transition_pair.layer_norm.bias
        self.arg224_1 = getattr(model.pairformer_stack.blocks, "3").transition_pair.linear_no_bias_ab.weight
        self.arg225_1 = getattr(model.pairformer_stack.blocks, "3").transition_pair.linear_out.weight
        self.arg226_1 = getattr(model.pairformer_stack.blocks, "3").triangle_multiplication.layernorm_z_in.weight
        self.arg227_1 = getattr(model.pairformer_stack.blocks, "3").triangle_multiplication.layernorm_z_in.bias
        self.arg228_1 = getattr(model.pairformer_stack.blocks, "3").triangle_multiplication.linear_z_out.weight
        self.arg229_1 = getattr(model.pairformer_stack.blocks, "3").triangle_multiplication.merged_linear_p.weight
        self.arg230_1 = getattr(model.pairformer_stack.blocks, "3").triangle_multiplication.merged_linear_g.weight
        self.arg231_1 = getattr(model.pairformer_stack.blocks, "3").triangle_attention.out_scalers
        self.arg232_1 = getattr(model.pairformer_stack.blocks, "3").triangle_attention.pair2b.weight
        self.arg233_1 = getattr(model.pairformer_stack.blocks, "3").triangle_attention.pair2qkvg1.weight
        self.arg234_1 = getattr(model.pairformer_stack.blocks, "3").triangle_attention.pair2qkvg2.weight
        self.arg235_1 = getattr(model.pairformer_stack.blocks, "3").triangle_attention.linear_out.weight
        self.arg236_1 = getattr(model.pairformer_stack.blocks, "3").transition_single.layer_norm.weight
        self.arg237_1 = getattr(model.pairformer_stack.blocks, "3").transition_single.layer_norm.bias
        self.arg238_1 = getattr(model.pairformer_stack.blocks, "3").transition_single.linear_no_bias_ab.weight
        self.arg239_1 = getattr(model.pairformer_stack.blocks, "3").transition_single.linear_out.weight
        self.arg240_1 = getattr(model.pairformer_stack.blocks, "3").attention_pair_bias.single_layer_norm.weight
        self.arg241_1 = getattr(model.pairformer_stack.blocks, "3").attention_pair_bias.single_layer_norm.bias
        self.arg242_1 = getattr(model.pairformer_stack.blocks, "3").attention_pair_bias.pair_layer_norm.weight
        self.arg243_1 = getattr(model.pairformer_stack.blocks, "3").attention_pair_bias.pair_layer_norm.bias
        self.arg244_1 = getattr(model.pairformer_stack.blocks, "3").attention_pair_bias.pair_linear.weight
        self.arg245_1 = getattr(model.pairformer_stack.blocks, "3").attention_pair_bias.attention.query_bias
        self.arg246_1 = getattr(model.pairformer_stack.blocks, "3").attention_pair_bias.attention.input2qkvg.weight
        self.arg247_1 = getattr(model.pairformer_stack.blocks, "3").attention_pair_bias.attention.output_proj.weight
        self.arg248_1 = getattr(model.pairformer_stack.blocks, "4").transition_pair.layer_norm.weight
        self.arg249_1 = getattr(model.pairformer_stack.blocks, "4").transition_pair.layer_norm.bias
        self.arg250_1 = getattr(model.pairformer_stack.blocks, "4").transition_pair.linear_no_bias_ab.weight
        self.arg251_1 = getattr(model.pairformer_stack.blocks, "4").transition_pair.linear_out.weight
        self.arg252_1 = getattr(model.pairformer_stack.blocks, "4").triangle_multiplication.layernorm_z_in.weight
        self.arg253_1 = getattr(model.pairformer_stack.blocks, "4").triangle_multiplication.layernorm_z_in.bias
        self.arg254_1 = getattr(model.pairformer_stack.blocks, "4").triangle_multiplication.linear_z_out.weight
        self.arg255_1 = getattr(model.pairformer_stack.blocks, "4").triangle_multiplication.merged_linear_p.weight
        self.arg256_1 = getattr(model.pairformer_stack.blocks, "4").triangle_multiplication.merged_linear_g.weight
        self.arg257_1 = getattr(model.pairformer_stack.blocks, "4").triangle_attention.out_scalers
        self.arg258_1 = getattr(model.pairformer_stack.blocks, "4").triangle_attention.pair2b.weight
        self.arg259_1 = getattr(model.pairformer_stack.blocks, "4").triangle_attention.pair2qkvg1.weight
        self.arg260_1 = getattr(model.pairformer_stack.blocks, "4").triangle_attention.pair2qkvg2.weight
        self.arg261_1 = getattr(model.pairformer_stack.blocks, "4").triangle_attention.linear_out.weight
        self.arg262_1 = getattr(model.pairformer_stack.blocks, "4").transition_single.layer_norm.weight
        self.arg263_1 = getattr(model.pairformer_stack.blocks, "4").transition_single.layer_norm.bias
        self.arg264_1 = getattr(model.pairformer_stack.blocks, "4").transition_single.linear_no_bias_ab.weight
        self.arg265_1 = getattr(model.pairformer_stack.blocks, "4").transition_single.linear_out.weight
        self.arg266_1 = getattr(model.pairformer_stack.blocks, "4").attention_pair_bias.single_layer_norm.weight
        self.arg267_1 = getattr(model.pairformer_stack.blocks, "4").attention_pair_bias.single_layer_norm.bias
        self.arg268_1 = getattr(model.pairformer_stack.blocks, "4").attention_pair_bias.pair_layer_norm.weight
        self.arg269_1 = getattr(model.pairformer_stack.blocks, "4").attention_pair_bias.pair_layer_norm.bias
        self.arg270_1 = getattr(model.pairformer_stack.blocks, "4").attention_pair_bias.pair_linear.weight
        self.arg271_1 = getattr(model.pairformer_stack.blocks, "4").attention_pair_bias.attention.query_bias
        self.arg272_1 = getattr(model.pairformer_stack.blocks, "4").attention_pair_bias.attention.input2qkvg.weight
        self.arg273_1 = getattr(model.pairformer_stack.blocks, "4").attention_pair_bias.attention.output_proj.weight
        self.arg274_1 = getattr(model.pairformer_stack.blocks, "5").transition_pair.layer_norm.weight
        self.arg275_1 = getattr(model.pairformer_stack.blocks, "5").transition_pair.layer_norm.bias
        self.arg276_1 = getattr(model.pairformer_stack.blocks, "5").transition_pair.linear_no_bias_ab.weight
        self.arg277_1 = getattr(model.pairformer_stack.blocks, "5").transition_pair.linear_out.weight
        self.arg278_1 = getattr(model.pairformer_stack.blocks, "5").triangle_multiplication.layernorm_z_in.weight
        self.arg279_1 = getattr(model.pairformer_stack.blocks, "5").triangle_multiplication.layernorm_z_in.bias
        self.arg280_1 = getattr(model.pairformer_stack.blocks, "5").triangle_multiplication.linear_z_out.weight
        self.arg281_1 = getattr(model.pairformer_stack.blocks, "5").triangle_multiplication.merged_linear_p.weight
        self.arg282_1 = getattr(model.pairformer_stack.blocks, "5").triangle_multiplication.merged_linear_g.weight
        self.arg283_1 = getattr(model.pairformer_stack.blocks, "5").triangle_attention.out_scalers
        self.arg284_1 = getattr(model.pairformer_stack.blocks, "5").triangle_attention.pair2b.weight
        self.arg285_1 = getattr(model.pairformer_stack.blocks, "5").triangle_attention.pair2qkvg1.weight
        self.arg286_1 = getattr(model.pairformer_stack.blocks, "5").triangle_attention.pair2qkvg2.weight
        self.arg287_1 = getattr(model.pairformer_stack.blocks, "5").triangle_attention.linear_out.weight
        self.arg288_1 = getattr(model.pairformer_stack.blocks, "5").transition_single.layer_norm.weight
        self.arg289_1 = getattr(model.pairformer_stack.blocks, "5").transition_single.layer_norm.bias
        self.arg290_1 = getattr(model.pairformer_stack.blocks, "5").transition_single.linear_no_bias_ab.weight
        self.arg291_1 = getattr(model.pairformer_stack.blocks, "5").transition_single.linear_out.weight
        self.arg292_1 = getattr(model.pairformer_stack.blocks, "5").attention_pair_bias.single_layer_norm.weight
        self.arg293_1 = getattr(model.pairformer_stack.blocks, "5").attention_pair_bias.single_layer_norm.bias
        self.arg294_1 = getattr(model.pairformer_stack.blocks, "5").attention_pair_bias.pair_layer_norm.weight
        self.arg295_1 = getattr(model.pairformer_stack.blocks, "5").attention_pair_bias.pair_layer_norm.bias
        self.arg296_1 = getattr(model.pairformer_stack.blocks, "5").attention_pair_bias.pair_linear.weight
        self.arg297_1 = getattr(model.pairformer_stack.blocks, "5").attention_pair_bias.attention.query_bias
        self.arg298_1 = getattr(model.pairformer_stack.blocks, "5").attention_pair_bias.attention.input2qkvg.weight
        self.arg299_1 = getattr(model.pairformer_stack.blocks, "5").attention_pair_bias.attention.output_proj.weight
        self.arg300_1 = getattr(model.pairformer_stack.blocks, "6").transition_pair.layer_norm.weight
        self.arg301_1 = getattr(model.pairformer_stack.blocks, "6").transition_pair.layer_norm.bias
        self.arg302_1 = getattr(model.pairformer_stack.blocks, "6").transition_pair.linear_no_bias_ab.weight
        self.arg303_1 = getattr(model.pairformer_stack.blocks, "6").transition_pair.linear_out.weight
        self.arg304_1 = getattr(model.pairformer_stack.blocks, "6").triangle_multiplication.layernorm_z_in.weight
        self.arg305_1 = getattr(model.pairformer_stack.blocks, "6").triangle_multiplication.layernorm_z_in.bias
        self.arg306_1 = getattr(model.pairformer_stack.blocks, "6").triangle_multiplication.linear_z_out.weight
        self.arg307_1 = getattr(model.pairformer_stack.blocks, "6").triangle_multiplication.merged_linear_p.weight
        self.arg308_1 = getattr(model.pairformer_stack.blocks, "6").triangle_multiplication.merged_linear_g.weight
        self.arg309_1 = getattr(model.pairformer_stack.blocks, "6").triangle_attention.out_scalers
        self.arg310_1 = getattr(model.pairformer_stack.blocks, "6").triangle_attention.pair2b.weight
        self.arg311_1 = getattr(model.pairformer_stack.blocks, "6").triangle_attention.pair2qkvg1.weight
        self.arg312_1 = getattr(model.pairformer_stack.blocks, "6").triangle_attention.pair2qkvg2.weight
        self.arg313_1 = getattr(model.pairformer_stack.blocks, "6").triangle_attention.linear_out.weight
        self.arg314_1 = getattr(model.pairformer_stack.blocks, "6").transition_single.layer_norm.weight
        self.arg315_1 = getattr(model.pairformer_stack.blocks, "6").transition_single.layer_norm.bias
        self.arg316_1 = getattr(model.pairformer_stack.blocks, "6").transition_single.linear_no_bias_ab.weight
        self.arg317_1 = getattr(model.pairformer_stack.blocks, "6").transition_single.linear_out.weight
        self.arg318_1 = getattr(model.pairformer_stack.blocks, "6").attention_pair_bias.single_layer_norm.weight
        self.arg319_1 = getattr(model.pairformer_stack.blocks, "6").attention_pair_bias.single_layer_norm.bias
        self.arg320_1 = getattr(model.pairformer_stack.blocks, "6").attention_pair_bias.pair_layer_norm.weight
        self.arg321_1 = getattr(model.pairformer_stack.blocks, "6").attention_pair_bias.pair_layer_norm.bias
        self.arg322_1 = getattr(model.pairformer_stack.blocks, "6").attention_pair_bias.pair_linear.weight
        self.arg323_1 = getattr(model.pairformer_stack.blocks, "6").attention_pair_bias.attention.query_bias
        self.arg324_1 = getattr(model.pairformer_stack.blocks, "6").attention_pair_bias.attention.input2qkvg.weight
        self.arg325_1 = getattr(model.pairformer_stack.blocks, "6").attention_pair_bias.attention.output_proj.weight
        self.arg326_1 = getattr(model.pairformer_stack.blocks, "7").transition_pair.layer_norm.weight
        self.arg327_1 = getattr(model.pairformer_stack.blocks, "7").transition_pair.layer_norm.bias
        self.arg328_1 = getattr(model.pairformer_stack.blocks, "7").transition_pair.linear_no_bias_ab.weight
        self.arg329_1 = getattr(model.pairformer_stack.blocks, "7").transition_pair.linear_out.weight
        self.arg330_1 = getattr(model.pairformer_stack.blocks, "7").triangle_multiplication.layernorm_z_in.weight
        self.arg331_1 = getattr(model.pairformer_stack.blocks, "7").triangle_multiplication.layernorm_z_in.bias
        self.arg332_1 = getattr(model.pairformer_stack.blocks, "7").triangle_multiplication.linear_z_out.weight
        self.arg333_1 = getattr(model.pairformer_stack.blocks, "7").triangle_multiplication.merged_linear_p.weight
        self.arg334_1 = getattr(model.pairformer_stack.blocks, "7").triangle_multiplication.merged_linear_g.weight
        self.arg335_1 = getattr(model.pairformer_stack.blocks, "7").triangle_attention.out_scalers
        self.arg336_1 = getattr(model.pairformer_stack.blocks, "7").triangle_attention.pair2b.weight
        self.arg337_1 = getattr(model.pairformer_stack.blocks, "7").triangle_attention.pair2qkvg1.weight
        self.arg338_1 = getattr(model.pairformer_stack.blocks, "7").triangle_attention.pair2qkvg2.weight
        self.arg339_1 = getattr(model.pairformer_stack.blocks, "7").triangle_attention.linear_out.weight
        self.arg340_1 = getattr(model.pairformer_stack.blocks, "7").transition_single.layer_norm.weight
        self.arg341_1 = getattr(model.pairformer_stack.blocks, "7").transition_single.layer_norm.bias
        self.arg342_1 = getattr(model.pairformer_stack.blocks, "7").transition_single.linear_no_bias_ab.weight
        self.arg343_1 = getattr(model.pairformer_stack.blocks, "7").transition_single.linear_out.weight
        self.arg344_1 = getattr(model.pairformer_stack.blocks, "7").attention_pair_bias.single_layer_norm.weight
        self.arg345_1 = getattr(model.pairformer_stack.blocks, "7").attention_pair_bias.single_layer_norm.bias
        self.arg346_1 = getattr(model.pairformer_stack.blocks, "7").attention_pair_bias.pair_layer_norm.weight
        self.arg347_1 = getattr(model.pairformer_stack.blocks, "7").attention_pair_bias.pair_layer_norm.bias
        self.arg348_1 = getattr(model.pairformer_stack.blocks, "7").attention_pair_bias.pair_linear.weight
        self.arg349_1 = getattr(model.pairformer_stack.blocks, "7").attention_pair_bias.attention.query_bias
        self.arg350_1 = getattr(model.pairformer_stack.blocks, "7").attention_pair_bias.attention.input2qkvg.weight
        self.arg351_1 = getattr(model.pairformer_stack.blocks, "7").attention_pair_bias.attention.output_proj.weight
        self.arg352_1 = getattr(model.pairformer_stack.blocks, "8").transition_pair.layer_norm.weight
        self.arg353_1 = getattr(model.pairformer_stack.blocks, "8").transition_pair.layer_norm.bias
        self.arg354_1 = getattr(model.pairformer_stack.blocks, "8").transition_pair.linear_no_bias_ab.weight
        self.arg355_1 = getattr(model.pairformer_stack.blocks, "8").transition_pair.linear_out.weight
        self.arg356_1 = getattr(model.pairformer_stack.blocks, "8").triangle_multiplication.layernorm_z_in.weight
        self.arg357_1 = getattr(model.pairformer_stack.blocks, "8").triangle_multiplication.layernorm_z_in.bias
        self.arg358_1 = getattr(model.pairformer_stack.blocks, "8").triangle_multiplication.linear_z_out.weight
        self.arg359_1 = getattr(model.pairformer_stack.blocks, "8").triangle_multiplication.merged_linear_p.weight
        self.arg360_1 = getattr(model.pairformer_stack.blocks, "8").triangle_multiplication.merged_linear_g.weight
        self.arg361_1 = getattr(model.pairformer_stack.blocks, "8").triangle_attention.out_scalers
        self.arg362_1 = getattr(model.pairformer_stack.blocks, "8").triangle_attention.pair2b.weight
        self.arg363_1 = getattr(model.pairformer_stack.blocks, "8").triangle_attention.pair2qkvg1.weight
        self.arg364_1 = getattr(model.pairformer_stack.blocks, "8").triangle_attention.pair2qkvg2.weight
        self.arg365_1 = getattr(model.pairformer_stack.blocks, "8").triangle_attention.linear_out.weight
        self.arg366_1 = getattr(model.pairformer_stack.blocks, "8").transition_single.layer_norm.weight
        self.arg367_1 = getattr(model.pairformer_stack.blocks, "8").transition_single.layer_norm.bias
        self.arg368_1 = getattr(model.pairformer_stack.blocks, "8").transition_single.linear_no_bias_ab.weight
        self.arg369_1 = getattr(model.pairformer_stack.blocks, "8").transition_single.linear_out.weight
        self.arg370_1 = getattr(model.pairformer_stack.blocks, "8").attention_pair_bias.single_layer_norm.weight
        self.arg371_1 = getattr(model.pairformer_stack.blocks, "8").attention_pair_bias.single_layer_norm.bias
        self.arg372_1 = getattr(model.pairformer_stack.blocks, "8").attention_pair_bias.pair_layer_norm.weight
        self.arg373_1 = getattr(model.pairformer_stack.blocks, "8").attention_pair_bias.pair_layer_norm.bias
        self.arg374_1 = getattr(model.pairformer_stack.blocks, "8").attention_pair_bias.pair_linear.weight
        self.arg375_1 = getattr(model.pairformer_stack.blocks, "8").attention_pair_bias.attention.query_bias
        self.arg376_1 = getattr(model.pairformer_stack.blocks, "8").attention_pair_bias.attention.input2qkvg.weight
        self.arg377_1 = getattr(model.pairformer_stack.blocks, "8").attention_pair_bias.attention.output_proj.weight
        self.arg378_1 = getattr(model.pairformer_stack.blocks, "9").transition_pair.layer_norm.weight
        self.arg379_1 = getattr(model.pairformer_stack.blocks, "9").transition_pair.layer_norm.bias
        self.arg380_1 = getattr(model.pairformer_stack.blocks, "9").transition_pair.linear_no_bias_ab.weight
        self.arg381_1 = getattr(model.pairformer_stack.blocks, "9").transition_pair.linear_out.weight
        self.arg382_1 = getattr(model.pairformer_stack.blocks, "9").triangle_multiplication.layernorm_z_in.weight
        self.arg383_1 = getattr(model.pairformer_stack.blocks, "9").triangle_multiplication.layernorm_z_in.bias
        self.arg384_1 = getattr(model.pairformer_stack.blocks, "9").triangle_multiplication.linear_z_out.weight
        self.arg385_1 = getattr(model.pairformer_stack.blocks, "9").triangle_multiplication.merged_linear_p.weight
        self.arg386_1 = getattr(model.pairformer_stack.blocks, "9").triangle_multiplication.merged_linear_g.weight
        self.arg387_1 = getattr(model.pairformer_stack.blocks, "9").triangle_attention.out_scalers
        self.arg388_1 = getattr(model.pairformer_stack.blocks, "9").triangle_attention.pair2b.weight
        self.arg389_1 = getattr(model.pairformer_stack.blocks, "9").triangle_attention.pair2qkvg1.weight
        self.arg390_1 = getattr(model.pairformer_stack.blocks, "9").triangle_attention.pair2qkvg2.weight
        self.arg391_1 = getattr(model.pairformer_stack.blocks, "9").triangle_attention.linear_out.weight
        self.arg392_1 = getattr(model.pairformer_stack.blocks, "9").transition_single.layer_norm.weight
        self.arg393_1 = getattr(model.pairformer_stack.blocks, "9").transition_single.layer_norm.bias
        self.arg394_1 = getattr(model.pairformer_stack.blocks, "9").transition_single.linear_no_bias_ab.weight
        self.arg395_1 = getattr(model.pairformer_stack.blocks, "9").transition_single.linear_out.weight
        self.arg396_1 = getattr(model.pairformer_stack.blocks, "9").attention_pair_bias.single_layer_norm.weight
        self.arg397_1 = getattr(model.pairformer_stack.blocks, "9").attention_pair_bias.single_layer_norm.bias
        self.arg398_1 = getattr(model.pairformer_stack.blocks, "9").attention_pair_bias.pair_layer_norm.weight
        self.arg399_1 = getattr(model.pairformer_stack.blocks, "9").attention_pair_bias.pair_layer_norm.bias
        self.arg400_1 = getattr(model.pairformer_stack.blocks, "9").attention_pair_bias.pair_linear.weight
        self.arg401_1 = getattr(model.pairformer_stack.blocks, "9").attention_pair_bias.attention.query_bias
        self.arg402_1 = getattr(model.pairformer_stack.blocks, "9").attention_pair_bias.attention.input2qkvg.weight
        self.arg403_1 = getattr(model.pairformer_stack.blocks, "9").attention_pair_bias.attention.output_proj.weight
        self.arg404_1 = getattr(model.pairformer_stack.blocks, "10").transition_pair.layer_norm.weight
        self.arg405_1 = getattr(model.pairformer_stack.blocks, "10").transition_pair.layer_norm.bias
        self.arg406_1 = getattr(model.pairformer_stack.blocks, "10").transition_pair.linear_no_bias_ab.weight
        self.arg407_1 = getattr(model.pairformer_stack.blocks, "10").transition_pair.linear_out.weight
        self.arg408_1 = getattr(model.pairformer_stack.blocks, "10").triangle_multiplication.layernorm_z_in.weight
        self.arg409_1 = getattr(model.pairformer_stack.blocks, "10").triangle_multiplication.layernorm_z_in.bias
        self.arg410_1 = getattr(model.pairformer_stack.blocks, "10").triangle_multiplication.linear_z_out.weight
        self.arg411_1 = getattr(model.pairformer_stack.blocks, "10").triangle_multiplication.merged_linear_p.weight
        self.arg412_1 = getattr(model.pairformer_stack.blocks, "10").triangle_multiplication.merged_linear_g.weight
        self.arg413_1 = getattr(model.pairformer_stack.blocks, "10").triangle_attention.out_scalers
        self.arg414_1 = getattr(model.pairformer_stack.blocks, "10").triangle_attention.pair2b.weight
        self.arg415_1 = getattr(model.pairformer_stack.blocks, "10").triangle_attention.pair2qkvg1.weight
        self.arg416_1 = getattr(model.pairformer_stack.blocks, "10").triangle_attention.pair2qkvg2.weight
        self.arg417_1 = getattr(model.pairformer_stack.blocks, "10").triangle_attention.linear_out.weight
        self.arg418_1 = getattr(model.pairformer_stack.blocks, "10").transition_single.layer_norm.weight
        self.arg419_1 = getattr(model.pairformer_stack.blocks, "10").transition_single.layer_norm.bias
        self.arg420_1 = getattr(model.pairformer_stack.blocks, "10").transition_single.linear_no_bias_ab.weight
        self.arg421_1 = getattr(model.pairformer_stack.blocks, "10").transition_single.linear_out.weight
        self.arg422_1 = getattr(model.pairformer_stack.blocks, "10").attention_pair_bias.single_layer_norm.weight
        self.arg423_1 = getattr(model.pairformer_stack.blocks, "10").attention_pair_bias.single_layer_norm.bias
        self.arg424_1 = getattr(model.pairformer_stack.blocks, "10").attention_pair_bias.pair_layer_norm.weight
        self.arg425_1 = getattr(model.pairformer_stack.blocks, "10").attention_pair_bias.pair_layer_norm.bias
        self.arg426_1 = getattr(model.pairformer_stack.blocks, "10").attention_pair_bias.pair_linear.weight
        self.arg427_1 = getattr(model.pairformer_stack.blocks, "10").attention_pair_bias.attention.query_bias
        self.arg428_1 = getattr(model.pairformer_stack.blocks, "10").attention_pair_bias.attention.input2qkvg.weight
        self.arg429_1 = getattr(model.pairformer_stack.blocks, "10").attention_pair_bias.attention.output_proj.weight
        self.arg430_1 = getattr(model.pairformer_stack.blocks, "11").transition_pair.layer_norm.weight
        self.arg431_1 = getattr(model.pairformer_stack.blocks, "11").transition_pair.layer_norm.bias
        self.arg432_1 = getattr(model.pairformer_stack.blocks, "11").transition_pair.linear_no_bias_ab.weight
        self.arg433_1 = getattr(model.pairformer_stack.blocks, "11").transition_pair.linear_out.weight
        self.arg434_1 = getattr(model.pairformer_stack.blocks, "11").triangle_multiplication.layernorm_z_in.weight
        self.arg435_1 = getattr(model.pairformer_stack.blocks, "11").triangle_multiplication.layernorm_z_in.bias
        self.arg436_1 = getattr(model.pairformer_stack.blocks, "11").triangle_multiplication.linear_z_out.weight
        self.arg437_1 = getattr(model.pairformer_stack.blocks, "11").triangle_multiplication.merged_linear_p.weight
        self.arg438_1 = getattr(model.pairformer_stack.blocks, "11").triangle_multiplication.merged_linear_g.weight
        self.arg439_1 = getattr(model.pairformer_stack.blocks, "11").triangle_attention.out_scalers
        self.arg440_1 = getattr(model.pairformer_stack.blocks, "11").triangle_attention.pair2b.weight
        self.arg441_1 = getattr(model.pairformer_stack.blocks, "11").triangle_attention.pair2qkvg1.weight
        self.arg442_1 = getattr(model.pairformer_stack.blocks, "11").triangle_attention.pair2qkvg2.weight
        self.arg443_1 = getattr(model.pairformer_stack.blocks, "11").triangle_attention.linear_out.weight
        self.arg444_1 = getattr(model.pairformer_stack.blocks, "11").transition_single.layer_norm.weight
        self.arg445_1 = getattr(model.pairformer_stack.blocks, "11").transition_single.layer_norm.bias
        self.arg446_1 = getattr(model.pairformer_stack.blocks, "11").transition_single.linear_no_bias_ab.weight
        self.arg447_1 = getattr(model.pairformer_stack.blocks, "11").transition_single.linear_out.weight
        self.arg448_1 = getattr(model.pairformer_stack.blocks, "11").attention_pair_bias.single_layer_norm.weight
        self.arg449_1 = getattr(model.pairformer_stack.blocks, "11").attention_pair_bias.single_layer_norm.bias
        self.arg450_1 = getattr(model.pairformer_stack.blocks, "11").attention_pair_bias.pair_layer_norm.weight
        self.arg451_1 = getattr(model.pairformer_stack.blocks, "11").attention_pair_bias.pair_layer_norm.bias
        self.arg452_1 = getattr(model.pairformer_stack.blocks, "11").attention_pair_bias.pair_linear.weight
        self.arg453_1 = getattr(model.pairformer_stack.blocks, "11").attention_pair_bias.attention.query_bias
        self.arg454_1 = getattr(model.pairformer_stack.blocks, "11").attention_pair_bias.attention.input2qkvg.weight
        self.arg455_1 = getattr(model.pairformer_stack.blocks, "11").attention_pair_bias.attention.output_proj.weight
        self.arg456_1 = getattr(model.pairformer_stack.blocks, "12").transition_pair.layer_norm.weight
        self.arg457_1 = getattr(model.pairformer_stack.blocks, "12").transition_pair.layer_norm.bias
        self.arg458_1 = getattr(model.pairformer_stack.blocks, "12").transition_pair.linear_no_bias_ab.weight
        self.arg459_1 = getattr(model.pairformer_stack.blocks, "12").transition_pair.linear_out.weight
        self.arg460_1 = getattr(model.pairformer_stack.blocks, "12").triangle_multiplication.layernorm_z_in.weight
        self.arg461_1 = getattr(model.pairformer_stack.blocks, "12").triangle_multiplication.layernorm_z_in.bias
        self.arg462_1 = getattr(model.pairformer_stack.blocks, "12").triangle_multiplication.linear_z_out.weight
        self.arg463_1 = getattr(model.pairformer_stack.blocks, "12").triangle_multiplication.merged_linear_p.weight
        self.arg464_1 = getattr(model.pairformer_stack.blocks, "12").triangle_multiplication.merged_linear_g.weight
        self.arg465_1 = getattr(model.pairformer_stack.blocks, "12").triangle_attention.out_scalers
        self.arg466_1 = getattr(model.pairformer_stack.blocks, "12").triangle_attention.pair2b.weight
        self.arg467_1 = getattr(model.pairformer_stack.blocks, "12").triangle_attention.pair2qkvg1.weight
        self.arg468_1 = getattr(model.pairformer_stack.blocks, "12").triangle_attention.pair2qkvg2.weight
        self.arg469_1 = getattr(model.pairformer_stack.blocks, "12").triangle_attention.linear_out.weight
        self.arg470_1 = getattr(model.pairformer_stack.blocks, "12").transition_single.layer_norm.weight
        self.arg471_1 = getattr(model.pairformer_stack.blocks, "12").transition_single.layer_norm.bias
        self.arg472_1 = getattr(model.pairformer_stack.blocks, "12").transition_single.linear_no_bias_ab.weight
        self.arg473_1 = getattr(model.pairformer_stack.blocks, "12").transition_single.linear_out.weight
        self.arg474_1 = getattr(model.pairformer_stack.blocks, "12").attention_pair_bias.single_layer_norm.weight
        self.arg475_1 = getattr(model.pairformer_stack.blocks, "12").attention_pair_bias.single_layer_norm.bias
        self.arg476_1 = getattr(model.pairformer_stack.blocks, "12").attention_pair_bias.pair_layer_norm.weight
        self.arg477_1 = getattr(model.pairformer_stack.blocks, "12").attention_pair_bias.pair_layer_norm.bias
        self.arg478_1 = getattr(model.pairformer_stack.blocks, "12").attention_pair_bias.pair_linear.weight
        self.arg479_1 = getattr(model.pairformer_stack.blocks, "12").attention_pair_bias.attention.query_bias
        self.arg480_1 = getattr(model.pairformer_stack.blocks, "12").attention_pair_bias.attention.input2qkvg.weight
        self.arg481_1 = getattr(model.pairformer_stack.blocks, "12").attention_pair_bias.attention.output_proj.weight
        self.arg482_1 = getattr(model.pairformer_stack.blocks, "13").transition_pair.layer_norm.weight
        self.arg483_1 = getattr(model.pairformer_stack.blocks, "13").transition_pair.layer_norm.bias
        self.arg484_1 = getattr(model.pairformer_stack.blocks, "13").transition_pair.linear_no_bias_ab.weight
        self.arg485_1 = getattr(model.pairformer_stack.blocks, "13").transition_pair.linear_out.weight
        self.arg486_1 = getattr(model.pairformer_stack.blocks, "13").triangle_multiplication.layernorm_z_in.weight
        self.arg487_1 = getattr(model.pairformer_stack.blocks, "13").triangle_multiplication.layernorm_z_in.bias
        self.arg488_1 = getattr(model.pairformer_stack.blocks, "13").triangle_multiplication.linear_z_out.weight
        self.arg489_1 = getattr(model.pairformer_stack.blocks, "13").triangle_multiplication.merged_linear_p.weight
        self.arg490_1 = getattr(model.pairformer_stack.blocks, "13").triangle_multiplication.merged_linear_g.weight
        self.arg491_1 = getattr(model.pairformer_stack.blocks, "13").triangle_attention.out_scalers
        self.arg492_1 = getattr(model.pairformer_stack.blocks, "13").triangle_attention.pair2b.weight
        self.arg493_1 = getattr(model.pairformer_stack.blocks, "13").triangle_attention.pair2qkvg1.weight
        self.arg494_1 = getattr(model.pairformer_stack.blocks, "13").triangle_attention.pair2qkvg2.weight
        self.arg495_1 = getattr(model.pairformer_stack.blocks, "13").triangle_attention.linear_out.weight
        self.arg496_1 = getattr(model.pairformer_stack.blocks, "13").transition_single.layer_norm.weight
        self.arg497_1 = getattr(model.pairformer_stack.blocks, "13").transition_single.layer_norm.bias
        self.arg498_1 = getattr(model.pairformer_stack.blocks, "13").transition_single.linear_no_bias_ab.weight
        self.arg499_1 = getattr(model.pairformer_stack.blocks, "13").transition_single.linear_out.weight
        self.arg500_1 = getattr(model.pairformer_stack.blocks, "13").attention_pair_bias.single_layer_norm.weight
        self.arg501_1 = getattr(model.pairformer_stack.blocks, "13").attention_pair_bias.single_layer_norm.bias
        self.arg502_1 = getattr(model.pairformer_stack.blocks, "13").attention_pair_bias.pair_layer_norm.weight
        self.arg503_1 = getattr(model.pairformer_stack.blocks, "13").attention_pair_bias.pair_layer_norm.bias
        self.arg504_1 = getattr(model.pairformer_stack.blocks, "13").attention_pair_bias.pair_linear.weight
        self.arg505_1 = getattr(model.pairformer_stack.blocks, "13").attention_pair_bias.attention.query_bias
        self.arg506_1 = getattr(model.pairformer_stack.blocks, "13").attention_pair_bias.attention.input2qkvg.weight
        self.arg507_1 = getattr(model.pairformer_stack.blocks, "13").attention_pair_bias.attention.output_proj.weight
        self.arg508_1 = getattr(model.pairformer_stack.blocks, "14").transition_pair.layer_norm.weight
        self.arg509_1 = getattr(model.pairformer_stack.blocks, "14").transition_pair.layer_norm.bias
        self.arg510_1 = getattr(model.pairformer_stack.blocks, "14").transition_pair.linear_no_bias_ab.weight
        self.arg511_1 = getattr(model.pairformer_stack.blocks, "14").transition_pair.linear_out.weight
        self.arg512_1 = getattr(model.pairformer_stack.blocks, "14").triangle_multiplication.layernorm_z_in.weight
        self.arg513_1 = getattr(model.pairformer_stack.blocks, "14").triangle_multiplication.layernorm_z_in.bias
        self.arg514_1 = getattr(model.pairformer_stack.blocks, "14").triangle_multiplication.linear_z_out.weight
        self.arg515_1 = getattr(model.pairformer_stack.blocks, "14").triangle_multiplication.merged_linear_p.weight
        self.arg516_1 = getattr(model.pairformer_stack.blocks, "14").triangle_multiplication.merged_linear_g.weight
        self.arg517_1 = getattr(model.pairformer_stack.blocks, "14").triangle_attention.out_scalers
        self.arg518_1 = getattr(model.pairformer_stack.blocks, "14").triangle_attention.pair2b.weight
        self.arg519_1 = getattr(model.pairformer_stack.blocks, "14").triangle_attention.pair2qkvg1.weight
        self.arg520_1 = getattr(model.pairformer_stack.blocks, "14").triangle_attention.pair2qkvg2.weight
        self.arg521_1 = getattr(model.pairformer_stack.blocks, "14").triangle_attention.linear_out.weight
        self.arg522_1 = getattr(model.pairformer_stack.blocks, "14").transition_single.layer_norm.weight
        self.arg523_1 = getattr(model.pairformer_stack.blocks, "14").transition_single.layer_norm.bias
        self.arg524_1 = getattr(model.pairformer_stack.blocks, "14").transition_single.linear_no_bias_ab.weight
        self.arg525_1 = getattr(model.pairformer_stack.blocks, "14").transition_single.linear_out.weight
        self.arg526_1 = getattr(model.pairformer_stack.blocks, "14").attention_pair_bias.single_layer_norm.weight
        self.arg527_1 = getattr(model.pairformer_stack.blocks, "14").attention_pair_bias.single_layer_norm.bias
        self.arg528_1 = getattr(model.pairformer_stack.blocks, "14").attention_pair_bias.pair_layer_norm.weight
        self.arg529_1 = getattr(model.pairformer_stack.blocks, "14").attention_pair_bias.pair_layer_norm.bias
        self.arg530_1 = getattr(model.pairformer_stack.blocks, "14").attention_pair_bias.pair_linear.weight
        self.arg531_1 = getattr(model.pairformer_stack.blocks, "14").attention_pair_bias.attention.query_bias
        self.arg532_1 = getattr(model.pairformer_stack.blocks, "14").attention_pair_bias.attention.input2qkvg.weight
        self.arg533_1 = getattr(model.pairformer_stack.blocks, "14").attention_pair_bias.attention.output_proj.weight
        self.arg534_1 = getattr(model.pairformer_stack.blocks, "15").transition_pair.layer_norm.weight
        self.arg535_1 = getattr(model.pairformer_stack.blocks, "15").transition_pair.layer_norm.bias
        self.arg536_1 = getattr(model.pairformer_stack.blocks, "15").transition_pair.linear_no_bias_ab.weight
        self.arg537_1 = getattr(model.pairformer_stack.blocks, "15").transition_pair.linear_out.weight
        self.arg538_1 = getattr(model.pairformer_stack.blocks, "15").triangle_multiplication.layernorm_z_in.weight
        self.arg539_1 = getattr(model.pairformer_stack.blocks, "15").triangle_multiplication.layernorm_z_in.bias
        self.arg540_1 = getattr(model.pairformer_stack.blocks, "15").triangle_multiplication.linear_z_out.weight
        self.arg541_1 = getattr(model.pairformer_stack.blocks, "15").triangle_multiplication.merged_linear_p.weight
        self.arg542_1 = getattr(model.pairformer_stack.blocks, "15").triangle_multiplication.merged_linear_g.weight
        self.arg543_1 = getattr(model.pairformer_stack.blocks, "15").triangle_attention.out_scalers
        self.arg544_1 = getattr(model.pairformer_stack.blocks, "15").triangle_attention.pair2b.weight
        self.arg545_1 = getattr(model.pairformer_stack.blocks, "15").triangle_attention.pair2qkvg1.weight
        self.arg546_1 = getattr(model.pairformer_stack.blocks, "15").triangle_attention.pair2qkvg2.weight
        self.arg547_1 = getattr(model.pairformer_stack.blocks, "15").triangle_attention.linear_out.weight
        self.arg548_1 = getattr(model.pairformer_stack.blocks, "15").transition_single.layer_norm.weight
        self.arg549_1 = getattr(model.pairformer_stack.blocks, "15").transition_single.layer_norm.bias
        self.arg550_1 = getattr(model.pairformer_stack.blocks, "15").transition_single.linear_no_bias_ab.weight
        self.arg551_1 = getattr(model.pairformer_stack.blocks, "15").transition_single.linear_out.weight
        self.arg552_1 = getattr(model.pairformer_stack.blocks, "15").attention_pair_bias.single_layer_norm.weight
        self.arg553_1 = getattr(model.pairformer_stack.blocks, "15").attention_pair_bias.single_layer_norm.bias
        self.arg554_1 = getattr(model.pairformer_stack.blocks, "15").attention_pair_bias.pair_layer_norm.weight
        self.arg555_1 = getattr(model.pairformer_stack.blocks, "15").attention_pair_bias.pair_layer_norm.bias
        self.arg556_1 = getattr(model.pairformer_stack.blocks, "15").attention_pair_bias.pair_linear.weight
        self.arg557_1 = getattr(model.pairformer_stack.blocks, "15").attention_pair_bias.attention.query_bias
        self.arg558_1 = getattr(model.pairformer_stack.blocks, "15").attention_pair_bias.attention.input2qkvg.weight
        self.arg559_1 = getattr(model.pairformer_stack.blocks, "15").attention_pair_bias.attention.output_proj.weight
        self.arg560_1 = getattr(model.pairformer_stack.blocks, "16").transition_pair.layer_norm.weight
        self.arg561_1 = getattr(model.pairformer_stack.blocks, "16").transition_pair.layer_norm.bias
        self.arg562_1 = getattr(model.pairformer_stack.blocks, "16").transition_pair.linear_no_bias_ab.weight
        self.arg563_1 = getattr(model.pairformer_stack.blocks, "16").transition_pair.linear_out.weight
        self.arg564_1 = getattr(model.pairformer_stack.blocks, "16").triangle_multiplication.layernorm_z_in.weight
        self.arg565_1 = getattr(model.pairformer_stack.blocks, "16").triangle_multiplication.layernorm_z_in.bias
        self.arg566_1 = getattr(model.pairformer_stack.blocks, "16").triangle_multiplication.linear_z_out.weight
        self.arg567_1 = getattr(model.pairformer_stack.blocks, "16").triangle_multiplication.merged_linear_p.weight
        self.arg568_1 = getattr(model.pairformer_stack.blocks, "16").triangle_multiplication.merged_linear_g.weight
        self.arg569_1 = getattr(model.pairformer_stack.blocks, "16").triangle_attention.out_scalers
        self.arg570_1 = getattr(model.pairformer_stack.blocks, "16").triangle_attention.pair2b.weight
        self.arg571_1 = getattr(model.pairformer_stack.blocks, "16").triangle_attention.pair2qkvg1.weight
        self.arg572_1 = getattr(model.pairformer_stack.blocks, "16").triangle_attention.pair2qkvg2.weight
        self.arg573_1 = getattr(model.pairformer_stack.blocks, "16").triangle_attention.linear_out.weight
        self.arg574_1 = getattr(model.pairformer_stack.blocks, "16").transition_single.layer_norm.weight
        self.arg575_1 = getattr(model.pairformer_stack.blocks, "16").transition_single.layer_norm.bias
        self.arg576_1 = getattr(model.pairformer_stack.blocks, "16").transition_single.linear_no_bias_ab.weight
        self.arg577_1 = getattr(model.pairformer_stack.blocks, "16").transition_single.linear_out.weight
        self.arg578_1 = getattr(model.pairformer_stack.blocks, "16").attention_pair_bias.single_layer_norm.weight
        self.arg579_1 = getattr(model.pairformer_stack.blocks, "16").attention_pair_bias.single_layer_norm.bias
        self.arg580_1 = getattr(model.pairformer_stack.blocks, "16").attention_pair_bias.pair_layer_norm.weight
        self.arg581_1 = getattr(model.pairformer_stack.blocks, "16").attention_pair_bias.pair_layer_norm.bias
        self.arg582_1 = getattr(model.pairformer_stack.blocks, "16").attention_pair_bias.pair_linear.weight
        self.arg583_1 = getattr(model.pairformer_stack.blocks, "16").attention_pair_bias.attention.query_bias
        self.arg584_1 = getattr(model.pairformer_stack.blocks, "16").attention_pair_bias.attention.input2qkvg.weight
        self.arg585_1 = getattr(model.pairformer_stack.blocks, "16").attention_pair_bias.attention.output_proj.weight
        self.arg586_1 = getattr(model.pairformer_stack.blocks, "17").transition_pair.layer_norm.weight
        self.arg587_1 = getattr(model.pairformer_stack.blocks, "17").transition_pair.layer_norm.bias
        self.arg588_1 = getattr(model.pairformer_stack.blocks, "17").transition_pair.linear_no_bias_ab.weight
        self.arg589_1 = getattr(model.pairformer_stack.blocks, "17").transition_pair.linear_out.weight
        self.arg590_1 = getattr(model.pairformer_stack.blocks, "17").triangle_multiplication.layernorm_z_in.weight
        self.arg591_1 = getattr(model.pairformer_stack.blocks, "17").triangle_multiplication.layernorm_z_in.bias
        self.arg592_1 = getattr(model.pairformer_stack.blocks, "17").triangle_multiplication.linear_z_out.weight
        self.arg593_1 = getattr(model.pairformer_stack.blocks, "17").triangle_multiplication.merged_linear_p.weight
        self.arg594_1 = getattr(model.pairformer_stack.blocks, "17").triangle_multiplication.merged_linear_g.weight
        self.arg595_1 = getattr(model.pairformer_stack.blocks, "17").triangle_attention.out_scalers
        self.arg596_1 = getattr(model.pairformer_stack.blocks, "17").triangle_attention.pair2b.weight
        self.arg597_1 = getattr(model.pairformer_stack.blocks, "17").triangle_attention.pair2qkvg1.weight
        self.arg598_1 = getattr(model.pairformer_stack.blocks, "17").triangle_attention.pair2qkvg2.weight
        self.arg599_1 = getattr(model.pairformer_stack.blocks, "17").triangle_attention.linear_out.weight
        self.arg600_1 = getattr(model.pairformer_stack.blocks, "17").transition_single.layer_norm.weight
        self.arg601_1 = getattr(model.pairformer_stack.blocks, "17").transition_single.layer_norm.bias
        self.arg602_1 = getattr(model.pairformer_stack.blocks, "17").transition_single.linear_no_bias_ab.weight
        self.arg603_1 = getattr(model.pairformer_stack.blocks, "17").transition_single.linear_out.weight
        self.arg604_1 = getattr(model.pairformer_stack.blocks, "17").attention_pair_bias.single_layer_norm.weight
        self.arg605_1 = getattr(model.pairformer_stack.blocks, "17").attention_pair_bias.single_layer_norm.bias
        self.arg606_1 = getattr(model.pairformer_stack.blocks, "17").attention_pair_bias.pair_layer_norm.weight
        self.arg607_1 = getattr(model.pairformer_stack.blocks, "17").attention_pair_bias.pair_layer_norm.bias
        self.arg608_1 = getattr(model.pairformer_stack.blocks, "17").attention_pair_bias.pair_linear.weight
        self.arg609_1 = getattr(model.pairformer_stack.blocks, "17").attention_pair_bias.attention.query_bias
        self.arg610_1 = getattr(model.pairformer_stack.blocks, "17").attention_pair_bias.attention.input2qkvg.weight
        self.arg611_1 = getattr(model.pairformer_stack.blocks, "17").attention_pair_bias.attention.output_proj.weight
        self.arg612_1 = getattr(model.pairformer_stack.blocks, "18").transition_pair.layer_norm.weight
        self.arg613_1 = getattr(model.pairformer_stack.blocks, "18").transition_pair.layer_norm.bias
        self.arg614_1 = getattr(model.pairformer_stack.blocks, "18").transition_pair.linear_no_bias_ab.weight
        self.arg615_1 = getattr(model.pairformer_stack.blocks, "18").transition_pair.linear_out.weight
        self.arg616_1 = getattr(model.pairformer_stack.blocks, "18").triangle_multiplication.layernorm_z_in.weight
        self.arg617_1 = getattr(model.pairformer_stack.blocks, "18").triangle_multiplication.layernorm_z_in.bias
        self.arg618_1 = getattr(model.pairformer_stack.blocks, "18").triangle_multiplication.linear_z_out.weight
        self.arg619_1 = getattr(model.pairformer_stack.blocks, "18").triangle_multiplication.merged_linear_p.weight
        self.arg620_1 = getattr(model.pairformer_stack.blocks, "18").triangle_multiplication.merged_linear_g.weight
        self.arg621_1 = getattr(model.pairformer_stack.blocks, "18").triangle_attention.out_scalers
        self.arg622_1 = getattr(model.pairformer_stack.blocks, "18").triangle_attention.pair2b.weight
        self.arg623_1 = getattr(model.pairformer_stack.blocks, "18").triangle_attention.pair2qkvg1.weight
        self.arg624_1 = getattr(model.pairformer_stack.blocks, "18").triangle_attention.pair2qkvg2.weight
        self.arg625_1 = getattr(model.pairformer_stack.blocks, "18").triangle_attention.linear_out.weight
        self.arg626_1 = getattr(model.pairformer_stack.blocks, "18").transition_single.layer_norm.weight
        self.arg627_1 = getattr(model.pairformer_stack.blocks, "18").transition_single.layer_norm.bias
        self.arg628_1 = getattr(model.pairformer_stack.blocks, "18").transition_single.linear_no_bias_ab.weight
        self.arg629_1 = getattr(model.pairformer_stack.blocks, "18").transition_single.linear_out.weight
        self.arg630_1 = getattr(model.pairformer_stack.blocks, "18").attention_pair_bias.single_layer_norm.weight
        self.arg631_1 = getattr(model.pairformer_stack.blocks, "18").attention_pair_bias.single_layer_norm.bias
        self.arg632_1 = getattr(model.pairformer_stack.blocks, "18").attention_pair_bias.pair_layer_norm.weight
        self.arg633_1 = getattr(model.pairformer_stack.blocks, "18").attention_pair_bias.pair_layer_norm.bias
        self.arg634_1 = getattr(model.pairformer_stack.blocks, "18").attention_pair_bias.pair_linear.weight
        self.arg635_1 = getattr(model.pairformer_stack.blocks, "18").attention_pair_bias.attention.query_bias
        self.arg636_1 = getattr(model.pairformer_stack.blocks, "18").attention_pair_bias.attention.input2qkvg.weight
        self.arg637_1 = getattr(model.pairformer_stack.blocks, "18").attention_pair_bias.attention.output_proj.weight
        self.arg638_1 = getattr(model.pairformer_stack.blocks, "19").transition_pair.layer_norm.weight
        self.arg639_1 = getattr(model.pairformer_stack.blocks, "19").transition_pair.layer_norm.bias
        self.arg640_1 = getattr(model.pairformer_stack.blocks, "19").transition_pair.linear_no_bias_ab.weight
        self.arg641_1 = getattr(model.pairformer_stack.blocks, "19").transition_pair.linear_out.weight
        self.arg642_1 = getattr(model.pairformer_stack.blocks, "19").triangle_multiplication.layernorm_z_in.weight
        self.arg643_1 = getattr(model.pairformer_stack.blocks, "19").triangle_multiplication.layernorm_z_in.bias
        self.arg644_1 = getattr(model.pairformer_stack.blocks, "19").triangle_multiplication.linear_z_out.weight
        self.arg645_1 = getattr(model.pairformer_stack.blocks, "19").triangle_multiplication.merged_linear_p.weight
        self.arg646_1 = getattr(model.pairformer_stack.blocks, "19").triangle_multiplication.merged_linear_g.weight
        self.arg647_1 = getattr(model.pairformer_stack.blocks, "19").triangle_attention.out_scalers
        self.arg648_1 = getattr(model.pairformer_stack.blocks, "19").triangle_attention.pair2b.weight
        self.arg649_1 = getattr(model.pairformer_stack.blocks, "19").triangle_attention.pair2qkvg1.weight
        self.arg650_1 = getattr(model.pairformer_stack.blocks, "19").triangle_attention.pair2qkvg2.weight
        self.arg651_1 = getattr(model.pairformer_stack.blocks, "19").triangle_attention.linear_out.weight
        self.arg652_1 = getattr(model.pairformer_stack.blocks, "19").transition_single.layer_norm.weight
        self.arg653_1 = getattr(model.pairformer_stack.blocks, "19").transition_single.layer_norm.bias
        self.arg654_1 = getattr(model.pairformer_stack.blocks, "19").transition_single.linear_no_bias_ab.weight
        self.arg655_1 = getattr(model.pairformer_stack.blocks, "19").transition_single.linear_out.weight
        self.arg656_1 = getattr(model.pairformer_stack.blocks, "19").attention_pair_bias.single_layer_norm.weight
        self.arg657_1 = getattr(model.pairformer_stack.blocks, "19").attention_pair_bias.single_layer_norm.bias
        self.arg658_1 = getattr(model.pairformer_stack.blocks, "19").attention_pair_bias.pair_layer_norm.weight
        self.arg659_1 = getattr(model.pairformer_stack.blocks, "19").attention_pair_bias.pair_layer_norm.bias
        self.arg660_1 = getattr(model.pairformer_stack.blocks, "19").attention_pair_bias.pair_linear.weight
        self.arg661_1 = getattr(model.pairformer_stack.blocks, "19").attention_pair_bias.attention.query_bias
        self.arg662_1 = getattr(model.pairformer_stack.blocks, "19").attention_pair_bias.attention.input2qkvg.weight
        self.arg663_1 = getattr(model.pairformer_stack.blocks, "19").attention_pair_bias.attention.output_proj.weight
        self.arg664_1 = getattr(model.pairformer_stack.blocks, "20").transition_pair.layer_norm.weight
        self.arg665_1 = getattr(model.pairformer_stack.blocks, "20").transition_pair.layer_norm.bias
        self.arg666_1 = getattr(model.pairformer_stack.blocks, "20").transition_pair.linear_no_bias_ab.weight
        self.arg667_1 = getattr(model.pairformer_stack.blocks, "20").transition_pair.linear_out.weight
        self.arg668_1 = getattr(model.pairformer_stack.blocks, "20").triangle_multiplication.layernorm_z_in.weight
        self.arg669_1 = getattr(model.pairformer_stack.blocks, "20").triangle_multiplication.layernorm_z_in.bias
        self.arg670_1 = getattr(model.pairformer_stack.blocks, "20").triangle_multiplication.linear_z_out.weight
        self.arg671_1 = getattr(model.pairformer_stack.blocks, "20").triangle_multiplication.merged_linear_p.weight
        self.arg672_1 = getattr(model.pairformer_stack.blocks, "20").triangle_multiplication.merged_linear_g.weight
        self.arg673_1 = getattr(model.pairformer_stack.blocks, "20").triangle_attention.out_scalers
        self.arg674_1 = getattr(model.pairformer_stack.blocks, "20").triangle_attention.pair2b.weight
        self.arg675_1 = getattr(model.pairformer_stack.blocks, "20").triangle_attention.pair2qkvg1.weight
        self.arg676_1 = getattr(model.pairformer_stack.blocks, "20").triangle_attention.pair2qkvg2.weight
        self.arg677_1 = getattr(model.pairformer_stack.blocks, "20").triangle_attention.linear_out.weight
        self.arg678_1 = getattr(model.pairformer_stack.blocks, "20").transition_single.layer_norm.weight
        self.arg679_1 = getattr(model.pairformer_stack.blocks, "20").transition_single.layer_norm.bias
        self.arg680_1 = getattr(model.pairformer_stack.blocks, "20").transition_single.linear_no_bias_ab.weight
        self.arg681_1 = getattr(model.pairformer_stack.blocks, "20").transition_single.linear_out.weight
        self.arg682_1 = getattr(model.pairformer_stack.blocks, "20").attention_pair_bias.single_layer_norm.weight
        self.arg683_1 = getattr(model.pairformer_stack.blocks, "20").attention_pair_bias.single_layer_norm.bias
        self.arg684_1 = getattr(model.pairformer_stack.blocks, "20").attention_pair_bias.pair_layer_norm.weight
        self.arg685_1 = getattr(model.pairformer_stack.blocks, "20").attention_pair_bias.pair_layer_norm.bias
        self.arg686_1 = getattr(model.pairformer_stack.blocks, "20").attention_pair_bias.pair_linear.weight
        self.arg687_1 = getattr(model.pairformer_stack.blocks, "20").attention_pair_bias.attention.query_bias
        self.arg688_1 = getattr(model.pairformer_stack.blocks, "20").attention_pair_bias.attention.input2qkvg.weight
        self.arg689_1 = getattr(model.pairformer_stack.blocks, "20").attention_pair_bias.attention.output_proj.weight
        self.arg690_1 = getattr(model.pairformer_stack.blocks, "21").transition_pair.layer_norm.weight
        self.arg691_1 = getattr(model.pairformer_stack.blocks, "21").transition_pair.layer_norm.bias
        self.arg692_1 = getattr(model.pairformer_stack.blocks, "21").transition_pair.linear_no_bias_ab.weight
        self.arg693_1 = getattr(model.pairformer_stack.blocks, "21").transition_pair.linear_out.weight
        self.arg694_1 = getattr(model.pairformer_stack.blocks, "21").triangle_multiplication.layernorm_z_in.weight
        self.arg695_1 = getattr(model.pairformer_stack.blocks, "21").triangle_multiplication.layernorm_z_in.bias
        self.arg696_1 = getattr(model.pairformer_stack.blocks, "21").triangle_multiplication.linear_z_out.weight
        self.arg697_1 = getattr(model.pairformer_stack.blocks, "21").triangle_multiplication.merged_linear_p.weight
        self.arg698_1 = getattr(model.pairformer_stack.blocks, "21").triangle_multiplication.merged_linear_g.weight
        self.arg699_1 = getattr(model.pairformer_stack.blocks, "21").triangle_attention.out_scalers
        self.arg700_1 = getattr(model.pairformer_stack.blocks, "21").triangle_attention.pair2b.weight
        self.arg701_1 = getattr(model.pairformer_stack.blocks, "21").triangle_attention.pair2qkvg1.weight
        self.arg702_1 = getattr(model.pairformer_stack.blocks, "21").triangle_attention.pair2qkvg2.weight
        self.arg703_1 = getattr(model.pairformer_stack.blocks, "21").triangle_attention.linear_out.weight
        self.arg704_1 = getattr(model.pairformer_stack.blocks, "21").transition_single.layer_norm.weight
        self.arg705_1 = getattr(model.pairformer_stack.blocks, "21").transition_single.layer_norm.bias
        self.arg706_1 = getattr(model.pairformer_stack.blocks, "21").transition_single.linear_no_bias_ab.weight
        self.arg707_1 = getattr(model.pairformer_stack.blocks, "21").transition_single.linear_out.weight
        self.arg708_1 = getattr(model.pairformer_stack.blocks, "21").attention_pair_bias.single_layer_norm.weight
        self.arg709_1 = getattr(model.pairformer_stack.blocks, "21").attention_pair_bias.single_layer_norm.bias
        self.arg710_1 = getattr(model.pairformer_stack.blocks, "21").attention_pair_bias.pair_layer_norm.weight
        self.arg711_1 = getattr(model.pairformer_stack.blocks, "21").attention_pair_bias.pair_layer_norm.bias
        self.arg712_1 = getattr(model.pairformer_stack.blocks, "21").attention_pair_bias.pair_linear.weight
        self.arg713_1 = getattr(model.pairformer_stack.blocks, "21").attention_pair_bias.attention.query_bias
        self.arg714_1 = getattr(model.pairformer_stack.blocks, "21").attention_pair_bias.attention.input2qkvg.weight
        self.arg715_1 = getattr(model.pairformer_stack.blocks, "21").attention_pair_bias.attention.output_proj.weight
        self.arg716_1 = getattr(model.pairformer_stack.blocks, "22").transition_pair.layer_norm.weight
        self.arg717_1 = getattr(model.pairformer_stack.blocks, "22").transition_pair.layer_norm.bias
        self.arg718_1 = getattr(model.pairformer_stack.blocks, "22").transition_pair.linear_no_bias_ab.weight
        self.arg719_1 = getattr(model.pairformer_stack.blocks, "22").transition_pair.linear_out.weight
        self.arg720_1 = getattr(model.pairformer_stack.blocks, "22").triangle_multiplication.layernorm_z_in.weight
        self.arg721_1 = getattr(model.pairformer_stack.blocks, "22").triangle_multiplication.layernorm_z_in.bias
        self.arg722_1 = getattr(model.pairformer_stack.blocks, "22").triangle_multiplication.linear_z_out.weight
        self.arg723_1 = getattr(model.pairformer_stack.blocks, "22").triangle_multiplication.merged_linear_p.weight
        self.arg724_1 = getattr(model.pairformer_stack.blocks, "22").triangle_multiplication.merged_linear_g.weight
        self.arg725_1 = getattr(model.pairformer_stack.blocks, "22").triangle_attention.out_scalers
        self.arg726_1 = getattr(model.pairformer_stack.blocks, "22").triangle_attention.pair2b.weight
        self.arg727_1 = getattr(model.pairformer_stack.blocks, "22").triangle_attention.pair2qkvg1.weight
        self.arg728_1 = getattr(model.pairformer_stack.blocks, "22").triangle_attention.pair2qkvg2.weight
        self.arg729_1 = getattr(model.pairformer_stack.blocks, "22").triangle_attention.linear_out.weight
        self.arg730_1 = getattr(model.pairformer_stack.blocks, "22").transition_single.layer_norm.weight
        self.arg731_1 = getattr(model.pairformer_stack.blocks, "22").transition_single.layer_norm.bias
        self.arg732_1 = getattr(model.pairformer_stack.blocks, "22").transition_single.linear_no_bias_ab.weight
        self.arg733_1 = getattr(model.pairformer_stack.blocks, "22").transition_single.linear_out.weight
        self.arg734_1 = getattr(model.pairformer_stack.blocks, "22").attention_pair_bias.single_layer_norm.weight
        self.arg735_1 = getattr(model.pairformer_stack.blocks, "22").attention_pair_bias.single_layer_norm.bias
        self.arg736_1 = getattr(model.pairformer_stack.blocks, "22").attention_pair_bias.pair_layer_norm.weight
        self.arg737_1 = getattr(model.pairformer_stack.blocks, "22").attention_pair_bias.pair_layer_norm.bias
        self.arg738_1 = getattr(model.pairformer_stack.blocks, "22").attention_pair_bias.pair_linear.weight
        self.arg739_1 = getattr(model.pairformer_stack.blocks, "22").attention_pair_bias.attention.query_bias
        self.arg740_1 = getattr(model.pairformer_stack.blocks, "22").attention_pair_bias.attention.input2qkvg.weight
        self.arg741_1 = getattr(model.pairformer_stack.blocks, "22").attention_pair_bias.attention.output_proj.weight
        self.arg742_1 = getattr(model.pairformer_stack.blocks, "23").transition_pair.layer_norm.weight
        self.arg743_1 = getattr(model.pairformer_stack.blocks, "23").transition_pair.layer_norm.bias
        self.arg744_1 = getattr(model.pairformer_stack.blocks, "23").transition_pair.linear_no_bias_ab.weight
        self.arg745_1 = getattr(model.pairformer_stack.blocks, "23").transition_pair.linear_out.weight
        self.arg746_1 = getattr(model.pairformer_stack.blocks, "23").triangle_multiplication.layernorm_z_in.weight
        self.arg747_1 = getattr(model.pairformer_stack.blocks, "23").triangle_multiplication.layernorm_z_in.bias
        self.arg748_1 = getattr(model.pairformer_stack.blocks, "23").triangle_multiplication.linear_z_out.weight
        self.arg749_1 = getattr(model.pairformer_stack.blocks, "23").triangle_multiplication.merged_linear_p.weight
        self.arg750_1 = getattr(model.pairformer_stack.blocks, "23").triangle_multiplication.merged_linear_g.weight
        self.arg751_1 = getattr(model.pairformer_stack.blocks, "23").triangle_attention.out_scalers
        self.arg752_1 = getattr(model.pairformer_stack.blocks, "23").triangle_attention.pair2b.weight
        self.arg753_1 = getattr(model.pairformer_stack.blocks, "23").triangle_attention.pair2qkvg1.weight
        self.arg754_1 = getattr(model.pairformer_stack.blocks, "23").triangle_attention.pair2qkvg2.weight
        self.arg755_1 = getattr(model.pairformer_stack.blocks, "23").triangle_attention.linear_out.weight
        self.arg756_1 = getattr(model.pairformer_stack.blocks, "23").transition_single.layer_norm.weight
        self.arg757_1 = getattr(model.pairformer_stack.blocks, "23").transition_single.layer_norm.bias
        self.arg758_1 = getattr(model.pairformer_stack.blocks, "23").transition_single.linear_no_bias_ab.weight
        self.arg759_1 = getattr(model.pairformer_stack.blocks, "23").transition_single.linear_out.weight
        self.arg760_1 = getattr(model.pairformer_stack.blocks, "23").attention_pair_bias.single_layer_norm.weight
        self.arg761_1 = getattr(model.pairformer_stack.blocks, "23").attention_pair_bias.single_layer_norm.bias
        self.arg762_1 = getattr(model.pairformer_stack.blocks, "23").attention_pair_bias.pair_layer_norm.weight
        self.arg763_1 = getattr(model.pairformer_stack.blocks, "23").attention_pair_bias.pair_layer_norm.bias
        self.arg764_1 = getattr(model.pairformer_stack.blocks, "23").attention_pair_bias.pair_linear.weight
        self.arg765_1 = getattr(model.pairformer_stack.blocks, "23").attention_pair_bias.attention.query_bias
        self.arg766_1 = getattr(model.pairformer_stack.blocks, "23").attention_pair_bias.attention.input2qkvg.weight
        self.arg767_1 = getattr(model.pairformer_stack.blocks, "23").attention_pair_bias.attention.output_proj.weight
        self.arg768_1 = getattr(model.pairformer_stack.blocks, "24").transition_pair.layer_norm.weight
        self.arg769_1 = getattr(model.pairformer_stack.blocks, "24").transition_pair.layer_norm.bias
        self.arg770_1 = getattr(model.pairformer_stack.blocks, "24").transition_pair.linear_no_bias_ab.weight
        self.arg771_1 = getattr(model.pairformer_stack.blocks, "24").transition_pair.linear_out.weight
        self.arg772_1 = getattr(model.pairformer_stack.blocks, "24").triangle_multiplication.layernorm_z_in.weight
        self.arg773_1 = getattr(model.pairformer_stack.blocks, "24").triangle_multiplication.layernorm_z_in.bias
        self.arg774_1 = getattr(model.pairformer_stack.blocks, "24").triangle_multiplication.linear_z_out.weight
        self.arg775_1 = getattr(model.pairformer_stack.blocks, "24").triangle_multiplication.merged_linear_p.weight
        self.arg776_1 = getattr(model.pairformer_stack.blocks, "24").triangle_multiplication.merged_linear_g.weight
        self.arg777_1 = getattr(model.pairformer_stack.blocks, "24").triangle_attention.out_scalers
        self.arg778_1 = getattr(model.pairformer_stack.blocks, "24").triangle_attention.pair2b.weight
        self.arg779_1 = getattr(model.pairformer_stack.blocks, "24").triangle_attention.pair2qkvg1.weight
        self.arg780_1 = getattr(model.pairformer_stack.blocks, "24").triangle_attention.pair2qkvg2.weight
        self.arg781_1 = getattr(model.pairformer_stack.blocks, "24").triangle_attention.linear_out.weight
        self.arg782_1 = getattr(model.pairformer_stack.blocks, "24").transition_single.layer_norm.weight
        self.arg783_1 = getattr(model.pairformer_stack.blocks, "24").transition_single.layer_norm.bias
        self.arg784_1 = getattr(model.pairformer_stack.blocks, "24").transition_single.linear_no_bias_ab.weight
        self.arg785_1 = getattr(model.pairformer_stack.blocks, "24").transition_single.linear_out.weight
        self.arg786_1 = getattr(model.pairformer_stack.blocks, "24").attention_pair_bias.single_layer_norm.weight
        self.arg787_1 = getattr(model.pairformer_stack.blocks, "24").attention_pair_bias.single_layer_norm.bias
        self.arg788_1 = getattr(model.pairformer_stack.blocks, "24").attention_pair_bias.pair_layer_norm.weight
        self.arg789_1 = getattr(model.pairformer_stack.blocks, "24").attention_pair_bias.pair_layer_norm.bias
        self.arg790_1 = getattr(model.pairformer_stack.blocks, "24").attention_pair_bias.pair_linear.weight
        self.arg791_1 = getattr(model.pairformer_stack.blocks, "24").attention_pair_bias.attention.query_bias
        self.arg792_1 = getattr(model.pairformer_stack.blocks, "24").attention_pair_bias.attention.input2qkvg.weight
        self.arg793_1 = getattr(model.pairformer_stack.blocks, "24").attention_pair_bias.attention.output_proj.weight
        self.arg794_1 = getattr(model.pairformer_stack.blocks, "25").transition_pair.layer_norm.weight
        self.arg795_1 = getattr(model.pairformer_stack.blocks, "25").transition_pair.layer_norm.bias
        self.arg796_1 = getattr(model.pairformer_stack.blocks, "25").transition_pair.linear_no_bias_ab.weight
        self.arg797_1 = getattr(model.pairformer_stack.blocks, "25").transition_pair.linear_out.weight
        self.arg798_1 = getattr(model.pairformer_stack.blocks, "25").triangle_multiplication.layernorm_z_in.weight
        self.arg799_1 = getattr(model.pairformer_stack.blocks, "25").triangle_multiplication.layernorm_z_in.bias
        self.arg800_1 = getattr(model.pairformer_stack.blocks, "25").triangle_multiplication.linear_z_out.weight
        self.arg801_1 = getattr(model.pairformer_stack.blocks, "25").triangle_multiplication.merged_linear_p.weight
        self.arg802_1 = getattr(model.pairformer_stack.blocks, "25").triangle_multiplication.merged_linear_g.weight
        self.arg803_1 = getattr(model.pairformer_stack.blocks, "25").triangle_attention.out_scalers
        self.arg804_1 = getattr(model.pairformer_stack.blocks, "25").triangle_attention.pair2b.weight
        self.arg805_1 = getattr(model.pairformer_stack.blocks, "25").triangle_attention.pair2qkvg1.weight
        self.arg806_1 = getattr(model.pairformer_stack.blocks, "25").triangle_attention.pair2qkvg2.weight
        self.arg807_1 = getattr(model.pairformer_stack.blocks, "25").triangle_attention.linear_out.weight
        self.arg808_1 = getattr(model.pairformer_stack.blocks, "25").transition_single.layer_norm.weight
        self.arg809_1 = getattr(model.pairformer_stack.blocks, "25").transition_single.layer_norm.bias
        self.arg810_1 = getattr(model.pairformer_stack.blocks, "25").transition_single.linear_no_bias_ab.weight
        self.arg811_1 = getattr(model.pairformer_stack.blocks, "25").transition_single.linear_out.weight
        self.arg812_1 = getattr(model.pairformer_stack.blocks, "25").attention_pair_bias.single_layer_norm.weight
        self.arg813_1 = getattr(model.pairformer_stack.blocks, "25").attention_pair_bias.single_layer_norm.bias
        self.arg814_1 = getattr(model.pairformer_stack.blocks, "25").attention_pair_bias.pair_layer_norm.weight
        self.arg815_1 = getattr(model.pairformer_stack.blocks, "25").attention_pair_bias.pair_layer_norm.bias
        self.arg816_1 = getattr(model.pairformer_stack.blocks, "25").attention_pair_bias.pair_linear.weight
        self.arg817_1 = getattr(model.pairformer_stack.blocks, "25").attention_pair_bias.attention.query_bias
        self.arg818_1 = getattr(model.pairformer_stack.blocks, "25").attention_pair_bias.attention.input2qkvg.weight
        self.arg819_1 = getattr(model.pairformer_stack.blocks, "25").attention_pair_bias.attention.output_proj.weight
        self.arg820_1 = getattr(model.pairformer_stack.blocks, "26").transition_pair.layer_norm.weight
        self.arg821_1 = getattr(model.pairformer_stack.blocks, "26").transition_pair.layer_norm.bias
        self.arg822_1 = getattr(model.pairformer_stack.blocks, "26").transition_pair.linear_no_bias_ab.weight
        self.arg823_1 = getattr(model.pairformer_stack.blocks, "26").transition_pair.linear_out.weight
        self.arg824_1 = getattr(model.pairformer_stack.blocks, "26").triangle_multiplication.layernorm_z_in.weight
        self.arg825_1 = getattr(model.pairformer_stack.blocks, "26").triangle_multiplication.layernorm_z_in.bias
        self.arg826_1 = getattr(model.pairformer_stack.blocks, "26").triangle_multiplication.linear_z_out.weight
        self.arg827_1 = getattr(model.pairformer_stack.blocks, "26").triangle_multiplication.merged_linear_p.weight
        self.arg828_1 = getattr(model.pairformer_stack.blocks, "26").triangle_multiplication.merged_linear_g.weight
        self.arg829_1 = getattr(model.pairformer_stack.blocks, "26").triangle_attention.out_scalers
        self.arg830_1 = getattr(model.pairformer_stack.blocks, "26").triangle_attention.pair2b.weight
        self.arg831_1 = getattr(model.pairformer_stack.blocks, "26").triangle_attention.pair2qkvg1.weight
        self.arg832_1 = getattr(model.pairformer_stack.blocks, "26").triangle_attention.pair2qkvg2.weight
        self.arg833_1 = getattr(model.pairformer_stack.blocks, "26").triangle_attention.linear_out.weight
        self.arg834_1 = getattr(model.pairformer_stack.blocks, "26").transition_single.layer_norm.weight
        self.arg835_1 = getattr(model.pairformer_stack.blocks, "26").transition_single.layer_norm.bias
        self.arg836_1 = getattr(model.pairformer_stack.blocks, "26").transition_single.linear_no_bias_ab.weight
        self.arg837_1 = getattr(model.pairformer_stack.blocks, "26").transition_single.linear_out.weight
        self.arg838_1 = getattr(model.pairformer_stack.blocks, "26").attention_pair_bias.single_layer_norm.weight
        self.arg839_1 = getattr(model.pairformer_stack.blocks, "26").attention_pair_bias.single_layer_norm.bias
        self.arg840_1 = getattr(model.pairformer_stack.blocks, "26").attention_pair_bias.pair_layer_norm.weight
        self.arg841_1 = getattr(model.pairformer_stack.blocks, "26").attention_pair_bias.pair_layer_norm.bias
        self.arg842_1 = getattr(model.pairformer_stack.blocks, "26").attention_pair_bias.pair_linear.weight
        self.arg843_1 = getattr(model.pairformer_stack.blocks, "26").attention_pair_bias.attention.query_bias
        self.arg844_1 = getattr(model.pairformer_stack.blocks, "26").attention_pair_bias.attention.input2qkvg.weight
        self.arg845_1 = getattr(model.pairformer_stack.blocks, "26").attention_pair_bias.attention.output_proj.weight
        self.arg846_1 = getattr(model.pairformer_stack.blocks, "27").transition_pair.layer_norm.weight
        self.arg847_1 = getattr(model.pairformer_stack.blocks, "27").transition_pair.layer_norm.bias
        self.arg848_1 = getattr(model.pairformer_stack.blocks, "27").transition_pair.linear_no_bias_ab.weight
        self.arg849_1 = getattr(model.pairformer_stack.blocks, "27").transition_pair.linear_out.weight
        self.arg850_1 = getattr(model.pairformer_stack.blocks, "27").triangle_multiplication.layernorm_z_in.weight
        self.arg851_1 = getattr(model.pairformer_stack.blocks, "27").triangle_multiplication.layernorm_z_in.bias
        self.arg852_1 = getattr(model.pairformer_stack.blocks, "27").triangle_multiplication.linear_z_out.weight
        self.arg853_1 = getattr(model.pairformer_stack.blocks, "27").triangle_multiplication.merged_linear_p.weight
        self.arg854_1 = getattr(model.pairformer_stack.blocks, "27").triangle_multiplication.merged_linear_g.weight
        self.arg855_1 = getattr(model.pairformer_stack.blocks, "27").triangle_attention.out_scalers
        self.arg856_1 = getattr(model.pairformer_stack.blocks, "27").triangle_attention.pair2b.weight
        self.arg857_1 = getattr(model.pairformer_stack.blocks, "27").triangle_attention.pair2qkvg1.weight
        self.arg858_1 = getattr(model.pairformer_stack.blocks, "27").triangle_attention.pair2qkvg2.weight
        self.arg859_1 = getattr(model.pairformer_stack.blocks, "27").triangle_attention.linear_out.weight
        self.arg860_1 = getattr(model.pairformer_stack.blocks, "27").transition_single.layer_norm.weight
        self.arg861_1 = getattr(model.pairformer_stack.blocks, "27").transition_single.layer_norm.bias
        self.arg862_1 = getattr(model.pairformer_stack.blocks, "27").transition_single.linear_no_bias_ab.weight
        self.arg863_1 = getattr(model.pairformer_stack.blocks, "27").transition_single.linear_out.weight
        self.arg864_1 = getattr(model.pairformer_stack.blocks, "27").attention_pair_bias.single_layer_norm.weight
        self.arg865_1 = getattr(model.pairformer_stack.blocks, "27").attention_pair_bias.single_layer_norm.bias
        self.arg866_1 = getattr(model.pairformer_stack.blocks, "27").attention_pair_bias.pair_layer_norm.weight
        self.arg867_1 = getattr(model.pairformer_stack.blocks, "27").attention_pair_bias.pair_layer_norm.bias
        self.arg868_1 = getattr(model.pairformer_stack.blocks, "27").attention_pair_bias.pair_linear.weight
        self.arg869_1 = getattr(model.pairformer_stack.blocks, "27").attention_pair_bias.attention.query_bias
        self.arg870_1 = getattr(model.pairformer_stack.blocks, "27").attention_pair_bias.attention.input2qkvg.weight
        self.arg871_1 = getattr(model.pairformer_stack.blocks, "27").attention_pair_bias.attention.output_proj.weight
        self.arg872_1 = getattr(model.pairformer_stack.blocks, "28").transition_pair.layer_norm.weight
        self.arg873_1 = getattr(model.pairformer_stack.blocks, "28").transition_pair.layer_norm.bias
        self.arg874_1 = getattr(model.pairformer_stack.blocks, "28").transition_pair.linear_no_bias_ab.weight
        self.arg875_1 = getattr(model.pairformer_stack.blocks, "28").transition_pair.linear_out.weight
        self.arg876_1 = getattr(model.pairformer_stack.blocks, "28").triangle_multiplication.layernorm_z_in.weight
        self.arg877_1 = getattr(model.pairformer_stack.blocks, "28").triangle_multiplication.layernorm_z_in.bias
        self.arg878_1 = getattr(model.pairformer_stack.blocks, "28").triangle_multiplication.linear_z_out.weight
        self.arg879_1 = getattr(model.pairformer_stack.blocks, "28").triangle_multiplication.merged_linear_p.weight
        self.arg880_1 = getattr(model.pairformer_stack.blocks, "28").triangle_multiplication.merged_linear_g.weight
        self.arg881_1 = getattr(model.pairformer_stack.blocks, "28").triangle_attention.out_scalers
        self.arg882_1 = getattr(model.pairformer_stack.blocks, "28").triangle_attention.pair2b.weight
        self.arg883_1 = getattr(model.pairformer_stack.blocks, "28").triangle_attention.pair2qkvg1.weight
        self.arg884_1 = getattr(model.pairformer_stack.blocks, "28").triangle_attention.pair2qkvg2.weight
        self.arg885_1 = getattr(model.pairformer_stack.blocks, "28").triangle_attention.linear_out.weight
        self.arg886_1 = getattr(model.pairformer_stack.blocks, "28").transition_single.layer_norm.weight
        self.arg887_1 = getattr(model.pairformer_stack.blocks, "28").transition_single.layer_norm.bias
        self.arg888_1 = getattr(model.pairformer_stack.blocks, "28").transition_single.linear_no_bias_ab.weight
        self.arg889_1 = getattr(model.pairformer_stack.blocks, "28").transition_single.linear_out.weight
        self.arg890_1 = getattr(model.pairformer_stack.blocks, "28").attention_pair_bias.single_layer_norm.weight
        self.arg891_1 = getattr(model.pairformer_stack.blocks, "28").attention_pair_bias.single_layer_norm.bias
        self.arg892_1 = getattr(model.pairformer_stack.blocks, "28").attention_pair_bias.pair_layer_norm.weight
        self.arg893_1 = getattr(model.pairformer_stack.blocks, "28").attention_pair_bias.pair_layer_norm.bias
        self.arg894_1 = getattr(model.pairformer_stack.blocks, "28").attention_pair_bias.pair_linear.weight
        self.arg895_1 = getattr(model.pairformer_stack.blocks, "28").attention_pair_bias.attention.query_bias
        self.arg896_1 = getattr(model.pairformer_stack.blocks, "28").attention_pair_bias.attention.input2qkvg.weight
        self.arg897_1 = getattr(model.pairformer_stack.blocks, "28").attention_pair_bias.attention.output_proj.weight
        self.arg898_1 = getattr(model.pairformer_stack.blocks, "29").transition_pair.layer_norm.weight
        self.arg899_1 = getattr(model.pairformer_stack.blocks, "29").transition_pair.layer_norm.bias
        self.arg900_1 = getattr(model.pairformer_stack.blocks, "29").transition_pair.linear_no_bias_ab.weight
        self.arg901_1 = getattr(model.pairformer_stack.blocks, "29").transition_pair.linear_out.weight
        self.arg902_1 = getattr(model.pairformer_stack.blocks, "29").triangle_multiplication.layernorm_z_in.weight
        self.arg903_1 = getattr(model.pairformer_stack.blocks, "29").triangle_multiplication.layernorm_z_in.bias
        self.arg904_1 = getattr(model.pairformer_stack.blocks, "29").triangle_multiplication.linear_z_out.weight
        self.arg905_1 = getattr(model.pairformer_stack.blocks, "29").triangle_multiplication.merged_linear_p.weight
        self.arg906_1 = getattr(model.pairformer_stack.blocks, "29").triangle_multiplication.merged_linear_g.weight
        self.arg907_1 = getattr(model.pairformer_stack.blocks, "29").triangle_attention.out_scalers
        self.arg908_1 = getattr(model.pairformer_stack.blocks, "29").triangle_attention.pair2b.weight
        self.arg909_1 = getattr(model.pairformer_stack.blocks, "29").triangle_attention.pair2qkvg1.weight
        self.arg910_1 = getattr(model.pairformer_stack.blocks, "29").triangle_attention.pair2qkvg2.weight
        self.arg911_1 = getattr(model.pairformer_stack.blocks, "29").triangle_attention.linear_out.weight
        self.arg912_1 = getattr(model.pairformer_stack.blocks, "29").transition_single.layer_norm.weight
        self.arg913_1 = getattr(model.pairformer_stack.blocks, "29").transition_single.layer_norm.bias
        self.arg914_1 = getattr(model.pairformer_stack.blocks, "29").transition_single.linear_no_bias_ab.weight
        self.arg915_1 = getattr(model.pairformer_stack.blocks, "29").transition_single.linear_out.weight
        self.arg916_1 = getattr(model.pairformer_stack.blocks, "29").attention_pair_bias.single_layer_norm.weight
        self.arg917_1 = getattr(model.pairformer_stack.blocks, "29").attention_pair_bias.single_layer_norm.bias
        self.arg918_1 = getattr(model.pairformer_stack.blocks, "29").attention_pair_bias.pair_layer_norm.weight
        self.arg919_1 = getattr(model.pairformer_stack.blocks, "29").attention_pair_bias.pair_layer_norm.bias
        self.arg920_1 = getattr(model.pairformer_stack.blocks, "29").attention_pair_bias.pair_linear.weight
        self.arg921_1 = getattr(model.pairformer_stack.blocks, "29").attention_pair_bias.attention.query_bias
        self.arg922_1 = getattr(model.pairformer_stack.blocks, "29").attention_pair_bias.attention.input2qkvg.weight
        self.arg923_1 = getattr(model.pairformer_stack.blocks, "29").attention_pair_bias.attention.output_proj.weight
        self.arg924_1 = getattr(model.pairformer_stack.blocks, "30").transition_pair.layer_norm.weight
        self.arg925_1 = getattr(model.pairformer_stack.blocks, "30").transition_pair.layer_norm.bias
        self.arg926_1 = getattr(model.pairformer_stack.blocks, "30").transition_pair.linear_no_bias_ab.weight
        self.arg927_1 = getattr(model.pairformer_stack.blocks, "30").transition_pair.linear_out.weight
        self.arg928_1 = getattr(model.pairformer_stack.blocks, "30").triangle_multiplication.layernorm_z_in.weight
        self.arg929_1 = getattr(model.pairformer_stack.blocks, "30").triangle_multiplication.layernorm_z_in.bias
        self.arg930_1 = getattr(model.pairformer_stack.blocks, "30").triangle_multiplication.linear_z_out.weight
        self.arg931_1 = getattr(model.pairformer_stack.blocks, "30").triangle_multiplication.merged_linear_p.weight
        self.arg932_1 = getattr(model.pairformer_stack.blocks, "30").triangle_multiplication.merged_linear_g.weight
        self.arg933_1 = getattr(model.pairformer_stack.blocks, "30").triangle_attention.out_scalers
        self.arg934_1 = getattr(model.pairformer_stack.blocks, "30").triangle_attention.pair2b.weight
        self.arg935_1 = getattr(model.pairformer_stack.blocks, "30").triangle_attention.pair2qkvg1.weight
        self.arg936_1 = getattr(model.pairformer_stack.blocks, "30").triangle_attention.pair2qkvg2.weight
        self.arg937_1 = getattr(model.pairformer_stack.blocks, "30").triangle_attention.linear_out.weight
        self.arg938_1 = getattr(model.pairformer_stack.blocks, "30").transition_single.layer_norm.weight
        self.arg939_1 = getattr(model.pairformer_stack.blocks, "30").transition_single.layer_norm.bias
        self.arg940_1 = getattr(model.pairformer_stack.blocks, "30").transition_single.linear_no_bias_ab.weight
        self.arg941_1 = getattr(model.pairformer_stack.blocks, "30").transition_single.linear_out.weight
        self.arg942_1 = getattr(model.pairformer_stack.blocks, "30").attention_pair_bias.single_layer_norm.weight
        self.arg943_1 = getattr(model.pairformer_stack.blocks, "30").attention_pair_bias.single_layer_norm.bias
        self.arg944_1 = getattr(model.pairformer_stack.blocks, "30").attention_pair_bias.pair_layer_norm.weight
        self.arg945_1 = getattr(model.pairformer_stack.blocks, "30").attention_pair_bias.pair_layer_norm.bias
        self.arg946_1 = getattr(model.pairformer_stack.blocks, "30").attention_pair_bias.pair_linear.weight
        self.arg947_1 = getattr(model.pairformer_stack.blocks, "30").attention_pair_bias.attention.query_bias
        self.arg948_1 = getattr(model.pairformer_stack.blocks, "30").attention_pair_bias.attention.input2qkvg.weight
        self.arg949_1 = getattr(model.pairformer_stack.blocks, "30").attention_pair_bias.attention.output_proj.weight
        self.arg950_1 = getattr(model.pairformer_stack.blocks, "31").transition_pair.layer_norm.weight
        self.arg951_1 = getattr(model.pairformer_stack.blocks, "31").transition_pair.layer_norm.bias
        self.arg952_1 = getattr(model.pairformer_stack.blocks, "31").transition_pair.linear_no_bias_ab.weight
        self.arg953_1 = getattr(model.pairformer_stack.blocks, "31").transition_pair.linear_out.weight
        self.arg954_1 = getattr(model.pairformer_stack.blocks, "31").triangle_multiplication.layernorm_z_in.weight
        self.arg955_1 = getattr(model.pairformer_stack.blocks, "31").triangle_multiplication.layernorm_z_in.bias
        self.arg956_1 = getattr(model.pairformer_stack.blocks, "31").triangle_multiplication.linear_z_out.weight
        self.arg957_1 = getattr(model.pairformer_stack.blocks, "31").triangle_multiplication.merged_linear_p.weight
        self.arg958_1 = getattr(model.pairformer_stack.blocks, "31").triangle_multiplication.merged_linear_g.weight
        self.arg959_1 = getattr(model.pairformer_stack.blocks, "31").triangle_attention.out_scalers
        self.arg960_1 = getattr(model.pairformer_stack.blocks, "31").triangle_attention.pair2b.weight
        self.arg961_1 = getattr(model.pairformer_stack.blocks, "31").triangle_attention.pair2qkvg1.weight
        self.arg962_1 = getattr(model.pairformer_stack.blocks, "31").triangle_attention.pair2qkvg2.weight
        self.arg963_1 = getattr(model.pairformer_stack.blocks, "31").triangle_attention.linear_out.weight
        self.arg964_1 = getattr(model.pairformer_stack.blocks, "31").transition_single.layer_norm.weight
        self.arg965_1 = getattr(model.pairformer_stack.blocks, "31").transition_single.layer_norm.bias
        self.arg966_1 = getattr(model.pairformer_stack.blocks, "31").transition_single.linear_no_bias_ab.weight
        self.arg967_1 = getattr(model.pairformer_stack.blocks, "31").transition_single.linear_out.weight
        self.arg968_1 = getattr(model.pairformer_stack.blocks, "31").attention_pair_bias.single_layer_norm.weight
        self.arg969_1 = getattr(model.pairformer_stack.blocks, "31").attention_pair_bias.single_layer_norm.bias
        self.arg970_1 = getattr(model.pairformer_stack.blocks, "31").attention_pair_bias.pair_layer_norm.weight
        self.arg971_1 = getattr(model.pairformer_stack.blocks, "31").attention_pair_bias.pair_layer_norm.bias
        self.arg972_1 = getattr(model.pairformer_stack.blocks, "31").attention_pair_bias.pair_linear.weight
        self.arg973_1 = getattr(model.pairformer_stack.blocks, "31").attention_pair_bias.attention.query_bias
        self.arg974_1 = getattr(model.pairformer_stack.blocks, "31").attention_pair_bias.attention.input2qkvg.weight
        self.arg975_1 = getattr(model.pairformer_stack.blocks, "31").attention_pair_bias.attention.output_proj.weight
        self.arg976_1 = getattr(model.pairformer_stack.blocks, "32").transition_pair.layer_norm.weight
        self.arg977_1 = getattr(model.pairformer_stack.blocks, "32").transition_pair.layer_norm.bias
        self.arg978_1 = getattr(model.pairformer_stack.blocks, "32").transition_pair.linear_no_bias_ab.weight
        self.arg979_1 = getattr(model.pairformer_stack.blocks, "32").transition_pair.linear_out.weight
        self.arg980_1 = getattr(model.pairformer_stack.blocks, "32").triangle_multiplication.layernorm_z_in.weight
        self.arg981_1 = getattr(model.pairformer_stack.blocks, "32").triangle_multiplication.layernorm_z_in.bias
        self.arg982_1 = getattr(model.pairformer_stack.blocks, "32").triangle_multiplication.linear_z_out.weight
        self.arg983_1 = getattr(model.pairformer_stack.blocks, "32").triangle_multiplication.merged_linear_p.weight
        self.arg984_1 = getattr(model.pairformer_stack.blocks, "32").triangle_multiplication.merged_linear_g.weight
        self.arg985_1 = getattr(model.pairformer_stack.blocks, "32").triangle_attention.out_scalers
        self.arg986_1 = getattr(model.pairformer_stack.blocks, "32").triangle_attention.pair2b.weight
        self.arg987_1 = getattr(model.pairformer_stack.blocks, "32").triangle_attention.pair2qkvg1.weight
        self.arg988_1 = getattr(model.pairformer_stack.blocks, "32").triangle_attention.pair2qkvg2.weight
        self.arg989_1 = getattr(model.pairformer_stack.blocks, "32").triangle_attention.linear_out.weight
        self.arg990_1 = getattr(model.pairformer_stack.blocks, "32").transition_single.layer_norm.weight
        self.arg991_1 = getattr(model.pairformer_stack.blocks, "32").transition_single.layer_norm.bias
        self.arg992_1 = getattr(model.pairformer_stack.blocks, "32").transition_single.linear_no_bias_ab.weight
        self.arg993_1 = getattr(model.pairformer_stack.blocks, "32").transition_single.linear_out.weight
        self.arg994_1 = getattr(model.pairformer_stack.blocks, "32").attention_pair_bias.single_layer_norm.weight
        self.arg995_1 = getattr(model.pairformer_stack.blocks, "32").attention_pair_bias.single_layer_norm.bias
        self.arg996_1 = getattr(model.pairformer_stack.blocks, "32").attention_pair_bias.pair_layer_norm.weight
        self.arg997_1 = getattr(model.pairformer_stack.blocks, "32").attention_pair_bias.pair_layer_norm.bias
        self.arg998_1 = getattr(model.pairformer_stack.blocks, "32").attention_pair_bias.pair_linear.weight
        self.arg999_1 = getattr(model.pairformer_stack.blocks, "32").attention_pair_bias.attention.query_bias
        self.arg1000_1 = getattr(model.pairformer_stack.blocks, "32").attention_pair_bias.attention.input2qkvg.weight
        self.arg1001_1 = getattr(model.pairformer_stack.blocks, "32").attention_pair_bias.attention.output_proj.weight
        self.arg1002_1 = getattr(model.pairformer_stack.blocks, "33").transition_pair.layer_norm.weight
        self.arg1003_1 = getattr(model.pairformer_stack.blocks, "33").transition_pair.layer_norm.bias
        self.arg1004_1 = getattr(model.pairformer_stack.blocks, "33").transition_pair.linear_no_bias_ab.weight
        self.arg1005_1 = getattr(model.pairformer_stack.blocks, "33").transition_pair.linear_out.weight
        self.arg1006_1 = getattr(model.pairformer_stack.blocks, "33").triangle_multiplication.layernorm_z_in.weight
        self.arg1007_1 = getattr(model.pairformer_stack.blocks, "33").triangle_multiplication.layernorm_z_in.bias
        self.arg1008_1 = getattr(model.pairformer_stack.blocks, "33").triangle_multiplication.linear_z_out.weight
        self.arg1009_1 = getattr(model.pairformer_stack.blocks, "33").triangle_multiplication.merged_linear_p.weight
        self.arg1010_1 = getattr(model.pairformer_stack.blocks, "33").triangle_multiplication.merged_linear_g.weight
        self.arg1011_1 = getattr(model.pairformer_stack.blocks, "33").triangle_attention.out_scalers
        self.arg1012_1 = getattr(model.pairformer_stack.blocks, "33").triangle_attention.pair2b.weight
        self.arg1013_1 = getattr(model.pairformer_stack.blocks, "33").triangle_attention.pair2qkvg1.weight
        self.arg1014_1 = getattr(model.pairformer_stack.blocks, "33").triangle_attention.pair2qkvg2.weight
        self.arg1015_1 = getattr(model.pairformer_stack.blocks, "33").triangle_attention.linear_out.weight
        self.arg1016_1 = getattr(model.pairformer_stack.blocks, "33").transition_single.layer_norm.weight
        self.arg1017_1 = getattr(model.pairformer_stack.blocks, "33").transition_single.layer_norm.bias
        self.arg1018_1 = getattr(model.pairformer_stack.blocks, "33").transition_single.linear_no_bias_ab.weight
        self.arg1019_1 = getattr(model.pairformer_stack.blocks, "33").transition_single.linear_out.weight
        self.arg1020_1 = getattr(model.pairformer_stack.blocks, "33").attention_pair_bias.single_layer_norm.weight
        self.arg1021_1 = getattr(model.pairformer_stack.blocks, "33").attention_pair_bias.single_layer_norm.bias
        self.arg1022_1 = getattr(model.pairformer_stack.blocks, "33").attention_pair_bias.pair_layer_norm.weight
        self.arg1023_1 = getattr(model.pairformer_stack.blocks, "33").attention_pair_bias.pair_layer_norm.bias
        self.arg1024_1 = getattr(model.pairformer_stack.blocks, "33").attention_pair_bias.pair_linear.weight
        self.arg1025_1 = getattr(model.pairformer_stack.blocks, "33").attention_pair_bias.attention.query_bias
        self.arg1026_1 = getattr(model.pairformer_stack.blocks, "33").attention_pair_bias.attention.input2qkvg.weight
        self.arg1027_1 = getattr(model.pairformer_stack.blocks, "33").attention_pair_bias.attention.output_proj.weight
        self.arg1028_1 = getattr(model.pairformer_stack.blocks, "34").transition_pair.layer_norm.weight
        self.arg1029_1 = getattr(model.pairformer_stack.blocks, "34").transition_pair.layer_norm.bias
        self.arg1030_1 = getattr(model.pairformer_stack.blocks, "34").transition_pair.linear_no_bias_ab.weight
        self.arg1031_1 = getattr(model.pairformer_stack.blocks, "34").transition_pair.linear_out.weight
        self.arg1032_1 = getattr(model.pairformer_stack.blocks, "34").triangle_multiplication.layernorm_z_in.weight
        self.arg1033_1 = getattr(model.pairformer_stack.blocks, "34").triangle_multiplication.layernorm_z_in.bias
        self.arg1034_1 = getattr(model.pairformer_stack.blocks, "34").triangle_multiplication.linear_z_out.weight
        self.arg1035_1 = getattr(model.pairformer_stack.blocks, "34").triangle_multiplication.merged_linear_p.weight
        self.arg1036_1 = getattr(model.pairformer_stack.blocks, "34").triangle_multiplication.merged_linear_g.weight
        self.arg1037_1 = getattr(model.pairformer_stack.blocks, "34").triangle_attention.out_scalers
        self.arg1038_1 = getattr(model.pairformer_stack.blocks, "34").triangle_attention.pair2b.weight
        self.arg1039_1 = getattr(model.pairformer_stack.blocks, "34").triangle_attention.pair2qkvg1.weight
        self.arg1040_1 = getattr(model.pairformer_stack.blocks, "34").triangle_attention.pair2qkvg2.weight
        self.arg1041_1 = getattr(model.pairformer_stack.blocks, "34").triangle_attention.linear_out.weight
        self.arg1042_1 = getattr(model.pairformer_stack.blocks, "34").transition_single.layer_norm.weight
        self.arg1043_1 = getattr(model.pairformer_stack.blocks, "34").transition_single.layer_norm.bias
        self.arg1044_1 = getattr(model.pairformer_stack.blocks, "34").transition_single.linear_no_bias_ab.weight
        self.arg1045_1 = getattr(model.pairformer_stack.blocks, "34").transition_single.linear_out.weight
        self.arg1046_1 = getattr(model.pairformer_stack.blocks, "34").attention_pair_bias.single_layer_norm.weight
        self.arg1047_1 = getattr(model.pairformer_stack.blocks, "34").attention_pair_bias.single_layer_norm.bias
        self.arg1048_1 = getattr(model.pairformer_stack.blocks, "34").attention_pair_bias.pair_layer_norm.weight
        self.arg1049_1 = getattr(model.pairformer_stack.blocks, "34").attention_pair_bias.pair_layer_norm.bias
        self.arg1050_1 = getattr(model.pairformer_stack.blocks, "34").attention_pair_bias.pair_linear.weight
        self.arg1051_1 = getattr(model.pairformer_stack.blocks, "34").attention_pair_bias.attention.query_bias
        self.arg1052_1 = getattr(model.pairformer_stack.blocks, "34").attention_pair_bias.attention.input2qkvg.weight
        self.arg1053_1 = getattr(model.pairformer_stack.blocks, "34").attention_pair_bias.attention.output_proj.weight
        self.arg1054_1 = getattr(model.pairformer_stack.blocks, "35").transition_pair.layer_norm.weight
        self.arg1055_1 = getattr(model.pairformer_stack.blocks, "35").transition_pair.layer_norm.bias
        self.arg1056_1 = getattr(model.pairformer_stack.blocks, "35").transition_pair.linear_no_bias_ab.weight
        self.arg1057_1 = getattr(model.pairformer_stack.blocks, "35").transition_pair.linear_out.weight
        self.arg1058_1 = getattr(model.pairformer_stack.blocks, "35").triangle_multiplication.layernorm_z_in.weight
        self.arg1059_1 = getattr(model.pairformer_stack.blocks, "35").triangle_multiplication.layernorm_z_in.bias
        self.arg1060_1 = getattr(model.pairformer_stack.blocks, "35").triangle_multiplication.linear_z_out.weight
        self.arg1061_1 = getattr(model.pairformer_stack.blocks, "35").triangle_multiplication.merged_linear_p.weight
        self.arg1062_1 = getattr(model.pairformer_stack.blocks, "35").triangle_multiplication.merged_linear_g.weight
        self.arg1063_1 = getattr(model.pairformer_stack.blocks, "35").triangle_attention.out_scalers
        self.arg1064_1 = getattr(model.pairformer_stack.blocks, "35").triangle_attention.pair2b.weight
        self.arg1065_1 = getattr(model.pairformer_stack.blocks, "35").triangle_attention.pair2qkvg1.weight
        self.arg1066_1 = getattr(model.pairformer_stack.blocks, "35").triangle_attention.pair2qkvg2.weight
        self.arg1067_1 = getattr(model.pairformer_stack.blocks, "35").triangle_attention.linear_out.weight
        self.arg1068_1 = getattr(model.pairformer_stack.blocks, "35").transition_single.layer_norm.weight
        self.arg1069_1 = getattr(model.pairformer_stack.blocks, "35").transition_single.layer_norm.bias
        self.arg1070_1 = getattr(model.pairformer_stack.blocks, "35").transition_single.linear_no_bias_ab.weight
        self.arg1071_1 = getattr(model.pairformer_stack.blocks, "35").transition_single.linear_out.weight
        self.arg1072_1 = getattr(model.pairformer_stack.blocks, "35").attention_pair_bias.single_layer_norm.weight
        self.arg1073_1 = getattr(model.pairformer_stack.blocks, "35").attention_pair_bias.single_layer_norm.bias
        self.arg1074_1 = getattr(model.pairformer_stack.blocks, "35").attention_pair_bias.pair_layer_norm.weight
        self.arg1075_1 = getattr(model.pairformer_stack.blocks, "35").attention_pair_bias.pair_layer_norm.bias
        self.arg1076_1 = getattr(model.pairformer_stack.blocks, "35").attention_pair_bias.pair_linear.weight
        self.arg1077_1 = getattr(model.pairformer_stack.blocks, "35").attention_pair_bias.attention.query_bias
        self.arg1078_1 = getattr(model.pairformer_stack.blocks, "35").attention_pair_bias.attention.input2qkvg.weight
        self.arg1079_1 = getattr(model.pairformer_stack.blocks, "35").attention_pair_bias.attention.output_proj.weight
        self.arg1080_1 = getattr(model.pairformer_stack.blocks, "36").transition_pair.layer_norm.weight
        self.arg1081_1 = getattr(model.pairformer_stack.blocks, "36").transition_pair.layer_norm.bias
        self.arg1082_1 = getattr(model.pairformer_stack.blocks, "36").transition_pair.linear_no_bias_ab.weight
        self.arg1083_1 = getattr(model.pairformer_stack.blocks, "36").transition_pair.linear_out.weight
        self.arg1084_1 = getattr(model.pairformer_stack.blocks, "36").triangle_multiplication.layernorm_z_in.weight
        self.arg1085_1 = getattr(model.pairformer_stack.blocks, "36").triangle_multiplication.layernorm_z_in.bias
        self.arg1086_1 = getattr(model.pairformer_stack.blocks, "36").triangle_multiplication.linear_z_out.weight
        self.arg1087_1 = getattr(model.pairformer_stack.blocks, "36").triangle_multiplication.merged_linear_p.weight
        self.arg1088_1 = getattr(model.pairformer_stack.blocks, "36").triangle_multiplication.merged_linear_g.weight
        self.arg1089_1 = getattr(model.pairformer_stack.blocks, "36").triangle_attention.out_scalers
        self.arg1090_1 = getattr(model.pairformer_stack.blocks, "36").triangle_attention.pair2b.weight
        self.arg1091_1 = getattr(model.pairformer_stack.blocks, "36").triangle_attention.pair2qkvg1.weight
        self.arg1092_1 = getattr(model.pairformer_stack.blocks, "36").triangle_attention.pair2qkvg2.weight
        self.arg1093_1 = getattr(model.pairformer_stack.blocks, "36").triangle_attention.linear_out.weight
        self.arg1094_1 = getattr(model.pairformer_stack.blocks, "36").transition_single.layer_norm.weight
        self.arg1095_1 = getattr(model.pairformer_stack.blocks, "36").transition_single.layer_norm.bias
        self.arg1096_1 = getattr(model.pairformer_stack.blocks, "36").transition_single.linear_no_bias_ab.weight
        self.arg1097_1 = getattr(model.pairformer_stack.blocks, "36").transition_single.linear_out.weight
        self.arg1098_1 = getattr(model.pairformer_stack.blocks, "36").attention_pair_bias.single_layer_norm.weight
        self.arg1099_1 = getattr(model.pairformer_stack.blocks, "36").attention_pair_bias.single_layer_norm.bias
        self.arg1100_1 = getattr(model.pairformer_stack.blocks, "36").attention_pair_bias.pair_layer_norm.weight
        self.arg1101_1 = getattr(model.pairformer_stack.blocks, "36").attention_pair_bias.pair_layer_norm.bias
        self.arg1102_1 = getattr(model.pairformer_stack.blocks, "36").attention_pair_bias.pair_linear.weight
        self.arg1103_1 = getattr(model.pairformer_stack.blocks, "36").attention_pair_bias.attention.query_bias
        self.arg1104_1 = getattr(model.pairformer_stack.blocks, "36").attention_pair_bias.attention.input2qkvg.weight
        self.arg1105_1 = getattr(model.pairformer_stack.blocks, "36").attention_pair_bias.attention.output_proj.weight
        self.arg1106_1 = getattr(model.pairformer_stack.blocks, "37").transition_pair.layer_norm.weight
        self.arg1107_1 = getattr(model.pairformer_stack.blocks, "37").transition_pair.layer_norm.bias
        self.arg1108_1 = getattr(model.pairformer_stack.blocks, "37").transition_pair.linear_no_bias_ab.weight
        self.arg1109_1 = getattr(model.pairformer_stack.blocks, "37").transition_pair.linear_out.weight
        self.arg1110_1 = getattr(model.pairformer_stack.blocks, "37").triangle_multiplication.layernorm_z_in.weight
        self.arg1111_1 = getattr(model.pairformer_stack.blocks, "37").triangle_multiplication.layernorm_z_in.bias
        self.arg1112_1 = getattr(model.pairformer_stack.blocks, "37").triangle_multiplication.linear_z_out.weight
        self.arg1113_1 = getattr(model.pairformer_stack.blocks, "37").triangle_multiplication.merged_linear_p.weight
        self.arg1114_1 = getattr(model.pairformer_stack.blocks, "37").triangle_multiplication.merged_linear_g.weight
        self.arg1115_1 = getattr(model.pairformer_stack.blocks, "37").triangle_attention.out_scalers
        self.arg1116_1 = getattr(model.pairformer_stack.blocks, "37").triangle_attention.pair2b.weight
        self.arg1117_1 = getattr(model.pairformer_stack.blocks, "37").triangle_attention.pair2qkvg1.weight
        self.arg1118_1 = getattr(model.pairformer_stack.blocks, "37").triangle_attention.pair2qkvg2.weight
        self.arg1119_1 = getattr(model.pairformer_stack.blocks, "37").triangle_attention.linear_out.weight
        self.arg1120_1 = getattr(model.pairformer_stack.blocks, "37").transition_single.layer_norm.weight
        self.arg1121_1 = getattr(model.pairformer_stack.blocks, "37").transition_single.layer_norm.bias
        self.arg1122_1 = getattr(model.pairformer_stack.blocks, "37").transition_single.linear_no_bias_ab.weight
        self.arg1123_1 = getattr(model.pairformer_stack.blocks, "37").transition_single.linear_out.weight
        self.arg1124_1 = getattr(model.pairformer_stack.blocks, "37").attention_pair_bias.single_layer_norm.weight
        self.arg1125_1 = getattr(model.pairformer_stack.blocks, "37").attention_pair_bias.single_layer_norm.bias
        self.arg1126_1 = getattr(model.pairformer_stack.blocks, "37").attention_pair_bias.pair_layer_norm.weight
        self.arg1127_1 = getattr(model.pairformer_stack.blocks, "37").attention_pair_bias.pair_layer_norm.bias
        self.arg1128_1 = getattr(model.pairformer_stack.blocks, "37").attention_pair_bias.pair_linear.weight
        self.arg1129_1 = getattr(model.pairformer_stack.blocks, "37").attention_pair_bias.attention.query_bias
        self.arg1130_1 = getattr(model.pairformer_stack.blocks, "37").attention_pair_bias.attention.input2qkvg.weight
        self.arg1131_1 = getattr(model.pairformer_stack.blocks, "37").attention_pair_bias.attention.output_proj.weight
        self.arg1132_1 = getattr(model.pairformer_stack.blocks, "38").transition_pair.layer_norm.weight
        self.arg1133_1 = getattr(model.pairformer_stack.blocks, "38").transition_pair.layer_norm.bias
        self.arg1134_1 = getattr(model.pairformer_stack.blocks, "38").transition_pair.linear_no_bias_ab.weight
        self.arg1135_1 = getattr(model.pairformer_stack.blocks, "38").transition_pair.linear_out.weight
        self.arg1136_1 = getattr(model.pairformer_stack.blocks, "38").triangle_multiplication.layernorm_z_in.weight
        self.arg1137_1 = getattr(model.pairformer_stack.blocks, "38").triangle_multiplication.layernorm_z_in.bias
        self.arg1138_1 = getattr(model.pairformer_stack.blocks, "38").triangle_multiplication.linear_z_out.weight
        self.arg1139_1 = getattr(model.pairformer_stack.blocks, "38").triangle_multiplication.merged_linear_p.weight
        self.arg1140_1 = getattr(model.pairformer_stack.blocks, "38").triangle_multiplication.merged_linear_g.weight
        self.arg1141_1 = getattr(model.pairformer_stack.blocks, "38").triangle_attention.out_scalers
        self.arg1142_1 = getattr(model.pairformer_stack.blocks, "38").triangle_attention.pair2b.weight
        self.arg1143_1 = getattr(model.pairformer_stack.blocks, "38").triangle_attention.pair2qkvg1.weight
        self.arg1144_1 = getattr(model.pairformer_stack.blocks, "38").triangle_attention.pair2qkvg2.weight
        self.arg1145_1 = getattr(model.pairformer_stack.blocks, "38").triangle_attention.linear_out.weight
        self.arg1146_1 = getattr(model.pairformer_stack.blocks, "38").transition_single.layer_norm.weight
        self.arg1147_1 = getattr(model.pairformer_stack.blocks, "38").transition_single.layer_norm.bias
        self.arg1148_1 = getattr(model.pairformer_stack.blocks, "38").transition_single.linear_no_bias_ab.weight
        self.arg1149_1 = getattr(model.pairformer_stack.blocks, "38").transition_single.linear_out.weight
        self.arg1150_1 = getattr(model.pairformer_stack.blocks, "38").attention_pair_bias.single_layer_norm.weight
        self.arg1151_1 = getattr(model.pairformer_stack.blocks, "38").attention_pair_bias.single_layer_norm.bias
        self.arg1152_1 = getattr(model.pairformer_stack.blocks, "38").attention_pair_bias.pair_layer_norm.weight
        self.arg1153_1 = getattr(model.pairformer_stack.blocks, "38").attention_pair_bias.pair_layer_norm.bias
        self.arg1154_1 = getattr(model.pairformer_stack.blocks, "38").attention_pair_bias.pair_linear.weight
        self.arg1155_1 = getattr(model.pairformer_stack.blocks, "38").attention_pair_bias.attention.query_bias
        self.arg1156_1 = getattr(model.pairformer_stack.blocks, "38").attention_pair_bias.attention.input2qkvg.weight
        self.arg1157_1 = getattr(model.pairformer_stack.blocks, "38").attention_pair_bias.attention.output_proj.weight
        self.arg1158_1 = getattr(model.pairformer_stack.blocks, "39").transition_pair.layer_norm.weight
        self.arg1159_1 = getattr(model.pairformer_stack.blocks, "39").transition_pair.layer_norm.bias
        self.arg1160_1 = getattr(model.pairformer_stack.blocks, "39").transition_pair.linear_no_bias_ab.weight
        self.arg1161_1 = getattr(model.pairformer_stack.blocks, "39").transition_pair.linear_out.weight
        self.arg1162_1 = getattr(model.pairformer_stack.blocks, "39").triangle_multiplication.layernorm_z_in.weight
        self.arg1163_1 = getattr(model.pairformer_stack.blocks, "39").triangle_multiplication.layernorm_z_in.bias
        self.arg1164_1 = getattr(model.pairformer_stack.blocks, "39").triangle_multiplication.linear_z_out.weight
        self.arg1165_1 = getattr(model.pairformer_stack.blocks, "39").triangle_multiplication.merged_linear_p.weight
        self.arg1166_1 = getattr(model.pairformer_stack.blocks, "39").triangle_multiplication.merged_linear_g.weight
        self.arg1167_1 = getattr(model.pairformer_stack.blocks, "39").triangle_attention.out_scalers
        self.arg1168_1 = getattr(model.pairformer_stack.blocks, "39").triangle_attention.pair2b.weight
        self.arg1169_1 = getattr(model.pairformer_stack.blocks, "39").triangle_attention.pair2qkvg1.weight
        self.arg1170_1 = getattr(model.pairformer_stack.blocks, "39").triangle_attention.pair2qkvg2.weight
        self.arg1171_1 = getattr(model.pairformer_stack.blocks, "39").triangle_attention.linear_out.weight
        self.arg1172_1 = getattr(model.pairformer_stack.blocks, "39").transition_single.layer_norm.weight
        self.arg1173_1 = getattr(model.pairformer_stack.blocks, "39").transition_single.layer_norm.bias
        self.arg1174_1 = getattr(model.pairformer_stack.blocks, "39").transition_single.linear_no_bias_ab.weight
        self.arg1175_1 = getattr(model.pairformer_stack.blocks, "39").transition_single.linear_out.weight
        self.arg1176_1 = getattr(model.pairformer_stack.blocks, "39").attention_pair_bias.single_layer_norm.weight
        self.arg1177_1 = getattr(model.pairformer_stack.blocks, "39").attention_pair_bias.single_layer_norm.bias
        self.arg1178_1 = getattr(model.pairformer_stack.blocks, "39").attention_pair_bias.pair_layer_norm.weight
        self.arg1179_1 = getattr(model.pairformer_stack.blocks, "39").attention_pair_bias.pair_layer_norm.bias
        self.arg1180_1 = getattr(model.pairformer_stack.blocks, "39").attention_pair_bias.pair_linear.weight
        self.arg1181_1 = getattr(model.pairformer_stack.blocks, "39").attention_pair_bias.attention.query_bias
        self.arg1182_1 = getattr(model.pairformer_stack.blocks, "39").attention_pair_bias.attention.input2qkvg.weight
        self.arg1183_1 = getattr(model.pairformer_stack.blocks, "39").attention_pair_bias.attention.output_proj.weight
        self.arg1184_1 = getattr(model.pairformer_stack.blocks, "40").transition_pair.layer_norm.weight
        self.arg1185_1 = getattr(model.pairformer_stack.blocks, "40").transition_pair.layer_norm.bias
        self.arg1186_1 = getattr(model.pairformer_stack.blocks, "40").transition_pair.linear_no_bias_ab.weight
        self.arg1187_1 = getattr(model.pairformer_stack.blocks, "40").transition_pair.linear_out.weight
        self.arg1188_1 = getattr(model.pairformer_stack.blocks, "40").triangle_multiplication.layernorm_z_in.weight
        self.arg1189_1 = getattr(model.pairformer_stack.blocks, "40").triangle_multiplication.layernorm_z_in.bias
        self.arg1190_1 = getattr(model.pairformer_stack.blocks, "40").triangle_multiplication.linear_z_out.weight
        self.arg1191_1 = getattr(model.pairformer_stack.blocks, "40").triangle_multiplication.merged_linear_p.weight
        self.arg1192_1 = getattr(model.pairformer_stack.blocks, "40").triangle_multiplication.merged_linear_g.weight
        self.arg1193_1 = getattr(model.pairformer_stack.blocks, "40").triangle_attention.out_scalers
        self.arg1194_1 = getattr(model.pairformer_stack.blocks, "40").triangle_attention.pair2b.weight
        self.arg1195_1 = getattr(model.pairformer_stack.blocks, "40").triangle_attention.pair2qkvg1.weight
        self.arg1196_1 = getattr(model.pairformer_stack.blocks, "40").triangle_attention.pair2qkvg2.weight
        self.arg1197_1 = getattr(model.pairformer_stack.blocks, "40").triangle_attention.linear_out.weight
        self.arg1198_1 = getattr(model.pairformer_stack.blocks, "40").transition_single.layer_norm.weight
        self.arg1199_1 = getattr(model.pairformer_stack.blocks, "40").transition_single.layer_norm.bias
        self.arg1200_1 = getattr(model.pairformer_stack.blocks, "40").transition_single.linear_no_bias_ab.weight
        self.arg1201_1 = getattr(model.pairformer_stack.blocks, "40").transition_single.linear_out.weight
        self.arg1202_1 = getattr(model.pairformer_stack.blocks, "40").attention_pair_bias.single_layer_norm.weight
        self.arg1203_1 = getattr(model.pairformer_stack.blocks, "40").attention_pair_bias.single_layer_norm.bias
        self.arg1204_1 = getattr(model.pairformer_stack.blocks, "40").attention_pair_bias.pair_layer_norm.weight
        self.arg1205_1 = getattr(model.pairformer_stack.blocks, "40").attention_pair_bias.pair_layer_norm.bias
        self.arg1206_1 = getattr(model.pairformer_stack.blocks, "40").attention_pair_bias.pair_linear.weight
        self.arg1207_1 = getattr(model.pairformer_stack.blocks, "40").attention_pair_bias.attention.query_bias
        self.arg1208_1 = getattr(model.pairformer_stack.blocks, "40").attention_pair_bias.attention.input2qkvg.weight
        self.arg1209_1 = getattr(model.pairformer_stack.blocks, "40").attention_pair_bias.attention.output_proj.weight
        self.arg1210_1 = getattr(model.pairformer_stack.blocks, "41").transition_pair.layer_norm.weight
        self.arg1211_1 = getattr(model.pairformer_stack.blocks, "41").transition_pair.layer_norm.bias
        self.arg1212_1 = getattr(model.pairformer_stack.blocks, "41").transition_pair.linear_no_bias_ab.weight
        self.arg1213_1 = getattr(model.pairformer_stack.blocks, "41").transition_pair.linear_out.weight
        self.arg1214_1 = getattr(model.pairformer_stack.blocks, "41").triangle_multiplication.layernorm_z_in.weight
        self.arg1215_1 = getattr(model.pairformer_stack.blocks, "41").triangle_multiplication.layernorm_z_in.bias
        self.arg1216_1 = getattr(model.pairformer_stack.blocks, "41").triangle_multiplication.linear_z_out.weight
        self.arg1217_1 = getattr(model.pairformer_stack.blocks, "41").triangle_multiplication.merged_linear_p.weight
        self.arg1218_1 = getattr(model.pairformer_stack.blocks, "41").triangle_multiplication.merged_linear_g.weight
        self.arg1219_1 = getattr(model.pairformer_stack.blocks, "41").triangle_attention.out_scalers
        self.arg1220_1 = getattr(model.pairformer_stack.blocks, "41").triangle_attention.pair2b.weight
        self.arg1221_1 = getattr(model.pairformer_stack.blocks, "41").triangle_attention.pair2qkvg1.weight
        self.arg1222_1 = getattr(model.pairformer_stack.blocks, "41").triangle_attention.pair2qkvg2.weight
        self.arg1223_1 = getattr(model.pairformer_stack.blocks, "41").triangle_attention.linear_out.weight
        self.arg1224_1 = getattr(model.pairformer_stack.blocks, "41").transition_single.layer_norm.weight
        self.arg1225_1 = getattr(model.pairformer_stack.blocks, "41").transition_single.layer_norm.bias
        self.arg1226_1 = getattr(model.pairformer_stack.blocks, "41").transition_single.linear_no_bias_ab.weight
        self.arg1227_1 = getattr(model.pairformer_stack.blocks, "41").transition_single.linear_out.weight
        self.arg1228_1 = getattr(model.pairformer_stack.blocks, "41").attention_pair_bias.single_layer_norm.weight
        self.arg1229_1 = getattr(model.pairformer_stack.blocks, "41").attention_pair_bias.single_layer_norm.bias
        self.arg1230_1 = getattr(model.pairformer_stack.blocks, "41").attention_pair_bias.pair_layer_norm.weight
        self.arg1231_1 = getattr(model.pairformer_stack.blocks, "41").attention_pair_bias.pair_layer_norm.bias
        self.arg1232_1 = getattr(model.pairformer_stack.blocks, "41").attention_pair_bias.pair_linear.weight
        self.arg1233_1 = getattr(model.pairformer_stack.blocks, "41").attention_pair_bias.attention.query_bias
        self.arg1234_1 = getattr(model.pairformer_stack.blocks, "41").attention_pair_bias.attention.input2qkvg.weight
        self.arg1235_1 = getattr(model.pairformer_stack.blocks, "41").attention_pair_bias.attention.output_proj.weight
        self.arg1236_1 = getattr(model.pairformer_stack.blocks, "42").transition_pair.layer_norm.weight
        self.arg1237_1 = getattr(model.pairformer_stack.blocks, "42").transition_pair.layer_norm.bias
        self.arg1238_1 = getattr(model.pairformer_stack.blocks, "42").transition_pair.linear_no_bias_ab.weight
        self.arg1239_1 = getattr(model.pairformer_stack.blocks, "42").transition_pair.linear_out.weight
        self.arg1240_1 = getattr(model.pairformer_stack.blocks, "42").triangle_multiplication.layernorm_z_in.weight
        self.arg1241_1 = getattr(model.pairformer_stack.blocks, "42").triangle_multiplication.layernorm_z_in.bias
        self.arg1242_1 = getattr(model.pairformer_stack.blocks, "42").triangle_multiplication.linear_z_out.weight
        self.arg1243_1 = getattr(model.pairformer_stack.blocks, "42").triangle_multiplication.merged_linear_p.weight
        self.arg1244_1 = getattr(model.pairformer_stack.blocks, "42").triangle_multiplication.merged_linear_g.weight
        self.arg1245_1 = getattr(model.pairformer_stack.blocks, "42").triangle_attention.out_scalers
        self.arg1246_1 = getattr(model.pairformer_stack.blocks, "42").triangle_attention.pair2b.weight
        self.arg1247_1 = getattr(model.pairformer_stack.blocks, "42").triangle_attention.pair2qkvg1.weight
        self.arg1248_1 = getattr(model.pairformer_stack.blocks, "42").triangle_attention.pair2qkvg2.weight
        self.arg1249_1 = getattr(model.pairformer_stack.blocks, "42").triangle_attention.linear_out.weight
        self.arg1250_1 = getattr(model.pairformer_stack.blocks, "42").transition_single.layer_norm.weight
        self.arg1251_1 = getattr(model.pairformer_stack.blocks, "42").transition_single.layer_norm.bias
        self.arg1252_1 = getattr(model.pairformer_stack.blocks, "42").transition_single.linear_no_bias_ab.weight
        self.arg1253_1 = getattr(model.pairformer_stack.blocks, "42").transition_single.linear_out.weight
        self.arg1254_1 = getattr(model.pairformer_stack.blocks, "42").attention_pair_bias.single_layer_norm.weight
        self.arg1255_1 = getattr(model.pairformer_stack.blocks, "42").attention_pair_bias.single_layer_norm.bias
        self.arg1256_1 = getattr(model.pairformer_stack.blocks, "42").attention_pair_bias.pair_layer_norm.weight
        self.arg1257_1 = getattr(model.pairformer_stack.blocks, "42").attention_pair_bias.pair_layer_norm.bias
        self.arg1258_1 = getattr(model.pairformer_stack.blocks, "42").attention_pair_bias.pair_linear.weight
        self.arg1259_1 = getattr(model.pairformer_stack.blocks, "42").attention_pair_bias.attention.query_bias
        self.arg1260_1 = getattr(model.pairformer_stack.blocks, "42").attention_pair_bias.attention.input2qkvg.weight
        self.arg1261_1 = getattr(model.pairformer_stack.blocks, "42").attention_pair_bias.attention.output_proj.weight
        self.arg1262_1 = getattr(model.pairformer_stack.blocks, "43").transition_pair.layer_norm.weight
        self.arg1263_1 = getattr(model.pairformer_stack.blocks, "43").transition_pair.layer_norm.bias
        self.arg1264_1 = getattr(model.pairformer_stack.blocks, "43").transition_pair.linear_no_bias_ab.weight
        self.arg1265_1 = getattr(model.pairformer_stack.blocks, "43").transition_pair.linear_out.weight
        self.arg1266_1 = getattr(model.pairformer_stack.blocks, "43").triangle_multiplication.layernorm_z_in.weight
        self.arg1267_1 = getattr(model.pairformer_stack.blocks, "43").triangle_multiplication.layernorm_z_in.bias
        self.arg1268_1 = getattr(model.pairformer_stack.blocks, "43").triangle_multiplication.linear_z_out.weight
        self.arg1269_1 = getattr(model.pairformer_stack.blocks, "43").triangle_multiplication.merged_linear_p.weight
        self.arg1270_1 = getattr(model.pairformer_stack.blocks, "43").triangle_multiplication.merged_linear_g.weight
        self.arg1271_1 = getattr(model.pairformer_stack.blocks, "43").triangle_attention.out_scalers
        self.arg1272_1 = getattr(model.pairformer_stack.blocks, "43").triangle_attention.pair2b.weight
        self.arg1273_1 = getattr(model.pairformer_stack.blocks, "43").triangle_attention.pair2qkvg1.weight
        self.arg1274_1 = getattr(model.pairformer_stack.blocks, "43").triangle_attention.pair2qkvg2.weight
        self.arg1275_1 = getattr(model.pairformer_stack.blocks, "43").triangle_attention.linear_out.weight
        self.arg1276_1 = getattr(model.pairformer_stack.blocks, "43").transition_single.layer_norm.weight
        self.arg1277_1 = getattr(model.pairformer_stack.blocks, "43").transition_single.layer_norm.bias
        self.arg1278_1 = getattr(model.pairformer_stack.blocks, "43").transition_single.linear_no_bias_ab.weight
        self.arg1279_1 = getattr(model.pairformer_stack.blocks, "43").transition_single.linear_out.weight
        self.arg1280_1 = getattr(model.pairformer_stack.blocks, "43").attention_pair_bias.single_layer_norm.weight
        self.arg1281_1 = getattr(model.pairformer_stack.blocks, "43").attention_pair_bias.single_layer_norm.bias
        self.arg1282_1 = getattr(model.pairformer_stack.blocks, "43").attention_pair_bias.pair_layer_norm.weight
        self.arg1283_1 = getattr(model.pairformer_stack.blocks, "43").attention_pair_bias.pair_layer_norm.bias
        self.arg1284_1 = getattr(model.pairformer_stack.blocks, "43").attention_pair_bias.pair_linear.weight
        self.arg1285_1 = getattr(model.pairformer_stack.blocks, "43").attention_pair_bias.attention.query_bias
        self.arg1286_1 = getattr(model.pairformer_stack.blocks, "43").attention_pair_bias.attention.input2qkvg.weight
        self.arg1287_1 = getattr(model.pairformer_stack.blocks, "43").attention_pair_bias.attention.output_proj.weight
        self.arg1288_1 = getattr(model.pairformer_stack.blocks, "44").transition_pair.layer_norm.weight
        self.arg1289_1 = getattr(model.pairformer_stack.blocks, "44").transition_pair.layer_norm.bias
        self.arg1290_1 = getattr(model.pairformer_stack.blocks, "44").transition_pair.linear_no_bias_ab.weight
        self.arg1291_1 = getattr(model.pairformer_stack.blocks, "44").transition_pair.linear_out.weight
        self.arg1292_1 = getattr(model.pairformer_stack.blocks, "44").triangle_multiplication.layernorm_z_in.weight
        self.arg1293_1 = getattr(model.pairformer_stack.blocks, "44").triangle_multiplication.layernorm_z_in.bias
        self.arg1294_1 = getattr(model.pairformer_stack.blocks, "44").triangle_multiplication.linear_z_out.weight
        self.arg1295_1 = getattr(model.pairformer_stack.blocks, "44").triangle_multiplication.merged_linear_p.weight
        self.arg1296_1 = getattr(model.pairformer_stack.blocks, "44").triangle_multiplication.merged_linear_g.weight
        self.arg1297_1 = getattr(model.pairformer_stack.blocks, "44").triangle_attention.out_scalers
        self.arg1298_1 = getattr(model.pairformer_stack.blocks, "44").triangle_attention.pair2b.weight
        self.arg1299_1 = getattr(model.pairformer_stack.blocks, "44").triangle_attention.pair2qkvg1.weight
        self.arg1300_1 = getattr(model.pairformer_stack.blocks, "44").triangle_attention.pair2qkvg2.weight
        self.arg1301_1 = getattr(model.pairformer_stack.blocks, "44").triangle_attention.linear_out.weight
        self.arg1302_1 = getattr(model.pairformer_stack.blocks, "44").transition_single.layer_norm.weight
        self.arg1303_1 = getattr(model.pairformer_stack.blocks, "44").transition_single.layer_norm.bias
        self.arg1304_1 = getattr(model.pairformer_stack.blocks, "44").transition_single.linear_no_bias_ab.weight
        self.arg1305_1 = getattr(model.pairformer_stack.blocks, "44").transition_single.linear_out.weight
        self.arg1306_1 = getattr(model.pairformer_stack.blocks, "44").attention_pair_bias.single_layer_norm.weight
        self.arg1307_1 = getattr(model.pairformer_stack.blocks, "44").attention_pair_bias.single_layer_norm.bias
        self.arg1308_1 = getattr(model.pairformer_stack.blocks, "44").attention_pair_bias.pair_layer_norm.weight
        self.arg1309_1 = getattr(model.pairformer_stack.blocks, "44").attention_pair_bias.pair_layer_norm.bias
        self.arg1310_1 = getattr(model.pairformer_stack.blocks, "44").attention_pair_bias.pair_linear.weight
        self.arg1311_1 = getattr(model.pairformer_stack.blocks, "44").attention_pair_bias.attention.query_bias
        self.arg1312_1 = getattr(model.pairformer_stack.blocks, "44").attention_pair_bias.attention.input2qkvg.weight
        self.arg1313_1 = getattr(model.pairformer_stack.blocks, "44").attention_pair_bias.attention.output_proj.weight
        self.arg1314_1 = getattr(model.pairformer_stack.blocks, "45").transition_pair.layer_norm.weight
        self.arg1315_1 = getattr(model.pairformer_stack.blocks, "45").transition_pair.layer_norm.bias
        self.arg1316_1 = getattr(model.pairformer_stack.blocks, "45").transition_pair.linear_no_bias_ab.weight
        self.arg1317_1 = getattr(model.pairformer_stack.blocks, "45").transition_pair.linear_out.weight
        self.arg1318_1 = getattr(model.pairformer_stack.blocks, "45").triangle_multiplication.layernorm_z_in.weight
        self.arg1319_1 = getattr(model.pairformer_stack.blocks, "45").triangle_multiplication.layernorm_z_in.bias
        self.arg1320_1 = getattr(model.pairformer_stack.blocks, "45").triangle_multiplication.linear_z_out.weight
        self.arg1321_1 = getattr(model.pairformer_stack.blocks, "45").triangle_multiplication.merged_linear_p.weight
        self.arg1322_1 = getattr(model.pairformer_stack.blocks, "45").triangle_multiplication.merged_linear_g.weight
        self.arg1323_1 = getattr(model.pairformer_stack.blocks, "45").triangle_attention.out_scalers
        self.arg1324_1 = getattr(model.pairformer_stack.blocks, "45").triangle_attention.pair2b.weight
        self.arg1325_1 = getattr(model.pairformer_stack.blocks, "45").triangle_attention.pair2qkvg1.weight
        self.arg1326_1 = getattr(model.pairformer_stack.blocks, "45").triangle_attention.pair2qkvg2.weight
        self.arg1327_1 = getattr(model.pairformer_stack.blocks, "45").triangle_attention.linear_out.weight
        self.arg1328_1 = getattr(model.pairformer_stack.blocks, "45").transition_single.layer_norm.weight
        self.arg1329_1 = getattr(model.pairformer_stack.blocks, "45").transition_single.layer_norm.bias
        self.arg1330_1 = getattr(model.pairformer_stack.blocks, "45").transition_single.linear_no_bias_ab.weight
        self.arg1331_1 = getattr(model.pairformer_stack.blocks, "45").transition_single.linear_out.weight
        self.arg1332_1 = getattr(model.pairformer_stack.blocks, "45").attention_pair_bias.single_layer_norm.weight
        self.arg1333_1 = getattr(model.pairformer_stack.blocks, "45").attention_pair_bias.single_layer_norm.bias
        self.arg1334_1 = getattr(model.pairformer_stack.blocks, "45").attention_pair_bias.pair_layer_norm.weight
        self.arg1335_1 = getattr(model.pairformer_stack.blocks, "45").attention_pair_bias.pair_layer_norm.bias
        self.arg1336_1 = getattr(model.pairformer_stack.blocks, "45").attention_pair_bias.pair_linear.weight
        self.arg1337_1 = getattr(model.pairformer_stack.blocks, "45").attention_pair_bias.attention.query_bias
        self.arg1338_1 = getattr(model.pairformer_stack.blocks, "45").attention_pair_bias.attention.input2qkvg.weight
        self.arg1339_1 = getattr(model.pairformer_stack.blocks, "45").attention_pair_bias.attention.output_proj.weight
        self.arg1340_1 = getattr(model.pairformer_stack.blocks, "46").transition_pair.layer_norm.weight
        self.arg1341_1 = getattr(model.pairformer_stack.blocks, "46").transition_pair.layer_norm.bias
        self.arg1342_1 = getattr(model.pairformer_stack.blocks, "46").transition_pair.linear_no_bias_ab.weight
        self.arg1343_1 = getattr(model.pairformer_stack.blocks, "46").transition_pair.linear_out.weight
        self.arg1344_1 = getattr(model.pairformer_stack.blocks, "46").triangle_multiplication.layernorm_z_in.weight
        self.arg1345_1 = getattr(model.pairformer_stack.blocks, "46").triangle_multiplication.layernorm_z_in.bias
        self.arg1346_1 = getattr(model.pairformer_stack.blocks, "46").triangle_multiplication.linear_z_out.weight
        self.arg1347_1 = getattr(model.pairformer_stack.blocks, "46").triangle_multiplication.merged_linear_p.weight
        self.arg1348_1 = getattr(model.pairformer_stack.blocks, "46").triangle_multiplication.merged_linear_g.weight
        self.arg1349_1 = getattr(model.pairformer_stack.blocks, "46").triangle_attention.out_scalers
        self.arg1350_1 = getattr(model.pairformer_stack.blocks, "46").triangle_attention.pair2b.weight
        self.arg1351_1 = getattr(model.pairformer_stack.blocks, "46").triangle_attention.pair2qkvg1.weight
        self.arg1352_1 = getattr(model.pairformer_stack.blocks, "46").triangle_attention.pair2qkvg2.weight
        self.arg1353_1 = getattr(model.pairformer_stack.blocks, "46").triangle_attention.linear_out.weight
        self.arg1354_1 = getattr(model.pairformer_stack.blocks, "46").transition_single.layer_norm.weight
        self.arg1355_1 = getattr(model.pairformer_stack.blocks, "46").transition_single.layer_norm.bias
        self.arg1356_1 = getattr(model.pairformer_stack.blocks, "46").transition_single.linear_no_bias_ab.weight
        self.arg1357_1 = getattr(model.pairformer_stack.blocks, "46").transition_single.linear_out.weight
        self.arg1358_1 = getattr(model.pairformer_stack.blocks, "46").attention_pair_bias.single_layer_norm.weight
        self.arg1359_1 = getattr(model.pairformer_stack.blocks, "46").attention_pair_bias.single_layer_norm.bias
        self.arg1360_1 = getattr(model.pairformer_stack.blocks, "46").attention_pair_bias.pair_layer_norm.weight
        self.arg1361_1 = getattr(model.pairformer_stack.blocks, "46").attention_pair_bias.pair_layer_norm.bias
        self.arg1362_1 = getattr(model.pairformer_stack.blocks, "46").attention_pair_bias.pair_linear.weight
        self.arg1363_1 = getattr(model.pairformer_stack.blocks, "46").attention_pair_bias.attention.query_bias
        self.arg1364_1 = getattr(model.pairformer_stack.blocks, "46").attention_pair_bias.attention.input2qkvg.weight
        self.arg1365_1 = getattr(model.pairformer_stack.blocks, "46").attention_pair_bias.attention.output_proj.weight
        self.arg1366_1 = getattr(model.pairformer_stack.blocks, "47").transition_pair.layer_norm.weight
        self.arg1367_1 = getattr(model.pairformer_stack.blocks, "47").transition_pair.layer_norm.bias
        self.arg1368_1 = getattr(model.pairformer_stack.blocks, "47").transition_pair.linear_no_bias_ab.weight
        self.arg1369_1 = getattr(model.pairformer_stack.blocks, "47").transition_pair.linear_out.weight
        self.arg1370_1 = getattr(model.pairformer_stack.blocks, "47").triangle_multiplication.layernorm_z_in.weight
        self.arg1371_1 = getattr(model.pairformer_stack.blocks, "47").triangle_multiplication.layernorm_z_in.bias
        self.arg1372_1 = getattr(model.pairformer_stack.blocks, "47").triangle_multiplication.linear_z_out.weight
        self.arg1373_1 = getattr(model.pairformer_stack.blocks, "47").triangle_multiplication.merged_linear_p.weight
        self.arg1374_1 = getattr(model.pairformer_stack.blocks, "47").triangle_multiplication.merged_linear_g.weight
        self.arg1375_1 = getattr(model.pairformer_stack.blocks, "47").triangle_attention.out_scalers
        self.arg1376_1 = getattr(model.pairformer_stack.blocks, "47").triangle_attention.pair2b.weight
        self.arg1377_1 = getattr(model.pairformer_stack.blocks, "47").triangle_attention.pair2qkvg1.weight
        self.arg1378_1 = getattr(model.pairformer_stack.blocks, "47").triangle_attention.pair2qkvg2.weight
        self.arg1379_1 = getattr(model.pairformer_stack.blocks, "47").triangle_attention.linear_out.weight
        self.arg1380_1 = getattr(model.pairformer_stack.blocks, "47").transition_single.layer_norm.weight
        self.arg1381_1 = getattr(model.pairformer_stack.blocks, "47").transition_single.layer_norm.bias
        self.arg1382_1 = getattr(model.pairformer_stack.blocks, "47").transition_single.linear_no_bias_ab.weight
        self.arg1383_1 = getattr(model.pairformer_stack.blocks, "47").transition_single.linear_out.weight
        self.arg1384_1 = getattr(model.pairformer_stack.blocks, "47").attention_pair_bias.single_layer_norm.weight
        self.arg1385_1 = getattr(model.pairformer_stack.blocks, "47").attention_pair_bias.single_layer_norm.bias
        self.arg1386_1 = getattr(model.pairformer_stack.blocks, "47").attention_pair_bias.pair_layer_norm.weight
        self.arg1387_1 = getattr(model.pairformer_stack.blocks, "47").attention_pair_bias.pair_layer_norm.bias
        self.arg1388_1 = getattr(model.pairformer_stack.blocks, "47").attention_pair_bias.pair_linear.weight
        self.arg1389_1 = getattr(model.pairformer_stack.blocks, "47").attention_pair_bias.attention.query_bias
        self.arg1390_1 = getattr(model.pairformer_stack.blocks, "47").attention_pair_bias.attention.input2qkvg.weight
        self.arg1391_1 = getattr(model.pairformer_stack.blocks, "47").attention_pair_bias.attention.output_proj.weight
        self.arg1392_1 = getattr(model.token_single_recycle_proj, "0").weight
        self.arg1393_1 = getattr(model.token_single_recycle_proj, "0").bias
        self.arg1394_1 = getattr(model.token_single_recycle_proj, "1").weight
        self.arg1395_1 = getattr(model.token_pair_recycle_proj, "0").weight
        self.arg1396_1 = getattr(model.token_pair_recycle_proj, "0").bias
        self.arg1397_1 = getattr(model.token_pair_recycle_proj, "1").weight

    def forward(self, arg1398_1, arg1399_1, arg1400_1, arg1401_1, arg1402_1, arg1403_1, arg1404_1, arg1405_1, arg1406_1, arg1407_1):
        arg0_1 = self.arg0_1
        arg1_1 = self.arg1_1
        arg2_1 = self.arg2_1
        arg3_1 = self.arg3_1
        arg4_1 = self.arg4_1
        arg5_1 = self.arg5_1
        arg6_1 = self.arg6_1
        arg7_1 = self.arg7_1
        arg8_1 = self.arg8_1
        arg9_1 = self.arg9_1
        arg10_1 = self.arg10_1
        arg11_1 = self.arg11_1
        arg12_1 = self.arg12_1
        arg13_1 = self.arg13_1
        arg14_1 = self.arg14_1
        arg15_1 = self.arg15_1
        arg16_1 = self.arg16_1
        arg17_1 = self.arg17_1
        arg18_1 = self.arg18_1
        arg19_1 = self.arg19_1
        arg20_1 = self.arg20_1
        arg21_1 = self.arg21_1
        arg22_1 = self.arg22_1
        arg23_1 = self.arg23_1
        arg24_1 = self.arg24_1
        arg25_1 = self.arg25_1
        arg26_1 = self.arg26_1
        arg27_1 = self.arg27_1
        arg28_1 = self.arg28_1
        arg29_1 = self.arg29_1
        arg30_1 = self.arg30_1
        arg31_1 = self.arg31_1
        arg32_1 = self.arg32_1
        arg33_1 = self.arg33_1
        arg34_1 = self.arg34_1
        arg35_1 = self.arg35_1
        arg36_1 = self.arg36_1
        arg37_1 = self.arg37_1
        arg38_1 = self.arg38_1
        arg39_1 = self.arg39_1
        arg40_1 = self.arg40_1
        arg41_1 = self.arg41_1
        arg42_1 = self.arg42_1
        arg43_1 = self.arg43_1
        arg44_1 = self.arg44_1
        arg45_1 = self.arg45_1
        arg46_1 = self.arg46_1
        arg47_1 = self.arg47_1
        arg48_1 = self.arg48_1
        arg49_1 = self.arg49_1
        arg50_1 = self.arg50_1
        arg51_1 = self.arg51_1
        arg52_1 = self.arg52_1
        arg53_1 = self.arg53_1
        arg54_1 = self.arg54_1
        arg55_1 = self.arg55_1
        arg56_1 = self.arg56_1
        arg57_1 = self.arg57_1
        arg58_1 = self.arg58_1
        arg59_1 = self.arg59_1
        arg60_1 = self.arg60_1
        arg61_1 = self.arg61_1
        arg62_1 = self.arg62_1
        arg63_1 = self.arg63_1
        arg64_1 = self.arg64_1
        arg65_1 = self.arg65_1
        arg66_1 = self.arg66_1
        arg67_1 = self.arg67_1
        arg68_1 = self.arg68_1
        arg69_1 = self.arg69_1
        arg70_1 = self.arg70_1
        arg71_1 = self.arg71_1
        arg72_1 = self.arg72_1
        arg73_1 = self.arg73_1
        arg74_1 = self.arg74_1
        arg75_1 = self.arg75_1
        arg76_1 = self.arg76_1
        arg77_1 = self.arg77_1
        arg78_1 = self.arg78_1
        arg79_1 = self.arg79_1
        arg80_1 = self.arg80_1
        arg81_1 = self.arg81_1
        arg82_1 = self.arg82_1
        arg83_1 = self.arg83_1
        arg84_1 = self.arg84_1
        arg85_1 = self.arg85_1
        arg86_1 = self.arg86_1
        arg87_1 = self.arg87_1
        arg88_1 = self.arg88_1
        arg89_1 = self.arg89_1
        arg90_1 = self.arg90_1
        arg91_1 = self.arg91_1
        arg92_1 = self.arg92_1
        arg93_1 = self.arg93_1
        arg94_1 = self.arg94_1
        arg95_1 = self.arg95_1
        arg96_1 = self.arg96_1
        arg97_1 = self.arg97_1
        arg98_1 = self.arg98_1
        arg99_1 = self.arg99_1
        arg100_1 = self.arg100_1
        arg101_1 = self.arg101_1
        arg102_1 = self.arg102_1
        arg103_1 = self.arg103_1
        arg104_1 = self.arg104_1
        arg105_1 = self.arg105_1
        arg106_1 = self.arg106_1
        arg107_1 = self.arg107_1
        arg108_1 = self.arg108_1
        arg109_1 = self.arg109_1
        arg110_1 = self.arg110_1
        arg111_1 = self.arg111_1
        arg112_1 = self.arg112_1
        arg113_1 = self.arg113_1
        arg114_1 = self.arg114_1
        arg115_1 = self.arg115_1
        arg116_1 = self.arg116_1
        arg117_1 = self.arg117_1
        arg118_1 = self.arg118_1
        arg119_1 = self.arg119_1
        arg120_1 = self.arg120_1
        arg121_1 = self.arg121_1
        arg122_1 = self.arg122_1
        arg123_1 = self.arg123_1
        arg124_1 = self.arg124_1
        arg125_1 = self.arg125_1
        arg126_1 = self.arg126_1
        arg127_1 = self.arg127_1
        arg128_1 = self.arg128_1
        arg129_1 = self.arg129_1
        arg130_1 = self.arg130_1
        arg131_1 = self.arg131_1
        arg132_1 = self.arg132_1
        arg133_1 = self.arg133_1
        arg134_1 = self.arg134_1
        arg135_1 = self.arg135_1
        arg136_1 = self.arg136_1
        arg137_1 = self.arg137_1
        arg138_1 = self.arg138_1
        arg139_1 = self.arg139_1
        arg140_1 = self.arg140_1
        arg141_1 = self.arg141_1
        arg142_1 = self.arg142_1
        arg143_1 = self.arg143_1
        arg144_1 = self.arg144_1
        arg145_1 = self.arg145_1
        arg146_1 = self.arg146_1
        arg147_1 = self.arg147_1
        arg148_1 = self.arg148_1
        arg149_1 = self.arg149_1
        arg150_1 = self.arg150_1
        arg151_1 = self.arg151_1
        arg152_1 = self.arg152_1
        arg153_1 = self.arg153_1
        arg154_1 = self.arg154_1
        arg155_1 = self.arg155_1
        arg156_1 = self.arg156_1
        arg157_1 = self.arg157_1
        arg158_1 = self.arg158_1
        arg159_1 = self.arg159_1
        arg160_1 = self.arg160_1
        arg161_1 = self.arg161_1
        arg162_1 = self.arg162_1
        arg163_1 = self.arg163_1
        arg164_1 = self.arg164_1
        arg165_1 = self.arg165_1
        arg166_1 = self.arg166_1
        arg167_1 = self.arg167_1
        arg168_1 = self.arg168_1
        arg169_1 = self.arg169_1
        arg170_1 = self.arg170_1
        arg171_1 = self.arg171_1
        arg172_1 = self.arg172_1
        arg173_1 = self.arg173_1
        arg174_1 = self.arg174_1
        arg175_1 = self.arg175_1
        arg176_1 = self.arg176_1
        arg177_1 = self.arg177_1
        arg178_1 = self.arg178_1
        arg179_1 = self.arg179_1
        arg180_1 = self.arg180_1
        arg181_1 = self.arg181_1
        arg182_1 = self.arg182_1
        arg183_1 = self.arg183_1
        arg184_1 = self.arg184_1
        arg185_1 = self.arg185_1
        arg186_1 = self.arg186_1
        arg187_1 = self.arg187_1
        arg188_1 = self.arg188_1
        arg189_1 = self.arg189_1
        arg190_1 = self.arg190_1
        arg191_1 = self.arg191_1
        arg192_1 = self.arg192_1
        arg193_1 = self.arg193_1
        arg194_1 = self.arg194_1
        arg195_1 = self.arg195_1
        arg196_1 = self.arg196_1
        arg197_1 = self.arg197_1
        arg198_1 = self.arg198_1
        arg199_1 = self.arg199_1
        arg200_1 = self.arg200_1
        arg201_1 = self.arg201_1
        arg202_1 = self.arg202_1
        arg203_1 = self.arg203_1
        arg204_1 = self.arg204_1
        arg205_1 = self.arg205_1
        arg206_1 = self.arg206_1
        arg207_1 = self.arg207_1
        arg208_1 = self.arg208_1
        arg209_1 = self.arg209_1
        arg210_1 = self.arg210_1
        arg211_1 = self.arg211_1
        arg212_1 = self.arg212_1
        arg213_1 = self.arg213_1
        arg214_1 = self.arg214_1
        arg215_1 = self.arg215_1
        arg216_1 = self.arg216_1
        arg217_1 = self.arg217_1
        arg218_1 = self.arg218_1
        arg219_1 = self.arg219_1
        arg220_1 = self.arg220_1
        arg221_1 = self.arg221_1
        arg222_1 = self.arg222_1
        arg223_1 = self.arg223_1
        arg224_1 = self.arg224_1
        arg225_1 = self.arg225_1
        arg226_1 = self.arg226_1
        arg227_1 = self.arg227_1
        arg228_1 = self.arg228_1
        arg229_1 = self.arg229_1
        arg230_1 = self.arg230_1
        arg231_1 = self.arg231_1
        arg232_1 = self.arg232_1
        arg233_1 = self.arg233_1
        arg234_1 = self.arg234_1
        arg235_1 = self.arg235_1
        arg236_1 = self.arg236_1
        arg237_1 = self.arg237_1
        arg238_1 = self.arg238_1
        arg239_1 = self.arg239_1
        arg240_1 = self.arg240_1
        arg241_1 = self.arg241_1
        arg242_1 = self.arg242_1
        arg243_1 = self.arg243_1
        arg244_1 = self.arg244_1
        arg245_1 = self.arg245_1
        arg246_1 = self.arg246_1
        arg247_1 = self.arg247_1
        arg248_1 = self.arg248_1
        arg249_1 = self.arg249_1
        arg250_1 = self.arg250_1
        arg251_1 = self.arg251_1
        arg252_1 = self.arg252_1
        arg253_1 = self.arg253_1
        arg254_1 = self.arg254_1
        arg255_1 = self.arg255_1
        arg256_1 = self.arg256_1
        arg257_1 = self.arg257_1
        arg258_1 = self.arg258_1
        arg259_1 = self.arg259_1
        arg260_1 = self.arg260_1
        arg261_1 = self.arg261_1
        arg262_1 = self.arg262_1
        arg263_1 = self.arg263_1
        arg264_1 = self.arg264_1
        arg265_1 = self.arg265_1
        arg266_1 = self.arg266_1
        arg267_1 = self.arg267_1
        arg268_1 = self.arg268_1
        arg269_1 = self.arg269_1
        arg270_1 = self.arg270_1
        arg271_1 = self.arg271_1
        arg272_1 = self.arg272_1
        arg273_1 = self.arg273_1
        arg274_1 = self.arg274_1
        arg275_1 = self.arg275_1
        arg276_1 = self.arg276_1
        arg277_1 = self.arg277_1
        arg278_1 = self.arg278_1
        arg279_1 = self.arg279_1
        arg280_1 = self.arg280_1
        arg281_1 = self.arg281_1
        arg282_1 = self.arg282_1
        arg283_1 = self.arg283_1
        arg284_1 = self.arg284_1
        arg285_1 = self.arg285_1
        arg286_1 = self.arg286_1
        arg287_1 = self.arg287_1
        arg288_1 = self.arg288_1
        arg289_1 = self.arg289_1
        arg290_1 = self.arg290_1
        arg291_1 = self.arg291_1
        arg292_1 = self.arg292_1
        arg293_1 = self.arg293_1
        arg294_1 = self.arg294_1
        arg295_1 = self.arg295_1
        arg296_1 = self.arg296_1
        arg297_1 = self.arg297_1
        arg298_1 = self.arg298_1
        arg299_1 = self.arg299_1
        arg300_1 = self.arg300_1
        arg301_1 = self.arg301_1
        arg302_1 = self.arg302_1
        arg303_1 = self.arg303_1
        arg304_1 = self.arg304_1
        arg305_1 = self.arg305_1
        arg306_1 = self.arg306_1
        arg307_1 = self.arg307_1
        arg308_1 = self.arg308_1
        arg309_1 = self.arg309_1
        arg310_1 = self.arg310_1
        arg311_1 = self.arg311_1
        arg312_1 = self.arg312_1
        arg313_1 = self.arg313_1
        arg314_1 = self.arg314_1
        arg315_1 = self.arg315_1
        arg316_1 = self.arg316_1
        arg317_1 = self.arg317_1
        arg318_1 = self.arg318_1
        arg319_1 = self.arg319_1
        arg320_1 = self.arg320_1
        arg321_1 = self.arg321_1
        arg322_1 = self.arg322_1
        arg323_1 = self.arg323_1
        arg324_1 = self.arg324_1
        arg325_1 = self.arg325_1
        arg326_1 = self.arg326_1
        arg327_1 = self.arg327_1
        arg328_1 = self.arg328_1
        arg329_1 = self.arg329_1
        arg330_1 = self.arg330_1
        arg331_1 = self.arg331_1
        arg332_1 = self.arg332_1
        arg333_1 = self.arg333_1
        arg334_1 = self.arg334_1
        arg335_1 = self.arg335_1
        arg336_1 = self.arg336_1
        arg337_1 = self.arg337_1
        arg338_1 = self.arg338_1
        arg339_1 = self.arg339_1
        arg340_1 = self.arg340_1
        arg341_1 = self.arg341_1
        arg342_1 = self.arg342_1
        arg343_1 = self.arg343_1
        arg344_1 = self.arg344_1
        arg345_1 = self.arg345_1
        arg346_1 = self.arg346_1
        arg347_1 = self.arg347_1
        arg348_1 = self.arg348_1
        arg349_1 = self.arg349_1
        arg350_1 = self.arg350_1
        arg351_1 = self.arg351_1
        arg352_1 = self.arg352_1
        arg353_1 = self.arg353_1
        arg354_1 = self.arg354_1
        arg355_1 = self.arg355_1
        arg356_1 = self.arg356_1
        arg357_1 = self.arg357_1
        arg358_1 = self.arg358_1
        arg359_1 = self.arg359_1
        arg360_1 = self.arg360_1
        arg361_1 = self.arg361_1
        arg362_1 = self.arg362_1
        arg363_1 = self.arg363_1
        arg364_1 = self.arg364_1
        arg365_1 = self.arg365_1
        arg366_1 = self.arg366_1
        arg367_1 = self.arg367_1
        arg368_1 = self.arg368_1
        arg369_1 = self.arg369_1
        arg370_1 = self.arg370_1
        arg371_1 = self.arg371_1
        arg372_1 = self.arg372_1
        arg373_1 = self.arg373_1
        arg374_1 = self.arg374_1
        arg375_1 = self.arg375_1
        arg376_1 = self.arg376_1
        arg377_1 = self.arg377_1
        arg378_1 = self.arg378_1
        arg379_1 = self.arg379_1
        arg380_1 = self.arg380_1
        arg381_1 = self.arg381_1
        arg382_1 = self.arg382_1
        arg383_1 = self.arg383_1
        arg384_1 = self.arg384_1
        arg385_1 = self.arg385_1
        arg386_1 = self.arg386_1
        arg387_1 = self.arg387_1
        arg388_1 = self.arg388_1
        arg389_1 = self.arg389_1
        arg390_1 = self.arg390_1
        arg391_1 = self.arg391_1
        arg392_1 = self.arg392_1
        arg393_1 = self.arg393_1
        arg394_1 = self.arg394_1
        arg395_1 = self.arg395_1
        arg396_1 = self.arg396_1
        arg397_1 = self.arg397_1
        arg398_1 = self.arg398_1
        arg399_1 = self.arg399_1
        arg400_1 = self.arg400_1
        arg401_1 = self.arg401_1
        arg402_1 = self.arg402_1
        arg403_1 = self.arg403_1
        arg404_1 = self.arg404_1
        arg405_1 = self.arg405_1
        arg406_1 = self.arg406_1
        arg407_1 = self.arg407_1
        arg408_1 = self.arg408_1
        arg409_1 = self.arg409_1
        arg410_1 = self.arg410_1
        arg411_1 = self.arg411_1
        arg412_1 = self.arg412_1
        arg413_1 = self.arg413_1
        arg414_1 = self.arg414_1
        arg415_1 = self.arg415_1
        arg416_1 = self.arg416_1
        arg417_1 = self.arg417_1
        arg418_1 = self.arg418_1
        arg419_1 = self.arg419_1
        arg420_1 = self.arg420_1
        arg421_1 = self.arg421_1
        arg422_1 = self.arg422_1
        arg423_1 = self.arg423_1
        arg424_1 = self.arg424_1
        arg425_1 = self.arg425_1
        arg426_1 = self.arg426_1
        arg427_1 = self.arg427_1
        arg428_1 = self.arg428_1
        arg429_1 = self.arg429_1
        arg430_1 = self.arg430_1
        arg431_1 = self.arg431_1
        arg432_1 = self.arg432_1
        arg433_1 = self.arg433_1
        arg434_1 = self.arg434_1
        arg435_1 = self.arg435_1
        arg436_1 = self.arg436_1
        arg437_1 = self.arg437_1
        arg438_1 = self.arg438_1
        arg439_1 = self.arg439_1
        arg440_1 = self.arg440_1
        arg441_1 = self.arg441_1
        arg442_1 = self.arg442_1
        arg443_1 = self.arg443_1
        arg444_1 = self.arg444_1
        arg445_1 = self.arg445_1
        arg446_1 = self.arg446_1
        arg447_1 = self.arg447_1
        arg448_1 = self.arg448_1
        arg449_1 = self.arg449_1
        arg450_1 = self.arg450_1
        arg451_1 = self.arg451_1
        arg452_1 = self.arg452_1
        arg453_1 = self.arg453_1
        arg454_1 = self.arg454_1
        arg455_1 = self.arg455_1
        arg456_1 = self.arg456_1
        arg457_1 = self.arg457_1
        arg458_1 = self.arg458_1
        arg459_1 = self.arg459_1
        arg460_1 = self.arg460_1
        arg461_1 = self.arg461_1
        arg462_1 = self.arg462_1
        arg463_1 = self.arg463_1
        arg464_1 = self.arg464_1
        arg465_1 = self.arg465_1
        arg466_1 = self.arg466_1
        arg467_1 = self.arg467_1
        arg468_1 = self.arg468_1
        arg469_1 = self.arg469_1
        arg470_1 = self.arg470_1
        arg471_1 = self.arg471_1
        arg472_1 = self.arg472_1
        arg473_1 = self.arg473_1
        arg474_1 = self.arg474_1
        arg475_1 = self.arg475_1
        arg476_1 = self.arg476_1
        arg477_1 = self.arg477_1
        arg478_1 = self.arg478_1
        arg479_1 = self.arg479_1
        arg480_1 = self.arg480_1
        arg481_1 = self.arg481_1
        arg482_1 = self.arg482_1
        arg483_1 = self.arg483_1
        arg484_1 = self.arg484_1
        arg485_1 = self.arg485_1
        arg486_1 = self.arg486_1
        arg487_1 = self.arg487_1
        arg488_1 = self.arg488_1
        arg489_1 = self.arg489_1
        arg490_1 = self.arg490_1
        arg491_1 = self.arg491_1
        arg492_1 = self.arg492_1
        arg493_1 = self.arg493_1
        arg494_1 = self.arg494_1
        arg495_1 = self.arg495_1
        arg496_1 = self.arg496_1
        arg497_1 = self.arg497_1
        arg498_1 = self.arg498_1
        arg499_1 = self.arg499_1
        arg500_1 = self.arg500_1
        arg501_1 = self.arg501_1
        arg502_1 = self.arg502_1
        arg503_1 = self.arg503_1
        arg504_1 = self.arg504_1
        arg505_1 = self.arg505_1
        arg506_1 = self.arg506_1
        arg507_1 = self.arg507_1
        arg508_1 = self.arg508_1
        arg509_1 = self.arg509_1
        arg510_1 = self.arg510_1
        arg511_1 = self.arg511_1
        arg512_1 = self.arg512_1
        arg513_1 = self.arg513_1
        arg514_1 = self.arg514_1
        arg515_1 = self.arg515_1
        arg516_1 = self.arg516_1
        arg517_1 = self.arg517_1
        arg518_1 = self.arg518_1
        arg519_1 = self.arg519_1
        arg520_1 = self.arg520_1
        arg521_1 = self.arg521_1
        arg522_1 = self.arg522_1
        arg523_1 = self.arg523_1
        arg524_1 = self.arg524_1
        arg525_1 = self.arg525_1
        arg526_1 = self.arg526_1
        arg527_1 = self.arg527_1
        arg528_1 = self.arg528_1
        arg529_1 = self.arg529_1
        arg530_1 = self.arg530_1
        arg531_1 = self.arg531_1
        arg532_1 = self.arg532_1
        arg533_1 = self.arg533_1
        arg534_1 = self.arg534_1
        arg535_1 = self.arg535_1
        arg536_1 = self.arg536_1
        arg537_1 = self.arg537_1
        arg538_1 = self.arg538_1
        arg539_1 = self.arg539_1
        arg540_1 = self.arg540_1
        arg541_1 = self.arg541_1
        arg542_1 = self.arg542_1
        arg543_1 = self.arg543_1
        arg544_1 = self.arg544_1
        arg545_1 = self.arg545_1
        arg546_1 = self.arg546_1
        arg547_1 = self.arg547_1
        arg548_1 = self.arg548_1
        arg549_1 = self.arg549_1
        arg550_1 = self.arg550_1
        arg551_1 = self.arg551_1
        arg552_1 = self.arg552_1
        arg553_1 = self.arg553_1
        arg554_1 = self.arg554_1
        arg555_1 = self.arg555_1
        arg556_1 = self.arg556_1
        arg557_1 = self.arg557_1
        arg558_1 = self.arg558_1
        arg559_1 = self.arg559_1
        arg560_1 = self.arg560_1
        arg561_1 = self.arg561_1
        arg562_1 = self.arg562_1
        arg563_1 = self.arg563_1
        arg564_1 = self.arg564_1
        arg565_1 = self.arg565_1
        arg566_1 = self.arg566_1
        arg567_1 = self.arg567_1
        arg568_1 = self.arg568_1
        arg569_1 = self.arg569_1
        arg570_1 = self.arg570_1
        arg571_1 = self.arg571_1
        arg572_1 = self.arg572_1
        arg573_1 = self.arg573_1
        arg574_1 = self.arg574_1
        arg575_1 = self.arg575_1
        arg576_1 = self.arg576_1
        arg577_1 = self.arg577_1
        arg578_1 = self.arg578_1
        arg579_1 = self.arg579_1
        arg580_1 = self.arg580_1
        arg581_1 = self.arg581_1
        arg582_1 = self.arg582_1
        arg583_1 = self.arg583_1
        arg584_1 = self.arg584_1
        arg585_1 = self.arg585_1
        arg586_1 = self.arg586_1
        arg587_1 = self.arg587_1
        arg588_1 = self.arg588_1
        arg589_1 = self.arg589_1
        arg590_1 = self.arg590_1
        arg591_1 = self.arg591_1
        arg592_1 = self.arg592_1
        arg593_1 = self.arg593_1
        arg594_1 = self.arg594_1
        arg595_1 = self.arg595_1
        arg596_1 = self.arg596_1
        arg597_1 = self.arg597_1
        arg598_1 = self.arg598_1
        arg599_1 = self.arg599_1
        arg600_1 = self.arg600_1
        arg601_1 = self.arg601_1
        arg602_1 = self.arg602_1
        arg603_1 = self.arg603_1
        arg604_1 = self.arg604_1
        arg605_1 = self.arg605_1
        arg606_1 = self.arg606_1
        arg607_1 = self.arg607_1
        arg608_1 = self.arg608_1
        arg609_1 = self.arg609_1
        arg610_1 = self.arg610_1
        arg611_1 = self.arg611_1
        arg612_1 = self.arg612_1
        arg613_1 = self.arg613_1
        arg614_1 = self.arg614_1
        arg615_1 = self.arg615_1
        arg616_1 = self.arg616_1
        arg617_1 = self.arg617_1
        arg618_1 = self.arg618_1
        arg619_1 = self.arg619_1
        arg620_1 = self.arg620_1
        arg621_1 = self.arg621_1
        arg622_1 = self.arg622_1
        arg623_1 = self.arg623_1
        arg624_1 = self.arg624_1
        arg625_1 = self.arg625_1
        arg626_1 = self.arg626_1
        arg627_1 = self.arg627_1
        arg628_1 = self.arg628_1
        arg629_1 = self.arg629_1
        arg630_1 = self.arg630_1
        arg631_1 = self.arg631_1
        arg632_1 = self.arg632_1
        arg633_1 = self.arg633_1
        arg634_1 = self.arg634_1
        arg635_1 = self.arg635_1
        arg636_1 = self.arg636_1
        arg637_1 = self.arg637_1
        arg638_1 = self.arg638_1
        arg639_1 = self.arg639_1
        arg640_1 = self.arg640_1
        arg641_1 = self.arg641_1
        arg642_1 = self.arg642_1
        arg643_1 = self.arg643_1
        arg644_1 = self.arg644_1
        arg645_1 = self.arg645_1
        arg646_1 = self.arg646_1
        arg647_1 = self.arg647_1
        arg648_1 = self.arg648_1
        arg649_1 = self.arg649_1
        arg650_1 = self.arg650_1
        arg651_1 = self.arg651_1
        arg652_1 = self.arg652_1
        arg653_1 = self.arg653_1
        arg654_1 = self.arg654_1
        arg655_1 = self.arg655_1
        arg656_1 = self.arg656_1
        arg657_1 = self.arg657_1
        arg658_1 = self.arg658_1
        arg659_1 = self.arg659_1
        arg660_1 = self.arg660_1
        arg661_1 = self.arg661_1
        arg662_1 = self.arg662_1
        arg663_1 = self.arg663_1
        arg664_1 = self.arg664_1
        arg665_1 = self.arg665_1
        arg666_1 = self.arg666_1
        arg667_1 = self.arg667_1
        arg668_1 = self.arg668_1
        arg669_1 = self.arg669_1
        arg670_1 = self.arg670_1
        arg671_1 = self.arg671_1
        arg672_1 = self.arg672_1
        arg673_1 = self.arg673_1
        arg674_1 = self.arg674_1
        arg675_1 = self.arg675_1
        arg676_1 = self.arg676_1
        arg677_1 = self.arg677_1
        arg678_1 = self.arg678_1
        arg679_1 = self.arg679_1
        arg680_1 = self.arg680_1
        arg681_1 = self.arg681_1
        arg682_1 = self.arg682_1
        arg683_1 = self.arg683_1
        arg684_1 = self.arg684_1
        arg685_1 = self.arg685_1
        arg686_1 = self.arg686_1
        arg687_1 = self.arg687_1
        arg688_1 = self.arg688_1
        arg689_1 = self.arg689_1
        arg690_1 = self.arg690_1
        arg691_1 = self.arg691_1
        arg692_1 = self.arg692_1
        arg693_1 = self.arg693_1
        arg694_1 = self.arg694_1
        arg695_1 = self.arg695_1
        arg696_1 = self.arg696_1
        arg697_1 = self.arg697_1
        arg698_1 = self.arg698_1
        arg699_1 = self.arg699_1
        arg700_1 = self.arg700_1
        arg701_1 = self.arg701_1
        arg702_1 = self.arg702_1
        arg703_1 = self.arg703_1
        arg704_1 = self.arg704_1
        arg705_1 = self.arg705_1
        arg706_1 = self.arg706_1
        arg707_1 = self.arg707_1
        arg708_1 = self.arg708_1
        arg709_1 = self.arg709_1
        arg710_1 = self.arg710_1
        arg711_1 = self.arg711_1
        arg712_1 = self.arg712_1
        arg713_1 = self.arg713_1
        arg714_1 = self.arg714_1
        arg715_1 = self.arg715_1
        arg716_1 = self.arg716_1
        arg717_1 = self.arg717_1
        arg718_1 = self.arg718_1
        arg719_1 = self.arg719_1
        arg720_1 = self.arg720_1
        arg721_1 = self.arg721_1
        arg722_1 = self.arg722_1
        arg723_1 = self.arg723_1
        arg724_1 = self.arg724_1
        arg725_1 = self.arg725_1
        arg726_1 = self.arg726_1
        arg727_1 = self.arg727_1
        arg728_1 = self.arg728_1
        arg729_1 = self.arg729_1
        arg730_1 = self.arg730_1
        arg731_1 = self.arg731_1
        arg732_1 = self.arg732_1
        arg733_1 = self.arg733_1
        arg734_1 = self.arg734_1
        arg735_1 = self.arg735_1
        arg736_1 = self.arg736_1
        arg737_1 = self.arg737_1
        arg738_1 = self.arg738_1
        arg739_1 = self.arg739_1
        arg740_1 = self.arg740_1
        arg741_1 = self.arg741_1
        arg742_1 = self.arg742_1
        arg743_1 = self.arg743_1
        arg744_1 = self.arg744_1
        arg745_1 = self.arg745_1
        arg746_1 = self.arg746_1
        arg747_1 = self.arg747_1
        arg748_1 = self.arg748_1
        arg749_1 = self.arg749_1
        arg750_1 = self.arg750_1
        arg751_1 = self.arg751_1
        arg752_1 = self.arg752_1
        arg753_1 = self.arg753_1
        arg754_1 = self.arg754_1
        arg755_1 = self.arg755_1
        arg756_1 = self.arg756_1
        arg757_1 = self.arg757_1
        arg758_1 = self.arg758_1
        arg759_1 = self.arg759_1
        arg760_1 = self.arg760_1
        arg761_1 = self.arg761_1
        arg762_1 = self.arg762_1
        arg763_1 = self.arg763_1
        arg764_1 = self.arg764_1
        arg765_1 = self.arg765_1
        arg766_1 = self.arg766_1
        arg767_1 = self.arg767_1
        arg768_1 = self.arg768_1
        arg769_1 = self.arg769_1
        arg770_1 = self.arg770_1
        arg771_1 = self.arg771_1
        arg772_1 = self.arg772_1
        arg773_1 = self.arg773_1
        arg774_1 = self.arg774_1
        arg775_1 = self.arg775_1
        arg776_1 = self.arg776_1
        arg777_1 = self.arg777_1
        arg778_1 = self.arg778_1
        arg779_1 = self.arg779_1
        arg780_1 = self.arg780_1
        arg781_1 = self.arg781_1
        arg782_1 = self.arg782_1
        arg783_1 = self.arg783_1
        arg784_1 = self.arg784_1
        arg785_1 = self.arg785_1
        arg786_1 = self.arg786_1
        arg787_1 = self.arg787_1
        arg788_1 = self.arg788_1
        arg789_1 = self.arg789_1
        arg790_1 = self.arg790_1
        arg791_1 = self.arg791_1
        arg792_1 = self.arg792_1
        arg793_1 = self.arg793_1
        arg794_1 = self.arg794_1
        arg795_1 = self.arg795_1
        arg796_1 = self.arg796_1
        arg797_1 = self.arg797_1
        arg798_1 = self.arg798_1
        arg799_1 = self.arg799_1
        arg800_1 = self.arg800_1
        arg801_1 = self.arg801_1
        arg802_1 = self.arg802_1
        arg803_1 = self.arg803_1
        arg804_1 = self.arg804_1
        arg805_1 = self.arg805_1
        arg806_1 = self.arg806_1
        arg807_1 = self.arg807_1
        arg808_1 = self.arg808_1
        arg809_1 = self.arg809_1
        arg810_1 = self.arg810_1
        arg811_1 = self.arg811_1
        arg812_1 = self.arg812_1
        arg813_1 = self.arg813_1
        arg814_1 = self.arg814_1
        arg815_1 = self.arg815_1
        arg816_1 = self.arg816_1
        arg817_1 = self.arg817_1
        arg818_1 = self.arg818_1
        arg819_1 = self.arg819_1
        arg820_1 = self.arg820_1
        arg821_1 = self.arg821_1
        arg822_1 = self.arg822_1
        arg823_1 = self.arg823_1
        arg824_1 = self.arg824_1
        arg825_1 = self.arg825_1
        arg826_1 = self.arg826_1
        arg827_1 = self.arg827_1
        arg828_1 = self.arg828_1
        arg829_1 = self.arg829_1
        arg830_1 = self.arg830_1
        arg831_1 = self.arg831_1
        arg832_1 = self.arg832_1
        arg833_1 = self.arg833_1
        arg834_1 = self.arg834_1
        arg835_1 = self.arg835_1
        arg836_1 = self.arg836_1
        arg837_1 = self.arg837_1
        arg838_1 = self.arg838_1
        arg839_1 = self.arg839_1
        arg840_1 = self.arg840_1
        arg841_1 = self.arg841_1
        arg842_1 = self.arg842_1
        arg843_1 = self.arg843_1
        arg844_1 = self.arg844_1
        arg845_1 = self.arg845_1
        arg846_1 = self.arg846_1
        arg847_1 = self.arg847_1
        arg848_1 = self.arg848_1
        arg849_1 = self.arg849_1
        arg850_1 = self.arg850_1
        arg851_1 = self.arg851_1
        arg852_1 = self.arg852_1
        arg853_1 = self.arg853_1
        arg854_1 = self.arg854_1
        arg855_1 = self.arg855_1
        arg856_1 = self.arg856_1
        arg857_1 = self.arg857_1
        arg858_1 = self.arg858_1
        arg859_1 = self.arg859_1
        arg860_1 = self.arg860_1
        arg861_1 = self.arg861_1
        arg862_1 = self.arg862_1
        arg863_1 = self.arg863_1
        arg864_1 = self.arg864_1
        arg865_1 = self.arg865_1
        arg866_1 = self.arg866_1
        arg867_1 = self.arg867_1
        arg868_1 = self.arg868_1
        arg869_1 = self.arg869_1
        arg870_1 = self.arg870_1
        arg871_1 = self.arg871_1
        arg872_1 = self.arg872_1
        arg873_1 = self.arg873_1
        arg874_1 = self.arg874_1
        arg875_1 = self.arg875_1
        arg876_1 = self.arg876_1
        arg877_1 = self.arg877_1
        arg878_1 = self.arg878_1
        arg879_1 = self.arg879_1
        arg880_1 = self.arg880_1
        arg881_1 = self.arg881_1
        arg882_1 = self.arg882_1
        arg883_1 = self.arg883_1
        arg884_1 = self.arg884_1
        arg885_1 = self.arg885_1
        arg886_1 = self.arg886_1
        arg887_1 = self.arg887_1
        arg888_1 = self.arg888_1
        arg889_1 = self.arg889_1
        arg890_1 = self.arg890_1
        arg891_1 = self.arg891_1
        arg892_1 = self.arg892_1
        arg893_1 = self.arg893_1
        arg894_1 = self.arg894_1
        arg895_1 = self.arg895_1
        arg896_1 = self.arg896_1
        arg897_1 = self.arg897_1
        arg898_1 = self.arg898_1
        arg899_1 = self.arg899_1
        arg900_1 = self.arg900_1
        arg901_1 = self.arg901_1
        arg902_1 = self.arg902_1
        arg903_1 = self.arg903_1
        arg904_1 = self.arg904_1
        arg905_1 = self.arg905_1
        arg906_1 = self.arg906_1
        arg907_1 = self.arg907_1
        arg908_1 = self.arg908_1
        arg909_1 = self.arg909_1
        arg910_1 = self.arg910_1
        arg911_1 = self.arg911_1
        arg912_1 = self.arg912_1
        arg913_1 = self.arg913_1
        arg914_1 = self.arg914_1
        arg915_1 = self.arg915_1
        arg916_1 = self.arg916_1
        arg917_1 = self.arg917_1
        arg918_1 = self.arg918_1
        arg919_1 = self.arg919_1
        arg920_1 = self.arg920_1
        arg921_1 = self.arg921_1
        arg922_1 = self.arg922_1
        arg923_1 = self.arg923_1
        arg924_1 = self.arg924_1
        arg925_1 = self.arg925_1
        arg926_1 = self.arg926_1
        arg927_1 = self.arg927_1
        arg928_1 = self.arg928_1
        arg929_1 = self.arg929_1
        arg930_1 = self.arg930_1
        arg931_1 = self.arg931_1
        arg932_1 = self.arg932_1
        arg933_1 = self.arg933_1
        arg934_1 = self.arg934_1
        arg935_1 = self.arg935_1
        arg936_1 = self.arg936_1
        arg937_1 = self.arg937_1
        arg938_1 = self.arg938_1
        arg939_1 = self.arg939_1
        arg940_1 = self.arg940_1
        arg941_1 = self.arg941_1
        arg942_1 = self.arg942_1
        arg943_1 = self.arg943_1
        arg944_1 = self.arg944_1
        arg945_1 = self.arg945_1
        arg946_1 = self.arg946_1
        arg947_1 = self.arg947_1
        arg948_1 = self.arg948_1
        arg949_1 = self.arg949_1
        arg950_1 = self.arg950_1
        arg951_1 = self.arg951_1
        arg952_1 = self.arg952_1
        arg953_1 = self.arg953_1
        arg954_1 = self.arg954_1
        arg955_1 = self.arg955_1
        arg956_1 = self.arg956_1
        arg957_1 = self.arg957_1
        arg958_1 = self.arg958_1
        arg959_1 = self.arg959_1
        arg960_1 = self.arg960_1
        arg961_1 = self.arg961_1
        arg962_1 = self.arg962_1
        arg963_1 = self.arg963_1
        arg964_1 = self.arg964_1
        arg965_1 = self.arg965_1
        arg966_1 = self.arg966_1
        arg967_1 = self.arg967_1
        arg968_1 = self.arg968_1
        arg969_1 = self.arg969_1
        arg970_1 = self.arg970_1
        arg971_1 = self.arg971_1
        arg972_1 = self.arg972_1
        arg973_1 = self.arg973_1
        arg974_1 = self.arg974_1
        arg975_1 = self.arg975_1
        arg976_1 = self.arg976_1
        arg977_1 = self.arg977_1
        arg978_1 = self.arg978_1
        arg979_1 = self.arg979_1
        arg980_1 = self.arg980_1
        arg981_1 = self.arg981_1
        arg982_1 = self.arg982_1
        arg983_1 = self.arg983_1
        arg984_1 = self.arg984_1
        arg985_1 = self.arg985_1
        arg986_1 = self.arg986_1
        arg987_1 = self.arg987_1
        arg988_1 = self.arg988_1
        arg989_1 = self.arg989_1
        arg990_1 = self.arg990_1
        arg991_1 = self.arg991_1
        arg992_1 = self.arg992_1
        arg993_1 = self.arg993_1
        arg994_1 = self.arg994_1
        arg995_1 = self.arg995_1
        arg996_1 = self.arg996_1
        arg997_1 = self.arg997_1
        arg998_1 = self.arg998_1
        arg999_1 = self.arg999_1
        arg1000_1 = self.arg1000_1
        arg1001_1 = self.arg1001_1
        arg1002_1 = self.arg1002_1
        arg1003_1 = self.arg1003_1
        arg1004_1 = self.arg1004_1
        arg1005_1 = self.arg1005_1
        arg1006_1 = self.arg1006_1
        arg1007_1 = self.arg1007_1
        arg1008_1 = self.arg1008_1
        arg1009_1 = self.arg1009_1
        arg1010_1 = self.arg1010_1
        arg1011_1 = self.arg1011_1
        arg1012_1 = self.arg1012_1
        arg1013_1 = self.arg1013_1
        arg1014_1 = self.arg1014_1
        arg1015_1 = self.arg1015_1
        arg1016_1 = self.arg1016_1
        arg1017_1 = self.arg1017_1
        arg1018_1 = self.arg1018_1
        arg1019_1 = self.arg1019_1
        arg1020_1 = self.arg1020_1
        arg1021_1 = self.arg1021_1
        arg1022_1 = self.arg1022_1
        arg1023_1 = self.arg1023_1
        arg1024_1 = self.arg1024_1
        arg1025_1 = self.arg1025_1
        arg1026_1 = self.arg1026_1
        arg1027_1 = self.arg1027_1
        arg1028_1 = self.arg1028_1
        arg1029_1 = self.arg1029_1
        arg1030_1 = self.arg1030_1
        arg1031_1 = self.arg1031_1
        arg1032_1 = self.arg1032_1
        arg1033_1 = self.arg1033_1
        arg1034_1 = self.arg1034_1
        arg1035_1 = self.arg1035_1
        arg1036_1 = self.arg1036_1
        arg1037_1 = self.arg1037_1
        arg1038_1 = self.arg1038_1
        arg1039_1 = self.arg1039_1
        arg1040_1 = self.arg1040_1
        arg1041_1 = self.arg1041_1
        arg1042_1 = self.arg1042_1
        arg1043_1 = self.arg1043_1
        arg1044_1 = self.arg1044_1
        arg1045_1 = self.arg1045_1
        arg1046_1 = self.arg1046_1
        arg1047_1 = self.arg1047_1
        arg1048_1 = self.arg1048_1
        arg1049_1 = self.arg1049_1
        arg1050_1 = self.arg1050_1
        arg1051_1 = self.arg1051_1
        arg1052_1 = self.arg1052_1
        arg1053_1 = self.arg1053_1
        arg1054_1 = self.arg1054_1
        arg1055_1 = self.arg1055_1
        arg1056_1 = self.arg1056_1
        arg1057_1 = self.arg1057_1
        arg1058_1 = self.arg1058_1
        arg1059_1 = self.arg1059_1
        arg1060_1 = self.arg1060_1
        arg1061_1 = self.arg1061_1
        arg1062_1 = self.arg1062_1
        arg1063_1 = self.arg1063_1
        arg1064_1 = self.arg1064_1
        arg1065_1 = self.arg1065_1
        arg1066_1 = self.arg1066_1
        arg1067_1 = self.arg1067_1
        arg1068_1 = self.arg1068_1
        arg1069_1 = self.arg1069_1
        arg1070_1 = self.arg1070_1
        arg1071_1 = self.arg1071_1
        arg1072_1 = self.arg1072_1
        arg1073_1 = self.arg1073_1
        arg1074_1 = self.arg1074_1
        arg1075_1 = self.arg1075_1
        arg1076_1 = self.arg1076_1
        arg1077_1 = self.arg1077_1
        arg1078_1 = self.arg1078_1
        arg1079_1 = self.arg1079_1
        arg1080_1 = self.arg1080_1
        arg1081_1 = self.arg1081_1
        arg1082_1 = self.arg1082_1
        arg1083_1 = self.arg1083_1
        arg1084_1 = self.arg1084_1
        arg1085_1 = self.arg1085_1
        arg1086_1 = self.arg1086_1
        arg1087_1 = self.arg1087_1
        arg1088_1 = self.arg1088_1
        arg1089_1 = self.arg1089_1
        arg1090_1 = self.arg1090_1
        arg1091_1 = self.arg1091_1
        arg1092_1 = self.arg1092_1
        arg1093_1 = self.arg1093_1
        arg1094_1 = self.arg1094_1
        arg1095_1 = self.arg1095_1
        arg1096_1 = self.arg1096_1
        arg1097_1 = self.arg1097_1
        arg1098_1 = self.arg1098_1
        arg1099_1 = self.arg1099_1
        arg1100_1 = self.arg1100_1
        arg1101_1 = self.arg1101_1
        arg1102_1 = self.arg1102_1
        arg1103_1 = self.arg1103_1
        arg1104_1 = self.arg1104_1
        arg1105_1 = self.arg1105_1
        arg1106_1 = self.arg1106_1
        arg1107_1 = self.arg1107_1
        arg1108_1 = self.arg1108_1
        arg1109_1 = self.arg1109_1
        arg1110_1 = self.arg1110_1
        arg1111_1 = self.arg1111_1
        arg1112_1 = self.arg1112_1
        arg1113_1 = self.arg1113_1
        arg1114_1 = self.arg1114_1
        arg1115_1 = self.arg1115_1
        arg1116_1 = self.arg1116_1
        arg1117_1 = self.arg1117_1
        arg1118_1 = self.arg1118_1
        arg1119_1 = self.arg1119_1
        arg1120_1 = self.arg1120_1
        arg1121_1 = self.arg1121_1
        arg1122_1 = self.arg1122_1
        arg1123_1 = self.arg1123_1
        arg1124_1 = self.arg1124_1
        arg1125_1 = self.arg1125_1
        arg1126_1 = self.arg1126_1
        arg1127_1 = self.arg1127_1
        arg1128_1 = self.arg1128_1
        arg1129_1 = self.arg1129_1
        arg1130_1 = self.arg1130_1
        arg1131_1 = self.arg1131_1
        arg1132_1 = self.arg1132_1
        arg1133_1 = self.arg1133_1
        arg1134_1 = self.arg1134_1
        arg1135_1 = self.arg1135_1
        arg1136_1 = self.arg1136_1
        arg1137_1 = self.arg1137_1
        arg1138_1 = self.arg1138_1
        arg1139_1 = self.arg1139_1
        arg1140_1 = self.arg1140_1
        arg1141_1 = self.arg1141_1
        arg1142_1 = self.arg1142_1
        arg1143_1 = self.arg1143_1
        arg1144_1 = self.arg1144_1
        arg1145_1 = self.arg1145_1
        arg1146_1 = self.arg1146_1
        arg1147_1 = self.arg1147_1
        arg1148_1 = self.arg1148_1
        arg1149_1 = self.arg1149_1
        arg1150_1 = self.arg1150_1
        arg1151_1 = self.arg1151_1
        arg1152_1 = self.arg1152_1
        arg1153_1 = self.arg1153_1
        arg1154_1 = self.arg1154_1
        arg1155_1 = self.arg1155_1
        arg1156_1 = self.arg1156_1
        arg1157_1 = self.arg1157_1
        arg1158_1 = self.arg1158_1
        arg1159_1 = self.arg1159_1
        arg1160_1 = self.arg1160_1
        arg1161_1 = self.arg1161_1
        arg1162_1 = self.arg1162_1
        arg1163_1 = self.arg1163_1
        arg1164_1 = self.arg1164_1
        arg1165_1 = self.arg1165_1
        arg1166_1 = self.arg1166_1
        arg1167_1 = self.arg1167_1
        arg1168_1 = self.arg1168_1
        arg1169_1 = self.arg1169_1
        arg1170_1 = self.arg1170_1
        arg1171_1 = self.arg1171_1
        arg1172_1 = self.arg1172_1
        arg1173_1 = self.arg1173_1
        arg1174_1 = self.arg1174_1
        arg1175_1 = self.arg1175_1
        arg1176_1 = self.arg1176_1
        arg1177_1 = self.arg1177_1
        arg1178_1 = self.arg1178_1
        arg1179_1 = self.arg1179_1
        arg1180_1 = self.arg1180_1
        arg1181_1 = self.arg1181_1
        arg1182_1 = self.arg1182_1
        arg1183_1 = self.arg1183_1
        arg1184_1 = self.arg1184_1
        arg1185_1 = self.arg1185_1
        arg1186_1 = self.arg1186_1
        arg1187_1 = self.arg1187_1
        arg1188_1 = self.arg1188_1
        arg1189_1 = self.arg1189_1
        arg1190_1 = self.arg1190_1
        arg1191_1 = self.arg1191_1
        arg1192_1 = self.arg1192_1
        arg1193_1 = self.arg1193_1
        arg1194_1 = self.arg1194_1
        arg1195_1 = self.arg1195_1
        arg1196_1 = self.arg1196_1
        arg1197_1 = self.arg1197_1
        arg1198_1 = self.arg1198_1
        arg1199_1 = self.arg1199_1
        arg1200_1 = self.arg1200_1
        arg1201_1 = self.arg1201_1
        arg1202_1 = self.arg1202_1
        arg1203_1 = self.arg1203_1
        arg1204_1 = self.arg1204_1
        arg1205_1 = self.arg1205_1
        arg1206_1 = self.arg1206_1
        arg1207_1 = self.arg1207_1
        arg1208_1 = self.arg1208_1
        arg1209_1 = self.arg1209_1
        arg1210_1 = self.arg1210_1
        arg1211_1 = self.arg1211_1
        arg1212_1 = self.arg1212_1
        arg1213_1 = self.arg1213_1
        arg1214_1 = self.arg1214_1
        arg1215_1 = self.arg1215_1
        arg1216_1 = self.arg1216_1
        arg1217_1 = self.arg1217_1
        arg1218_1 = self.arg1218_1
        arg1219_1 = self.arg1219_1
        arg1220_1 = self.arg1220_1
        arg1221_1 = self.arg1221_1
        arg1222_1 = self.arg1222_1
        arg1223_1 = self.arg1223_1
        arg1224_1 = self.arg1224_1
        arg1225_1 = self.arg1225_1
        arg1226_1 = self.arg1226_1
        arg1227_1 = self.arg1227_1
        arg1228_1 = self.arg1228_1
        arg1229_1 = self.arg1229_1
        arg1230_1 = self.arg1230_1
        arg1231_1 = self.arg1231_1
        arg1232_1 = self.arg1232_1
        arg1233_1 = self.arg1233_1
        arg1234_1 = self.arg1234_1
        arg1235_1 = self.arg1235_1
        arg1236_1 = self.arg1236_1
        arg1237_1 = self.arg1237_1
        arg1238_1 = self.arg1238_1
        arg1239_1 = self.arg1239_1
        arg1240_1 = self.arg1240_1
        arg1241_1 = self.arg1241_1
        arg1242_1 = self.arg1242_1
        arg1243_1 = self.arg1243_1
        arg1244_1 = self.arg1244_1
        arg1245_1 = self.arg1245_1
        arg1246_1 = self.arg1246_1
        arg1247_1 = self.arg1247_1
        arg1248_1 = self.arg1248_1
        arg1249_1 = self.arg1249_1
        arg1250_1 = self.arg1250_1
        arg1251_1 = self.arg1251_1
        arg1252_1 = self.arg1252_1
        arg1253_1 = self.arg1253_1
        arg1254_1 = self.arg1254_1
        arg1255_1 = self.arg1255_1
        arg1256_1 = self.arg1256_1
        arg1257_1 = self.arg1257_1
        arg1258_1 = self.arg1258_1
        arg1259_1 = self.arg1259_1
        arg1260_1 = self.arg1260_1
        arg1261_1 = self.arg1261_1
        arg1262_1 = self.arg1262_1
        arg1263_1 = self.arg1263_1
        arg1264_1 = self.arg1264_1
        arg1265_1 = self.arg1265_1
        arg1266_1 = self.arg1266_1
        arg1267_1 = self.arg1267_1
        arg1268_1 = self.arg1268_1
        arg1269_1 = self.arg1269_1
        arg1270_1 = self.arg1270_1
        arg1271_1 = self.arg1271_1
        arg1272_1 = self.arg1272_1
        arg1273_1 = self.arg1273_1
        arg1274_1 = self.arg1274_1
        arg1275_1 = self.arg1275_1
        arg1276_1 = self.arg1276_1
        arg1277_1 = self.arg1277_1
        arg1278_1 = self.arg1278_1
        arg1279_1 = self.arg1279_1
        arg1280_1 = self.arg1280_1
        arg1281_1 = self.arg1281_1
        arg1282_1 = self.arg1282_1
        arg1283_1 = self.arg1283_1
        arg1284_1 = self.arg1284_1
        arg1285_1 = self.arg1285_1
        arg1286_1 = self.arg1286_1
        arg1287_1 = self.arg1287_1
        arg1288_1 = self.arg1288_1
        arg1289_1 = self.arg1289_1
        arg1290_1 = self.arg1290_1
        arg1291_1 = self.arg1291_1
        arg1292_1 = self.arg1292_1
        arg1293_1 = self.arg1293_1
        arg1294_1 = self.arg1294_1
        arg1295_1 = self.arg1295_1
        arg1296_1 = self.arg1296_1
        arg1297_1 = self.arg1297_1
        arg1298_1 = self.arg1298_1
        arg1299_1 = self.arg1299_1
        arg1300_1 = self.arg1300_1
        arg1301_1 = self.arg1301_1
        arg1302_1 = self.arg1302_1
        arg1303_1 = self.arg1303_1
        arg1304_1 = self.arg1304_1
        arg1305_1 = self.arg1305_1
        arg1306_1 = self.arg1306_1
        arg1307_1 = self.arg1307_1
        arg1308_1 = self.arg1308_1
        arg1309_1 = self.arg1309_1
        arg1310_1 = self.arg1310_1
        arg1311_1 = self.arg1311_1
        arg1312_1 = self.arg1312_1
        arg1313_1 = self.arg1313_1
        arg1314_1 = self.arg1314_1
        arg1315_1 = self.arg1315_1
        arg1316_1 = self.arg1316_1
        arg1317_1 = self.arg1317_1
        arg1318_1 = self.arg1318_1
        arg1319_1 = self.arg1319_1
        arg1320_1 = self.arg1320_1
        arg1321_1 = self.arg1321_1
        arg1322_1 = self.arg1322_1
        arg1323_1 = self.arg1323_1
        arg1324_1 = self.arg1324_1
        arg1325_1 = self.arg1325_1
        arg1326_1 = self.arg1326_1
        arg1327_1 = self.arg1327_1
        arg1328_1 = self.arg1328_1
        arg1329_1 = self.arg1329_1
        arg1330_1 = self.arg1330_1
        arg1331_1 = self.arg1331_1
        arg1332_1 = self.arg1332_1
        arg1333_1 = self.arg1333_1
        arg1334_1 = self.arg1334_1
        arg1335_1 = self.arg1335_1
        arg1336_1 = self.arg1336_1
        arg1337_1 = self.arg1337_1
        arg1338_1 = self.arg1338_1
        arg1339_1 = self.arg1339_1
        arg1340_1 = self.arg1340_1
        arg1341_1 = self.arg1341_1
        arg1342_1 = self.arg1342_1
        arg1343_1 = self.arg1343_1
        arg1344_1 = self.arg1344_1
        arg1345_1 = self.arg1345_1
        arg1346_1 = self.arg1346_1
        arg1347_1 = self.arg1347_1
        arg1348_1 = self.arg1348_1
        arg1349_1 = self.arg1349_1
        arg1350_1 = self.arg1350_1
        arg1351_1 = self.arg1351_1
        arg1352_1 = self.arg1352_1
        arg1353_1 = self.arg1353_1
        arg1354_1 = self.arg1354_1
        arg1355_1 = self.arg1355_1
        arg1356_1 = self.arg1356_1
        arg1357_1 = self.arg1357_1
        arg1358_1 = self.arg1358_1
        arg1359_1 = self.arg1359_1
        arg1360_1 = self.arg1360_1
        arg1361_1 = self.arg1361_1
        arg1362_1 = self.arg1362_1
        arg1363_1 = self.arg1363_1
        arg1364_1 = self.arg1364_1
        arg1365_1 = self.arg1365_1
        arg1366_1 = self.arg1366_1
        arg1367_1 = self.arg1367_1
        arg1368_1 = self.arg1368_1
        arg1369_1 = self.arg1369_1
        arg1370_1 = self.arg1370_1
        arg1371_1 = self.arg1371_1
        arg1372_1 = self.arg1372_1
        arg1373_1 = self.arg1373_1
        arg1374_1 = self.arg1374_1
        arg1375_1 = self.arg1375_1
        arg1376_1 = self.arg1376_1
        arg1377_1 = self.arg1377_1
        arg1378_1 = self.arg1378_1
        arg1379_1 = self.arg1379_1
        arg1380_1 = self.arg1380_1
        arg1381_1 = self.arg1381_1
        arg1382_1 = self.arg1382_1
        arg1383_1 = self.arg1383_1
        arg1384_1 = self.arg1384_1
        arg1385_1 = self.arg1385_1
        arg1386_1 = self.arg1386_1
        arg1387_1 = self.arg1387_1
        arg1388_1 = self.arg1388_1
        arg1389_1 = self.arg1389_1
        arg1390_1 = self.arg1390_1
        arg1391_1 = self.arg1391_1
        arg1392_1 = self.arg1392_1
        arg1393_1 = self.arg1393_1
        arg1394_1 = self.arg1394_1
        arg1395_1 = self.arg1395_1
        arg1396_1 = self.arg1396_1
        arg1397_1 = self.arg1397_1

        _to_copy = torch.ops.aten._to_copy.default(arg1401_1, dtype = torch.float32);  arg1401_1 = None
        native_layer_norm_default = torch.ops.aten.native_layer_norm.default(_to_copy, [256], arg1395_1, arg1396_1, 1e-05);  _to_copy = arg1395_1 = arg1396_1 = None
        getitem = native_layer_norm_default[0]
        _to_copy_1 = torch.ops.aten._to_copy.default(arg1397_1, dtype = torch.bfloat16);  arg1397_1 = None
        _to_copy_2 = torch.ops.aten._to_copy.default(getitem, dtype = torch.bfloat16);  getitem = None
        t = torch.ops.aten.t.default(_to_copy_1);  _to_copy_1 = None
        view = torch.ops.aten.view.default(_to_copy_2, [262144, 256]);  _to_copy_2 = None
        mm = torch.ops.aten.mm.default(view, t);  view = t = None
        view_1 = torch.ops.aten.view.default(mm, [1, 512, 512, 256]);  mm = None
        add = torch.ops.aten.add.Tensor(arg1399_1, view_1);  arg1399_1 = view_1 = None
        _to_copy_3 = torch.ops.aten._to_copy.default(arg1400_1, dtype = torch.float32);  arg1400_1 = None
        native_layer_norm_default_1 = torch.ops.aten.native_layer_norm.default(_to_copy_3, [384], arg1392_1, arg1393_1, 1e-05);  _to_copy_3 = arg1392_1 = arg1393_1 = None
        getitem_3 = native_layer_norm_default_1[0]
        _to_copy_4 = torch.ops.aten._to_copy.default(arg1394_1, dtype = torch.bfloat16);  arg1394_1 = None
        _to_copy_5 = torch.ops.aten._to_copy.default(getitem_3, dtype = torch.bfloat16);  getitem_3 = None
        t_1 = torch.ops.aten.t.default(_to_copy_4);  _to_copy_4 = None
        view_2 = torch.ops.aten.view.default(_to_copy_5, [512, 384]);  _to_copy_5 = None
        mm_1 = torch.ops.aten.mm.default(view_2, t_1);  view_2 = t_1 = None
        view_3 = torch.ops.aten.view.default(mm_1, [1, 512, 384]);  mm_1 = None
        add_1 = torch.ops.aten.add.Tensor(arg1398_1, view_3);  arg1398_1 = view_3 = None
        any_1 = torch.ops.aten.any.dims(arg1405_1, dim = [-2, -1])
        sum_1 = torch.ops.aten.sum.dim_IntList(any_1, [1]);  any_1 = None
        _to_copy_6 = torch.ops.aten._to_copy.default(add, dtype = torch.float32)
        native_layer_norm_default_2 = torch.ops.aten.native_layer_norm.default(_to_copy_6, [256], arg28_1, arg29_1, 1e-05);  _to_copy_6 = arg28_1 = arg29_1 = None
        getitem_6 = native_layer_norm_default_2[0]
        _to_copy_7 = torch.ops.aten._to_copy.default(arg30_1, dtype = torch.bfloat16);  arg30_1 = None
        _to_copy_8 = torch.ops.aten._to_copy.default(getitem_6, dtype = torch.bfloat16);  getitem_6 = None
        t_2 = torch.ops.aten.t.default(_to_copy_7);  _to_copy_7 = None
        view_4 = torch.ops.aten.view.default(_to_copy_8, [262144, 256]);  _to_copy_8 = None
        mm_2 = torch.ops.aten.mm.default(view_4, t_2);  view_4 = t_2 = None
        view_5 = torch.ops.aten.view.default(mm_2, [1, 512, 512, 64]);  mm_2 = None
        unsqueeze = torch.ops.aten.unsqueeze.default(view_5, 1);  view_5 = None
        expand = torch.ops.aten.expand.default(unsqueeze, [-1, 4, -1, -1, -1]);  unsqueeze = None
        view_6 = torch.ops.aten.view.default(expand, [4, 512, 512, 64]);  expand = None
        unsqueeze_2 = torch.ops.aten.unsqueeze.default(arg1407_1, 1)
        expand_2 = torch.ops.aten.expand.default(unsqueeze_2, [-1, 4, -1, -1]);  unsqueeze_2 = None
        view_8 = torch.ops.aten.view.default(expand_2, [4, 512, 512]);  expand_2 = None
        view_9 = torch.ops.aten.view.default(arg1404_1, [4, 512, 512, 64]);  arg1404_1 = None
        view_10 = torch.ops.aten.view.default(arg1405_1, [4, 512, 512]);  arg1405_1 = None
        bitwise_and = torch.ops.aten.bitwise_and.Tensor(view_10, view_8);  view_10 = view_8 = None
        clone = torch.ops.aten.clone.default(view_6);  view_6 = None
        add_2 = torch.ops.aten.add.Tensor(clone, view_9);  clone = view_9 = None
        _to_copy_9 = torch.ops.aten._to_copy.default(add_2, dtype = torch.float32)
        native_layer_norm_default_3 = torch.ops.aten.native_layer_norm.default(_to_copy_9, [64], arg4_1, arg5_1, 1e-05);  _to_copy_9 = arg4_1 = arg5_1 = None
        getitem_9 = native_layer_norm_default_3[0]
        split_with_sizes_default = torch.ops.aten.split_with_sizes.default(arg7_1, [128, 128]);  arg7_1 = None
        getitem_12 = split_with_sizes_default[0]
        getitem_13 = split_with_sizes_default[1];  split_with_sizes_default = None
        split_with_sizes_default_1 = torch.ops.aten.split_with_sizes.default(arg8_1, [128, 128, 64]);  arg8_1 = None
        getitem_14 = split_with_sizes_default_1[0]
        getitem_15 = split_with_sizes_default_1[1]
        getitem_16 = split_with_sizes_default_1[2];  split_with_sizes_default_1 = None
        _to_copy_10 = torch.ops.aten._to_copy.default(getitem_12, dtype = torch.bfloat16);  getitem_12 = None
        _to_copy_11 = torch.ops.aten._to_copy.default(getitem_9, dtype = torch.bfloat16)
        t_3 = torch.ops.aten.t.default(_to_copy_10);  _to_copy_10 = None
        view_11 = torch.ops.aten.view.default(_to_copy_11, [1048576, 64]);  _to_copy_11 = None
        mm_3 = torch.ops.aten.mm.default(view_11, t_3);  view_11 = t_3 = None
        view_12 = torch.ops.aten.view.default(mm_3, [4, 512, 512, 128]);  mm_3 = None
        _to_copy_12 = torch.ops.aten._to_copy.default(getitem_14, dtype = torch.bfloat16);  getitem_14 = None
        _to_copy_13 = torch.ops.aten._to_copy.default(getitem_9, dtype = torch.bfloat16)
        t_4 = torch.ops.aten.t.default(_to_copy_12);  _to_copy_12 = None
        view_13 = torch.ops.aten.view.default(_to_copy_13, [1048576, 64]);  _to_copy_13 = None
        mm_4 = torch.ops.aten.mm.default(view_13, t_4);  view_13 = t_4 = None
        view_14 = torch.ops.aten.view.default(mm_4, [4, 512, 512, 128]);  mm_4 = None
        sigmoid = torch.ops.aten.sigmoid.default(view_14);  view_14 = None
        mul = torch.ops.aten.mul.Tensor(view_12, sigmoid);  view_12 = sigmoid = None
        unsqueeze_3 = torch.ops.aten.unsqueeze.default(bitwise_and, 3)
        bitwise_not = torch.ops.aten.bitwise_not.default(unsqueeze_3);  unsqueeze_3 = None
        masked_fill = torch.ops.aten.masked_fill.Scalar(mul, bitwise_not, 0);  mul = bitwise_not = None
        split_tensor = torch.ops.aten.split.Tensor(masked_fill, 64, dim = -1)
        getitem_19 = split_tensor[0]
        unsqueeze_6 = torch.ops.aten.unsqueeze.default(getitem_19, 4);  getitem_19 = None
        permute_3 = torch.ops.aten.permute.default(unsqueeze_6, [0, 1, 4, 3, 2]);  unsqueeze_6 = None
        permute_4 = torch.ops.aten.permute.default(permute_3, [0, 3, 1, 4, 2]);  permute_3 = None
        clone_1 = torch.ops.aten.clone.default(permute_4, memory_format = torch.contiguous_format);  permute_4 = None
        _unsafe_view = torch.ops.aten._unsafe_view.default(clone_1, [256, 512, 512]);  clone_1 = None
        split_tensor_1 = torch.ops.aten.split.Tensor(masked_fill, 64, dim = -1);  masked_fill = None
        getitem_22 = split_tensor_1[1];  split_tensor_1 = None
        unsqueeze_7 = torch.ops.aten.unsqueeze.default(getitem_22, 4);  getitem_22 = None
        permute_6 = torch.ops.aten.permute.default(unsqueeze_7, [0, 4, 1, 3, 2]);  unsqueeze_7 = None
        permute_7 = torch.ops.aten.permute.default(permute_6, [0, 3, 4, 2, 1]);  permute_6 = None
        clone_2 = torch.ops.aten.clone.default(permute_7, memory_format = torch.contiguous_format);  permute_7 = None
        _unsafe_view_1 = torch.ops.aten._unsafe_view.default(clone_2, [256, 512, 512]);  clone_2 = None
        bmm = torch.ops.aten.bmm.default(_unsafe_view, _unsafe_view_1);  _unsafe_view = _unsafe_view_1 = None
        view_15 = torch.ops.aten.view.default(bmm, [4, 64, 512, 1, 512]);  bmm = None
        permute_8 = torch.ops.aten.permute.default(view_15, [0, 2, 4, 1, 3]);  view_15 = None
        view_16 = torch.ops.aten.view.default(permute_8, [4, 512, 512, 64]);  permute_8 = None
        _to_copy_14 = torch.ops.aten._to_copy.default(getitem_13, dtype = torch.bfloat16);  getitem_13 = None
        _to_copy_15 = torch.ops.aten._to_copy.default(getitem_9, dtype = torch.bfloat16)
        t_5 = torch.ops.aten.t.default(_to_copy_14);  _to_copy_14 = None
        view_17 = torch.ops.aten.view.default(_to_copy_15, [1048576, 64]);  _to_copy_15 = None
        mm_5 = torch.ops.aten.mm.default(view_17, t_5);  view_17 = t_5 = None
        view_18 = torch.ops.aten.view.default(mm_5, [4, 512, 512, 128]);  mm_5 = None
        _to_copy_16 = torch.ops.aten._to_copy.default(getitem_15, dtype = torch.bfloat16);  getitem_15 = None
        _to_copy_17 = torch.ops.aten._to_copy.default(getitem_9, dtype = torch.bfloat16)
        t_6 = torch.ops.aten.t.default(_to_copy_16);  _to_copy_16 = None
        view_19 = torch.ops.aten.view.default(_to_copy_17, [1048576, 64]);  _to_copy_17 = None
        mm_6 = torch.ops.aten.mm.default(view_19, t_6);  view_19 = t_6 = None
        view_20 = torch.ops.aten.view.default(mm_6, [4, 512, 512, 128]);  mm_6 = None
        sigmoid_1 = torch.ops.aten.sigmoid.default(view_20);  view_20 = None
        mul_1 = torch.ops.aten.mul.Tensor(view_18, sigmoid_1);  view_18 = sigmoid_1 = None
        view_21 = torch.ops.aten.view.default(mul_1, [1048576, 128]);  mul_1 = None
        view_22 = torch.ops.aten.view.default(view_21, [4, 512, 512, 128]);  view_21 = None
        transpose = torch.ops.aten.transpose.int(bitwise_and, 1, 2)
        unsqueeze_8 = torch.ops.aten.unsqueeze.default(transpose, 3);  transpose = None
        clone_3 = torch.ops.aten.clone.default(unsqueeze_8, memory_format = torch.contiguous_format);  unsqueeze_8 = None
        bitwise_not_1 = torch.ops.aten.bitwise_not.default(clone_3);  clone_3 = None
        masked_fill_1 = torch.ops.aten.masked_fill.Scalar(view_22, bitwise_not_1, 0);  view_22 = bitwise_not_1 = None
        view_23 = torch.ops.aten.view.default(masked_fill_1, [1048576, 128]);  masked_fill_1 = None
        view_25 = torch.ops.aten.view.default(view_23, [4, 512, 512, 128])
        split_tensor_2 = torch.ops.aten.split.Tensor(view_25, 64, dim = -1);  view_25 = None
        getitem_25 = split_tensor_2[0]
        unsqueeze_11 = torch.ops.aten.unsqueeze.default(getitem_25, 4);  getitem_25 = None
        permute_12 = torch.ops.aten.permute.default(unsqueeze_11, [0, 2, 4, 3, 1]);  unsqueeze_11 = None
        permute_13 = torch.ops.aten.permute.default(permute_12, [0, 3, 1, 4, 2]);  permute_12 = None
        clone_4 = torch.ops.aten.clone.default(permute_13, memory_format = torch.contiguous_format);  permute_13 = None
        _unsafe_view_2 = torch.ops.aten._unsafe_view.default(clone_4, [256, 512, 512]);  clone_4 = None
        view_26 = torch.ops.aten.view.default(view_23, [4, 512, 512, 128]);  view_23 = None
        split_tensor_3 = torch.ops.aten.split.Tensor(view_26, 64, dim = -1);  view_26 = None
        getitem_28 = split_tensor_3[1];  split_tensor_3 = None
        unsqueeze_12 = torch.ops.aten.unsqueeze.default(getitem_28, 4);  getitem_28 = None
        permute_15 = torch.ops.aten.permute.default(unsqueeze_12, [0, 4, 2, 3, 1]);  unsqueeze_12 = None
        permute_16 = torch.ops.aten.permute.default(permute_15, [0, 3, 4, 2, 1]);  permute_15 = None
        clone_5 = torch.ops.aten.clone.default(permute_16, memory_format = torch.contiguous_format);  permute_16 = None
        _unsafe_view_3 = torch.ops.aten._unsafe_view.default(clone_5, [256, 512, 512]);  clone_5 = None
        bmm_1 = torch.ops.aten.bmm.default(_unsafe_view_2, _unsafe_view_3);  _unsafe_view_2 = _unsafe_view_3 = None
        view_27 = torch.ops.aten.view.default(bmm_1, [4, 64, 512, 1, 512]);  bmm_1 = None
        permute_17 = torch.ops.aten.permute.default(view_27, [0, 2, 4, 1, 3]);  view_27 = None
        view_28 = torch.ops.aten.view.default(permute_17, [4, 512, 512, 64]);  permute_17 = None
        _to_copy_18 = torch.ops.aten._to_copy.default(view_16, dtype = torch.float32);  view_16 = None
        native_layer_norm_default_4 = torch.ops.aten.native_layer_norm.default(_to_copy_18, [64], None, None, 1e-05);  _to_copy_18 = None
        getitem_29 = native_layer_norm_default_4[0]
        _to_copy_19 = torch.ops.aten._to_copy.default(view_28, dtype = torch.float32);  view_28 = None
        native_layer_norm_default_5 = torch.ops.aten.native_layer_norm.default(_to_copy_19, [64], None, None, 1e-05);  _to_copy_19 = None
        getitem_32 = native_layer_norm_default_5[0]
        add_3 = torch.ops.aten.add.Tensor(getitem_29, getitem_32);  getitem_29 = getitem_32 = None
        _to_copy_20 = torch.ops.aten._to_copy.default(arg6_1, dtype = torch.bfloat16);  arg6_1 = None
        _to_copy_21 = torch.ops.aten._to_copy.default(add_3, dtype = torch.bfloat16);  add_3 = None
        t_7 = torch.ops.aten.t.default(_to_copy_20);  _to_copy_20 = None
        view_29 = torch.ops.aten.view.default(_to_copy_21, [1048576, 64]);  _to_copy_21 = None
        mm_7 = torch.ops.aten.mm.default(view_29, t_7);  view_29 = t_7 = None
        view_30 = torch.ops.aten.view.default(mm_7, [4, 512, 512, 64]);  mm_7 = None
        _to_copy_22 = torch.ops.aten._to_copy.default(getitem_16, dtype = torch.bfloat16);  getitem_16 = None
        _to_copy_23 = torch.ops.aten._to_copy.default(getitem_9, dtype = torch.bfloat16);  getitem_9 = None
        t_8 = torch.ops.aten.t.default(_to_copy_22);  _to_copy_22 = None
        view_31 = torch.ops.aten.view.default(_to_copy_23, [1048576, 64]);  _to_copy_23 = None
        mm_8 = torch.ops.aten.mm.default(view_31, t_8);  view_31 = t_8 = None
        view_32 = torch.ops.aten.view.default(mm_8, [4, 512, 512, 64]);  mm_8 = None
        sigmoid_2 = torch.ops.aten.sigmoid.default(view_32);  view_32 = None
        mul_2 = torch.ops.aten.mul.Tensor(view_30, sigmoid_2);  view_30 = sigmoid_2 = None
        add_4 = torch.ops.aten.add.Tensor(add_2, mul_2);  mul_2 = None
        _to_copy_24 = torch.ops.aten._to_copy.default(add_2, dtype = torch.float32)
        native_layer_norm_default_6 = torch.ops.aten.native_layer_norm.default(_to_copy_24, [64], None, None, 1e-05);  _to_copy_24 = None
        getitem_35 = native_layer_norm_default_6[0]
        _to_copy_25 = torch.ops.aten._to_copy.default(arg10_1, dtype = torch.bfloat16);  arg10_1 = None
        _to_copy_26 = torch.ops.aten._to_copy.default(getitem_35, dtype = torch.bfloat16)
        t_9 = torch.ops.aten.t.default(_to_copy_25);  _to_copy_25 = None
        view_33 = torch.ops.aten.view.default(_to_copy_26, [1048576, 64]);  _to_copy_26 = None
        mm_9 = torch.ops.aten.mm.default(view_33, t_9);  view_33 = t_9 = None
        view_34 = torch.ops.aten.view.default(mm_9, [4, 512, 512, 8]);  mm_9 = None
        view_35 = torch.ops.aten.view.default(view_34, [4, 512, 512, 2, 4]);  view_34 = None
        permute_18 = torch.ops.aten.permute.default(view_35, [0, 3, 4, 1, 2]);  view_35 = None
        view_36 = torch.ops.aten.view.default(permute_18, [4, 2, 4, 1, 512, 512]);  permute_18 = None
        view_37 = torch.ops.aten.view.default(bitwise_and, [4, 1, 1, 1, 512, 512])
        bitwise_not_2 = torch.ops.aten.bitwise_not.default(view_37);  view_37 = None
        masked_fill_2 = torch.ops.aten.masked_fill.Scalar(view_36, bitwise_not_2, -10000);  view_36 = bitwise_not_2 = None
        view_38 = torch.ops.aten.view.default(masked_fill_2, [4, 2, 4, 512, 512]);  masked_fill_2 = None
        permute_19 = torch.ops.aten.permute.default(view_38, [1, 0, 2, 3, 4]);  view_38 = None
        clone_6 = torch.ops.aten.clone.default(permute_19, memory_format = torch.contiguous_format);  permute_19 = None
        _unsafe_view_4 = torch.ops.aten._unsafe_view.default(clone_6, [2, 16, 1, 512, 512]);  clone_6 = None
        _to_copy_27 = torch.ops.aten._to_copy.default(arg11_1, dtype = torch.bfloat16);  arg11_1 = None
        _to_copy_28 = torch.ops.aten._to_copy.default(getitem_35, dtype = torch.bfloat16)
        t_10 = torch.ops.aten.t.default(_to_copy_27);  _to_copy_27 = None
        view_39 = torch.ops.aten.view.default(_to_copy_28, [1048576, 64]);  _to_copy_28 = None
        mm_10 = torch.ops.aten.mm.default(view_39, t_10);  view_39 = t_10 = None
        view_40 = torch.ops.aten.view.default(mm_10, [4, 512, 512, 512]);  mm_10 = None
        select_1 = torch.ops.aten.select.int(_unsafe_view_4, 0, 0)
        view_41 = torch.ops.aten.view.default(view_40, [4, 512, 512, 4, 4, 32]);  view_40 = None
        permute_20 = torch.ops.aten.permute.default(view_41, [4, 0, 3, 1, 2, 5]);  view_41 = None
        clone_7 = torch.ops.aten.clone.default(permute_20, memory_format = torch.contiguous_format);  permute_20 = None
        _unsafe_view_5 = torch.ops.aten._unsafe_view.default(clone_7, [4, 16, 512, 512, 32]);  clone_7 = None
        unbind_int = torch.ops.aten.unbind.int(_unsafe_view_5);  _unsafe_view_5 = None
        getitem_38 = unbind_int[0]
        getitem_39 = unbind_int[1]
        getitem_40 = unbind_int[2]
        getitem_41 = unbind_int[3];  unbind_int = None
        expand_3 = torch.ops.aten.expand.default(select_1, [16, 512, 512, 512]);  select_1 = None
        _scaled_dot_product_efficient_attention_default = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_38, getitem_39, getitem_40, expand_3, False);  getitem_38 = getitem_39 = getitem_40 = expand_3 = None
        getitem_42 = _scaled_dot_product_efficient_attention_default[0]
        sigmoid_3 = torch.ops.aten.sigmoid.default(getitem_41);  getitem_41 = None
        mul_3 = torch.ops.aten.mul.Tensor(getitem_42, sigmoid_3);  getitem_42 = sigmoid_3 = None
        view_42 = torch.ops.aten.view.default(mul_3, [4, 4, 512, 512, 32]);  mul_3 = None
        permute_21 = torch.ops.aten.permute.default(view_42, [0, 2, 3, 1, 4]);  view_42 = None
        clone_8 = torch.ops.aten.clone.default(permute_21, memory_format = torch.contiguous_format);  permute_21 = None
        _unsafe_view_6 = torch.ops.aten._unsafe_view.default(clone_8, [4, 512, 512, 128]);  clone_8 = None
        transpose_1 = torch.ops.aten.transpose.int(getitem_35, 1, 2);  getitem_35 = None
        _to_copy_29 = torch.ops.aten._to_copy.default(arg12_1, dtype = torch.bfloat16);  arg12_1 = None
        _to_copy_30 = torch.ops.aten._to_copy.default(transpose_1, dtype = torch.bfloat16);  transpose_1 = None
        t_11 = torch.ops.aten.t.default(_to_copy_29);  _to_copy_29 = None
        expand_4 = torch.ops.aten.expand.default(_to_copy_30, [4, 512, 512, 64]);  _to_copy_30 = None
        clone_9 = torch.ops.aten.clone.default(expand_4, memory_format = torch.contiguous_format);  expand_4 = None
        _unsafe_view_7 = torch.ops.aten._unsafe_view.default(clone_9, [2048, 512, 64]);  clone_9 = None
        expand_5 = torch.ops.aten.expand.default(t_11, [4, 512, 64, 512]);  t_11 = None
        view_43 = torch.ops.aten.view.default(expand_5, [2048, 64, 512]);  expand_5 = None
        bmm_2 = torch.ops.aten.bmm.default(_unsafe_view_7, view_43);  _unsafe_view_7 = view_43 = None
        view_44 = torch.ops.aten.view.default(bmm_2, [4, 512, 512, 512]);  bmm_2 = None
        select_2 = torch.ops.aten.select.int(_unsafe_view_4, 0, 1);  _unsafe_view_4 = None
        view_45 = torch.ops.aten.view.default(view_44, [4, 512, 512, 4, 4, 32]);  view_44 = None
        permute_22 = torch.ops.aten.permute.default(view_45, [4, 0, 3, 1, 2, 5]);  view_45 = None
        clone_10 = torch.ops.aten.clone.default(permute_22, memory_format = torch.contiguous_format);  permute_22 = None
        _unsafe_view_8 = torch.ops.aten._unsafe_view.default(clone_10, [4, 16, 512, 512, 32]);  clone_10 = None
        unbind_int_1 = torch.ops.aten.unbind.int(_unsafe_view_8);  _unsafe_view_8 = None
        getitem_46 = unbind_int_1[0]
        getitem_47 = unbind_int_1[1]
        getitem_48 = unbind_int_1[2]
        getitem_49 = unbind_int_1[3];  unbind_int_1 = None
        expand_6 = torch.ops.aten.expand.default(select_2, [16, 512, 512, 512]);  select_2 = None
        _scaled_dot_product_efficient_attention_default_1 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_46, getitem_47, getitem_48, expand_6, False);  getitem_46 = getitem_47 = getitem_48 = expand_6 = None
        getitem_50 = _scaled_dot_product_efficient_attention_default_1[0]
        sigmoid_4 = torch.ops.aten.sigmoid.default(getitem_49);  getitem_49 = None
        mul_4 = torch.ops.aten.mul.Tensor(getitem_50, sigmoid_4);  getitem_50 = sigmoid_4 = None
        view_46 = torch.ops.aten.view.default(mul_4, [4, 4, 512, 512, 32]);  mul_4 = None
        permute_23 = torch.ops.aten.permute.default(view_46, [0, 2, 3, 1, 4]);  view_46 = None
        clone_11 = torch.ops.aten.clone.default(permute_23, memory_format = torch.contiguous_format);  permute_23 = None
        _unsafe_view_9 = torch.ops.aten._unsafe_view.default(clone_11, [4, 512, 512, 128]);  clone_11 = None
        cat = torch.ops.aten.cat.default([_unsafe_view_6, _unsafe_view_9], dim = -1);  _unsafe_view_6 = _unsafe_view_9 = None
        slice_3 = torch.ops.aten.slice.Tensor(arg9_1, dim = 0, start = 0, end = 9223372036854775807);  arg9_1 = None
        unsqueeze_13 = torch.ops.aten.unsqueeze.default(slice_3, 1);  slice_3 = None
        mul_5 = torch.ops.aten.mul.Tensor(arg13_1, unsqueeze_13);  arg13_1 = unsqueeze_13 = None
        _to_copy_31 = torch.ops.aten._to_copy.default(mul_5, dtype = torch.bfloat16);  mul_5 = None
        t_12 = torch.ops.aten.t.default(_to_copy_31);  _to_copy_31 = None
        view_47 = torch.ops.aten.view.default(cat, [1048576, 256]);  cat = None
        mm_11 = torch.ops.aten.mm.default(view_47, t_12);  view_47 = t_12 = None
        view_48 = torch.ops.aten.view.default(mm_11, [4, 512, 512, 64]);  mm_11 = None
        add_5 = torch.ops.aten.add.Tensor(add_4, view_48);  add_4 = view_48 = None
        split_tensor_4 = torch.ops.aten.split.Tensor(add_2, 512, dim = -2);  add_2 = None
        getitem_54 = split_tensor_4[0];  split_tensor_4 = None
        _to_copy_32 = torch.ops.aten._to_copy.default(getitem_54, dtype = torch.float32);  getitem_54 = None
        native_layer_norm_default_7 = torch.ops.aten.native_layer_norm.default(_to_copy_32, [64], arg0_1, arg1_1, 1e-05);  _to_copy_32 = arg0_1 = arg1_1 = None
        getitem_55 = native_layer_norm_default_7[0]
        _to_copy_33 = torch.ops.aten._to_copy.default(arg2_1, dtype = torch.bfloat16);  arg2_1 = None
        _to_copy_34 = torch.ops.aten._to_copy.default(getitem_55, dtype = torch.bfloat16);  getitem_55 = None
        t_13 = torch.ops.aten.t.default(_to_copy_33);  _to_copy_33 = None
        view_49 = torch.ops.aten.view.default(_to_copy_34, [1048576, 64]);  _to_copy_34 = None
        mm_12 = torch.ops.aten.mm.default(view_49, t_13);  view_49 = t_13 = None
        view_50 = torch.ops.aten.view.default(mm_12, [4, 512, 512, 256]);  mm_12 = None
        split_tensor_5 = torch.ops.aten.split.Tensor(view_50, 128, dim = -1);  view_50 = None
        getitem_58 = split_tensor_5[0]
        getitem_59 = split_tensor_5[1];  split_tensor_5 = None
        silu = torch.ops.aten.silu.default(getitem_58);  getitem_58 = None
        mul_6 = torch.ops.aten.mul.Tensor(silu, getitem_59);  silu = getitem_59 = None
        _to_copy_35 = torch.ops.aten._to_copy.default(arg3_1, dtype = torch.bfloat16);  arg3_1 = None
        t_14 = torch.ops.aten.t.default(_to_copy_35);  _to_copy_35 = None
        view_52 = torch.ops.aten.view.default(mul_6, [1048576, 128]);  mul_6 = None
        mm_13 = torch.ops.aten.mm.default(view_52, t_14);  view_52 = t_14 = None
        view_53 = torch.ops.aten.view.default(mm_13, [4, 512, 512, 64]);  mm_13 = None
        add_6 = torch.ops.aten.add.Tensor(add_5, view_53);  add_5 = view_53 = None
        _to_copy_36 = torch.ops.aten._to_copy.default(add_6, dtype = torch.float32)
        native_layer_norm_default_8 = torch.ops.aten.native_layer_norm.default(_to_copy_36, [64], arg18_1, arg19_1, 1e-05);  _to_copy_36 = arg18_1 = arg19_1 = None
        getitem_60 = native_layer_norm_default_8[0]
        split_with_sizes_default_2 = torch.ops.aten.split_with_sizes.default(arg21_1, [128, 128]);  arg21_1 = None
        getitem_63 = split_with_sizes_default_2[0]
        getitem_64 = split_with_sizes_default_2[1];  split_with_sizes_default_2 = None
        split_with_sizes_default_3 = torch.ops.aten.split_with_sizes.default(arg22_1, [128, 128, 64]);  arg22_1 = None
        getitem_65 = split_with_sizes_default_3[0]
        getitem_66 = split_with_sizes_default_3[1]
        getitem_67 = split_with_sizes_default_3[2];  split_with_sizes_default_3 = None
        _to_copy_37 = torch.ops.aten._to_copy.default(getitem_63, dtype = torch.bfloat16);  getitem_63 = None
        _to_copy_38 = torch.ops.aten._to_copy.default(getitem_60, dtype = torch.bfloat16)
        t_15 = torch.ops.aten.t.default(_to_copy_37);  _to_copy_37 = None
        view_54 = torch.ops.aten.view.default(_to_copy_38, [1048576, 64]);  _to_copy_38 = None
        mm_14 = torch.ops.aten.mm.default(view_54, t_15);  view_54 = t_15 = None
        view_55 = torch.ops.aten.view.default(mm_14, [4, 512, 512, 128]);  mm_14 = None
        _to_copy_39 = torch.ops.aten._to_copy.default(getitem_65, dtype = torch.bfloat16);  getitem_65 = None
        _to_copy_40 = torch.ops.aten._to_copy.default(getitem_60, dtype = torch.bfloat16)
        t_16 = torch.ops.aten.t.default(_to_copy_39);  _to_copy_39 = None
        view_56 = torch.ops.aten.view.default(_to_copy_40, [1048576, 64]);  _to_copy_40 = None
        mm_15 = torch.ops.aten.mm.default(view_56, t_16);  view_56 = t_16 = None
        view_57 = torch.ops.aten.view.default(mm_15, [4, 512, 512, 128]);  mm_15 = None
        sigmoid_5 = torch.ops.aten.sigmoid.default(view_57);  view_57 = None
        mul_7 = torch.ops.aten.mul.Tensor(view_55, sigmoid_5);  view_55 = sigmoid_5 = None
        unsqueeze_14 = torch.ops.aten.unsqueeze.default(bitwise_and, 3)
        bitwise_not_3 = torch.ops.aten.bitwise_not.default(unsqueeze_14);  unsqueeze_14 = None
        masked_fill_3 = torch.ops.aten.masked_fill.Scalar(mul_7, bitwise_not_3, 0);  mul_7 = bitwise_not_3 = None
        split_tensor_6 = torch.ops.aten.split.Tensor(masked_fill_3, 64, dim = -1)
        getitem_70 = split_tensor_6[0]
        unsqueeze_17 = torch.ops.aten.unsqueeze.default(getitem_70, 4);  getitem_70 = None
        permute_27 = torch.ops.aten.permute.default(unsqueeze_17, [0, 1, 4, 3, 2]);  unsqueeze_17 = None
        permute_28 = torch.ops.aten.permute.default(permute_27, [0, 3, 1, 4, 2]);  permute_27 = None
        clone_12 = torch.ops.aten.clone.default(permute_28, memory_format = torch.contiguous_format);  permute_28 = None
        _unsafe_view_10 = torch.ops.aten._unsafe_view.default(clone_12, [256, 512, 512]);  clone_12 = None
        split_tensor_7 = torch.ops.aten.split.Tensor(masked_fill_3, 64, dim = -1);  masked_fill_3 = None
        getitem_73 = split_tensor_7[1];  split_tensor_7 = None
        unsqueeze_18 = torch.ops.aten.unsqueeze.default(getitem_73, 4);  getitem_73 = None
        permute_30 = torch.ops.aten.permute.default(unsqueeze_18, [0, 4, 1, 3, 2]);  unsqueeze_18 = None
        permute_31 = torch.ops.aten.permute.default(permute_30, [0, 3, 4, 2, 1]);  permute_30 = None
        clone_13 = torch.ops.aten.clone.default(permute_31, memory_format = torch.contiguous_format);  permute_31 = None
        _unsafe_view_11 = torch.ops.aten._unsafe_view.default(clone_13, [256, 512, 512]);  clone_13 = None
        bmm_3 = torch.ops.aten.bmm.default(_unsafe_view_10, _unsafe_view_11);  _unsafe_view_10 = _unsafe_view_11 = None
        view_58 = torch.ops.aten.view.default(bmm_3, [4, 64, 512, 1, 512]);  bmm_3 = None
        permute_32 = torch.ops.aten.permute.default(view_58, [0, 2, 4, 1, 3]);  view_58 = None
        view_59 = torch.ops.aten.view.default(permute_32, [4, 512, 512, 64]);  permute_32 = None
        _to_copy_41 = torch.ops.aten._to_copy.default(getitem_64, dtype = torch.bfloat16);  getitem_64 = None
        _to_copy_42 = torch.ops.aten._to_copy.default(getitem_60, dtype = torch.bfloat16)
        t_17 = torch.ops.aten.t.default(_to_copy_41);  _to_copy_41 = None
        view_60 = torch.ops.aten.view.default(_to_copy_42, [1048576, 64]);  _to_copy_42 = None
        mm_16 = torch.ops.aten.mm.default(view_60, t_17);  view_60 = t_17 = None
        view_61 = torch.ops.aten.view.default(mm_16, [4, 512, 512, 128]);  mm_16 = None
        _to_copy_43 = torch.ops.aten._to_copy.default(getitem_66, dtype = torch.bfloat16);  getitem_66 = None
        _to_copy_44 = torch.ops.aten._to_copy.default(getitem_60, dtype = torch.bfloat16)
        t_18 = torch.ops.aten.t.default(_to_copy_43);  _to_copy_43 = None
        view_62 = torch.ops.aten.view.default(_to_copy_44, [1048576, 64]);  _to_copy_44 = None
        mm_17 = torch.ops.aten.mm.default(view_62, t_18);  view_62 = t_18 = None
        view_63 = torch.ops.aten.view.default(mm_17, [4, 512, 512, 128]);  mm_17 = None
        sigmoid_6 = torch.ops.aten.sigmoid.default(view_63);  view_63 = None
        mul_8 = torch.ops.aten.mul.Tensor(view_61, sigmoid_6);  view_61 = sigmoid_6 = None
        view_64 = torch.ops.aten.view.default(mul_8, [1048576, 128]);  mul_8 = None
        view_65 = torch.ops.aten.view.default(view_64, [4, 512, 512, 128]);  view_64 = None
        transpose_2 = torch.ops.aten.transpose.int(bitwise_and, 1, 2)
        unsqueeze_19 = torch.ops.aten.unsqueeze.default(transpose_2, 3);  transpose_2 = None
        clone_14 = torch.ops.aten.clone.default(unsqueeze_19, memory_format = torch.contiguous_format);  unsqueeze_19 = None
        bitwise_not_4 = torch.ops.aten.bitwise_not.default(clone_14);  clone_14 = None
        masked_fill_4 = torch.ops.aten.masked_fill.Scalar(view_65, bitwise_not_4, 0);  view_65 = bitwise_not_4 = None
        view_66 = torch.ops.aten.view.default(masked_fill_4, [1048576, 128]);  masked_fill_4 = None
        view_68 = torch.ops.aten.view.default(view_66, [4, 512, 512, 128])
        split_tensor_8 = torch.ops.aten.split.Tensor(view_68, 64, dim = -1);  view_68 = None
        getitem_76 = split_tensor_8[0]
        unsqueeze_22 = torch.ops.aten.unsqueeze.default(getitem_76, 4);  getitem_76 = None
        permute_36 = torch.ops.aten.permute.default(unsqueeze_22, [0, 2, 4, 3, 1]);  unsqueeze_22 = None
        permute_37 = torch.ops.aten.permute.default(permute_36, [0, 3, 1, 4, 2]);  permute_36 = None
        clone_15 = torch.ops.aten.clone.default(permute_37, memory_format = torch.contiguous_format);  permute_37 = None
        _unsafe_view_12 = torch.ops.aten._unsafe_view.default(clone_15, [256, 512, 512]);  clone_15 = None
        view_69 = torch.ops.aten.view.default(view_66, [4, 512, 512, 128]);  view_66 = None
        split_tensor_9 = torch.ops.aten.split.Tensor(view_69, 64, dim = -1);  view_69 = None
        getitem_79 = split_tensor_9[1];  split_tensor_9 = None
        unsqueeze_23 = torch.ops.aten.unsqueeze.default(getitem_79, 4);  getitem_79 = None
        permute_39 = torch.ops.aten.permute.default(unsqueeze_23, [0, 4, 2, 3, 1]);  unsqueeze_23 = None
        permute_40 = torch.ops.aten.permute.default(permute_39, [0, 3, 4, 2, 1]);  permute_39 = None
        clone_16 = torch.ops.aten.clone.default(permute_40, memory_format = torch.contiguous_format);  permute_40 = None
        _unsafe_view_13 = torch.ops.aten._unsafe_view.default(clone_16, [256, 512, 512]);  clone_16 = None
        bmm_4 = torch.ops.aten.bmm.default(_unsafe_view_12, _unsafe_view_13);  _unsafe_view_12 = _unsafe_view_13 = None
        view_70 = torch.ops.aten.view.default(bmm_4, [4, 64, 512, 1, 512]);  bmm_4 = None
        permute_41 = torch.ops.aten.permute.default(view_70, [0, 2, 4, 1, 3]);  view_70 = None
        view_71 = torch.ops.aten.view.default(permute_41, [4, 512, 512, 64]);  permute_41 = None
        _to_copy_45 = torch.ops.aten._to_copy.default(view_59, dtype = torch.float32);  view_59 = None
        native_layer_norm_default_9 = torch.ops.aten.native_layer_norm.default(_to_copy_45, [64], None, None, 1e-05);  _to_copy_45 = None
        getitem_80 = native_layer_norm_default_9[0]
        _to_copy_46 = torch.ops.aten._to_copy.default(view_71, dtype = torch.float32);  view_71 = None
        native_layer_norm_default_10 = torch.ops.aten.native_layer_norm.default(_to_copy_46, [64], None, None, 1e-05);  _to_copy_46 = None
        getitem_83 = native_layer_norm_default_10[0]
        add_7 = torch.ops.aten.add.Tensor(getitem_80, getitem_83);  getitem_80 = getitem_83 = None
        _to_copy_47 = torch.ops.aten._to_copy.default(arg20_1, dtype = torch.bfloat16);  arg20_1 = None
        _to_copy_48 = torch.ops.aten._to_copy.default(add_7, dtype = torch.bfloat16);  add_7 = None
        t_19 = torch.ops.aten.t.default(_to_copy_47);  _to_copy_47 = None
        view_72 = torch.ops.aten.view.default(_to_copy_48, [1048576, 64]);  _to_copy_48 = None
        mm_18 = torch.ops.aten.mm.default(view_72, t_19);  view_72 = t_19 = None
        view_73 = torch.ops.aten.view.default(mm_18, [4, 512, 512, 64]);  mm_18 = None
        _to_copy_49 = torch.ops.aten._to_copy.default(getitem_67, dtype = torch.bfloat16);  getitem_67 = None
        _to_copy_50 = torch.ops.aten._to_copy.default(getitem_60, dtype = torch.bfloat16);  getitem_60 = None
        t_20 = torch.ops.aten.t.default(_to_copy_49);  _to_copy_49 = None
        view_74 = torch.ops.aten.view.default(_to_copy_50, [1048576, 64]);  _to_copy_50 = None
        mm_19 = torch.ops.aten.mm.default(view_74, t_20);  view_74 = t_20 = None
        view_75 = torch.ops.aten.view.default(mm_19, [4, 512, 512, 64]);  mm_19 = None
        sigmoid_7 = torch.ops.aten.sigmoid.default(view_75);  view_75 = None
        mul_9 = torch.ops.aten.mul.Tensor(view_73, sigmoid_7);  view_73 = sigmoid_7 = None
        add_8 = torch.ops.aten.add.Tensor(add_6, mul_9);  mul_9 = None
        _to_copy_51 = torch.ops.aten._to_copy.default(add_6, dtype = torch.float32)
        native_layer_norm_default_11 = torch.ops.aten.native_layer_norm.default(_to_copy_51, [64], None, None, 1e-05);  _to_copy_51 = None
        getitem_86 = native_layer_norm_default_11[0]
        _to_copy_52 = torch.ops.aten._to_copy.default(arg24_1, dtype = torch.bfloat16);  arg24_1 = None
        _to_copy_53 = torch.ops.aten._to_copy.default(getitem_86, dtype = torch.bfloat16)
        t_21 = torch.ops.aten.t.default(_to_copy_52);  _to_copy_52 = None
        view_76 = torch.ops.aten.view.default(_to_copy_53, [1048576, 64]);  _to_copy_53 = None
        mm_20 = torch.ops.aten.mm.default(view_76, t_21);  view_76 = t_21 = None
        view_77 = torch.ops.aten.view.default(mm_20, [4, 512, 512, 8]);  mm_20 = None
        view_78 = torch.ops.aten.view.default(view_77, [4, 512, 512, 2, 4]);  view_77 = None
        permute_42 = torch.ops.aten.permute.default(view_78, [0, 3, 4, 1, 2]);  view_78 = None
        view_79 = torch.ops.aten.view.default(permute_42, [4, 2, 4, 1, 512, 512]);  permute_42 = None
        view_80 = torch.ops.aten.view.default(bitwise_and, [4, 1, 1, 1, 512, 512])
        bitwise_not_5 = torch.ops.aten.bitwise_not.default(view_80);  view_80 = None
        masked_fill_5 = torch.ops.aten.masked_fill.Scalar(view_79, bitwise_not_5, -10000);  view_79 = bitwise_not_5 = None
        view_81 = torch.ops.aten.view.default(masked_fill_5, [4, 2, 4, 512, 512]);  masked_fill_5 = None
        permute_43 = torch.ops.aten.permute.default(view_81, [1, 0, 2, 3, 4]);  view_81 = None
        clone_17 = torch.ops.aten.clone.default(permute_43, memory_format = torch.contiguous_format);  permute_43 = None
        _unsafe_view_14 = torch.ops.aten._unsafe_view.default(clone_17, [2, 16, 1, 512, 512]);  clone_17 = None
        _to_copy_54 = torch.ops.aten._to_copy.default(arg25_1, dtype = torch.bfloat16);  arg25_1 = None
        _to_copy_55 = torch.ops.aten._to_copy.default(getitem_86, dtype = torch.bfloat16)
        t_22 = torch.ops.aten.t.default(_to_copy_54);  _to_copy_54 = None
        view_82 = torch.ops.aten.view.default(_to_copy_55, [1048576, 64]);  _to_copy_55 = None
        mm_21 = torch.ops.aten.mm.default(view_82, t_22);  view_82 = t_22 = None
        view_83 = torch.ops.aten.view.default(mm_21, [4, 512, 512, 512]);  mm_21 = None
        select_3 = torch.ops.aten.select.int(_unsafe_view_14, 0, 0)
        view_84 = torch.ops.aten.view.default(view_83, [4, 512, 512, 4, 4, 32]);  view_83 = None
        permute_44 = torch.ops.aten.permute.default(view_84, [4, 0, 3, 1, 2, 5]);  view_84 = None
        clone_18 = torch.ops.aten.clone.default(permute_44, memory_format = torch.contiguous_format);  permute_44 = None
        _unsafe_view_15 = torch.ops.aten._unsafe_view.default(clone_18, [4, 16, 512, 512, 32]);  clone_18 = None
        unbind_int_2 = torch.ops.aten.unbind.int(_unsafe_view_15);  _unsafe_view_15 = None
        getitem_89 = unbind_int_2[0]
        getitem_90 = unbind_int_2[1]
        getitem_91 = unbind_int_2[2]
        getitem_92 = unbind_int_2[3];  unbind_int_2 = None
        expand_7 = torch.ops.aten.expand.default(select_3, [16, 512, 512, 512]);  select_3 = None
        _scaled_dot_product_efficient_attention_default_2 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_89, getitem_90, getitem_91, expand_7, False);  getitem_89 = getitem_90 = getitem_91 = expand_7 = None
        getitem_93 = _scaled_dot_product_efficient_attention_default_2[0]
        sigmoid_8 = torch.ops.aten.sigmoid.default(getitem_92);  getitem_92 = None
        mul_10 = torch.ops.aten.mul.Tensor(getitem_93, sigmoid_8);  getitem_93 = sigmoid_8 = None
        view_85 = torch.ops.aten.view.default(mul_10, [4, 4, 512, 512, 32]);  mul_10 = None
        permute_45 = torch.ops.aten.permute.default(view_85, [0, 2, 3, 1, 4]);  view_85 = None
        clone_19 = torch.ops.aten.clone.default(permute_45, memory_format = torch.contiguous_format);  permute_45 = None
        _unsafe_view_16 = torch.ops.aten._unsafe_view.default(clone_19, [4, 512, 512, 128]);  clone_19 = None
        transpose_3 = torch.ops.aten.transpose.int(getitem_86, 1, 2);  getitem_86 = None
        _to_copy_56 = torch.ops.aten._to_copy.default(arg26_1, dtype = torch.bfloat16);  arg26_1 = None
        _to_copy_57 = torch.ops.aten._to_copy.default(transpose_3, dtype = torch.bfloat16);  transpose_3 = None
        t_23 = torch.ops.aten.t.default(_to_copy_56);  _to_copy_56 = None
        expand_8 = torch.ops.aten.expand.default(_to_copy_57, [4, 512, 512, 64]);  _to_copy_57 = None
        clone_20 = torch.ops.aten.clone.default(expand_8, memory_format = torch.contiguous_format);  expand_8 = None
        _unsafe_view_17 = torch.ops.aten._unsafe_view.default(clone_20, [2048, 512, 64]);  clone_20 = None
        expand_9 = torch.ops.aten.expand.default(t_23, [4, 512, 64, 512]);  t_23 = None
        view_86 = torch.ops.aten.view.default(expand_9, [2048, 64, 512]);  expand_9 = None
        bmm_5 = torch.ops.aten.bmm.default(_unsafe_view_17, view_86);  _unsafe_view_17 = view_86 = None
        view_87 = torch.ops.aten.view.default(bmm_5, [4, 512, 512, 512]);  bmm_5 = None
        select_4 = torch.ops.aten.select.int(_unsafe_view_14, 0, 1);  _unsafe_view_14 = None
        view_88 = torch.ops.aten.view.default(view_87, [4, 512, 512, 4, 4, 32]);  view_87 = None
        permute_46 = torch.ops.aten.permute.default(view_88, [4, 0, 3, 1, 2, 5]);  view_88 = None
        clone_21 = torch.ops.aten.clone.default(permute_46, memory_format = torch.contiguous_format);  permute_46 = None
        _unsafe_view_18 = torch.ops.aten._unsafe_view.default(clone_21, [4, 16, 512, 512, 32]);  clone_21 = None
        unbind_int_3 = torch.ops.aten.unbind.int(_unsafe_view_18);  _unsafe_view_18 = None
        getitem_97 = unbind_int_3[0]
        getitem_98 = unbind_int_3[1]
        getitem_99 = unbind_int_3[2]
        getitem_100 = unbind_int_3[3];  unbind_int_3 = None
        expand_10 = torch.ops.aten.expand.default(select_4, [16, 512, 512, 512]);  select_4 = None
        _scaled_dot_product_efficient_attention_default_3 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_97, getitem_98, getitem_99, expand_10, False);  getitem_97 = getitem_98 = getitem_99 = expand_10 = None
        getitem_101 = _scaled_dot_product_efficient_attention_default_3[0]
        sigmoid_9 = torch.ops.aten.sigmoid.default(getitem_100);  getitem_100 = None
        mul_11 = torch.ops.aten.mul.Tensor(getitem_101, sigmoid_9);  getitem_101 = sigmoid_9 = None
        view_89 = torch.ops.aten.view.default(mul_11, [4, 4, 512, 512, 32]);  mul_11 = None
        permute_47 = torch.ops.aten.permute.default(view_89, [0, 2, 3, 1, 4]);  view_89 = None
        clone_22 = torch.ops.aten.clone.default(permute_47, memory_format = torch.contiguous_format);  permute_47 = None
        _unsafe_view_19 = torch.ops.aten._unsafe_view.default(clone_22, [4, 512, 512, 128]);  clone_22 = None
        cat_1 = torch.ops.aten.cat.default([_unsafe_view_16, _unsafe_view_19], dim = -1);  _unsafe_view_16 = _unsafe_view_19 = None
        slice_4 = torch.ops.aten.slice.Tensor(arg23_1, dim = 0, start = 0, end = 9223372036854775807);  arg23_1 = None
        unsqueeze_24 = torch.ops.aten.unsqueeze.default(slice_4, 1);  slice_4 = None
        mul_12 = torch.ops.aten.mul.Tensor(arg27_1, unsqueeze_24);  arg27_1 = unsqueeze_24 = None
        _to_copy_58 = torch.ops.aten._to_copy.default(mul_12, dtype = torch.bfloat16);  mul_12 = None
        t_24 = torch.ops.aten.t.default(_to_copy_58);  _to_copy_58 = None
        view_90 = torch.ops.aten.view.default(cat_1, [1048576, 256]);  cat_1 = None
        mm_22 = torch.ops.aten.mm.default(view_90, t_24);  view_90 = t_24 = None
        view_91 = torch.ops.aten.view.default(mm_22, [4, 512, 512, 64]);  mm_22 = None
        add_9 = torch.ops.aten.add.Tensor(add_8, view_91);  add_8 = view_91 = None
        split_tensor_10 = torch.ops.aten.split.Tensor(add_6, 512, dim = -2);  add_6 = None
        getitem_105 = split_tensor_10[0];  split_tensor_10 = None
        _to_copy_59 = torch.ops.aten._to_copy.default(getitem_105, dtype = torch.float32);  getitem_105 = None
        native_layer_norm_default_12 = torch.ops.aten.native_layer_norm.default(_to_copy_59, [64], arg14_1, arg15_1, 1e-05);  _to_copy_59 = arg14_1 = arg15_1 = None
        getitem_106 = native_layer_norm_default_12[0]
        _to_copy_60 = torch.ops.aten._to_copy.default(arg16_1, dtype = torch.bfloat16);  arg16_1 = None
        _to_copy_61 = torch.ops.aten._to_copy.default(getitem_106, dtype = torch.bfloat16);  getitem_106 = None
        t_25 = torch.ops.aten.t.default(_to_copy_60);  _to_copy_60 = None
        view_92 = torch.ops.aten.view.default(_to_copy_61, [1048576, 64]);  _to_copy_61 = None
        mm_23 = torch.ops.aten.mm.default(view_92, t_25);  view_92 = t_25 = None
        view_93 = torch.ops.aten.view.default(mm_23, [4, 512, 512, 256]);  mm_23 = None
        split_tensor_11 = torch.ops.aten.split.Tensor(view_93, 128, dim = -1);  view_93 = None
        getitem_109 = split_tensor_11[0]
        getitem_110 = split_tensor_11[1];  split_tensor_11 = None
        silu_1 = torch.ops.aten.silu.default(getitem_109);  getitem_109 = None
        mul_13 = torch.ops.aten.mul.Tensor(silu_1, getitem_110);  silu_1 = getitem_110 = None
        _to_copy_62 = torch.ops.aten._to_copy.default(arg17_1, dtype = torch.bfloat16);  arg17_1 = None
        t_26 = torch.ops.aten.t.default(_to_copy_62);  _to_copy_62 = None
        view_95 = torch.ops.aten.view.default(mul_13, [1048576, 128]);  mul_13 = None
        mm_24 = torch.ops.aten.mm.default(view_95, t_26);  view_95 = t_26 = None
        view_96 = torch.ops.aten.view.default(mm_24, [4, 512, 512, 64]);  mm_24 = None
        add_10 = torch.ops.aten.add.Tensor(add_9, view_96);  add_9 = view_96 = None
        view_97 = torch.ops.aten.view.default(add_10, [1, 4, 512, 512, 64]);  add_10 = None
        view_98 = torch.ops.aten.view.default(bitwise_and, [1, 4, 512, 512]);  bitwise_and = None
        _to_copy_63 = torch.ops.aten._to_copy.default(view_97, dtype = torch.float32);  view_97 = None
        native_layer_norm_default_13 = torch.ops.aten.native_layer_norm.default(_to_copy_63, [64], arg31_1, arg32_1, 1e-05);  _to_copy_63 = arg31_1 = arg32_1 = None
        getitem_111 = native_layer_norm_default_13[0]
        unsqueeze_25 = torch.ops.aten.unsqueeze.default(view_98, -1);  view_98 = None
        mul_14 = torch.ops.aten.mul.Tensor(getitem_111, unsqueeze_25);  getitem_111 = unsqueeze_25 = None
        sum_2 = torch.ops.aten.sum.dim_IntList(mul_14, [1], dtype = torch.float32);  mul_14 = None
        clamp_min = torch.ops.aten.clamp_min.default(sum_1, 1);  sum_1 = None
        view_99 = torch.ops.aten.view.default(clamp_min, [1, 1, 1, 1]);  clamp_min = None
        div = torch.ops.aten.div.Tensor(sum_2, view_99);  sum_2 = view_99 = None
        relu = torch.ops.aten.relu.default(div);  div = None
        _to_copy_64 = torch.ops.aten._to_copy.default(arg33_1, dtype = torch.bfloat16);  arg33_1 = None
        _to_copy_65 = torch.ops.aten._to_copy.default(relu, dtype = torch.bfloat16);  relu = None
        t_27 = torch.ops.aten.t.default(_to_copy_64);  _to_copy_64 = None
        view_100 = torch.ops.aten.view.default(_to_copy_65, [262144, 64]);  _to_copy_65 = None
        mm_25 = torch.ops.aten.mm.default(view_100, t_27);  view_100 = t_27 = None
        view_101 = torch.ops.aten.view.default(mm_25, [1, 512, 512, 256]);  mm_25 = None
        add_11 = torch.ops.aten.add.Tensor(add, view_101);  add = view_101 = None
        _to_copy_66 = torch.ops.aten._to_copy.default(arg34_1, dtype = torch.bfloat16);  arg34_1 = None
        t_28 = torch.ops.aten.t.default(_to_copy_66);  _to_copy_66 = None
        view_102 = torch.ops.aten.view.default(add_1, [512, 384])
        mm_26 = torch.ops.aten.mm.default(view_102, t_28);  view_102 = t_28 = None
        view_103 = torch.ops.aten.view.default(mm_26, [1, 512, 64]);  mm_26 = None
        view_104 = torch.ops.aten.view.default(view_103, [1, 1, 512, 64]);  view_103 = None
        add_12 = torch.ops.aten.add.Tensor(arg1402_1, view_104);  arg1402_1 = view_104 = None
        slice_5 = torch.ops.aten.slice.Tensor(add_12, dim = 0, start = 0, end = 9223372036854775807)
        slice_6 = torch.ops.aten.slice.Tensor(slice_5, dim = 1, start = 0, end = 4096);  slice_5 = None
        slice_7 = torch.ops.aten.slice.Tensor(slice_6, dim = 2, start = 0, end = 9223372036854775807);  slice_6 = None
        slice_8 = torch.ops.aten.slice.Tensor(slice_7, dim = 3, start = 0, end = 9223372036854775807);  slice_7 = None
        slice_9 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
        slice_10 = torch.ops.aten.slice.Tensor(slice_9, dim = 1, start = 0, end = 4096);  slice_9 = None
        slice_11 = torch.ops.aten.slice.Tensor(slice_10, dim = 2, start = 0, end = 9223372036854775807);  slice_10 = None
        _to_copy_67 = torch.ops.aten._to_copy.default(slice_8, dtype = torch.float32);  slice_8 = None
        native_layer_norm_default_14 = torch.ops.aten.native_layer_norm.default(_to_copy_67, [64], None, None, 1e-05);  _to_copy_67 = None
        getitem_114 = native_layer_norm_default_14[0]
        view_105 = torch.ops.aten.view.default(slice_11, [1, 4096, 512, 1]);  slice_11 = None
        bitwise_not_6 = torch.ops.aten.bitwise_not.default(view_105);  view_105 = None
        masked_fill_6 = torch.ops.aten.masked_fill.Scalar(getitem_114, bitwise_not_6, 0);  getitem_114 = bitwise_not_6 = None
        unbind_int_4 = torch.ops.aten.unbind.int(arg35_1)
        getitem_117 = unbind_int_4[0]
        getitem_118 = unbind_int_4[1];  unbind_int_4 = None
        _to_copy_68 = torch.ops.aten._to_copy.default(getitem_117, dtype = torch.bfloat16);  getitem_117 = None
        _to_copy_69 = torch.ops.aten._to_copy.default(masked_fill_6, dtype = torch.bfloat16)
        unsqueeze_26 = torch.ops.aten.unsqueeze.default(_to_copy_68, 3);  _to_copy_68 = None
        unsqueeze_27 = torch.ops.aten.unsqueeze.default(unsqueeze_26, 4);  unsqueeze_26 = None
        unsqueeze_28 = torch.ops.aten.unsqueeze.default(unsqueeze_27, 5);  unsqueeze_27 = None
        permute_48 = torch.ops.aten.permute.default(unsqueeze_28, [0, 1, 3, 4, 5, 2]);  unsqueeze_28 = None
        unsqueeze_29 = torch.ops.aten.unsqueeze.default(_to_copy_69, 4);  _to_copy_69 = None
        unsqueeze_30 = torch.ops.aten.unsqueeze.default(unsqueeze_29, 5);  unsqueeze_29 = None
        permute_49 = torch.ops.aten.permute.default(unsqueeze_30, [4, 5, 0, 1, 2, 3]);  unsqueeze_30 = None
        permute_50 = torch.ops.aten.permute.default(permute_48, [0, 1, 5, 2, 3, 4]);  permute_48 = None
        view_106 = torch.ops.aten.view.default(permute_50, [1, 64, 64]);  permute_50 = None
        permute_51 = torch.ops.aten.permute.default(permute_49, [5, 2, 3, 4, 0, 1]);  permute_49 = None
        view_107 = torch.ops.aten.view.default(permute_51, [1, 64, 2097152]);  permute_51 = None
        bmm_6 = torch.ops.aten.bmm.default(view_106, view_107);  view_106 = view_107 = None
        view_108 = torch.ops.aten.view.default(bmm_6, [8, 8, 1, 1, 4096, 512]);  bmm_6 = None
        permute_52 = torch.ops.aten.permute.default(view_108, [0, 1, 3, 4, 5, 2]);  view_108 = None
        view_109 = torch.ops.aten.view.default(permute_52, [8, 8, 1, 4096, 512]);  permute_52 = None
        _to_copy_70 = torch.ops.aten._to_copy.default(getitem_118, dtype = torch.bfloat16);  getitem_118 = None
        _to_copy_71 = torch.ops.aten._to_copy.default(masked_fill_6, dtype = torch.bfloat16);  masked_fill_6 = None
        unsqueeze_31 = torch.ops.aten.unsqueeze.default(_to_copy_70, 3);  _to_copy_70 = None
        unsqueeze_32 = torch.ops.aten.unsqueeze.default(unsqueeze_31, 4);  unsqueeze_31 = None
        unsqueeze_33 = torch.ops.aten.unsqueeze.default(unsqueeze_32, 5);  unsqueeze_32 = None
        permute_53 = torch.ops.aten.permute.default(unsqueeze_33, [0, 1, 3, 4, 5, 2]);  unsqueeze_33 = None
        unsqueeze_34 = torch.ops.aten.unsqueeze.default(_to_copy_71, 4);  _to_copy_71 = None
        unsqueeze_35 = torch.ops.aten.unsqueeze.default(unsqueeze_34, 5);  unsqueeze_34 = None
        permute_54 = torch.ops.aten.permute.default(unsqueeze_35, [4, 5, 0, 1, 2, 3]);  unsqueeze_35 = None
        permute_55 = torch.ops.aten.permute.default(permute_53, [0, 1, 5, 2, 3, 4]);  permute_53 = None
        view_110 = torch.ops.aten.view.default(permute_55, [1, 64, 64]);  permute_55 = None
        permute_56 = torch.ops.aten.permute.default(permute_54, [5, 2, 3, 4, 0, 1]);  permute_54 = None
        view_111 = torch.ops.aten.view.default(permute_56, [1, 64, 2097152]);  permute_56 = None
        bmm_7 = torch.ops.aten.bmm.default(view_110, view_111);  view_110 = view_111 = None
        view_112 = torch.ops.aten.view.default(bmm_7, [8, 8, 1, 1, 4096, 512]);  bmm_7 = None
        permute_57 = torch.ops.aten.permute.default(view_112, [0, 1, 3, 4, 5, 2]);  view_112 = None
        view_113 = torch.ops.aten.view.default(permute_57, [8, 8, 1, 4096, 512]);  permute_57 = None
        unsqueeze_36 = torch.ops.aten.unsqueeze.default(view_109, 5);  view_109 = None
        unsqueeze_37 = torch.ops.aten.unsqueeze.default(unsqueeze_36, 6);  unsqueeze_36 = None
        permute_58 = torch.ops.aten.permute.default(unsqueeze_37, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_37 = None
        unsqueeze_38 = torch.ops.aten.unsqueeze.default(view_113, 5);  view_113 = None
        unsqueeze_39 = torch.ops.aten.unsqueeze.default(unsqueeze_38, 6);  unsqueeze_38 = None
        permute_59 = torch.ops.aten.permute.default(unsqueeze_39, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_39 = None
        permute_60 = torch.ops.aten.permute.default(permute_58, [3, 1, 4, 6, 0, 2, 5]);  permute_58 = None
        clone_23 = torch.ops.aten.clone.default(permute_60, memory_format = torch.contiguous_format);  permute_60 = None
        _unsafe_view_20 = torch.ops.aten._unsafe_view.default(clone_23, [8, 4096, 4096]);  clone_23 = None
        permute_61 = torch.ops.aten.permute.default(permute_59, [3, 6, 0, 2, 5, 1, 4]);  permute_59 = None
        clone_24 = torch.ops.aten.clone.default(permute_61, memory_format = torch.contiguous_format);  permute_61 = None
        _unsafe_view_21 = torch.ops.aten._unsafe_view.default(clone_24, [8, 4096, 4096]);  clone_24 = None
        bmm_8 = torch.ops.aten.bmm.default(_unsafe_view_20, _unsafe_view_21);  _unsafe_view_20 = _unsafe_view_21 = None
        view_114 = torch.ops.aten.view.default(bmm_8, [8, 512, 8, 1, 1, 512, 8]);  bmm_8 = None
        permute_62 = torch.ops.aten.permute.default(view_114, [4, 1, 5, 0, 2, 6, 3]);  view_114 = None
        view_115 = torch.ops.aten.view.default(permute_62, [1, 512, 512, 8, 8, 8]);  permute_62 = None
        clone_25 = torch.ops.aten.clone.default(view_115, memory_format = torch.contiguous_format);  view_115 = None
        _unsafe_view_22 = torch.ops.aten._unsafe_view.default(clone_25, [1, 512, 512, 512]);  clone_25 = None
        add_13 = torch.ops.aten.add.Tensor(_unsafe_view_22, 0);  _unsafe_view_22 = None
        slice_12 = torch.ops.aten.slice.Tensor(add_12, dim = 0, start = 0, end = 9223372036854775807)
        slice_13 = torch.ops.aten.slice.Tensor(slice_12, dim = 1, start = 4096, end = 8192);  slice_12 = None
        slice_14 = torch.ops.aten.slice.Tensor(slice_13, dim = 2, start = 0, end = 9223372036854775807);  slice_13 = None
        slice_15 = torch.ops.aten.slice.Tensor(slice_14, dim = 3, start = 0, end = 9223372036854775807);  slice_14 = None
        slice_16 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
        slice_17 = torch.ops.aten.slice.Tensor(slice_16, dim = 1, start = 4096, end = 8192);  slice_16 = None
        slice_18 = torch.ops.aten.slice.Tensor(slice_17, dim = 2, start = 0, end = 9223372036854775807);  slice_17 = None
        _to_copy_72 = torch.ops.aten._to_copy.default(slice_15, dtype = torch.float32);  slice_15 = None
        native_layer_norm_default_15 = torch.ops.aten.native_layer_norm.default(_to_copy_72, [64], None, None, 1e-05);  _to_copy_72 = None
        getitem_119 = native_layer_norm_default_15[0]
        view_116 = torch.ops.aten.view.default(slice_18, [1, 4096, 512, 1]);  slice_18 = None
        bitwise_not_7 = torch.ops.aten.bitwise_not.default(view_116);  view_116 = None
        masked_fill_7 = torch.ops.aten.masked_fill.Scalar(getitem_119, bitwise_not_7, 0);  getitem_119 = bitwise_not_7 = None
        unbind_int_5 = torch.ops.aten.unbind.int(arg35_1)
        getitem_122 = unbind_int_5[0]
        getitem_123 = unbind_int_5[1];  unbind_int_5 = None
        _to_copy_73 = torch.ops.aten._to_copy.default(getitem_122, dtype = torch.bfloat16);  getitem_122 = None
        _to_copy_74 = torch.ops.aten._to_copy.default(masked_fill_7, dtype = torch.bfloat16)
        unsqueeze_40 = torch.ops.aten.unsqueeze.default(_to_copy_73, 3);  _to_copy_73 = None
        unsqueeze_41 = torch.ops.aten.unsqueeze.default(unsqueeze_40, 4);  unsqueeze_40 = None
        unsqueeze_42 = torch.ops.aten.unsqueeze.default(unsqueeze_41, 5);  unsqueeze_41 = None
        permute_63 = torch.ops.aten.permute.default(unsqueeze_42, [0, 1, 3, 4, 5, 2]);  unsqueeze_42 = None
        unsqueeze_43 = torch.ops.aten.unsqueeze.default(_to_copy_74, 4);  _to_copy_74 = None
        unsqueeze_44 = torch.ops.aten.unsqueeze.default(unsqueeze_43, 5);  unsqueeze_43 = None
        permute_64 = torch.ops.aten.permute.default(unsqueeze_44, [4, 5, 0, 1, 2, 3]);  unsqueeze_44 = None
        permute_65 = torch.ops.aten.permute.default(permute_63, [0, 1, 5, 2, 3, 4]);  permute_63 = None
        view_117 = torch.ops.aten.view.default(permute_65, [1, 64, 64]);  permute_65 = None
        permute_66 = torch.ops.aten.permute.default(permute_64, [5, 2, 3, 4, 0, 1]);  permute_64 = None
        view_118 = torch.ops.aten.view.default(permute_66, [1, 64, 2097152]);  permute_66 = None
        bmm_9 = torch.ops.aten.bmm.default(view_117, view_118);  view_117 = view_118 = None
        view_119 = torch.ops.aten.view.default(bmm_9, [8, 8, 1, 1, 4096, 512]);  bmm_9 = None
        permute_67 = torch.ops.aten.permute.default(view_119, [0, 1, 3, 4, 5, 2]);  view_119 = None
        view_120 = torch.ops.aten.view.default(permute_67, [8, 8, 1, 4096, 512]);  permute_67 = None
        _to_copy_75 = torch.ops.aten._to_copy.default(getitem_123, dtype = torch.bfloat16);  getitem_123 = None
        _to_copy_76 = torch.ops.aten._to_copy.default(masked_fill_7, dtype = torch.bfloat16);  masked_fill_7 = None
        unsqueeze_45 = torch.ops.aten.unsqueeze.default(_to_copy_75, 3);  _to_copy_75 = None
        unsqueeze_46 = torch.ops.aten.unsqueeze.default(unsqueeze_45, 4);  unsqueeze_45 = None
        unsqueeze_47 = torch.ops.aten.unsqueeze.default(unsqueeze_46, 5);  unsqueeze_46 = None
        permute_68 = torch.ops.aten.permute.default(unsqueeze_47, [0, 1, 3, 4, 5, 2]);  unsqueeze_47 = None
        unsqueeze_48 = torch.ops.aten.unsqueeze.default(_to_copy_76, 4);  _to_copy_76 = None
        unsqueeze_49 = torch.ops.aten.unsqueeze.default(unsqueeze_48, 5);  unsqueeze_48 = None
        permute_69 = torch.ops.aten.permute.default(unsqueeze_49, [4, 5, 0, 1, 2, 3]);  unsqueeze_49 = None
        permute_70 = torch.ops.aten.permute.default(permute_68, [0, 1, 5, 2, 3, 4]);  permute_68 = None
        view_121 = torch.ops.aten.view.default(permute_70, [1, 64, 64]);  permute_70 = None
        permute_71 = torch.ops.aten.permute.default(permute_69, [5, 2, 3, 4, 0, 1]);  permute_69 = None
        view_122 = torch.ops.aten.view.default(permute_71, [1, 64, 2097152]);  permute_71 = None
        bmm_10 = torch.ops.aten.bmm.default(view_121, view_122);  view_121 = view_122 = None
        view_123 = torch.ops.aten.view.default(bmm_10, [8, 8, 1, 1, 4096, 512]);  bmm_10 = None
        permute_72 = torch.ops.aten.permute.default(view_123, [0, 1, 3, 4, 5, 2]);  view_123 = None
        view_124 = torch.ops.aten.view.default(permute_72, [8, 8, 1, 4096, 512]);  permute_72 = None
        unsqueeze_50 = torch.ops.aten.unsqueeze.default(view_120, 5);  view_120 = None
        unsqueeze_51 = torch.ops.aten.unsqueeze.default(unsqueeze_50, 6);  unsqueeze_50 = None
        permute_73 = torch.ops.aten.permute.default(unsqueeze_51, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_51 = None
        unsqueeze_52 = torch.ops.aten.unsqueeze.default(view_124, 5);  view_124 = None
        unsqueeze_53 = torch.ops.aten.unsqueeze.default(unsqueeze_52, 6);  unsqueeze_52 = None
        permute_74 = torch.ops.aten.permute.default(unsqueeze_53, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_53 = None
        permute_75 = torch.ops.aten.permute.default(permute_73, [3, 1, 4, 6, 0, 2, 5]);  permute_73 = None
        clone_26 = torch.ops.aten.clone.default(permute_75, memory_format = torch.contiguous_format);  permute_75 = None
        _unsafe_view_23 = torch.ops.aten._unsafe_view.default(clone_26, [8, 4096, 4096]);  clone_26 = None
        permute_76 = torch.ops.aten.permute.default(permute_74, [3, 6, 0, 2, 5, 1, 4]);  permute_74 = None
        clone_27 = torch.ops.aten.clone.default(permute_76, memory_format = torch.contiguous_format);  permute_76 = None
        _unsafe_view_24 = torch.ops.aten._unsafe_view.default(clone_27, [8, 4096, 4096]);  clone_27 = None
        bmm_11 = torch.ops.aten.bmm.default(_unsafe_view_23, _unsafe_view_24);  _unsafe_view_23 = _unsafe_view_24 = None
        view_125 = torch.ops.aten.view.default(bmm_11, [8, 512, 8, 1, 1, 512, 8]);  bmm_11 = None
        permute_77 = torch.ops.aten.permute.default(view_125, [4, 1, 5, 0, 2, 6, 3]);  view_125 = None
        view_126 = torch.ops.aten.view.default(permute_77, [1, 512, 512, 8, 8, 8]);  permute_77 = None
        clone_28 = torch.ops.aten.clone.default(view_126, memory_format = torch.contiguous_format);  view_126 = None
        _unsafe_view_25 = torch.ops.aten._unsafe_view.default(clone_28, [1, 512, 512, 512]);  clone_28 = None
        add_14 = torch.ops.aten.add.Tensor(add_13, _unsafe_view_25);  add_13 = _unsafe_view_25 = None
        slice_19 = torch.ops.aten.slice.Tensor(add_12, dim = 0, start = 0, end = 9223372036854775807)
        slice_20 = torch.ops.aten.slice.Tensor(slice_19, dim = 1, start = 8192, end = 12288);  slice_19 = None
        slice_21 = torch.ops.aten.slice.Tensor(slice_20, dim = 2, start = 0, end = 9223372036854775807);  slice_20 = None
        slice_22 = torch.ops.aten.slice.Tensor(slice_21, dim = 3, start = 0, end = 9223372036854775807);  slice_21 = None
        slice_23 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
        slice_24 = torch.ops.aten.slice.Tensor(slice_23, dim = 1, start = 8192, end = 12288);  slice_23 = None
        slice_25 = torch.ops.aten.slice.Tensor(slice_24, dim = 2, start = 0, end = 9223372036854775807);  slice_24 = None
        _to_copy_77 = torch.ops.aten._to_copy.default(slice_22, dtype = torch.float32);  slice_22 = None
        native_layer_norm_default_16 = torch.ops.aten.native_layer_norm.default(_to_copy_77, [64], None, None, 1e-05);  _to_copy_77 = None
        getitem_124 = native_layer_norm_default_16[0]
        view_127 = torch.ops.aten.view.default(slice_25, [1, 4096, 512, 1]);  slice_25 = None
        bitwise_not_8 = torch.ops.aten.bitwise_not.default(view_127);  view_127 = None
        masked_fill_8 = torch.ops.aten.masked_fill.Scalar(getitem_124, bitwise_not_8, 0);  getitem_124 = bitwise_not_8 = None
        unbind_int_6 = torch.ops.aten.unbind.int(arg35_1)
        getitem_127 = unbind_int_6[0]
        getitem_128 = unbind_int_6[1];  unbind_int_6 = None
        _to_copy_78 = torch.ops.aten._to_copy.default(getitem_127, dtype = torch.bfloat16);  getitem_127 = None
        _to_copy_79 = torch.ops.aten._to_copy.default(masked_fill_8, dtype = torch.bfloat16)
        unsqueeze_54 = torch.ops.aten.unsqueeze.default(_to_copy_78, 3);  _to_copy_78 = None
        unsqueeze_55 = torch.ops.aten.unsqueeze.default(unsqueeze_54, 4);  unsqueeze_54 = None
        unsqueeze_56 = torch.ops.aten.unsqueeze.default(unsqueeze_55, 5);  unsqueeze_55 = None
        permute_78 = torch.ops.aten.permute.default(unsqueeze_56, [0, 1, 3, 4, 5, 2]);  unsqueeze_56 = None
        unsqueeze_57 = torch.ops.aten.unsqueeze.default(_to_copy_79, 4);  _to_copy_79 = None
        unsqueeze_58 = torch.ops.aten.unsqueeze.default(unsqueeze_57, 5);  unsqueeze_57 = None
        permute_79 = torch.ops.aten.permute.default(unsqueeze_58, [4, 5, 0, 1, 2, 3]);  unsqueeze_58 = None
        permute_80 = torch.ops.aten.permute.default(permute_78, [0, 1, 5, 2, 3, 4]);  permute_78 = None
        view_128 = torch.ops.aten.view.default(permute_80, [1, 64, 64]);  permute_80 = None
        permute_81 = torch.ops.aten.permute.default(permute_79, [5, 2, 3, 4, 0, 1]);  permute_79 = None
        view_129 = torch.ops.aten.view.default(permute_81, [1, 64, 2097152]);  permute_81 = None
        bmm_12 = torch.ops.aten.bmm.default(view_128, view_129);  view_128 = view_129 = None
        view_130 = torch.ops.aten.view.default(bmm_12, [8, 8, 1, 1, 4096, 512]);  bmm_12 = None
        permute_82 = torch.ops.aten.permute.default(view_130, [0, 1, 3, 4, 5, 2]);  view_130 = None
        view_131 = torch.ops.aten.view.default(permute_82, [8, 8, 1, 4096, 512]);  permute_82 = None
        _to_copy_80 = torch.ops.aten._to_copy.default(getitem_128, dtype = torch.bfloat16);  getitem_128 = None
        _to_copy_81 = torch.ops.aten._to_copy.default(masked_fill_8, dtype = torch.bfloat16);  masked_fill_8 = None
        unsqueeze_59 = torch.ops.aten.unsqueeze.default(_to_copy_80, 3);  _to_copy_80 = None
        unsqueeze_60 = torch.ops.aten.unsqueeze.default(unsqueeze_59, 4);  unsqueeze_59 = None
        unsqueeze_61 = torch.ops.aten.unsqueeze.default(unsqueeze_60, 5);  unsqueeze_60 = None
        permute_83 = torch.ops.aten.permute.default(unsqueeze_61, [0, 1, 3, 4, 5, 2]);  unsqueeze_61 = None
        unsqueeze_62 = torch.ops.aten.unsqueeze.default(_to_copy_81, 4);  _to_copy_81 = None
        unsqueeze_63 = torch.ops.aten.unsqueeze.default(unsqueeze_62, 5);  unsqueeze_62 = None
        permute_84 = torch.ops.aten.permute.default(unsqueeze_63, [4, 5, 0, 1, 2, 3]);  unsqueeze_63 = None
        permute_85 = torch.ops.aten.permute.default(permute_83, [0, 1, 5, 2, 3, 4]);  permute_83 = None
        view_132 = torch.ops.aten.view.default(permute_85, [1, 64, 64]);  permute_85 = None
        permute_86 = torch.ops.aten.permute.default(permute_84, [5, 2, 3, 4, 0, 1]);  permute_84 = None
        view_133 = torch.ops.aten.view.default(permute_86, [1, 64, 2097152]);  permute_86 = None
        bmm_13 = torch.ops.aten.bmm.default(view_132, view_133);  view_132 = view_133 = None
        view_134 = torch.ops.aten.view.default(bmm_13, [8, 8, 1, 1, 4096, 512]);  bmm_13 = None
        permute_87 = torch.ops.aten.permute.default(view_134, [0, 1, 3, 4, 5, 2]);  view_134 = None
        view_135 = torch.ops.aten.view.default(permute_87, [8, 8, 1, 4096, 512]);  permute_87 = None
        unsqueeze_64 = torch.ops.aten.unsqueeze.default(view_131, 5);  view_131 = None
        unsqueeze_65 = torch.ops.aten.unsqueeze.default(unsqueeze_64, 6);  unsqueeze_64 = None
        permute_88 = torch.ops.aten.permute.default(unsqueeze_65, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_65 = None
        unsqueeze_66 = torch.ops.aten.unsqueeze.default(view_135, 5);  view_135 = None
        unsqueeze_67 = torch.ops.aten.unsqueeze.default(unsqueeze_66, 6);  unsqueeze_66 = None
        permute_89 = torch.ops.aten.permute.default(unsqueeze_67, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_67 = None
        permute_90 = torch.ops.aten.permute.default(permute_88, [3, 1, 4, 6, 0, 2, 5]);  permute_88 = None
        clone_29 = torch.ops.aten.clone.default(permute_90, memory_format = torch.contiguous_format);  permute_90 = None
        _unsafe_view_26 = torch.ops.aten._unsafe_view.default(clone_29, [8, 4096, 4096]);  clone_29 = None
        permute_91 = torch.ops.aten.permute.default(permute_89, [3, 6, 0, 2, 5, 1, 4]);  permute_89 = None
        clone_30 = torch.ops.aten.clone.default(permute_91, memory_format = torch.contiguous_format);  permute_91 = None
        _unsafe_view_27 = torch.ops.aten._unsafe_view.default(clone_30, [8, 4096, 4096]);  clone_30 = None
        bmm_14 = torch.ops.aten.bmm.default(_unsafe_view_26, _unsafe_view_27);  _unsafe_view_26 = _unsafe_view_27 = None
        view_136 = torch.ops.aten.view.default(bmm_14, [8, 512, 8, 1, 1, 512, 8]);  bmm_14 = None
        permute_92 = torch.ops.aten.permute.default(view_136, [4, 1, 5, 0, 2, 6, 3]);  view_136 = None
        view_137 = torch.ops.aten.view.default(permute_92, [1, 512, 512, 8, 8, 8]);  permute_92 = None
        clone_31 = torch.ops.aten.clone.default(view_137, memory_format = torch.contiguous_format);  view_137 = None
        _unsafe_view_28 = torch.ops.aten._unsafe_view.default(clone_31, [1, 512, 512, 512]);  clone_31 = None
        add_15 = torch.ops.aten.add.Tensor(add_14, _unsafe_view_28);  add_14 = _unsafe_view_28 = None
        slice_26 = torch.ops.aten.slice.Tensor(add_12, dim = 0, start = 0, end = 9223372036854775807)
        slice_27 = torch.ops.aten.slice.Tensor(slice_26, dim = 1, start = 12288, end = 16384);  slice_26 = None
        slice_28 = torch.ops.aten.slice.Tensor(slice_27, dim = 2, start = 0, end = 9223372036854775807);  slice_27 = None
        slice_29 = torch.ops.aten.slice.Tensor(slice_28, dim = 3, start = 0, end = 9223372036854775807);  slice_28 = None
        slice_30 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
        slice_31 = torch.ops.aten.slice.Tensor(slice_30, dim = 1, start = 12288, end = 16384);  slice_30 = None
        slice_32 = torch.ops.aten.slice.Tensor(slice_31, dim = 2, start = 0, end = 9223372036854775807);  slice_31 = None
        _to_copy_82 = torch.ops.aten._to_copy.default(slice_29, dtype = torch.float32);  slice_29 = None
        native_layer_norm_default_17 = torch.ops.aten.native_layer_norm.default(_to_copy_82, [64], None, None, 1e-05);  _to_copy_82 = None
        getitem_129 = native_layer_norm_default_17[0]
        view_138 = torch.ops.aten.view.default(slice_32, [1, 4096, 512, 1]);  slice_32 = None
        bitwise_not_9 = torch.ops.aten.bitwise_not.default(view_138);  view_138 = None
        masked_fill_9 = torch.ops.aten.masked_fill.Scalar(getitem_129, bitwise_not_9, 0);  getitem_129 = bitwise_not_9 = None
        unbind_int_7 = torch.ops.aten.unbind.int(arg35_1);  arg35_1 = None
        getitem_132 = unbind_int_7[0]
        getitem_133 = unbind_int_7[1];  unbind_int_7 = None
        _to_copy_83 = torch.ops.aten._to_copy.default(getitem_132, dtype = torch.bfloat16);  getitem_132 = None
        _to_copy_84 = torch.ops.aten._to_copy.default(masked_fill_9, dtype = torch.bfloat16)
        unsqueeze_68 = torch.ops.aten.unsqueeze.default(_to_copy_83, 3);  _to_copy_83 = None
        unsqueeze_69 = torch.ops.aten.unsqueeze.default(unsqueeze_68, 4);  unsqueeze_68 = None
        unsqueeze_70 = torch.ops.aten.unsqueeze.default(unsqueeze_69, 5);  unsqueeze_69 = None
        permute_93 = torch.ops.aten.permute.default(unsqueeze_70, [0, 1, 3, 4, 5, 2]);  unsqueeze_70 = None
        unsqueeze_71 = torch.ops.aten.unsqueeze.default(_to_copy_84, 4);  _to_copy_84 = None
        unsqueeze_72 = torch.ops.aten.unsqueeze.default(unsqueeze_71, 5);  unsqueeze_71 = None
        permute_94 = torch.ops.aten.permute.default(unsqueeze_72, [4, 5, 0, 1, 2, 3]);  unsqueeze_72 = None
        permute_95 = torch.ops.aten.permute.default(permute_93, [0, 1, 5, 2, 3, 4]);  permute_93 = None
        view_139 = torch.ops.aten.view.default(permute_95, [1, 64, 64]);  permute_95 = None
        permute_96 = torch.ops.aten.permute.default(permute_94, [5, 2, 3, 4, 0, 1]);  permute_94 = None
        view_140 = torch.ops.aten.view.default(permute_96, [1, 64, 2097152]);  permute_96 = None
        bmm_15 = torch.ops.aten.bmm.default(view_139, view_140);  view_139 = view_140 = None
        view_141 = torch.ops.aten.view.default(bmm_15, [8, 8, 1, 1, 4096, 512]);  bmm_15 = None
        permute_97 = torch.ops.aten.permute.default(view_141, [0, 1, 3, 4, 5, 2]);  view_141 = None
        view_142 = torch.ops.aten.view.default(permute_97, [8, 8, 1, 4096, 512]);  permute_97 = None
        _to_copy_85 = torch.ops.aten._to_copy.default(getitem_133, dtype = torch.bfloat16);  getitem_133 = None
        _to_copy_86 = torch.ops.aten._to_copy.default(masked_fill_9, dtype = torch.bfloat16);  masked_fill_9 = None
        unsqueeze_73 = torch.ops.aten.unsqueeze.default(_to_copy_85, 3);  _to_copy_85 = None
        unsqueeze_74 = torch.ops.aten.unsqueeze.default(unsqueeze_73, 4);  unsqueeze_73 = None
        unsqueeze_75 = torch.ops.aten.unsqueeze.default(unsqueeze_74, 5);  unsqueeze_74 = None
        permute_98 = torch.ops.aten.permute.default(unsqueeze_75, [0, 1, 3, 4, 5, 2]);  unsqueeze_75 = None
        unsqueeze_76 = torch.ops.aten.unsqueeze.default(_to_copy_86, 4);  _to_copy_86 = None
        unsqueeze_77 = torch.ops.aten.unsqueeze.default(unsqueeze_76, 5);  unsqueeze_76 = None
        permute_99 = torch.ops.aten.permute.default(unsqueeze_77, [4, 5, 0, 1, 2, 3]);  unsqueeze_77 = None
        permute_100 = torch.ops.aten.permute.default(permute_98, [0, 1, 5, 2, 3, 4]);  permute_98 = None
        view_143 = torch.ops.aten.view.default(permute_100, [1, 64, 64]);  permute_100 = None
        permute_101 = torch.ops.aten.permute.default(permute_99, [5, 2, 3, 4, 0, 1]);  permute_99 = None
        view_144 = torch.ops.aten.view.default(permute_101, [1, 64, 2097152]);  permute_101 = None
        bmm_16 = torch.ops.aten.bmm.default(view_143, view_144);  view_143 = view_144 = None
        view_145 = torch.ops.aten.view.default(bmm_16, [8, 8, 1, 1, 4096, 512]);  bmm_16 = None
        permute_102 = torch.ops.aten.permute.default(view_145, [0, 1, 3, 4, 5, 2]);  view_145 = None
        view_146 = torch.ops.aten.view.default(permute_102, [8, 8, 1, 4096, 512]);  permute_102 = None
        unsqueeze_78 = torch.ops.aten.unsqueeze.default(view_142, 5);  view_142 = None
        unsqueeze_79 = torch.ops.aten.unsqueeze.default(unsqueeze_78, 6);  unsqueeze_78 = None
        permute_103 = torch.ops.aten.permute.default(unsqueeze_79, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_79 = None
        unsqueeze_80 = torch.ops.aten.unsqueeze.default(view_146, 5);  view_146 = None
        unsqueeze_81 = torch.ops.aten.unsqueeze.default(unsqueeze_80, 6);  unsqueeze_80 = None
        permute_104 = torch.ops.aten.permute.default(unsqueeze_81, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_81 = None
        permute_105 = torch.ops.aten.permute.default(permute_103, [3, 1, 4, 6, 0, 2, 5]);  permute_103 = None
        clone_32 = torch.ops.aten.clone.default(permute_105, memory_format = torch.contiguous_format);  permute_105 = None
        _unsafe_view_29 = torch.ops.aten._unsafe_view.default(clone_32, [8, 4096, 4096]);  clone_32 = None
        permute_106 = torch.ops.aten.permute.default(permute_104, [3, 6, 0, 2, 5, 1, 4]);  permute_104 = None
        clone_33 = torch.ops.aten.clone.default(permute_106, memory_format = torch.contiguous_format);  permute_106 = None
        _unsafe_view_30 = torch.ops.aten._unsafe_view.default(clone_33, [8, 4096, 4096]);  clone_33 = None
        bmm_17 = torch.ops.aten.bmm.default(_unsafe_view_29, _unsafe_view_30);  _unsafe_view_29 = _unsafe_view_30 = None
        view_147 = torch.ops.aten.view.default(bmm_17, [8, 512, 8, 1, 1, 512, 8]);  bmm_17 = None
        permute_107 = torch.ops.aten.permute.default(view_147, [4, 1, 5, 0, 2, 6, 3]);  view_147 = None
        view_148 = torch.ops.aten.view.default(permute_107, [1, 512, 512, 8, 8, 8]);  permute_107 = None
        clone_34 = torch.ops.aten.clone.default(view_148, memory_format = torch.contiguous_format);  view_148 = None
        _unsafe_view_31 = torch.ops.aten._unsafe_view.default(clone_34, [1, 512, 512, 512]);  clone_34 = None
        add_16 = torch.ops.aten.add.Tensor(add_15, _unsafe_view_31);  add_15 = _unsafe_view_31 = None
        _to_copy_87 = torch.ops.aten._to_copy.default(add_16, dtype = torch.float32);  add_16 = None
        native_layer_norm_default_18 = torch.ops.aten.native_layer_norm.default(_to_copy_87, [512], arg36_1, arg37_1, 0.1);  _to_copy_87 = arg36_1 = arg37_1 = None
        getitem_134 = native_layer_norm_default_18[0]
        _to_copy_88 = torch.ops.aten._to_copy.default(arg39_1, dtype = torch.bfloat16);  arg39_1 = None
        _to_copy_89 = torch.ops.aten._to_copy.default(arg38_1, dtype = torch.bfloat16);  arg38_1 = None
        _to_copy_90 = torch.ops.aten._to_copy.default(getitem_134, dtype = torch.bfloat16);  getitem_134 = None
        view_149 = torch.ops.aten.view.default(_to_copy_90, [262144, 512]);  _to_copy_90 = None
        t_29 = torch.ops.aten.t.default(_to_copy_89);  _to_copy_89 = None
        addmm = torch.ops.aten.addmm.default(_to_copy_88, view_149, t_29);  _to_copy_88 = view_149 = t_29 = None
        view_150 = torch.ops.aten.view.default(addmm, [1, 512, 512, 256]);  addmm = None
        add_17 = torch.ops.aten.add.Tensor(add_11, view_150);  view_150 = None
        split_tensor_12 = torch.ops.aten.split.Tensor(add_12, 128, dim = -2)
        getitem_137 = split_tensor_12[0]
        getitem_138 = split_tensor_12[1]
        getitem_139 = split_tensor_12[2]
        getitem_140 = split_tensor_12[3];  split_tensor_12 = None
        _to_copy_91 = torch.ops.aten._to_copy.default(getitem_137, dtype = torch.float32);  getitem_137 = None
        native_layer_norm_default_19 = torch.ops.aten.native_layer_norm.default(_to_copy_91, [64], arg76_1, arg77_1, 1e-05);  _to_copy_91 = None
        getitem_141 = native_layer_norm_default_19[0]
        _to_copy_92 = torch.ops.aten._to_copy.default(arg78_1, dtype = torch.bfloat16)
        _to_copy_93 = torch.ops.aten._to_copy.default(getitem_141, dtype = torch.bfloat16);  getitem_141 = None
        t_30 = torch.ops.aten.t.default(_to_copy_92);  _to_copy_92 = None
        view_151 = torch.ops.aten.view.default(_to_copy_93, [2097152, 64]);  _to_copy_93 = None
        mm_27 = torch.ops.aten.mm.default(view_151, t_30);  view_151 = t_30 = None
        view_152 = torch.ops.aten.view.default(mm_27, [1, 16384, 128, 512]);  mm_27 = None
        split_tensor_13 = torch.ops.aten.split.Tensor(view_152, 256, dim = -1);  view_152 = None
        getitem_144 = split_tensor_13[0]
        getitem_145 = split_tensor_13[1];  split_tensor_13 = None
        silu_2 = torch.ops.aten.silu.default(getitem_144);  getitem_144 = None
        mul_15 = torch.ops.aten.mul.Tensor(silu_2, getitem_145);  silu_2 = getitem_145 = None
        _to_copy_94 = torch.ops.aten._to_copy.default(arg79_1, dtype = torch.bfloat16)
        t_31 = torch.ops.aten.t.default(_to_copy_94);  _to_copy_94 = None
        view_154 = torch.ops.aten.view.default(mul_15, [2097152, 256]);  mul_15 = None
        mm_28 = torch.ops.aten.mm.default(view_154, t_31);  view_154 = t_31 = None
        view_155 = torch.ops.aten.view.default(mm_28, [1, 16384, 128, 64]);  mm_28 = None
        _to_copy_95 = torch.ops.aten._to_copy.default(getitem_138, dtype = torch.float32);  getitem_138 = None
        native_layer_norm_default_20 = torch.ops.aten.native_layer_norm.default(_to_copy_95, [64], arg76_1, arg77_1, 1e-05);  _to_copy_95 = None
        getitem_146 = native_layer_norm_default_20[0]
        _to_copy_96 = torch.ops.aten._to_copy.default(arg78_1, dtype = torch.bfloat16)
        _to_copy_97 = torch.ops.aten._to_copy.default(getitem_146, dtype = torch.bfloat16);  getitem_146 = None
        t_32 = torch.ops.aten.t.default(_to_copy_96);  _to_copy_96 = None
        view_156 = torch.ops.aten.view.default(_to_copy_97, [2097152, 64]);  _to_copy_97 = None
        mm_29 = torch.ops.aten.mm.default(view_156, t_32);  view_156 = t_32 = None
        view_157 = torch.ops.aten.view.default(mm_29, [1, 16384, 128, 512]);  mm_29 = None
        split_tensor_14 = torch.ops.aten.split.Tensor(view_157, 256, dim = -1);  view_157 = None
        getitem_149 = split_tensor_14[0]
        getitem_150 = split_tensor_14[1];  split_tensor_14 = None
        silu_3 = torch.ops.aten.silu.default(getitem_149);  getitem_149 = None
        mul_16 = torch.ops.aten.mul.Tensor(silu_3, getitem_150);  silu_3 = getitem_150 = None
        _to_copy_98 = torch.ops.aten._to_copy.default(arg79_1, dtype = torch.bfloat16)
        t_33 = torch.ops.aten.t.default(_to_copy_98);  _to_copy_98 = None
        view_159 = torch.ops.aten.view.default(mul_16, [2097152, 256]);  mul_16 = None
        mm_30 = torch.ops.aten.mm.default(view_159, t_33);  view_159 = t_33 = None
        view_160 = torch.ops.aten.view.default(mm_30, [1, 16384, 128, 64]);  mm_30 = None
        _to_copy_99 = torch.ops.aten._to_copy.default(getitem_139, dtype = torch.float32);  getitem_139 = None
        native_layer_norm_default_21 = torch.ops.aten.native_layer_norm.default(_to_copy_99, [64], arg76_1, arg77_1, 1e-05);  _to_copy_99 = None
        getitem_151 = native_layer_norm_default_21[0]
        _to_copy_100 = torch.ops.aten._to_copy.default(arg78_1, dtype = torch.bfloat16)
        _to_copy_101 = torch.ops.aten._to_copy.default(getitem_151, dtype = torch.bfloat16);  getitem_151 = None
        t_34 = torch.ops.aten.t.default(_to_copy_100);  _to_copy_100 = None
        view_161 = torch.ops.aten.view.default(_to_copy_101, [2097152, 64]);  _to_copy_101 = None
        mm_31 = torch.ops.aten.mm.default(view_161, t_34);  view_161 = t_34 = None
        view_162 = torch.ops.aten.view.default(mm_31, [1, 16384, 128, 512]);  mm_31 = None
        split_tensor_15 = torch.ops.aten.split.Tensor(view_162, 256, dim = -1);  view_162 = None
        getitem_154 = split_tensor_15[0]
        getitem_155 = split_tensor_15[1];  split_tensor_15 = None
        silu_4 = torch.ops.aten.silu.default(getitem_154);  getitem_154 = None
        mul_17 = torch.ops.aten.mul.Tensor(silu_4, getitem_155);  silu_4 = getitem_155 = None
        _to_copy_102 = torch.ops.aten._to_copy.default(arg79_1, dtype = torch.bfloat16)
        t_35 = torch.ops.aten.t.default(_to_copy_102);  _to_copy_102 = None
        view_164 = torch.ops.aten.view.default(mul_17, [2097152, 256]);  mul_17 = None
        mm_32 = torch.ops.aten.mm.default(view_164, t_35);  view_164 = t_35 = None
        view_165 = torch.ops.aten.view.default(mm_32, [1, 16384, 128, 64]);  mm_32 = None
        _to_copy_103 = torch.ops.aten._to_copy.default(getitem_140, dtype = torch.float32);  getitem_140 = None
        native_layer_norm_default_22 = torch.ops.aten.native_layer_norm.default(_to_copy_103, [64], arg76_1, arg77_1, 1e-05);  _to_copy_103 = arg76_1 = arg77_1 = None
        getitem_156 = native_layer_norm_default_22[0]
        _to_copy_104 = torch.ops.aten._to_copy.default(arg78_1, dtype = torch.bfloat16);  arg78_1 = None
        _to_copy_105 = torch.ops.aten._to_copy.default(getitem_156, dtype = torch.bfloat16);  getitem_156 = None
        t_36 = torch.ops.aten.t.default(_to_copy_104);  _to_copy_104 = None
        view_166 = torch.ops.aten.view.default(_to_copy_105, [2097152, 64]);  _to_copy_105 = None
        mm_33 = torch.ops.aten.mm.default(view_166, t_36);  view_166 = t_36 = None
        view_167 = torch.ops.aten.view.default(mm_33, [1, 16384, 128, 512]);  mm_33 = None
        split_tensor_16 = torch.ops.aten.split.Tensor(view_167, 256, dim = -1);  view_167 = None
        getitem_159 = split_tensor_16[0]
        getitem_160 = split_tensor_16[1];  split_tensor_16 = None
        silu_5 = torch.ops.aten.silu.default(getitem_159);  getitem_159 = None
        mul_18 = torch.ops.aten.mul.Tensor(silu_5, getitem_160);  silu_5 = getitem_160 = None
        _to_copy_106 = torch.ops.aten._to_copy.default(arg79_1, dtype = torch.bfloat16);  arg79_1 = None
        t_37 = torch.ops.aten.t.default(_to_copy_106);  _to_copy_106 = None
        view_169 = torch.ops.aten.view.default(mul_18, [2097152, 256]);  mul_18 = None
        mm_34 = torch.ops.aten.mm.default(view_169, t_37);  view_169 = t_37 = None
        view_170 = torch.ops.aten.view.default(mm_34, [1, 16384, 128, 64]);  mm_34 = None
        cat_2 = torch.ops.aten.cat.default([view_155, view_160, view_165, view_170], dim = -2);  view_155 = view_160 = view_165 = view_170 = None
        add_18 = torch.ops.aten.add.Tensor(add_12, cat_2);  cat_2 = None
        slice_33 = torch.ops.aten.slice.Tensor(add_12, dim = 0, start = 0, end = 9223372036854775807)
        slice_34 = torch.ops.aten.slice.Tensor(slice_33, dim = 1, start = 0, end = 4096);  slice_33 = None
        slice_35 = torch.ops.aten.slice.Tensor(slice_34, dim = 2, start = 0, end = 9223372036854775807);  slice_34 = None
        slice_36 = torch.ops.aten.slice.Tensor(slice_35, dim = 3, start = 0, end = 9223372036854775807);  slice_35 = None
        slice_37 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
        slice_38 = torch.ops.aten.slice.Tensor(slice_37, dim = 1, start = 0, end = 4096);  slice_37 = None
        slice_39 = torch.ops.aten.slice.Tensor(slice_38, dim = 2, start = 0, end = 9223372036854775807);  slice_38 = None
        _to_copy_107 = torch.ops.aten._to_copy.default(add_17, dtype = torch.float32)
        native_layer_norm_default_23 = torch.ops.aten.native_layer_norm.default(_to_copy_107, [256], arg58_1, arg59_1, 1e-05);  _to_copy_107 = None
        getitem_161 = native_layer_norm_default_23[0]
        _to_copy_108 = torch.ops.aten._to_copy.default(arg60_1, dtype = torch.bfloat16)
        _to_copy_109 = torch.ops.aten._to_copy.default(getitem_161, dtype = torch.bfloat16);  getitem_161 = None
        t_38 = torch.ops.aten.t.default(_to_copy_108);  _to_copy_108 = None
        view_171 = torch.ops.aten.view.default(_to_copy_109, [262144, 256]);  _to_copy_109 = None
        mm_35 = torch.ops.aten.mm.default(view_171, t_38);  view_171 = t_38 = None
        view_172 = torch.ops.aten.view.default(mm_35, [1, 512, 512, 8]);  mm_35 = None
        permute_108 = torch.ops.aten.permute.default(view_172, [0, 3, 1, 2]);  view_172 = None
        view_173 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_10 = torch.ops.aten.bitwise_not.default(view_173);  view_173 = None
        masked_fill_10 = torch.ops.aten.masked_fill.Scalar(permute_108, bitwise_not_10, -10000);  permute_108 = bitwise_not_10 = None
        _to_copy_110 = torch.ops.aten._to_copy.default(masked_fill_10, dtype = torch.float32, layout = torch.strided, device = self.device);  masked_fill_10 = None
        _softmax = torch.ops.aten._softmax.default(_to_copy_110, -1, False);  _to_copy_110 = None
        _to_copy_111 = torch.ops.aten._to_copy.default(slice_36, dtype = torch.float32);  slice_36 = None
        native_layer_norm_default_24 = torch.ops.aten.native_layer_norm.default(_to_copy_111, [64], arg55_1, arg56_1, 1e-05);  _to_copy_111 = None
        getitem_164 = native_layer_norm_default_24[0]
        _to_copy_112 = torch.ops.aten._to_copy.default(arg57_1, dtype = torch.bfloat16)
        _to_copy_113 = torch.ops.aten._to_copy.default(getitem_164, dtype = torch.bfloat16);  getitem_164 = None
        t_39 = torch.ops.aten.t.default(_to_copy_112);  _to_copy_112 = None
        view_174 = torch.ops.aten.view.default(_to_copy_113, [2097152, 64]);  _to_copy_113 = None
        mm_36 = torch.ops.aten.mm.default(view_174, t_39);  view_174 = t_39 = None
        view_175 = torch.ops.aten.view.default(mm_36, [1, 4096, 512, 512]);  mm_36 = None
        view_176 = torch.ops.aten.view.default(view_175, [1, 4096, 512, 2, 8, 32]);  view_175 = None
        permute_109 = torch.ops.aten.permute.default(view_176, [3, 0, 1, 2, 4, 5]);  view_176 = None
        unbind_int_8 = torch.ops.aten.unbind.int(permute_109);  permute_109 = None
        getitem_167 = unbind_int_8[0]
        getitem_168 = unbind_int_8[1];  unbind_int_8 = None
        sigmoid_10 = torch.ops.aten.sigmoid.default(getitem_168);  getitem_168 = None
        bitwise_not_11 = torch.ops.aten.bitwise_not.default(slice_39);  slice_39 = None
        view_177 = torch.ops.aten.view.default(bitwise_not_11, [1, 4096, 512, 1, 1]);  bitwise_not_11 = None
        masked_fill_11 = torch.ops.aten.masked_fill.Scalar(getitem_167, view_177, 0);  getitem_167 = view_177 = None
        _to_copy_114 = torch.ops.aten._to_copy.default(_softmax, dtype = torch.bfloat16);  _softmax = None
        unsqueeze_82 = torch.ops.aten.unsqueeze.default(_to_copy_114, 4);  _to_copy_114 = None
        unsqueeze_83 = torch.ops.aten.unsqueeze.default(unsqueeze_82, 5);  unsqueeze_82 = None
        permute_110 = torch.ops.aten.permute.default(unsqueeze_83, [0, 4, 2, 1, 5, 3]);  unsqueeze_83 = None
        unsqueeze_84 = torch.ops.aten.unsqueeze.default(masked_fill_11, 5);  masked_fill_11 = None
        permute_111 = torch.ops.aten.permute.default(unsqueeze_84, [0, 1, 5, 3, 4, 2]);  unsqueeze_84 = None
        permute_112 = torch.ops.aten.permute.default(permute_110, [3, 2, 5, 0, 1, 4]);  permute_110 = None
        view_178 = torch.ops.aten.view.default(permute_112, [8, 512, 512]);  permute_112 = None
        permute_113 = torch.ops.aten.permute.default(permute_111, [3, 5, 0, 1, 4, 2]);  permute_111 = None
        clone_35 = torch.ops.aten.clone.default(permute_113, memory_format = torch.contiguous_format);  permute_113 = None
        _unsafe_view_32 = torch.ops.aten._unsafe_view.default(clone_35, [8, 512, 131072]);  clone_35 = None
        bmm_18 = torch.ops.aten.bmm.default(view_178, _unsafe_view_32);  view_178 = _unsafe_view_32 = None
        view_179 = torch.ops.aten.view.default(bmm_18, [8, 512, 1, 1, 4096, 32]);  bmm_18 = None
        permute_114 = torch.ops.aten.permute.default(view_179, [3, 4, 1, 0, 5, 2]);  view_179 = None
        view_180 = torch.ops.aten.view.default(permute_114, [1, 4096, 512, 8, 32]);  permute_114 = None
        mul_19 = torch.ops.aten.mul.Tensor(sigmoid_10, view_180);  sigmoid_10 = view_180 = None
        view_181 = torch.ops.aten.view.default(mul_19, [1, 4096, 512, 256]);  mul_19 = None
        _to_copy_115 = torch.ops.aten._to_copy.default(arg61_1, dtype = torch.bfloat16)
        t_40 = torch.ops.aten.t.default(_to_copy_115);  _to_copy_115 = None
        view_182 = torch.ops.aten.view.default(view_181, [2097152, 256]);  view_181 = None
        mm_37 = torch.ops.aten.mm.default(view_182, t_40);  view_182 = t_40 = None
        view_183 = torch.ops.aten.view.default(mm_37, [1, 4096, 512, 64]);  mm_37 = None
        slice_40 = torch.ops.aten.slice.Tensor(add_12, dim = 0, start = 0, end = 9223372036854775807)
        slice_41 = torch.ops.aten.slice.Tensor(slice_40, dim = 1, start = 4096, end = 8192);  slice_40 = None
        slice_42 = torch.ops.aten.slice.Tensor(slice_41, dim = 2, start = 0, end = 9223372036854775807);  slice_41 = None
        slice_43 = torch.ops.aten.slice.Tensor(slice_42, dim = 3, start = 0, end = 9223372036854775807);  slice_42 = None
        slice_44 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
        slice_45 = torch.ops.aten.slice.Tensor(slice_44, dim = 1, start = 4096, end = 8192);  slice_44 = None
        slice_46 = torch.ops.aten.slice.Tensor(slice_45, dim = 2, start = 0, end = 9223372036854775807);  slice_45 = None
        _to_copy_116 = torch.ops.aten._to_copy.default(add_17, dtype = torch.float32)
        native_layer_norm_default_25 = torch.ops.aten.native_layer_norm.default(_to_copy_116, [256], arg58_1, arg59_1, 1e-05);  _to_copy_116 = None
        getitem_169 = native_layer_norm_default_25[0]
        _to_copy_117 = torch.ops.aten._to_copy.default(arg60_1, dtype = torch.bfloat16)
        _to_copy_118 = torch.ops.aten._to_copy.default(getitem_169, dtype = torch.bfloat16);  getitem_169 = None
        t_41 = torch.ops.aten.t.default(_to_copy_117);  _to_copy_117 = None
        view_184 = torch.ops.aten.view.default(_to_copy_118, [262144, 256]);  _to_copy_118 = None
        mm_38 = torch.ops.aten.mm.default(view_184, t_41);  view_184 = t_41 = None
        view_185 = torch.ops.aten.view.default(mm_38, [1, 512, 512, 8]);  mm_38 = None
        permute_115 = torch.ops.aten.permute.default(view_185, [0, 3, 1, 2]);  view_185 = None
        view_186 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_12 = torch.ops.aten.bitwise_not.default(view_186);  view_186 = None
        masked_fill_12 = torch.ops.aten.masked_fill.Scalar(permute_115, bitwise_not_12, -10000);  permute_115 = bitwise_not_12 = None
        _to_copy_119 = torch.ops.aten._to_copy.default(masked_fill_12, dtype = torch.float32, layout = torch.strided, device = self.device);  masked_fill_12 = None
        _softmax_1 = torch.ops.aten._softmax.default(_to_copy_119, -1, False);  _to_copy_119 = None
        _to_copy_120 = torch.ops.aten._to_copy.default(slice_43, dtype = torch.float32);  slice_43 = None
        native_layer_norm_default_26 = torch.ops.aten.native_layer_norm.default(_to_copy_120, [64], arg55_1, arg56_1, 1e-05);  _to_copy_120 = None
        getitem_172 = native_layer_norm_default_26[0]
        _to_copy_121 = torch.ops.aten._to_copy.default(arg57_1, dtype = torch.bfloat16)
        _to_copy_122 = torch.ops.aten._to_copy.default(getitem_172, dtype = torch.bfloat16);  getitem_172 = None
        t_42 = torch.ops.aten.t.default(_to_copy_121);  _to_copy_121 = None
        view_187 = torch.ops.aten.view.default(_to_copy_122, [2097152, 64]);  _to_copy_122 = None
        mm_39 = torch.ops.aten.mm.default(view_187, t_42);  view_187 = t_42 = None
        view_188 = torch.ops.aten.view.default(mm_39, [1, 4096, 512, 512]);  mm_39 = None
        view_189 = torch.ops.aten.view.default(view_188, [1, 4096, 512, 2, 8, 32]);  view_188 = None
        permute_116 = torch.ops.aten.permute.default(view_189, [3, 0, 1, 2, 4, 5]);  view_189 = None
        unbind_int_9 = torch.ops.aten.unbind.int(permute_116);  permute_116 = None
        getitem_175 = unbind_int_9[0]
        getitem_176 = unbind_int_9[1];  unbind_int_9 = None
        sigmoid_11 = torch.ops.aten.sigmoid.default(getitem_176);  getitem_176 = None
        bitwise_not_13 = torch.ops.aten.bitwise_not.default(slice_46);  slice_46 = None
        view_190 = torch.ops.aten.view.default(bitwise_not_13, [1, 4096, 512, 1, 1]);  bitwise_not_13 = None
        masked_fill_13 = torch.ops.aten.masked_fill.Scalar(getitem_175, view_190, 0);  getitem_175 = view_190 = None
        _to_copy_123 = torch.ops.aten._to_copy.default(_softmax_1, dtype = torch.bfloat16);  _softmax_1 = None
        unsqueeze_85 = torch.ops.aten.unsqueeze.default(_to_copy_123, 4);  _to_copy_123 = None
        unsqueeze_86 = torch.ops.aten.unsqueeze.default(unsqueeze_85, 5);  unsqueeze_85 = None
        permute_117 = torch.ops.aten.permute.default(unsqueeze_86, [0, 4, 2, 1, 5, 3]);  unsqueeze_86 = None
        unsqueeze_87 = torch.ops.aten.unsqueeze.default(masked_fill_13, 5);  masked_fill_13 = None
        permute_118 = torch.ops.aten.permute.default(unsqueeze_87, [0, 1, 5, 3, 4, 2]);  unsqueeze_87 = None
        permute_119 = torch.ops.aten.permute.default(permute_117, [3, 2, 5, 0, 1, 4]);  permute_117 = None
        view_191 = torch.ops.aten.view.default(permute_119, [8, 512, 512]);  permute_119 = None
        permute_120 = torch.ops.aten.permute.default(permute_118, [3, 5, 0, 1, 4, 2]);  permute_118 = None
        clone_36 = torch.ops.aten.clone.default(permute_120, memory_format = torch.contiguous_format);  permute_120 = None
        _unsafe_view_33 = torch.ops.aten._unsafe_view.default(clone_36, [8, 512, 131072]);  clone_36 = None
        bmm_19 = torch.ops.aten.bmm.default(view_191, _unsafe_view_33);  view_191 = _unsafe_view_33 = None
        view_192 = torch.ops.aten.view.default(bmm_19, [8, 512, 1, 1, 4096, 32]);  bmm_19 = None
        permute_121 = torch.ops.aten.permute.default(view_192, [3, 4, 1, 0, 5, 2]);  view_192 = None
        view_193 = torch.ops.aten.view.default(permute_121, [1, 4096, 512, 8, 32]);  permute_121 = None
        mul_20 = torch.ops.aten.mul.Tensor(sigmoid_11, view_193);  sigmoid_11 = view_193 = None
        view_194 = torch.ops.aten.view.default(mul_20, [1, 4096, 512, 256]);  mul_20 = None
        _to_copy_124 = torch.ops.aten._to_copy.default(arg61_1, dtype = torch.bfloat16)
        t_43 = torch.ops.aten.t.default(_to_copy_124);  _to_copy_124 = None
        view_195 = torch.ops.aten.view.default(view_194, [2097152, 256]);  view_194 = None
        mm_40 = torch.ops.aten.mm.default(view_195, t_43);  view_195 = t_43 = None
        view_196 = torch.ops.aten.view.default(mm_40, [1, 4096, 512, 64]);  mm_40 = None
        slice_47 = torch.ops.aten.slice.Tensor(add_12, dim = 0, start = 0, end = 9223372036854775807)
        slice_48 = torch.ops.aten.slice.Tensor(slice_47, dim = 1, start = 8192, end = 12288);  slice_47 = None
        slice_49 = torch.ops.aten.slice.Tensor(slice_48, dim = 2, start = 0, end = 9223372036854775807);  slice_48 = None
        slice_50 = torch.ops.aten.slice.Tensor(slice_49, dim = 3, start = 0, end = 9223372036854775807);  slice_49 = None
        slice_51 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
        slice_52 = torch.ops.aten.slice.Tensor(slice_51, dim = 1, start = 8192, end = 12288);  slice_51 = None
        slice_53 = torch.ops.aten.slice.Tensor(slice_52, dim = 2, start = 0, end = 9223372036854775807);  slice_52 = None
        _to_copy_125 = torch.ops.aten._to_copy.default(add_17, dtype = torch.float32)
        native_layer_norm_default_27 = torch.ops.aten.native_layer_norm.default(_to_copy_125, [256], arg58_1, arg59_1, 1e-05);  _to_copy_125 = None
        getitem_177 = native_layer_norm_default_27[0]
        _to_copy_126 = torch.ops.aten._to_copy.default(arg60_1, dtype = torch.bfloat16)
        _to_copy_127 = torch.ops.aten._to_copy.default(getitem_177, dtype = torch.bfloat16);  getitem_177 = None
        t_44 = torch.ops.aten.t.default(_to_copy_126);  _to_copy_126 = None
        view_197 = torch.ops.aten.view.default(_to_copy_127, [262144, 256]);  _to_copy_127 = None
        mm_41 = torch.ops.aten.mm.default(view_197, t_44);  view_197 = t_44 = None
        view_198 = torch.ops.aten.view.default(mm_41, [1, 512, 512, 8]);  mm_41 = None
        permute_122 = torch.ops.aten.permute.default(view_198, [0, 3, 1, 2]);  view_198 = None
        view_199 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_14 = torch.ops.aten.bitwise_not.default(view_199);  view_199 = None
        masked_fill_14 = torch.ops.aten.masked_fill.Scalar(permute_122, bitwise_not_14, -10000);  permute_122 = bitwise_not_14 = None
        _to_copy_128 = torch.ops.aten._to_copy.default(masked_fill_14, dtype = torch.float32, layout = torch.strided, device = self.device);  masked_fill_14 = None
        _softmax_2 = torch.ops.aten._softmax.default(_to_copy_128, -1, False);  _to_copy_128 = None
        _to_copy_129 = torch.ops.aten._to_copy.default(slice_50, dtype = torch.float32);  slice_50 = None
        native_layer_norm_default_28 = torch.ops.aten.native_layer_norm.default(_to_copy_129, [64], arg55_1, arg56_1, 1e-05);  _to_copy_129 = None
        getitem_180 = native_layer_norm_default_28[0]
        _to_copy_130 = torch.ops.aten._to_copy.default(arg57_1, dtype = torch.bfloat16)
        _to_copy_131 = torch.ops.aten._to_copy.default(getitem_180, dtype = torch.bfloat16);  getitem_180 = None
        t_45 = torch.ops.aten.t.default(_to_copy_130);  _to_copy_130 = None
        view_200 = torch.ops.aten.view.default(_to_copy_131, [2097152, 64]);  _to_copy_131 = None
        mm_42 = torch.ops.aten.mm.default(view_200, t_45);  view_200 = t_45 = None
        view_201 = torch.ops.aten.view.default(mm_42, [1, 4096, 512, 512]);  mm_42 = None
        view_202 = torch.ops.aten.view.default(view_201, [1, 4096, 512, 2, 8, 32]);  view_201 = None
        permute_123 = torch.ops.aten.permute.default(view_202, [3, 0, 1, 2, 4, 5]);  view_202 = None
        unbind_int_10 = torch.ops.aten.unbind.int(permute_123);  permute_123 = None
        getitem_183 = unbind_int_10[0]
        getitem_184 = unbind_int_10[1];  unbind_int_10 = None
        sigmoid_12 = torch.ops.aten.sigmoid.default(getitem_184);  getitem_184 = None
        bitwise_not_15 = torch.ops.aten.bitwise_not.default(slice_53);  slice_53 = None
        view_203 = torch.ops.aten.view.default(bitwise_not_15, [1, 4096, 512, 1, 1]);  bitwise_not_15 = None
        masked_fill_15 = torch.ops.aten.masked_fill.Scalar(getitem_183, view_203, 0);  getitem_183 = view_203 = None
        _to_copy_132 = torch.ops.aten._to_copy.default(_softmax_2, dtype = torch.bfloat16);  _softmax_2 = None
        unsqueeze_88 = torch.ops.aten.unsqueeze.default(_to_copy_132, 4);  _to_copy_132 = None
        unsqueeze_89 = torch.ops.aten.unsqueeze.default(unsqueeze_88, 5);  unsqueeze_88 = None
        permute_124 = torch.ops.aten.permute.default(unsqueeze_89, [0, 4, 2, 1, 5, 3]);  unsqueeze_89 = None
        unsqueeze_90 = torch.ops.aten.unsqueeze.default(masked_fill_15, 5);  masked_fill_15 = None
        permute_125 = torch.ops.aten.permute.default(unsqueeze_90, [0, 1, 5, 3, 4, 2]);  unsqueeze_90 = None
        permute_126 = torch.ops.aten.permute.default(permute_124, [3, 2, 5, 0, 1, 4]);  permute_124 = None
        view_204 = torch.ops.aten.view.default(permute_126, [8, 512, 512]);  permute_126 = None
        permute_127 = torch.ops.aten.permute.default(permute_125, [3, 5, 0, 1, 4, 2]);  permute_125 = None
        clone_37 = torch.ops.aten.clone.default(permute_127, memory_format = torch.contiguous_format);  permute_127 = None
        _unsafe_view_34 = torch.ops.aten._unsafe_view.default(clone_37, [8, 512, 131072]);  clone_37 = None
        bmm_20 = torch.ops.aten.bmm.default(view_204, _unsafe_view_34);  view_204 = _unsafe_view_34 = None
        view_205 = torch.ops.aten.view.default(bmm_20, [8, 512, 1, 1, 4096, 32]);  bmm_20 = None
        permute_128 = torch.ops.aten.permute.default(view_205, [3, 4, 1, 0, 5, 2]);  view_205 = None
        view_206 = torch.ops.aten.view.default(permute_128, [1, 4096, 512, 8, 32]);  permute_128 = None
        mul_21 = torch.ops.aten.mul.Tensor(sigmoid_12, view_206);  sigmoid_12 = view_206 = None
        view_207 = torch.ops.aten.view.default(mul_21, [1, 4096, 512, 256]);  mul_21 = None
        _to_copy_133 = torch.ops.aten._to_copy.default(arg61_1, dtype = torch.bfloat16)
        t_46 = torch.ops.aten.t.default(_to_copy_133);  _to_copy_133 = None
        view_208 = torch.ops.aten.view.default(view_207, [2097152, 256]);  view_207 = None
        mm_43 = torch.ops.aten.mm.default(view_208, t_46);  view_208 = t_46 = None
        view_209 = torch.ops.aten.view.default(mm_43, [1, 4096, 512, 64]);  mm_43 = None
        slice_54 = torch.ops.aten.slice.Tensor(add_12, dim = 0, start = 0, end = 9223372036854775807);  add_12 = None
        slice_55 = torch.ops.aten.slice.Tensor(slice_54, dim = 1, start = 12288, end = 16384);  slice_54 = None
        slice_56 = torch.ops.aten.slice.Tensor(slice_55, dim = 2, start = 0, end = 9223372036854775807);  slice_55 = None
        slice_57 = torch.ops.aten.slice.Tensor(slice_56, dim = 3, start = 0, end = 9223372036854775807);  slice_56 = None
        slice_58 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
        slice_59 = torch.ops.aten.slice.Tensor(slice_58, dim = 1, start = 12288, end = 16384);  slice_58 = None
        slice_60 = torch.ops.aten.slice.Tensor(slice_59, dim = 2, start = 0, end = 9223372036854775807);  slice_59 = None
        _to_copy_134 = torch.ops.aten._to_copy.default(add_17, dtype = torch.float32)
        native_layer_norm_default_29 = torch.ops.aten.native_layer_norm.default(_to_copy_134, [256], arg58_1, arg59_1, 1e-05);  _to_copy_134 = arg58_1 = arg59_1 = None
        getitem_185 = native_layer_norm_default_29[0]
        _to_copy_135 = torch.ops.aten._to_copy.default(arg60_1, dtype = torch.bfloat16);  arg60_1 = None
        _to_copy_136 = torch.ops.aten._to_copy.default(getitem_185, dtype = torch.bfloat16);  getitem_185 = None
        t_47 = torch.ops.aten.t.default(_to_copy_135);  _to_copy_135 = None
        view_210 = torch.ops.aten.view.default(_to_copy_136, [262144, 256]);  _to_copy_136 = None
        mm_44 = torch.ops.aten.mm.default(view_210, t_47);  view_210 = t_47 = None
        view_211 = torch.ops.aten.view.default(mm_44, [1, 512, 512, 8]);  mm_44 = None
        permute_129 = torch.ops.aten.permute.default(view_211, [0, 3, 1, 2]);  view_211 = None
        view_212 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_16 = torch.ops.aten.bitwise_not.default(view_212);  view_212 = None
        masked_fill_16 = torch.ops.aten.masked_fill.Scalar(permute_129, bitwise_not_16, -10000);  permute_129 = bitwise_not_16 = None
        _to_copy_137 = torch.ops.aten._to_copy.default(masked_fill_16, dtype = torch.float32, layout = torch.strided, device = self.device);  masked_fill_16 = None
        _softmax_3 = torch.ops.aten._softmax.default(_to_copy_137, -1, False);  _to_copy_137 = None
        _to_copy_138 = torch.ops.aten._to_copy.default(slice_57, dtype = torch.float32);  slice_57 = None
        native_layer_norm_default_30 = torch.ops.aten.native_layer_norm.default(_to_copy_138, [64], arg55_1, arg56_1, 1e-05);  _to_copy_138 = arg55_1 = arg56_1 = None
        getitem_188 = native_layer_norm_default_30[0]
        _to_copy_139 = torch.ops.aten._to_copy.default(arg57_1, dtype = torch.bfloat16);  arg57_1 = None
        _to_copy_140 = torch.ops.aten._to_copy.default(getitem_188, dtype = torch.bfloat16);  getitem_188 = None
        t_48 = torch.ops.aten.t.default(_to_copy_139);  _to_copy_139 = None
        view_213 = torch.ops.aten.view.default(_to_copy_140, [2097152, 64]);  _to_copy_140 = None
        mm_45 = torch.ops.aten.mm.default(view_213, t_48);  view_213 = t_48 = None
        view_214 = torch.ops.aten.view.default(mm_45, [1, 4096, 512, 512]);  mm_45 = None
        view_215 = torch.ops.aten.view.default(view_214, [1, 4096, 512, 2, 8, 32]);  view_214 = None
        permute_130 = torch.ops.aten.permute.default(view_215, [3, 0, 1, 2, 4, 5]);  view_215 = None
        unbind_int_11 = torch.ops.aten.unbind.int(permute_130);  permute_130 = None
        getitem_191 = unbind_int_11[0]
        getitem_192 = unbind_int_11[1];  unbind_int_11 = None
        sigmoid_13 = torch.ops.aten.sigmoid.default(getitem_192);  getitem_192 = None
        bitwise_not_17 = torch.ops.aten.bitwise_not.default(slice_60);  slice_60 = None
        view_216 = torch.ops.aten.view.default(bitwise_not_17, [1, 4096, 512, 1, 1]);  bitwise_not_17 = None
        masked_fill_17 = torch.ops.aten.masked_fill.Scalar(getitem_191, view_216, 0);  getitem_191 = view_216 = None
        _to_copy_141 = torch.ops.aten._to_copy.default(_softmax_3, dtype = torch.bfloat16);  _softmax_3 = None
        unsqueeze_91 = torch.ops.aten.unsqueeze.default(_to_copy_141, 4);  _to_copy_141 = None
        unsqueeze_92 = torch.ops.aten.unsqueeze.default(unsqueeze_91, 5);  unsqueeze_91 = None
        permute_131 = torch.ops.aten.permute.default(unsqueeze_92, [0, 4, 2, 1, 5, 3]);  unsqueeze_92 = None
        unsqueeze_93 = torch.ops.aten.unsqueeze.default(masked_fill_17, 5);  masked_fill_17 = None
        permute_132 = torch.ops.aten.permute.default(unsqueeze_93, [0, 1, 5, 3, 4, 2]);  unsqueeze_93 = None
        permute_133 = torch.ops.aten.permute.default(permute_131, [3, 2, 5, 0, 1, 4]);  permute_131 = None
        view_217 = torch.ops.aten.view.default(permute_133, [8, 512, 512]);  permute_133 = None
        permute_134 = torch.ops.aten.permute.default(permute_132, [3, 5, 0, 1, 4, 2]);  permute_132 = None
        clone_38 = torch.ops.aten.clone.default(permute_134, memory_format = torch.contiguous_format);  permute_134 = None
        _unsafe_view_35 = torch.ops.aten._unsafe_view.default(clone_38, [8, 512, 131072]);  clone_38 = None
        bmm_21 = torch.ops.aten.bmm.default(view_217, _unsafe_view_35);  view_217 = _unsafe_view_35 = None
        view_218 = torch.ops.aten.view.default(bmm_21, [8, 512, 1, 1, 4096, 32]);  bmm_21 = None
        permute_135 = torch.ops.aten.permute.default(view_218, [3, 4, 1, 0, 5, 2]);  view_218 = None
        view_219 = torch.ops.aten.view.default(permute_135, [1, 4096, 512, 8, 32]);  permute_135 = None
        mul_22 = torch.ops.aten.mul.Tensor(sigmoid_13, view_219);  sigmoid_13 = view_219 = None
        view_220 = torch.ops.aten.view.default(mul_22, [1, 4096, 512, 256]);  mul_22 = None
        _to_copy_142 = torch.ops.aten._to_copy.default(arg61_1, dtype = torch.bfloat16);  arg61_1 = None
        t_49 = torch.ops.aten.t.default(_to_copy_142);  _to_copy_142 = None
        view_221 = torch.ops.aten.view.default(view_220, [2097152, 256]);  view_220 = None
        mm_46 = torch.ops.aten.mm.default(view_221, t_49);  view_221 = t_49 = None
        view_222 = torch.ops.aten.view.default(mm_46, [1, 4096, 512, 64]);  mm_46 = None
        cat_3 = torch.ops.aten.cat.default([view_183, view_196, view_209, view_222], dim = 1);  view_183 = view_196 = view_209 = view_222 = None
        add_19 = torch.ops.aten.add.Tensor(add_18, cat_3);  add_18 = cat_3 = None
        _to_copy_143 = torch.ops.aten._to_copy.default(add_17, dtype = torch.float32)
        native_layer_norm_default_31 = torch.ops.aten.native_layer_norm.default(_to_copy_143, [256], arg104_1, arg105_1, 1e-05);  _to_copy_143 = arg104_1 = arg105_1 = None
        getitem_193 = native_layer_norm_default_31[0]
        split_with_sizes_default_4 = torch.ops.aten.split_with_sizes.default(arg107_1, [512, 512]);  arg107_1 = None
        getitem_196 = split_with_sizes_default_4[0]
        getitem_197 = split_with_sizes_default_4[1];  split_with_sizes_default_4 = None
        split_with_sizes_default_5 = torch.ops.aten.split_with_sizes.default(arg108_1, [512, 512, 256]);  arg108_1 = None
        getitem_198 = split_with_sizes_default_5[0]
        getitem_199 = split_with_sizes_default_5[1]
        getitem_200 = split_with_sizes_default_5[2];  split_with_sizes_default_5 = None
        _to_copy_144 = torch.ops.aten._to_copy.default(getitem_196, dtype = torch.bfloat16);  getitem_196 = None
        _to_copy_145 = torch.ops.aten._to_copy.default(getitem_193, dtype = torch.bfloat16)
        t_50 = torch.ops.aten.t.default(_to_copy_144);  _to_copy_144 = None
        view_223 = torch.ops.aten.view.default(_to_copy_145, [262144, 256]);  _to_copy_145 = None
        mm_47 = torch.ops.aten.mm.default(view_223, t_50);  view_223 = t_50 = None
        view_224 = torch.ops.aten.view.default(mm_47, [1, 512, 512, 512]);  mm_47 = None
        _to_copy_146 = torch.ops.aten._to_copy.default(getitem_198, dtype = torch.bfloat16);  getitem_198 = None
        _to_copy_147 = torch.ops.aten._to_copy.default(getitem_193, dtype = torch.bfloat16)
        t_51 = torch.ops.aten.t.default(_to_copy_146);  _to_copy_146 = None
        view_225 = torch.ops.aten.view.default(_to_copy_147, [262144, 256]);  _to_copy_147 = None
        mm_48 = torch.ops.aten.mm.default(view_225, t_51);  view_225 = t_51 = None
        view_226 = torch.ops.aten.view.default(mm_48, [1, 512, 512, 512]);  mm_48 = None
        sigmoid_14 = torch.ops.aten.sigmoid.default(view_226);  view_226 = None
        mul_23 = torch.ops.aten.mul.Tensor(view_224, sigmoid_14);  view_224 = sigmoid_14 = None
        unsqueeze_94 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_18 = torch.ops.aten.bitwise_not.default(unsqueeze_94);  unsqueeze_94 = None
        masked_fill_18 = torch.ops.aten.masked_fill.Scalar(mul_23, bitwise_not_18, 0);  mul_23 = bitwise_not_18 = None
        split_tensor_17 = torch.ops.aten.split.Tensor(masked_fill_18, 256, dim = -1)
        getitem_203 = split_tensor_17[0]
        unsqueeze_97 = torch.ops.aten.unsqueeze.default(getitem_203, 4);  getitem_203 = None
        permute_140 = torch.ops.aten.permute.default(unsqueeze_97, [0, 1, 4, 3, 2]);  unsqueeze_97 = None
        permute_141 = torch.ops.aten.permute.default(permute_140, [3, 1, 4, 0, 2]);  permute_140 = None
        view_229 = torch.ops.aten.view.default(permute_141, [256, 512, 512]);  permute_141 = None
        split_tensor_18 = torch.ops.aten.split.Tensor(masked_fill_18, 256, dim = -1);  masked_fill_18 = None
        getitem_206 = split_tensor_18[1];  split_tensor_18 = None
        unsqueeze_98 = torch.ops.aten.unsqueeze.default(getitem_206, 4);  getitem_206 = None
        permute_142 = torch.ops.aten.permute.default(unsqueeze_98, [0, 4, 1, 3, 2]);  unsqueeze_98 = None
        permute_143 = torch.ops.aten.permute.default(permute_142, [3, 4, 0, 2, 1]);  permute_142 = None
        view_230 = torch.ops.aten.view.default(permute_143, [256, 512, 512]);  permute_143 = None
        bmm_22 = torch.ops.aten.bmm.default(view_229, view_230);  view_229 = view_230 = None
        view_231 = torch.ops.aten.view.default(bmm_22, [256, 512, 1, 1, 512]);  bmm_22 = None
        permute_144 = torch.ops.aten.permute.default(view_231, [3, 1, 4, 0, 2]);  view_231 = None
        view_232 = torch.ops.aten.view.default(permute_144, [1, 512, 512, 256]);  permute_144 = None
        _to_copy_148 = torch.ops.aten._to_copy.default(getitem_197, dtype = torch.bfloat16);  getitem_197 = None
        _to_copy_149 = torch.ops.aten._to_copy.default(getitem_193, dtype = torch.bfloat16)
        t_52 = torch.ops.aten.t.default(_to_copy_148);  _to_copy_148 = None
        view_233 = torch.ops.aten.view.default(_to_copy_149, [262144, 256]);  _to_copy_149 = None
        mm_49 = torch.ops.aten.mm.default(view_233, t_52);  view_233 = t_52 = None
        view_234 = torch.ops.aten.view.default(mm_49, [1, 512, 512, 512]);  mm_49 = None
        _to_copy_150 = torch.ops.aten._to_copy.default(getitem_199, dtype = torch.bfloat16);  getitem_199 = None
        _to_copy_151 = torch.ops.aten._to_copy.default(getitem_193, dtype = torch.bfloat16)
        t_53 = torch.ops.aten.t.default(_to_copy_150);  _to_copy_150 = None
        view_235 = torch.ops.aten.view.default(_to_copy_151, [262144, 256]);  _to_copy_151 = None
        mm_50 = torch.ops.aten.mm.default(view_235, t_53);  view_235 = t_53 = None
        view_236 = torch.ops.aten.view.default(mm_50, [1, 512, 512, 512]);  mm_50 = None
        sigmoid_15 = torch.ops.aten.sigmoid.default(view_236);  view_236 = None
        mul_24 = torch.ops.aten.mul.Tensor(view_234, sigmoid_15);  view_234 = sigmoid_15 = None
        view_237 = torch.ops.aten.view.default(mul_24, [262144, 512]);  mul_24 = None
        view_238 = torch.ops.aten.view.default(view_237, [1, 512, 512, 512]);  view_237 = None
        transpose_4 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_99 = torch.ops.aten.unsqueeze.default(transpose_4, 3);  transpose_4 = None
        clone_39 = torch.ops.aten.clone.default(unsqueeze_99, memory_format = torch.contiguous_format);  unsqueeze_99 = None
        bitwise_not_19 = torch.ops.aten.bitwise_not.default(clone_39);  clone_39 = None
        masked_fill_19 = torch.ops.aten.masked_fill.Scalar(view_238, bitwise_not_19, 0);  view_238 = bitwise_not_19 = None
        view_239 = torch.ops.aten.view.default(masked_fill_19, [262144, 512]);  masked_fill_19 = None
        view_243 = torch.ops.aten.view.default(view_239, [1, 512, 512, 512])
        split_tensor_19 = torch.ops.aten.split.Tensor(view_243, 256, dim = -1);  view_243 = None
        getitem_209 = split_tensor_19[0]
        unsqueeze_102 = torch.ops.aten.unsqueeze.default(getitem_209, 4);  getitem_209 = None
        permute_149 = torch.ops.aten.permute.default(unsqueeze_102, [0, 2, 4, 3, 1]);  unsqueeze_102 = None
        permute_150 = torch.ops.aten.permute.default(permute_149, [3, 1, 4, 0, 2]);  permute_149 = None
        view_244 = torch.ops.aten.view.default(permute_150, [256, 512, 512]);  permute_150 = None
        view_245 = torch.ops.aten.view.default(view_239, [1, 512, 512, 512]);  view_239 = None
        split_tensor_20 = torch.ops.aten.split.Tensor(view_245, 256, dim = -1);  view_245 = None
        getitem_212 = split_tensor_20[1];  split_tensor_20 = None
        unsqueeze_103 = torch.ops.aten.unsqueeze.default(getitem_212, 4);  getitem_212 = None
        permute_151 = torch.ops.aten.permute.default(unsqueeze_103, [0, 4, 2, 3, 1]);  unsqueeze_103 = None
        permute_152 = torch.ops.aten.permute.default(permute_151, [3, 4, 0, 2, 1]);  permute_151 = None
        view_246 = torch.ops.aten.view.default(permute_152, [256, 512, 512]);  permute_152 = None
        bmm_23 = torch.ops.aten.bmm.default(view_244, view_246);  view_244 = view_246 = None
        view_247 = torch.ops.aten.view.default(bmm_23, [256, 512, 1, 1, 512]);  bmm_23 = None
        permute_153 = torch.ops.aten.permute.default(view_247, [3, 1, 4, 0, 2]);  view_247 = None
        view_248 = torch.ops.aten.view.default(permute_153, [1, 512, 512, 256]);  permute_153 = None
        _to_copy_152 = torch.ops.aten._to_copy.default(view_232, dtype = torch.float32);  view_232 = None
        native_layer_norm_default_32 = torch.ops.aten.native_layer_norm.default(_to_copy_152, [256], None, None, 1e-05);  _to_copy_152 = None
        getitem_213 = native_layer_norm_default_32[0]
        _to_copy_153 = torch.ops.aten._to_copy.default(view_248, dtype = torch.float32);  view_248 = None
        native_layer_norm_default_33 = torch.ops.aten.native_layer_norm.default(_to_copy_153, [256], None, None, 1e-05);  _to_copy_153 = None
        getitem_216 = native_layer_norm_default_33[0]
        add_20 = torch.ops.aten.add.Tensor(getitem_213, getitem_216);  getitem_213 = getitem_216 = None
        _to_copy_154 = torch.ops.aten._to_copy.default(arg106_1, dtype = torch.bfloat16);  arg106_1 = None
        _to_copy_155 = torch.ops.aten._to_copy.default(add_20, dtype = torch.bfloat16);  add_20 = None
        t_54 = torch.ops.aten.t.default(_to_copy_154);  _to_copy_154 = None
        view_249 = torch.ops.aten.view.default(_to_copy_155, [262144, 256]);  _to_copy_155 = None
        mm_51 = torch.ops.aten.mm.default(view_249, t_54);  view_249 = t_54 = None
        view_250 = torch.ops.aten.view.default(mm_51, [1, 512, 512, 256]);  mm_51 = None
        _to_copy_156 = torch.ops.aten._to_copy.default(getitem_200, dtype = torch.bfloat16);  getitem_200 = None
        _to_copy_157 = torch.ops.aten._to_copy.default(getitem_193, dtype = torch.bfloat16);  getitem_193 = None
        t_55 = torch.ops.aten.t.default(_to_copy_156);  _to_copy_156 = None
        view_251 = torch.ops.aten.view.default(_to_copy_157, [262144, 256]);  _to_copy_157 = None
        mm_52 = torch.ops.aten.mm.default(view_251, t_55);  view_251 = t_55 = None
        view_252 = torch.ops.aten.view.default(mm_52, [1, 512, 512, 256]);  mm_52 = None
        sigmoid_16 = torch.ops.aten.sigmoid.default(view_252);  view_252 = None
        mul_25 = torch.ops.aten.mul.Tensor(view_250, sigmoid_16);  view_250 = sigmoid_16 = None
        add_21 = torch.ops.aten.add.Tensor(add_17, mul_25);  mul_25 = None
        split_tensor_21 = torch.ops.aten.split.Tensor(add_17, 512, dim = -2);  add_17 = None
        getitem_219 = split_tensor_21[0];  split_tensor_21 = None
        _to_copy_158 = torch.ops.aten._to_copy.default(getitem_219, dtype = torch.float32);  getitem_219 = None
        native_layer_norm_default_34 = torch.ops.aten.native_layer_norm.default(_to_copy_158, [256], arg88_1, arg89_1, 1e-05);  _to_copy_158 = arg88_1 = arg89_1 = None
        getitem_220 = native_layer_norm_default_34[0]
        _to_copy_159 = torch.ops.aten._to_copy.default(arg90_1, dtype = torch.bfloat16);  arg90_1 = None
        _to_copy_160 = torch.ops.aten._to_copy.default(getitem_220, dtype = torch.bfloat16);  getitem_220 = None
        t_56 = torch.ops.aten.t.default(_to_copy_159);  _to_copy_159 = None
        view_253 = torch.ops.aten.view.default(_to_copy_160, [262144, 256]);  _to_copy_160 = None
        mm_53 = torch.ops.aten.mm.default(view_253, t_56);  view_253 = t_56 = None
        view_254 = torch.ops.aten.view.default(mm_53, [1, 512, 512, 2048]);  mm_53 = None
        split_tensor_22 = torch.ops.aten.split.Tensor(view_254, 1024, dim = -1);  view_254 = None
        getitem_223 = split_tensor_22[0]
        getitem_224 = split_tensor_22[1];  split_tensor_22 = None
        silu_6 = torch.ops.aten.silu.default(getitem_223);  getitem_223 = None
        mul_26 = torch.ops.aten.mul.Tensor(silu_6, getitem_224);  silu_6 = getitem_224 = None
        _to_copy_161 = torch.ops.aten._to_copy.default(arg91_1, dtype = torch.bfloat16);  arg91_1 = None
        t_57 = torch.ops.aten.t.default(_to_copy_161);  _to_copy_161 = None
        view_256 = torch.ops.aten.view.default(mul_26, [262144, 1024]);  mul_26 = None
        mm_54 = torch.ops.aten.mm.default(view_256, t_57);  view_256 = t_57 = None
        view_257 = torch.ops.aten.view.default(mm_54, [1, 512, 512, 256]);  mm_54 = None
        add_22 = torch.ops.aten.add.Tensor(add_21, view_257);  add_21 = view_257 = None
        _to_copy_162 = torch.ops.aten._to_copy.default(add_22, dtype = torch.float32)
        native_layer_norm_default_35 = torch.ops.aten.native_layer_norm.default(_to_copy_162, [256], None, None, 1e-05);  _to_copy_162 = None
        getitem_225 = native_layer_norm_default_35[0]
        _to_copy_163 = torch.ops.aten._to_copy.default(arg125_1, dtype = torch.bfloat16);  arg125_1 = None
        _to_copy_164 = torch.ops.aten._to_copy.default(getitem_225, dtype = torch.bfloat16)
        t_58 = torch.ops.aten.t.default(_to_copy_163);  _to_copy_163 = None
        view_258 = torch.ops.aten.view.default(_to_copy_164, [262144, 256]);  _to_copy_164 = None
        mm_55 = torch.ops.aten.mm.default(view_258, t_58);  view_258 = t_58 = None
        view_259 = torch.ops.aten.view.default(mm_55, [1, 512, 512, 8]);  mm_55 = None
        view_260 = torch.ops.aten.view.default(view_259, [1, 512, 512, 2, 4]);  view_259 = None
        permute_154 = torch.ops.aten.permute.default(view_260, [0, 3, 4, 1, 2]);  view_260 = None
        view_261 = torch.ops.aten.view.default(permute_154, [1, 2, 4, 1, 512, 512]);  permute_154 = None
        view_262 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_20 = torch.ops.aten.bitwise_not.default(view_262);  view_262 = None
        masked_fill_20 = torch.ops.aten.masked_fill.Scalar(view_261, bitwise_not_20, -10000);  view_261 = bitwise_not_20 = None
        view_263 = torch.ops.aten.view.default(masked_fill_20, [1, 2, 4, 512, 512]);  masked_fill_20 = None
        permute_155 = torch.ops.aten.permute.default(view_263, [1, 0, 2, 3, 4]);  view_263 = None
        view_264 = torch.ops.aten.view.default(permute_155, [2, 4, 1, 512, 512]);  permute_155 = None
        _to_copy_165 = torch.ops.aten._to_copy.default(arg126_1, dtype = torch.bfloat16);  arg126_1 = None
        _to_copy_166 = torch.ops.aten._to_copy.default(getitem_225, dtype = torch.bfloat16)
        t_59 = torch.ops.aten.t.default(_to_copy_165);  _to_copy_165 = None
        view_265 = torch.ops.aten.view.default(_to_copy_166, [262144, 256]);  _to_copy_166 = None
        mm_56 = torch.ops.aten.mm.default(view_265, t_59);  view_265 = t_59 = None
        view_266 = torch.ops.aten.view.default(mm_56, [1, 512, 512, 1024]);  mm_56 = None
        select_5 = torch.ops.aten.select.int(view_264, 0, 0)
        view_267 = torch.ops.aten.view.default(view_266, [1, 512, 512, 4, 4, 64]);  view_266 = None
        permute_156 = torch.ops.aten.permute.default(view_267, [4, 0, 3, 1, 2, 5]);  view_267 = None
        view_268 = torch.ops.aten.view.default(permute_156, [4, 4, 512, 512, 64]);  permute_156 = None
        unbind_int_12 = torch.ops.aten.unbind.int(view_268);  view_268 = None
        getitem_228 = unbind_int_12[0]
        getitem_229 = unbind_int_12[1]
        getitem_230 = unbind_int_12[2]
        getitem_231 = unbind_int_12[3];  unbind_int_12 = None
        expand_11 = torch.ops.aten.expand.default(select_5, [4, 512, 512, 512]);  select_5 = None
        _scaled_dot_product_efficient_attention_default_4 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_228, getitem_229, getitem_230, expand_11, False);  getitem_228 = getitem_229 = getitem_230 = expand_11 = None
        getitem_232 = _scaled_dot_product_efficient_attention_default_4[0]
        sigmoid_17 = torch.ops.aten.sigmoid.default(getitem_231);  getitem_231 = None
        mul_27 = torch.ops.aten.mul.Tensor(getitem_232, sigmoid_17);  getitem_232 = sigmoid_17 = None
        view_269 = torch.ops.aten.view.default(mul_27, [1, 4, 512, 512, 64]);  mul_27 = None
        permute_157 = torch.ops.aten.permute.default(view_269, [0, 2, 3, 1, 4]);  view_269 = None
        clone_40 = torch.ops.aten.clone.default(permute_157, memory_format = torch.contiguous_format);  permute_157 = None
        _unsafe_view_36 = torch.ops.aten._unsafe_view.default(clone_40, [1, 512, 512, 256]);  clone_40 = None
        transpose_5 = torch.ops.aten.transpose.int(getitem_225, 1, 2);  getitem_225 = None
        _to_copy_167 = torch.ops.aten._to_copy.default(arg127_1, dtype = torch.bfloat16);  arg127_1 = None
        _to_copy_168 = torch.ops.aten._to_copy.default(transpose_5, dtype = torch.bfloat16);  transpose_5 = None
        t_60 = torch.ops.aten.t.default(_to_copy_167);  _to_copy_167 = None
        expand_12 = torch.ops.aten.expand.default(_to_copy_168, [1, 512, 512, 256]);  _to_copy_168 = None
        view_270 = torch.ops.aten.view.default(expand_12, [512, 512, 256]);  expand_12 = None
        expand_13 = torch.ops.aten.expand.default(t_60, [1, 512, 256, 1024]);  t_60 = None
        view_271 = torch.ops.aten.view.default(expand_13, [512, 256, 1024]);  expand_13 = None
        bmm_24 = torch.ops.aten.bmm.default(view_270, view_271);  view_270 = view_271 = None
        view_272 = torch.ops.aten.view.default(bmm_24, [1, 512, 512, 1024]);  bmm_24 = None
        select_6 = torch.ops.aten.select.int(view_264, 0, 1);  view_264 = None
        view_273 = torch.ops.aten.view.default(view_272, [1, 512, 512, 4, 4, 64]);  view_272 = None
        permute_158 = torch.ops.aten.permute.default(view_273, [4, 0, 3, 1, 2, 5]);  view_273 = None
        view_274 = torch.ops.aten.view.default(permute_158, [4, 4, 512, 512, 64]);  permute_158 = None
        unbind_int_13 = torch.ops.aten.unbind.int(view_274);  view_274 = None
        getitem_236 = unbind_int_13[0]
        getitem_237 = unbind_int_13[1]
        getitem_238 = unbind_int_13[2]
        getitem_239 = unbind_int_13[3];  unbind_int_13 = None
        expand_14 = torch.ops.aten.expand.default(select_6, [4, 512, 512, 512]);  select_6 = None
        _scaled_dot_product_efficient_attention_default_5 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_236, getitem_237, getitem_238, expand_14, False);  getitem_236 = getitem_237 = getitem_238 = expand_14 = None
        getitem_240 = _scaled_dot_product_efficient_attention_default_5[0]
        sigmoid_18 = torch.ops.aten.sigmoid.default(getitem_239);  getitem_239 = None
        mul_28 = torch.ops.aten.mul.Tensor(getitem_240, sigmoid_18);  getitem_240 = sigmoid_18 = None
        view_275 = torch.ops.aten.view.default(mul_28, [1, 4, 512, 512, 64]);  mul_28 = None
        permute_159 = torch.ops.aten.permute.default(view_275, [0, 2, 3, 1, 4]);  view_275 = None
        clone_41 = torch.ops.aten.clone.default(permute_159, memory_format = torch.contiguous_format);  permute_159 = None
        _unsafe_view_37 = torch.ops.aten._unsafe_view.default(clone_41, [1, 512, 512, 256]);  clone_41 = None
        cat_4 = torch.ops.aten.cat.default([_unsafe_view_36, _unsafe_view_37], dim = -1);  _unsafe_view_36 = _unsafe_view_37 = None
        slice_61 = torch.ops.aten.slice.Tensor(arg124_1, dim = 0, start = 0, end = 9223372036854775807);  arg124_1 = None
        unsqueeze_104 = torch.ops.aten.unsqueeze.default(slice_61, 1);  slice_61 = None
        mul_29 = torch.ops.aten.mul.Tensor(arg128_1, unsqueeze_104);  arg128_1 = unsqueeze_104 = None
        _to_copy_169 = torch.ops.aten._to_copy.default(mul_29, dtype = torch.bfloat16);  mul_29 = None
        t_61 = torch.ops.aten.t.default(_to_copy_169);  _to_copy_169 = None
        view_276 = torch.ops.aten.view.default(cat_4, [262144, 512]);  cat_4 = None
        mm_57 = torch.ops.aten.mm.default(view_276, t_61);  view_276 = t_61 = None
        view_277 = torch.ops.aten.view.default(mm_57, [1, 512, 512, 256]);  mm_57 = None
        add_23 = torch.ops.aten.add.Tensor(add_22, view_277);  add_22 = view_277 = None
        slice_62 = torch.ops.aten.slice.Tensor(add_19, dim = 0, start = 0, end = 9223372036854775807)
        slice_63 = torch.ops.aten.slice.Tensor(slice_62, dim = 1, start = 0, end = 4096);  slice_62 = None
        slice_64 = torch.ops.aten.slice.Tensor(slice_63, dim = 2, start = 0, end = 9223372036854775807);  slice_63 = None
        slice_65 = torch.ops.aten.slice.Tensor(slice_64, dim = 3, start = 0, end = 9223372036854775807);  slice_64 = None
        slice_66 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
        slice_67 = torch.ops.aten.slice.Tensor(slice_66, dim = 1, start = 0, end = 4096);  slice_66 = None
        slice_68 = torch.ops.aten.slice.Tensor(slice_67, dim = 2, start = 0, end = 9223372036854775807);  slice_67 = None
        _to_copy_170 = torch.ops.aten._to_copy.default(slice_65, dtype = torch.float32);  slice_65 = None
        native_layer_norm_default_36 = torch.ops.aten.native_layer_norm.default(_to_copy_170, [64], None, None, 1e-05);  _to_copy_170 = None
        getitem_244 = native_layer_norm_default_36[0]
        view_278 = torch.ops.aten.view.default(slice_68, [1, 4096, 512, 1]);  slice_68 = None
        bitwise_not_21 = torch.ops.aten.bitwise_not.default(view_278);  view_278 = None
        masked_fill_21 = torch.ops.aten.masked_fill.Scalar(getitem_244, bitwise_not_21, 0);  getitem_244 = bitwise_not_21 = None
        unbind_int_14 = torch.ops.aten.unbind.int(arg40_1)
        getitem_247 = unbind_int_14[0]
        getitem_248 = unbind_int_14[1];  unbind_int_14 = None
        _to_copy_171 = torch.ops.aten._to_copy.default(getitem_247, dtype = torch.bfloat16);  getitem_247 = None
        _to_copy_172 = torch.ops.aten._to_copy.default(masked_fill_21, dtype = torch.bfloat16)
        unsqueeze_105 = torch.ops.aten.unsqueeze.default(_to_copy_171, 3);  _to_copy_171 = None
        unsqueeze_106 = torch.ops.aten.unsqueeze.default(unsqueeze_105, 4);  unsqueeze_105 = None
        unsqueeze_107 = torch.ops.aten.unsqueeze.default(unsqueeze_106, 5);  unsqueeze_106 = None
        permute_160 = torch.ops.aten.permute.default(unsqueeze_107, [0, 1, 3, 4, 5, 2]);  unsqueeze_107 = None
        unsqueeze_108 = torch.ops.aten.unsqueeze.default(_to_copy_172, 4);  _to_copy_172 = None
        unsqueeze_109 = torch.ops.aten.unsqueeze.default(unsqueeze_108, 5);  unsqueeze_108 = None
        permute_161 = torch.ops.aten.permute.default(unsqueeze_109, [4, 5, 0, 1, 2, 3]);  unsqueeze_109 = None
        permute_162 = torch.ops.aten.permute.default(permute_160, [0, 1, 5, 2, 3, 4]);  permute_160 = None
        view_279 = torch.ops.aten.view.default(permute_162, [1, 64, 64]);  permute_162 = None
        permute_163 = torch.ops.aten.permute.default(permute_161, [5, 2, 3, 4, 0, 1]);  permute_161 = None
        view_280 = torch.ops.aten.view.default(permute_163, [1, 64, 2097152]);  permute_163 = None
        bmm_25 = torch.ops.aten.bmm.default(view_279, view_280);  view_279 = view_280 = None
        view_281 = torch.ops.aten.view.default(bmm_25, [8, 8, 1, 1, 4096, 512]);  bmm_25 = None
        permute_164 = torch.ops.aten.permute.default(view_281, [0, 1, 3, 4, 5, 2]);  view_281 = None
        view_282 = torch.ops.aten.view.default(permute_164, [8, 8, 1, 4096, 512]);  permute_164 = None
        _to_copy_173 = torch.ops.aten._to_copy.default(getitem_248, dtype = torch.bfloat16);  getitem_248 = None
        _to_copy_174 = torch.ops.aten._to_copy.default(masked_fill_21, dtype = torch.bfloat16);  masked_fill_21 = None
        unsqueeze_110 = torch.ops.aten.unsqueeze.default(_to_copy_173, 3);  _to_copy_173 = None
        unsqueeze_111 = torch.ops.aten.unsqueeze.default(unsqueeze_110, 4);  unsqueeze_110 = None
        unsqueeze_112 = torch.ops.aten.unsqueeze.default(unsqueeze_111, 5);  unsqueeze_111 = None
        permute_165 = torch.ops.aten.permute.default(unsqueeze_112, [0, 1, 3, 4, 5, 2]);  unsqueeze_112 = None
        unsqueeze_113 = torch.ops.aten.unsqueeze.default(_to_copy_174, 4);  _to_copy_174 = None
        unsqueeze_114 = torch.ops.aten.unsqueeze.default(unsqueeze_113, 5);  unsqueeze_113 = None
        permute_166 = torch.ops.aten.permute.default(unsqueeze_114, [4, 5, 0, 1, 2, 3]);  unsqueeze_114 = None
        permute_167 = torch.ops.aten.permute.default(permute_165, [0, 1, 5, 2, 3, 4]);  permute_165 = None
        view_283 = torch.ops.aten.view.default(permute_167, [1, 64, 64]);  permute_167 = None
        permute_168 = torch.ops.aten.permute.default(permute_166, [5, 2, 3, 4, 0, 1]);  permute_166 = None
        view_284 = torch.ops.aten.view.default(permute_168, [1, 64, 2097152]);  permute_168 = None
        bmm_26 = torch.ops.aten.bmm.default(view_283, view_284);  view_283 = view_284 = None
        view_285 = torch.ops.aten.view.default(bmm_26, [8, 8, 1, 1, 4096, 512]);  bmm_26 = None
        permute_169 = torch.ops.aten.permute.default(view_285, [0, 1, 3, 4, 5, 2]);  view_285 = None
        view_286 = torch.ops.aten.view.default(permute_169, [8, 8, 1, 4096, 512]);  permute_169 = None
        unsqueeze_115 = torch.ops.aten.unsqueeze.default(view_282, 5);  view_282 = None
        unsqueeze_116 = torch.ops.aten.unsqueeze.default(unsqueeze_115, 6);  unsqueeze_115 = None
        permute_170 = torch.ops.aten.permute.default(unsqueeze_116, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_116 = None
        unsqueeze_117 = torch.ops.aten.unsqueeze.default(view_286, 5);  view_286 = None
        unsqueeze_118 = torch.ops.aten.unsqueeze.default(unsqueeze_117, 6);  unsqueeze_117 = None
        permute_171 = torch.ops.aten.permute.default(unsqueeze_118, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_118 = None
        permute_172 = torch.ops.aten.permute.default(permute_170, [3, 1, 4, 6, 0, 2, 5]);  permute_170 = None
        clone_42 = torch.ops.aten.clone.default(permute_172, memory_format = torch.contiguous_format);  permute_172 = None
        _unsafe_view_38 = torch.ops.aten._unsafe_view.default(clone_42, [8, 4096, 4096]);  clone_42 = None
        permute_173 = torch.ops.aten.permute.default(permute_171, [3, 6, 0, 2, 5, 1, 4]);  permute_171 = None
        clone_43 = torch.ops.aten.clone.default(permute_173, memory_format = torch.contiguous_format);  permute_173 = None
        _unsafe_view_39 = torch.ops.aten._unsafe_view.default(clone_43, [8, 4096, 4096]);  clone_43 = None
        bmm_27 = torch.ops.aten.bmm.default(_unsafe_view_38, _unsafe_view_39);  _unsafe_view_38 = _unsafe_view_39 = None
        view_287 = torch.ops.aten.view.default(bmm_27, [8, 512, 8, 1, 1, 512, 8]);  bmm_27 = None
        permute_174 = torch.ops.aten.permute.default(view_287, [4, 1, 5, 0, 2, 6, 3]);  view_287 = None
        view_288 = torch.ops.aten.view.default(permute_174, [1, 512, 512, 8, 8, 8]);  permute_174 = None
        clone_44 = torch.ops.aten.clone.default(view_288, memory_format = torch.contiguous_format);  view_288 = None
        _unsafe_view_40 = torch.ops.aten._unsafe_view.default(clone_44, [1, 512, 512, 512]);  clone_44 = None
        add_24 = torch.ops.aten.add.Tensor(_unsafe_view_40, 0);  _unsafe_view_40 = None
        slice_69 = torch.ops.aten.slice.Tensor(add_19, dim = 0, start = 0, end = 9223372036854775807)
        slice_70 = torch.ops.aten.slice.Tensor(slice_69, dim = 1, start = 4096, end = 8192);  slice_69 = None
        slice_71 = torch.ops.aten.slice.Tensor(slice_70, dim = 2, start = 0, end = 9223372036854775807);  slice_70 = None
        slice_72 = torch.ops.aten.slice.Tensor(slice_71, dim = 3, start = 0, end = 9223372036854775807);  slice_71 = None
        slice_73 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
        slice_74 = torch.ops.aten.slice.Tensor(slice_73, dim = 1, start = 4096, end = 8192);  slice_73 = None
        slice_75 = torch.ops.aten.slice.Tensor(slice_74, dim = 2, start = 0, end = 9223372036854775807);  slice_74 = None
        _to_copy_175 = torch.ops.aten._to_copy.default(slice_72, dtype = torch.float32);  slice_72 = None
        native_layer_norm_default_37 = torch.ops.aten.native_layer_norm.default(_to_copy_175, [64], None, None, 1e-05);  _to_copy_175 = None
        getitem_249 = native_layer_norm_default_37[0]
        view_289 = torch.ops.aten.view.default(slice_75, [1, 4096, 512, 1]);  slice_75 = None
        bitwise_not_22 = torch.ops.aten.bitwise_not.default(view_289);  view_289 = None
        masked_fill_22 = torch.ops.aten.masked_fill.Scalar(getitem_249, bitwise_not_22, 0);  getitem_249 = bitwise_not_22 = None
        unbind_int_15 = torch.ops.aten.unbind.int(arg40_1)
        getitem_252 = unbind_int_15[0]
        getitem_253 = unbind_int_15[1];  unbind_int_15 = None
        _to_copy_176 = torch.ops.aten._to_copy.default(getitem_252, dtype = torch.bfloat16);  getitem_252 = None
        _to_copy_177 = torch.ops.aten._to_copy.default(masked_fill_22, dtype = torch.bfloat16)
        unsqueeze_119 = torch.ops.aten.unsqueeze.default(_to_copy_176, 3);  _to_copy_176 = None
        unsqueeze_120 = torch.ops.aten.unsqueeze.default(unsqueeze_119, 4);  unsqueeze_119 = None
        unsqueeze_121 = torch.ops.aten.unsqueeze.default(unsqueeze_120, 5);  unsqueeze_120 = None
        permute_175 = torch.ops.aten.permute.default(unsqueeze_121, [0, 1, 3, 4, 5, 2]);  unsqueeze_121 = None
        unsqueeze_122 = torch.ops.aten.unsqueeze.default(_to_copy_177, 4);  _to_copy_177 = None
        unsqueeze_123 = torch.ops.aten.unsqueeze.default(unsqueeze_122, 5);  unsqueeze_122 = None
        permute_176 = torch.ops.aten.permute.default(unsqueeze_123, [4, 5, 0, 1, 2, 3]);  unsqueeze_123 = None
        permute_177 = torch.ops.aten.permute.default(permute_175, [0, 1, 5, 2, 3, 4]);  permute_175 = None
        view_290 = torch.ops.aten.view.default(permute_177, [1, 64, 64]);  permute_177 = None
        permute_178 = torch.ops.aten.permute.default(permute_176, [5, 2, 3, 4, 0, 1]);  permute_176 = None
        view_291 = torch.ops.aten.view.default(permute_178, [1, 64, 2097152]);  permute_178 = None
        bmm_28 = torch.ops.aten.bmm.default(view_290, view_291);  view_290 = view_291 = None
        view_292 = torch.ops.aten.view.default(bmm_28, [8, 8, 1, 1, 4096, 512]);  bmm_28 = None
        permute_179 = torch.ops.aten.permute.default(view_292, [0, 1, 3, 4, 5, 2]);  view_292 = None
        view_293 = torch.ops.aten.view.default(permute_179, [8, 8, 1, 4096, 512]);  permute_179 = None
        _to_copy_178 = torch.ops.aten._to_copy.default(getitem_253, dtype = torch.bfloat16);  getitem_253 = None
        _to_copy_179 = torch.ops.aten._to_copy.default(masked_fill_22, dtype = torch.bfloat16);  masked_fill_22 = None
        unsqueeze_124 = torch.ops.aten.unsqueeze.default(_to_copy_178, 3);  _to_copy_178 = None
        unsqueeze_125 = torch.ops.aten.unsqueeze.default(unsqueeze_124, 4);  unsqueeze_124 = None
        unsqueeze_126 = torch.ops.aten.unsqueeze.default(unsqueeze_125, 5);  unsqueeze_125 = None
        permute_180 = torch.ops.aten.permute.default(unsqueeze_126, [0, 1, 3, 4, 5, 2]);  unsqueeze_126 = None
        unsqueeze_127 = torch.ops.aten.unsqueeze.default(_to_copy_179, 4);  _to_copy_179 = None
        unsqueeze_128 = torch.ops.aten.unsqueeze.default(unsqueeze_127, 5);  unsqueeze_127 = None
        permute_181 = torch.ops.aten.permute.default(unsqueeze_128, [4, 5, 0, 1, 2, 3]);  unsqueeze_128 = None
        permute_182 = torch.ops.aten.permute.default(permute_180, [0, 1, 5, 2, 3, 4]);  permute_180 = None
        view_294 = torch.ops.aten.view.default(permute_182, [1, 64, 64]);  permute_182 = None
        permute_183 = torch.ops.aten.permute.default(permute_181, [5, 2, 3, 4, 0, 1]);  permute_181 = None
        view_295 = torch.ops.aten.view.default(permute_183, [1, 64, 2097152]);  permute_183 = None
        bmm_29 = torch.ops.aten.bmm.default(view_294, view_295);  view_294 = view_295 = None
        view_296 = torch.ops.aten.view.default(bmm_29, [8, 8, 1, 1, 4096, 512]);  bmm_29 = None
        permute_184 = torch.ops.aten.permute.default(view_296, [0, 1, 3, 4, 5, 2]);  view_296 = None
        view_297 = torch.ops.aten.view.default(permute_184, [8, 8, 1, 4096, 512]);  permute_184 = None
        unsqueeze_129 = torch.ops.aten.unsqueeze.default(view_293, 5);  view_293 = None
        unsqueeze_130 = torch.ops.aten.unsqueeze.default(unsqueeze_129, 6);  unsqueeze_129 = None
        permute_185 = torch.ops.aten.permute.default(unsqueeze_130, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_130 = None
        unsqueeze_131 = torch.ops.aten.unsqueeze.default(view_297, 5);  view_297 = None
        unsqueeze_132 = torch.ops.aten.unsqueeze.default(unsqueeze_131, 6);  unsqueeze_131 = None
        permute_186 = torch.ops.aten.permute.default(unsqueeze_132, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_132 = None
        permute_187 = torch.ops.aten.permute.default(permute_185, [3, 1, 4, 6, 0, 2, 5]);  permute_185 = None
        clone_45 = torch.ops.aten.clone.default(permute_187, memory_format = torch.contiguous_format);  permute_187 = None
        _unsafe_view_41 = torch.ops.aten._unsafe_view.default(clone_45, [8, 4096, 4096]);  clone_45 = None
        permute_188 = torch.ops.aten.permute.default(permute_186, [3, 6, 0, 2, 5, 1, 4]);  permute_186 = None
        clone_46 = torch.ops.aten.clone.default(permute_188, memory_format = torch.contiguous_format);  permute_188 = None
        _unsafe_view_42 = torch.ops.aten._unsafe_view.default(clone_46, [8, 4096, 4096]);  clone_46 = None
        bmm_30 = torch.ops.aten.bmm.default(_unsafe_view_41, _unsafe_view_42);  _unsafe_view_41 = _unsafe_view_42 = None
        view_298 = torch.ops.aten.view.default(bmm_30, [8, 512, 8, 1, 1, 512, 8]);  bmm_30 = None
        permute_189 = torch.ops.aten.permute.default(view_298, [4, 1, 5, 0, 2, 6, 3]);  view_298 = None
        view_299 = torch.ops.aten.view.default(permute_189, [1, 512, 512, 8, 8, 8]);  permute_189 = None
        clone_47 = torch.ops.aten.clone.default(view_299, memory_format = torch.contiguous_format);  view_299 = None
        _unsafe_view_43 = torch.ops.aten._unsafe_view.default(clone_47, [1, 512, 512, 512]);  clone_47 = None
        add_25 = torch.ops.aten.add.Tensor(add_24, _unsafe_view_43);  add_24 = _unsafe_view_43 = None
        slice_76 = torch.ops.aten.slice.Tensor(add_19, dim = 0, start = 0, end = 9223372036854775807)
        slice_77 = torch.ops.aten.slice.Tensor(slice_76, dim = 1, start = 8192, end = 12288);  slice_76 = None
        slice_78 = torch.ops.aten.slice.Tensor(slice_77, dim = 2, start = 0, end = 9223372036854775807);  slice_77 = None
        slice_79 = torch.ops.aten.slice.Tensor(slice_78, dim = 3, start = 0, end = 9223372036854775807);  slice_78 = None
        slice_80 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
        slice_81 = torch.ops.aten.slice.Tensor(slice_80, dim = 1, start = 8192, end = 12288);  slice_80 = None
        slice_82 = torch.ops.aten.slice.Tensor(slice_81, dim = 2, start = 0, end = 9223372036854775807);  slice_81 = None
        _to_copy_180 = torch.ops.aten._to_copy.default(slice_79, dtype = torch.float32);  slice_79 = None
        native_layer_norm_default_38 = torch.ops.aten.native_layer_norm.default(_to_copy_180, [64], None, None, 1e-05);  _to_copy_180 = None
        getitem_254 = native_layer_norm_default_38[0]
        view_300 = torch.ops.aten.view.default(slice_82, [1, 4096, 512, 1]);  slice_82 = None
        bitwise_not_23 = torch.ops.aten.bitwise_not.default(view_300);  view_300 = None
        masked_fill_23 = torch.ops.aten.masked_fill.Scalar(getitem_254, bitwise_not_23, 0);  getitem_254 = bitwise_not_23 = None
        unbind_int_16 = torch.ops.aten.unbind.int(arg40_1)
        getitem_257 = unbind_int_16[0]
        getitem_258 = unbind_int_16[1];  unbind_int_16 = None
        _to_copy_181 = torch.ops.aten._to_copy.default(getitem_257, dtype = torch.bfloat16);  getitem_257 = None
        _to_copy_182 = torch.ops.aten._to_copy.default(masked_fill_23, dtype = torch.bfloat16)
        unsqueeze_133 = torch.ops.aten.unsqueeze.default(_to_copy_181, 3);  _to_copy_181 = None
        unsqueeze_134 = torch.ops.aten.unsqueeze.default(unsqueeze_133, 4);  unsqueeze_133 = None
        unsqueeze_135 = torch.ops.aten.unsqueeze.default(unsqueeze_134, 5);  unsqueeze_134 = None
        permute_190 = torch.ops.aten.permute.default(unsqueeze_135, [0, 1, 3, 4, 5, 2]);  unsqueeze_135 = None
        unsqueeze_136 = torch.ops.aten.unsqueeze.default(_to_copy_182, 4);  _to_copy_182 = None
        unsqueeze_137 = torch.ops.aten.unsqueeze.default(unsqueeze_136, 5);  unsqueeze_136 = None
        permute_191 = torch.ops.aten.permute.default(unsqueeze_137, [4, 5, 0, 1, 2, 3]);  unsqueeze_137 = None
        permute_192 = torch.ops.aten.permute.default(permute_190, [0, 1, 5, 2, 3, 4]);  permute_190 = None
        view_301 = torch.ops.aten.view.default(permute_192, [1, 64, 64]);  permute_192 = None
        permute_193 = torch.ops.aten.permute.default(permute_191, [5, 2, 3, 4, 0, 1]);  permute_191 = None
        view_302 = torch.ops.aten.view.default(permute_193, [1, 64, 2097152]);  permute_193 = None
        bmm_31 = torch.ops.aten.bmm.default(view_301, view_302);  view_301 = view_302 = None
        view_303 = torch.ops.aten.view.default(bmm_31, [8, 8, 1, 1, 4096, 512]);  bmm_31 = None
        permute_194 = torch.ops.aten.permute.default(view_303, [0, 1, 3, 4, 5, 2]);  view_303 = None
        view_304 = torch.ops.aten.view.default(permute_194, [8, 8, 1, 4096, 512]);  permute_194 = None
        _to_copy_183 = torch.ops.aten._to_copy.default(getitem_258, dtype = torch.bfloat16);  getitem_258 = None
        _to_copy_184 = torch.ops.aten._to_copy.default(masked_fill_23, dtype = torch.bfloat16);  masked_fill_23 = None
        unsqueeze_138 = torch.ops.aten.unsqueeze.default(_to_copy_183, 3);  _to_copy_183 = None
        unsqueeze_139 = torch.ops.aten.unsqueeze.default(unsqueeze_138, 4);  unsqueeze_138 = None
        unsqueeze_140 = torch.ops.aten.unsqueeze.default(unsqueeze_139, 5);  unsqueeze_139 = None
        permute_195 = torch.ops.aten.permute.default(unsqueeze_140, [0, 1, 3, 4, 5, 2]);  unsqueeze_140 = None
        unsqueeze_141 = torch.ops.aten.unsqueeze.default(_to_copy_184, 4);  _to_copy_184 = None
        unsqueeze_142 = torch.ops.aten.unsqueeze.default(unsqueeze_141, 5);  unsqueeze_141 = None
        permute_196 = torch.ops.aten.permute.default(unsqueeze_142, [4, 5, 0, 1, 2, 3]);  unsqueeze_142 = None
        permute_197 = torch.ops.aten.permute.default(permute_195, [0, 1, 5, 2, 3, 4]);  permute_195 = None
        view_305 = torch.ops.aten.view.default(permute_197, [1, 64, 64]);  permute_197 = None
        permute_198 = torch.ops.aten.permute.default(permute_196, [5, 2, 3, 4, 0, 1]);  permute_196 = None
        view_306 = torch.ops.aten.view.default(permute_198, [1, 64, 2097152]);  permute_198 = None
        bmm_32 = torch.ops.aten.bmm.default(view_305, view_306);  view_305 = view_306 = None
        view_307 = torch.ops.aten.view.default(bmm_32, [8, 8, 1, 1, 4096, 512]);  bmm_32 = None
        permute_199 = torch.ops.aten.permute.default(view_307, [0, 1, 3, 4, 5, 2]);  view_307 = None
        view_308 = torch.ops.aten.view.default(permute_199, [8, 8, 1, 4096, 512]);  permute_199 = None
        unsqueeze_143 = torch.ops.aten.unsqueeze.default(view_304, 5);  view_304 = None
        unsqueeze_144 = torch.ops.aten.unsqueeze.default(unsqueeze_143, 6);  unsqueeze_143 = None
        permute_200 = torch.ops.aten.permute.default(unsqueeze_144, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_144 = None
        unsqueeze_145 = torch.ops.aten.unsqueeze.default(view_308, 5);  view_308 = None
        unsqueeze_146 = torch.ops.aten.unsqueeze.default(unsqueeze_145, 6);  unsqueeze_145 = None
        permute_201 = torch.ops.aten.permute.default(unsqueeze_146, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_146 = None
        permute_202 = torch.ops.aten.permute.default(permute_200, [3, 1, 4, 6, 0, 2, 5]);  permute_200 = None
        clone_48 = torch.ops.aten.clone.default(permute_202, memory_format = torch.contiguous_format);  permute_202 = None
        _unsafe_view_44 = torch.ops.aten._unsafe_view.default(clone_48, [8, 4096, 4096]);  clone_48 = None
        permute_203 = torch.ops.aten.permute.default(permute_201, [3, 6, 0, 2, 5, 1, 4]);  permute_201 = None
        clone_49 = torch.ops.aten.clone.default(permute_203, memory_format = torch.contiguous_format);  permute_203 = None
        _unsafe_view_45 = torch.ops.aten._unsafe_view.default(clone_49, [8, 4096, 4096]);  clone_49 = None
        bmm_33 = torch.ops.aten.bmm.default(_unsafe_view_44, _unsafe_view_45);  _unsafe_view_44 = _unsafe_view_45 = None
        view_309 = torch.ops.aten.view.default(bmm_33, [8, 512, 8, 1, 1, 512, 8]);  bmm_33 = None
        permute_204 = torch.ops.aten.permute.default(view_309, [4, 1, 5, 0, 2, 6, 3]);  view_309 = None
        view_310 = torch.ops.aten.view.default(permute_204, [1, 512, 512, 8, 8, 8]);  permute_204 = None
        clone_50 = torch.ops.aten.clone.default(view_310, memory_format = torch.contiguous_format);  view_310 = None
        _unsafe_view_46 = torch.ops.aten._unsafe_view.default(clone_50, [1, 512, 512, 512]);  clone_50 = None
        add_26 = torch.ops.aten.add.Tensor(add_25, _unsafe_view_46);  add_25 = _unsafe_view_46 = None
        slice_83 = torch.ops.aten.slice.Tensor(add_19, dim = 0, start = 0, end = 9223372036854775807)
        slice_84 = torch.ops.aten.slice.Tensor(slice_83, dim = 1, start = 12288, end = 16384);  slice_83 = None
        slice_85 = torch.ops.aten.slice.Tensor(slice_84, dim = 2, start = 0, end = 9223372036854775807);  slice_84 = None
        slice_86 = torch.ops.aten.slice.Tensor(slice_85, dim = 3, start = 0, end = 9223372036854775807);  slice_85 = None
        slice_87 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
        slice_88 = torch.ops.aten.slice.Tensor(slice_87, dim = 1, start = 12288, end = 16384);  slice_87 = None
        slice_89 = torch.ops.aten.slice.Tensor(slice_88, dim = 2, start = 0, end = 9223372036854775807);  slice_88 = None
        _to_copy_185 = torch.ops.aten._to_copy.default(slice_86, dtype = torch.float32);  slice_86 = None
        native_layer_norm_default_39 = torch.ops.aten.native_layer_norm.default(_to_copy_185, [64], None, None, 1e-05);  _to_copy_185 = None
        getitem_259 = native_layer_norm_default_39[0]
        view_311 = torch.ops.aten.view.default(slice_89, [1, 4096, 512, 1]);  slice_89 = None
        bitwise_not_24 = torch.ops.aten.bitwise_not.default(view_311);  view_311 = None
        masked_fill_24 = torch.ops.aten.masked_fill.Scalar(getitem_259, bitwise_not_24, 0);  getitem_259 = bitwise_not_24 = None
        unbind_int_17 = torch.ops.aten.unbind.int(arg40_1);  arg40_1 = None
        getitem_262 = unbind_int_17[0]
        getitem_263 = unbind_int_17[1];  unbind_int_17 = None
        _to_copy_186 = torch.ops.aten._to_copy.default(getitem_262, dtype = torch.bfloat16);  getitem_262 = None
        _to_copy_187 = torch.ops.aten._to_copy.default(masked_fill_24, dtype = torch.bfloat16)
        unsqueeze_147 = torch.ops.aten.unsqueeze.default(_to_copy_186, 3);  _to_copy_186 = None
        unsqueeze_148 = torch.ops.aten.unsqueeze.default(unsqueeze_147, 4);  unsqueeze_147 = None
        unsqueeze_149 = torch.ops.aten.unsqueeze.default(unsqueeze_148, 5);  unsqueeze_148 = None
        permute_205 = torch.ops.aten.permute.default(unsqueeze_149, [0, 1, 3, 4, 5, 2]);  unsqueeze_149 = None
        unsqueeze_150 = torch.ops.aten.unsqueeze.default(_to_copy_187, 4);  _to_copy_187 = None
        unsqueeze_151 = torch.ops.aten.unsqueeze.default(unsqueeze_150, 5);  unsqueeze_150 = None
        permute_206 = torch.ops.aten.permute.default(unsqueeze_151, [4, 5, 0, 1, 2, 3]);  unsqueeze_151 = None
        permute_207 = torch.ops.aten.permute.default(permute_205, [0, 1, 5, 2, 3, 4]);  permute_205 = None
        view_312 = torch.ops.aten.view.default(permute_207, [1, 64, 64]);  permute_207 = None
        permute_208 = torch.ops.aten.permute.default(permute_206, [5, 2, 3, 4, 0, 1]);  permute_206 = None
        view_313 = torch.ops.aten.view.default(permute_208, [1, 64, 2097152]);  permute_208 = None
        bmm_34 = torch.ops.aten.bmm.default(view_312, view_313);  view_312 = view_313 = None
        view_314 = torch.ops.aten.view.default(bmm_34, [8, 8, 1, 1, 4096, 512]);  bmm_34 = None
        permute_209 = torch.ops.aten.permute.default(view_314, [0, 1, 3, 4, 5, 2]);  view_314 = None
        view_315 = torch.ops.aten.view.default(permute_209, [8, 8, 1, 4096, 512]);  permute_209 = None
        _to_copy_188 = torch.ops.aten._to_copy.default(getitem_263, dtype = torch.bfloat16);  getitem_263 = None
        _to_copy_189 = torch.ops.aten._to_copy.default(masked_fill_24, dtype = torch.bfloat16);  masked_fill_24 = None
        unsqueeze_152 = torch.ops.aten.unsqueeze.default(_to_copy_188, 3);  _to_copy_188 = None
        unsqueeze_153 = torch.ops.aten.unsqueeze.default(unsqueeze_152, 4);  unsqueeze_152 = None
        unsqueeze_154 = torch.ops.aten.unsqueeze.default(unsqueeze_153, 5);  unsqueeze_153 = None
        permute_210 = torch.ops.aten.permute.default(unsqueeze_154, [0, 1, 3, 4, 5, 2]);  unsqueeze_154 = None
        unsqueeze_155 = torch.ops.aten.unsqueeze.default(_to_copy_189, 4);  _to_copy_189 = None
        unsqueeze_156 = torch.ops.aten.unsqueeze.default(unsqueeze_155, 5);  unsqueeze_155 = None
        permute_211 = torch.ops.aten.permute.default(unsqueeze_156, [4, 5, 0, 1, 2, 3]);  unsqueeze_156 = None
        permute_212 = torch.ops.aten.permute.default(permute_210, [0, 1, 5, 2, 3, 4]);  permute_210 = None
        view_316 = torch.ops.aten.view.default(permute_212, [1, 64, 64]);  permute_212 = None
        permute_213 = torch.ops.aten.permute.default(permute_211, [5, 2, 3, 4, 0, 1]);  permute_211 = None
        view_317 = torch.ops.aten.view.default(permute_213, [1, 64, 2097152]);  permute_213 = None
        bmm_35 = torch.ops.aten.bmm.default(view_316, view_317);  view_316 = view_317 = None
        view_318 = torch.ops.aten.view.default(bmm_35, [8, 8, 1, 1, 4096, 512]);  bmm_35 = None
        permute_214 = torch.ops.aten.permute.default(view_318, [0, 1, 3, 4, 5, 2]);  view_318 = None
        view_319 = torch.ops.aten.view.default(permute_214, [8, 8, 1, 4096, 512]);  permute_214 = None
        unsqueeze_157 = torch.ops.aten.unsqueeze.default(view_315, 5);  view_315 = None
        unsqueeze_158 = torch.ops.aten.unsqueeze.default(unsqueeze_157, 6);  unsqueeze_157 = None
        permute_215 = torch.ops.aten.permute.default(unsqueeze_158, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_158 = None
        unsqueeze_159 = torch.ops.aten.unsqueeze.default(view_319, 5);  view_319 = None
        unsqueeze_160 = torch.ops.aten.unsqueeze.default(unsqueeze_159, 6);  unsqueeze_159 = None
        permute_216 = torch.ops.aten.permute.default(unsqueeze_160, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_160 = None
        permute_217 = torch.ops.aten.permute.default(permute_215, [3, 1, 4, 6, 0, 2, 5]);  permute_215 = None
        clone_51 = torch.ops.aten.clone.default(permute_217, memory_format = torch.contiguous_format);  permute_217 = None
        _unsafe_view_47 = torch.ops.aten._unsafe_view.default(clone_51, [8, 4096, 4096]);  clone_51 = None
        permute_218 = torch.ops.aten.permute.default(permute_216, [3, 6, 0, 2, 5, 1, 4]);  permute_216 = None
        clone_52 = torch.ops.aten.clone.default(permute_218, memory_format = torch.contiguous_format);  permute_218 = None
        _unsafe_view_48 = torch.ops.aten._unsafe_view.default(clone_52, [8, 4096, 4096]);  clone_52 = None
        bmm_36 = torch.ops.aten.bmm.default(_unsafe_view_47, _unsafe_view_48);  _unsafe_view_47 = _unsafe_view_48 = None
        view_320 = torch.ops.aten.view.default(bmm_36, [8, 512, 8, 1, 1, 512, 8]);  bmm_36 = None
        permute_219 = torch.ops.aten.permute.default(view_320, [4, 1, 5, 0, 2, 6, 3]);  view_320 = None
        view_321 = torch.ops.aten.view.default(permute_219, [1, 512, 512, 8, 8, 8]);  permute_219 = None
        clone_53 = torch.ops.aten.clone.default(view_321, memory_format = torch.contiguous_format);  view_321 = None
        _unsafe_view_49 = torch.ops.aten._unsafe_view.default(clone_53, [1, 512, 512, 512]);  clone_53 = None
        add_27 = torch.ops.aten.add.Tensor(add_26, _unsafe_view_49);  add_26 = _unsafe_view_49 = None
        _to_copy_190 = torch.ops.aten._to_copy.default(add_27, dtype = torch.float32);  add_27 = None
        native_layer_norm_default_40 = torch.ops.aten.native_layer_norm.default(_to_copy_190, [512], arg41_1, arg42_1, 0.1);  _to_copy_190 = arg41_1 = arg42_1 = None
        getitem_264 = native_layer_norm_default_40[0]
        _to_copy_191 = torch.ops.aten._to_copy.default(arg44_1, dtype = torch.bfloat16);  arg44_1 = None
        _to_copy_192 = torch.ops.aten._to_copy.default(arg43_1, dtype = torch.bfloat16);  arg43_1 = None
        _to_copy_193 = torch.ops.aten._to_copy.default(getitem_264, dtype = torch.bfloat16);  getitem_264 = None
        view_322 = torch.ops.aten.view.default(_to_copy_193, [262144, 512]);  _to_copy_193 = None
        t_62 = torch.ops.aten.t.default(_to_copy_192);  _to_copy_192 = None
        addmm_1 = torch.ops.aten.addmm.default(_to_copy_191, view_322, t_62);  _to_copy_191 = view_322 = t_62 = None
        view_323 = torch.ops.aten.view.default(addmm_1, [1, 512, 512, 256]);  addmm_1 = None
        add_28 = torch.ops.aten.add.Tensor(add_23, view_323);  add_23 = view_323 = None
        split_tensor_23 = torch.ops.aten.split.Tensor(add_19, 128, dim = -2)
        getitem_267 = split_tensor_23[0]
        getitem_268 = split_tensor_23[1]
        getitem_269 = split_tensor_23[2]
        getitem_270 = split_tensor_23[3];  split_tensor_23 = None
        _to_copy_194 = torch.ops.aten._to_copy.default(getitem_267, dtype = torch.float32);  getitem_267 = None
        native_layer_norm_default_41 = torch.ops.aten.native_layer_norm.default(_to_copy_194, [64], arg80_1, arg81_1, 1e-05);  _to_copy_194 = None
        getitem_271 = native_layer_norm_default_41[0]
        _to_copy_195 = torch.ops.aten._to_copy.default(arg82_1, dtype = torch.bfloat16)
        _to_copy_196 = torch.ops.aten._to_copy.default(getitem_271, dtype = torch.bfloat16);  getitem_271 = None
        t_63 = torch.ops.aten.t.default(_to_copy_195);  _to_copy_195 = None
        view_324 = torch.ops.aten.view.default(_to_copy_196, [2097152, 64]);  _to_copy_196 = None
        mm_58 = torch.ops.aten.mm.default(view_324, t_63);  view_324 = t_63 = None
        view_325 = torch.ops.aten.view.default(mm_58, [1, 16384, 128, 512]);  mm_58 = None
        split_tensor_24 = torch.ops.aten.split.Tensor(view_325, 256, dim = -1);  view_325 = None
        getitem_274 = split_tensor_24[0]
        getitem_275 = split_tensor_24[1];  split_tensor_24 = None
        silu_7 = torch.ops.aten.silu.default(getitem_274);  getitem_274 = None
        mul_30 = torch.ops.aten.mul.Tensor(silu_7, getitem_275);  silu_7 = getitem_275 = None
        _to_copy_197 = torch.ops.aten._to_copy.default(arg83_1, dtype = torch.bfloat16)
        t_64 = torch.ops.aten.t.default(_to_copy_197);  _to_copy_197 = None
        view_327 = torch.ops.aten.view.default(mul_30, [2097152, 256]);  mul_30 = None
        mm_59 = torch.ops.aten.mm.default(view_327, t_64);  view_327 = t_64 = None
        view_328 = torch.ops.aten.view.default(mm_59, [1, 16384, 128, 64]);  mm_59 = None
        _to_copy_198 = torch.ops.aten._to_copy.default(getitem_268, dtype = torch.float32);  getitem_268 = None
        native_layer_norm_default_42 = torch.ops.aten.native_layer_norm.default(_to_copy_198, [64], arg80_1, arg81_1, 1e-05);  _to_copy_198 = None
        getitem_276 = native_layer_norm_default_42[0]
        _to_copy_199 = torch.ops.aten._to_copy.default(arg82_1, dtype = torch.bfloat16)
        _to_copy_200 = torch.ops.aten._to_copy.default(getitem_276, dtype = torch.bfloat16);  getitem_276 = None
        t_65 = torch.ops.aten.t.default(_to_copy_199);  _to_copy_199 = None
        view_329 = torch.ops.aten.view.default(_to_copy_200, [2097152, 64]);  _to_copy_200 = None
        mm_60 = torch.ops.aten.mm.default(view_329, t_65);  view_329 = t_65 = None
        view_330 = torch.ops.aten.view.default(mm_60, [1, 16384, 128, 512]);  mm_60 = None
        split_tensor_25 = torch.ops.aten.split.Tensor(view_330, 256, dim = -1);  view_330 = None
        getitem_279 = split_tensor_25[0]
        getitem_280 = split_tensor_25[1];  split_tensor_25 = None
        silu_8 = torch.ops.aten.silu.default(getitem_279);  getitem_279 = None
        mul_31 = torch.ops.aten.mul.Tensor(silu_8, getitem_280);  silu_8 = getitem_280 = None
        _to_copy_201 = torch.ops.aten._to_copy.default(arg83_1, dtype = torch.bfloat16)
        t_66 = torch.ops.aten.t.default(_to_copy_201);  _to_copy_201 = None
        view_332 = torch.ops.aten.view.default(mul_31, [2097152, 256]);  mul_31 = None
        mm_61 = torch.ops.aten.mm.default(view_332, t_66);  view_332 = t_66 = None
        view_333 = torch.ops.aten.view.default(mm_61, [1, 16384, 128, 64]);  mm_61 = None
        _to_copy_202 = torch.ops.aten._to_copy.default(getitem_269, dtype = torch.float32);  getitem_269 = None
        native_layer_norm_default_43 = torch.ops.aten.native_layer_norm.default(_to_copy_202, [64], arg80_1, arg81_1, 1e-05);  _to_copy_202 = None
        getitem_281 = native_layer_norm_default_43[0]
        _to_copy_203 = torch.ops.aten._to_copy.default(arg82_1, dtype = torch.bfloat16)
        _to_copy_204 = torch.ops.aten._to_copy.default(getitem_281, dtype = torch.bfloat16);  getitem_281 = None
        t_67 = torch.ops.aten.t.default(_to_copy_203);  _to_copy_203 = None
        view_334 = torch.ops.aten.view.default(_to_copy_204, [2097152, 64]);  _to_copy_204 = None
        mm_62 = torch.ops.aten.mm.default(view_334, t_67);  view_334 = t_67 = None
        view_335 = torch.ops.aten.view.default(mm_62, [1, 16384, 128, 512]);  mm_62 = None
        split_tensor_26 = torch.ops.aten.split.Tensor(view_335, 256, dim = -1);  view_335 = None
        getitem_284 = split_tensor_26[0]
        getitem_285 = split_tensor_26[1];  split_tensor_26 = None
        silu_9 = torch.ops.aten.silu.default(getitem_284);  getitem_284 = None
        mul_32 = torch.ops.aten.mul.Tensor(silu_9, getitem_285);  silu_9 = getitem_285 = None
        _to_copy_205 = torch.ops.aten._to_copy.default(arg83_1, dtype = torch.bfloat16)
        t_68 = torch.ops.aten.t.default(_to_copy_205);  _to_copy_205 = None
        view_337 = torch.ops.aten.view.default(mul_32, [2097152, 256]);  mul_32 = None
        mm_63 = torch.ops.aten.mm.default(view_337, t_68);  view_337 = t_68 = None
        view_338 = torch.ops.aten.view.default(mm_63, [1, 16384, 128, 64]);  mm_63 = None
        _to_copy_206 = torch.ops.aten._to_copy.default(getitem_270, dtype = torch.float32);  getitem_270 = None
        native_layer_norm_default_44 = torch.ops.aten.native_layer_norm.default(_to_copy_206, [64], arg80_1, arg81_1, 1e-05);  _to_copy_206 = arg80_1 = arg81_1 = None
        getitem_286 = native_layer_norm_default_44[0]
        _to_copy_207 = torch.ops.aten._to_copy.default(arg82_1, dtype = torch.bfloat16);  arg82_1 = None
        _to_copy_208 = torch.ops.aten._to_copy.default(getitem_286, dtype = torch.bfloat16);  getitem_286 = None
        t_69 = torch.ops.aten.t.default(_to_copy_207);  _to_copy_207 = None
        view_339 = torch.ops.aten.view.default(_to_copy_208, [2097152, 64]);  _to_copy_208 = None
        mm_64 = torch.ops.aten.mm.default(view_339, t_69);  view_339 = t_69 = None
        view_340 = torch.ops.aten.view.default(mm_64, [1, 16384, 128, 512]);  mm_64 = None
        split_tensor_27 = torch.ops.aten.split.Tensor(view_340, 256, dim = -1);  view_340 = None
        getitem_289 = split_tensor_27[0]
        getitem_290 = split_tensor_27[1];  split_tensor_27 = None
        silu_10 = torch.ops.aten.silu.default(getitem_289);  getitem_289 = None
        mul_33 = torch.ops.aten.mul.Tensor(silu_10, getitem_290);  silu_10 = getitem_290 = None
        _to_copy_209 = torch.ops.aten._to_copy.default(arg83_1, dtype = torch.bfloat16);  arg83_1 = None
        t_70 = torch.ops.aten.t.default(_to_copy_209);  _to_copy_209 = None
        view_342 = torch.ops.aten.view.default(mul_33, [2097152, 256]);  mul_33 = None
        mm_65 = torch.ops.aten.mm.default(view_342, t_70);  view_342 = t_70 = None
        view_343 = torch.ops.aten.view.default(mm_65, [1, 16384, 128, 64]);  mm_65 = None
        cat_5 = torch.ops.aten.cat.default([view_328, view_333, view_338, view_343], dim = -2);  view_328 = view_333 = view_338 = view_343 = None
        add_29 = torch.ops.aten.add.Tensor(add_19, cat_5);  cat_5 = None
        slice_90 = torch.ops.aten.slice.Tensor(add_19, dim = 0, start = 0, end = 9223372036854775807)
        slice_91 = torch.ops.aten.slice.Tensor(slice_90, dim = 1, start = 0, end = 4096);  slice_90 = None
        slice_92 = torch.ops.aten.slice.Tensor(slice_91, dim = 2, start = 0, end = 9223372036854775807);  slice_91 = None
        slice_93 = torch.ops.aten.slice.Tensor(slice_92, dim = 3, start = 0, end = 9223372036854775807);  slice_92 = None
        slice_94 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
        slice_95 = torch.ops.aten.slice.Tensor(slice_94, dim = 1, start = 0, end = 4096);  slice_94 = None
        slice_96 = torch.ops.aten.slice.Tensor(slice_95, dim = 2, start = 0, end = 9223372036854775807);  slice_95 = None
        _to_copy_210 = torch.ops.aten._to_copy.default(add_28, dtype = torch.float32)
        native_layer_norm_default_45 = torch.ops.aten.native_layer_norm.default(_to_copy_210, [256], arg65_1, arg66_1, 1e-05);  _to_copy_210 = None
        getitem_291 = native_layer_norm_default_45[0]
        _to_copy_211 = torch.ops.aten._to_copy.default(arg67_1, dtype = torch.bfloat16)
        _to_copy_212 = torch.ops.aten._to_copy.default(getitem_291, dtype = torch.bfloat16);  getitem_291 = None
        t_71 = torch.ops.aten.t.default(_to_copy_211);  _to_copy_211 = None
        view_344 = torch.ops.aten.view.default(_to_copy_212, [262144, 256]);  _to_copy_212 = None
        mm_66 = torch.ops.aten.mm.default(view_344, t_71);  view_344 = t_71 = None
        view_345 = torch.ops.aten.view.default(mm_66, [1, 512, 512, 8]);  mm_66 = None
        permute_220 = torch.ops.aten.permute.default(view_345, [0, 3, 1, 2]);  view_345 = None
        view_346 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_25 = torch.ops.aten.bitwise_not.default(view_346);  view_346 = None
        masked_fill_25 = torch.ops.aten.masked_fill.Scalar(permute_220, bitwise_not_25, -10000);  permute_220 = bitwise_not_25 = None
        _to_copy_213 = torch.ops.aten._to_copy.default(masked_fill_25, dtype = torch.float32, layout = torch.strided, device = self.device);  masked_fill_25 = None
        _softmax_4 = torch.ops.aten._softmax.default(_to_copy_213, -1, False);  _to_copy_213 = None
        _to_copy_214 = torch.ops.aten._to_copy.default(slice_93, dtype = torch.float32);  slice_93 = None
        native_layer_norm_default_46 = torch.ops.aten.native_layer_norm.default(_to_copy_214, [64], arg62_1, arg63_1, 1e-05);  _to_copy_214 = None
        getitem_294 = native_layer_norm_default_46[0]
        _to_copy_215 = torch.ops.aten._to_copy.default(arg64_1, dtype = torch.bfloat16)
        _to_copy_216 = torch.ops.aten._to_copy.default(getitem_294, dtype = torch.bfloat16);  getitem_294 = None
        t_72 = torch.ops.aten.t.default(_to_copy_215);  _to_copy_215 = None
        view_347 = torch.ops.aten.view.default(_to_copy_216, [2097152, 64]);  _to_copy_216 = None
        mm_67 = torch.ops.aten.mm.default(view_347, t_72);  view_347 = t_72 = None
        view_348 = torch.ops.aten.view.default(mm_67, [1, 4096, 512, 512]);  mm_67 = None
        view_349 = torch.ops.aten.view.default(view_348, [1, 4096, 512, 2, 8, 32]);  view_348 = None
        permute_221 = torch.ops.aten.permute.default(view_349, [3, 0, 1, 2, 4, 5]);  view_349 = None
        unbind_int_18 = torch.ops.aten.unbind.int(permute_221);  permute_221 = None
        getitem_297 = unbind_int_18[0]
        getitem_298 = unbind_int_18[1];  unbind_int_18 = None
        sigmoid_19 = torch.ops.aten.sigmoid.default(getitem_298);  getitem_298 = None
        bitwise_not_26 = torch.ops.aten.bitwise_not.default(slice_96);  slice_96 = None
        view_350 = torch.ops.aten.view.default(bitwise_not_26, [1, 4096, 512, 1, 1]);  bitwise_not_26 = None
        masked_fill_26 = torch.ops.aten.masked_fill.Scalar(getitem_297, view_350, 0);  getitem_297 = view_350 = None
        _to_copy_217 = torch.ops.aten._to_copy.default(_softmax_4, dtype = torch.bfloat16);  _softmax_4 = None
        unsqueeze_161 = torch.ops.aten.unsqueeze.default(_to_copy_217, 4);  _to_copy_217 = None
        unsqueeze_162 = torch.ops.aten.unsqueeze.default(unsqueeze_161, 5);  unsqueeze_161 = None
        permute_222 = torch.ops.aten.permute.default(unsqueeze_162, [0, 4, 2, 1, 5, 3]);  unsqueeze_162 = None
        unsqueeze_163 = torch.ops.aten.unsqueeze.default(masked_fill_26, 5);  masked_fill_26 = None
        permute_223 = torch.ops.aten.permute.default(unsqueeze_163, [0, 1, 5, 3, 4, 2]);  unsqueeze_163 = None
        permute_224 = torch.ops.aten.permute.default(permute_222, [3, 2, 5, 0, 1, 4]);  permute_222 = None
        view_351 = torch.ops.aten.view.default(permute_224, [8, 512, 512]);  permute_224 = None
        permute_225 = torch.ops.aten.permute.default(permute_223, [3, 5, 0, 1, 4, 2]);  permute_223 = None
        clone_54 = torch.ops.aten.clone.default(permute_225, memory_format = torch.contiguous_format);  permute_225 = None
        _unsafe_view_50 = torch.ops.aten._unsafe_view.default(clone_54, [8, 512, 131072]);  clone_54 = None
        bmm_37 = torch.ops.aten.bmm.default(view_351, _unsafe_view_50);  view_351 = _unsafe_view_50 = None
        view_352 = torch.ops.aten.view.default(bmm_37, [8, 512, 1, 1, 4096, 32]);  bmm_37 = None
        permute_226 = torch.ops.aten.permute.default(view_352, [3, 4, 1, 0, 5, 2]);  view_352 = None
        view_353 = torch.ops.aten.view.default(permute_226, [1, 4096, 512, 8, 32]);  permute_226 = None
        mul_34 = torch.ops.aten.mul.Tensor(sigmoid_19, view_353);  sigmoid_19 = view_353 = None
        view_354 = torch.ops.aten.view.default(mul_34, [1, 4096, 512, 256]);  mul_34 = None
        _to_copy_218 = torch.ops.aten._to_copy.default(arg68_1, dtype = torch.bfloat16)
        t_73 = torch.ops.aten.t.default(_to_copy_218);  _to_copy_218 = None
        view_355 = torch.ops.aten.view.default(view_354, [2097152, 256]);  view_354 = None
        mm_68 = torch.ops.aten.mm.default(view_355, t_73);  view_355 = t_73 = None
        view_356 = torch.ops.aten.view.default(mm_68, [1, 4096, 512, 64]);  mm_68 = None
        slice_97 = torch.ops.aten.slice.Tensor(add_19, dim = 0, start = 0, end = 9223372036854775807)
        slice_98 = torch.ops.aten.slice.Tensor(slice_97, dim = 1, start = 4096, end = 8192);  slice_97 = None
        slice_99 = torch.ops.aten.slice.Tensor(slice_98, dim = 2, start = 0, end = 9223372036854775807);  slice_98 = None
        slice_100 = torch.ops.aten.slice.Tensor(slice_99, dim = 3, start = 0, end = 9223372036854775807);  slice_99 = None
        slice_101 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
        slice_102 = torch.ops.aten.slice.Tensor(slice_101, dim = 1, start = 4096, end = 8192);  slice_101 = None
        slice_103 = torch.ops.aten.slice.Tensor(slice_102, dim = 2, start = 0, end = 9223372036854775807);  slice_102 = None
        _to_copy_219 = torch.ops.aten._to_copy.default(add_28, dtype = torch.float32)
        native_layer_norm_default_47 = torch.ops.aten.native_layer_norm.default(_to_copy_219, [256], arg65_1, arg66_1, 1e-05);  _to_copy_219 = None
        getitem_299 = native_layer_norm_default_47[0]
        _to_copy_220 = torch.ops.aten._to_copy.default(arg67_1, dtype = torch.bfloat16)
        _to_copy_221 = torch.ops.aten._to_copy.default(getitem_299, dtype = torch.bfloat16);  getitem_299 = None
        t_74 = torch.ops.aten.t.default(_to_copy_220);  _to_copy_220 = None
        view_357 = torch.ops.aten.view.default(_to_copy_221, [262144, 256]);  _to_copy_221 = None
        mm_69 = torch.ops.aten.mm.default(view_357, t_74);  view_357 = t_74 = None
        view_358 = torch.ops.aten.view.default(mm_69, [1, 512, 512, 8]);  mm_69 = None
        permute_227 = torch.ops.aten.permute.default(view_358, [0, 3, 1, 2]);  view_358 = None
        view_359 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_27 = torch.ops.aten.bitwise_not.default(view_359);  view_359 = None
        masked_fill_27 = torch.ops.aten.masked_fill.Scalar(permute_227, bitwise_not_27, -10000);  permute_227 = bitwise_not_27 = None
        _to_copy_222 = torch.ops.aten._to_copy.default(masked_fill_27, dtype = torch.float32, layout = torch.strided, device = self.device);  masked_fill_27 = None
        _softmax_5 = torch.ops.aten._softmax.default(_to_copy_222, -1, False);  _to_copy_222 = None
        _to_copy_223 = torch.ops.aten._to_copy.default(slice_100, dtype = torch.float32);  slice_100 = None
        native_layer_norm_default_48 = torch.ops.aten.native_layer_norm.default(_to_copy_223, [64], arg62_1, arg63_1, 1e-05);  _to_copy_223 = None
        getitem_302 = native_layer_norm_default_48[0]
        _to_copy_224 = torch.ops.aten._to_copy.default(arg64_1, dtype = torch.bfloat16)
        _to_copy_225 = torch.ops.aten._to_copy.default(getitem_302, dtype = torch.bfloat16);  getitem_302 = None
        t_75 = torch.ops.aten.t.default(_to_copy_224);  _to_copy_224 = None
        view_360 = torch.ops.aten.view.default(_to_copy_225, [2097152, 64]);  _to_copy_225 = None
        mm_70 = torch.ops.aten.mm.default(view_360, t_75);  view_360 = t_75 = None
        view_361 = torch.ops.aten.view.default(mm_70, [1, 4096, 512, 512]);  mm_70 = None
        view_362 = torch.ops.aten.view.default(view_361, [1, 4096, 512, 2, 8, 32]);  view_361 = None
        permute_228 = torch.ops.aten.permute.default(view_362, [3, 0, 1, 2, 4, 5]);  view_362 = None
        unbind_int_19 = torch.ops.aten.unbind.int(permute_228);  permute_228 = None
        getitem_305 = unbind_int_19[0]
        getitem_306 = unbind_int_19[1];  unbind_int_19 = None
        sigmoid_20 = torch.ops.aten.sigmoid.default(getitem_306);  getitem_306 = None
        bitwise_not_28 = torch.ops.aten.bitwise_not.default(slice_103);  slice_103 = None
        view_363 = torch.ops.aten.view.default(bitwise_not_28, [1, 4096, 512, 1, 1]);  bitwise_not_28 = None
        masked_fill_28 = torch.ops.aten.masked_fill.Scalar(getitem_305, view_363, 0);  getitem_305 = view_363 = None
        _to_copy_226 = torch.ops.aten._to_copy.default(_softmax_5, dtype = torch.bfloat16);  _softmax_5 = None
        unsqueeze_164 = torch.ops.aten.unsqueeze.default(_to_copy_226, 4);  _to_copy_226 = None
        unsqueeze_165 = torch.ops.aten.unsqueeze.default(unsqueeze_164, 5);  unsqueeze_164 = None
        permute_229 = torch.ops.aten.permute.default(unsqueeze_165, [0, 4, 2, 1, 5, 3]);  unsqueeze_165 = None
        unsqueeze_166 = torch.ops.aten.unsqueeze.default(masked_fill_28, 5);  masked_fill_28 = None
        permute_230 = torch.ops.aten.permute.default(unsqueeze_166, [0, 1, 5, 3, 4, 2]);  unsqueeze_166 = None
        permute_231 = torch.ops.aten.permute.default(permute_229, [3, 2, 5, 0, 1, 4]);  permute_229 = None
        view_364 = torch.ops.aten.view.default(permute_231, [8, 512, 512]);  permute_231 = None
        permute_232 = torch.ops.aten.permute.default(permute_230, [3, 5, 0, 1, 4, 2]);  permute_230 = None
        clone_55 = torch.ops.aten.clone.default(permute_232, memory_format = torch.contiguous_format);  permute_232 = None
        _unsafe_view_51 = torch.ops.aten._unsafe_view.default(clone_55, [8, 512, 131072]);  clone_55 = None
        bmm_38 = torch.ops.aten.bmm.default(view_364, _unsafe_view_51);  view_364 = _unsafe_view_51 = None
        view_365 = torch.ops.aten.view.default(bmm_38, [8, 512, 1, 1, 4096, 32]);  bmm_38 = None
        permute_233 = torch.ops.aten.permute.default(view_365, [3, 4, 1, 0, 5, 2]);  view_365 = None
        view_366 = torch.ops.aten.view.default(permute_233, [1, 4096, 512, 8, 32]);  permute_233 = None
        mul_35 = torch.ops.aten.mul.Tensor(sigmoid_20, view_366);  sigmoid_20 = view_366 = None
        view_367 = torch.ops.aten.view.default(mul_35, [1, 4096, 512, 256]);  mul_35 = None
        _to_copy_227 = torch.ops.aten._to_copy.default(arg68_1, dtype = torch.bfloat16)
        t_76 = torch.ops.aten.t.default(_to_copy_227);  _to_copy_227 = None
        view_368 = torch.ops.aten.view.default(view_367, [2097152, 256]);  view_367 = None
        mm_71 = torch.ops.aten.mm.default(view_368, t_76);  view_368 = t_76 = None
        view_369 = torch.ops.aten.view.default(mm_71, [1, 4096, 512, 64]);  mm_71 = None
        slice_104 = torch.ops.aten.slice.Tensor(add_19, dim = 0, start = 0, end = 9223372036854775807)
        slice_105 = torch.ops.aten.slice.Tensor(slice_104, dim = 1, start = 8192, end = 12288);  slice_104 = None
        slice_106 = torch.ops.aten.slice.Tensor(slice_105, dim = 2, start = 0, end = 9223372036854775807);  slice_105 = None
        slice_107 = torch.ops.aten.slice.Tensor(slice_106, dim = 3, start = 0, end = 9223372036854775807);  slice_106 = None
        slice_108 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
        slice_109 = torch.ops.aten.slice.Tensor(slice_108, dim = 1, start = 8192, end = 12288);  slice_108 = None
        slice_110 = torch.ops.aten.slice.Tensor(slice_109, dim = 2, start = 0, end = 9223372036854775807);  slice_109 = None
        _to_copy_228 = torch.ops.aten._to_copy.default(add_28, dtype = torch.float32)
        native_layer_norm_default_49 = torch.ops.aten.native_layer_norm.default(_to_copy_228, [256], arg65_1, arg66_1, 1e-05);  _to_copy_228 = None
        getitem_307 = native_layer_norm_default_49[0]
        _to_copy_229 = torch.ops.aten._to_copy.default(arg67_1, dtype = torch.bfloat16)
        _to_copy_230 = torch.ops.aten._to_copy.default(getitem_307, dtype = torch.bfloat16);  getitem_307 = None
        t_77 = torch.ops.aten.t.default(_to_copy_229);  _to_copy_229 = None
        view_370 = torch.ops.aten.view.default(_to_copy_230, [262144, 256]);  _to_copy_230 = None
        mm_72 = torch.ops.aten.mm.default(view_370, t_77);  view_370 = t_77 = None
        view_371 = torch.ops.aten.view.default(mm_72, [1, 512, 512, 8]);  mm_72 = None
        permute_234 = torch.ops.aten.permute.default(view_371, [0, 3, 1, 2]);  view_371 = None
        view_372 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_29 = torch.ops.aten.bitwise_not.default(view_372);  view_372 = None
        masked_fill_29 = torch.ops.aten.masked_fill.Scalar(permute_234, bitwise_not_29, -10000);  permute_234 = bitwise_not_29 = None
        _to_copy_231 = torch.ops.aten._to_copy.default(masked_fill_29, dtype = torch.float32, layout = torch.strided, device = self.device);  masked_fill_29 = None
        _softmax_6 = torch.ops.aten._softmax.default(_to_copy_231, -1, False);  _to_copy_231 = None
        _to_copy_232 = torch.ops.aten._to_copy.default(slice_107, dtype = torch.float32);  slice_107 = None
        native_layer_norm_default_50 = torch.ops.aten.native_layer_norm.default(_to_copy_232, [64], arg62_1, arg63_1, 1e-05);  _to_copy_232 = None
        getitem_310 = native_layer_norm_default_50[0]
        _to_copy_233 = torch.ops.aten._to_copy.default(arg64_1, dtype = torch.bfloat16)
        _to_copy_234 = torch.ops.aten._to_copy.default(getitem_310, dtype = torch.bfloat16);  getitem_310 = None
        t_78 = torch.ops.aten.t.default(_to_copy_233);  _to_copy_233 = None
        view_373 = torch.ops.aten.view.default(_to_copy_234, [2097152, 64]);  _to_copy_234 = None
        mm_73 = torch.ops.aten.mm.default(view_373, t_78);  view_373 = t_78 = None
        view_374 = torch.ops.aten.view.default(mm_73, [1, 4096, 512, 512]);  mm_73 = None
        view_375 = torch.ops.aten.view.default(view_374, [1, 4096, 512, 2, 8, 32]);  view_374 = None
        permute_235 = torch.ops.aten.permute.default(view_375, [3, 0, 1, 2, 4, 5]);  view_375 = None
        unbind_int_20 = torch.ops.aten.unbind.int(permute_235);  permute_235 = None
        getitem_313 = unbind_int_20[0]
        getitem_314 = unbind_int_20[1];  unbind_int_20 = None
        sigmoid_21 = torch.ops.aten.sigmoid.default(getitem_314);  getitem_314 = None
        bitwise_not_30 = torch.ops.aten.bitwise_not.default(slice_110);  slice_110 = None
        view_376 = torch.ops.aten.view.default(bitwise_not_30, [1, 4096, 512, 1, 1]);  bitwise_not_30 = None
        masked_fill_30 = torch.ops.aten.masked_fill.Scalar(getitem_313, view_376, 0);  getitem_313 = view_376 = None
        _to_copy_235 = torch.ops.aten._to_copy.default(_softmax_6, dtype = torch.bfloat16);  _softmax_6 = None
        unsqueeze_167 = torch.ops.aten.unsqueeze.default(_to_copy_235, 4);  _to_copy_235 = None
        unsqueeze_168 = torch.ops.aten.unsqueeze.default(unsqueeze_167, 5);  unsqueeze_167 = None
        permute_236 = torch.ops.aten.permute.default(unsqueeze_168, [0, 4, 2, 1, 5, 3]);  unsqueeze_168 = None
        unsqueeze_169 = torch.ops.aten.unsqueeze.default(masked_fill_30, 5);  masked_fill_30 = None
        permute_237 = torch.ops.aten.permute.default(unsqueeze_169, [0, 1, 5, 3, 4, 2]);  unsqueeze_169 = None
        permute_238 = torch.ops.aten.permute.default(permute_236, [3, 2, 5, 0, 1, 4]);  permute_236 = None
        view_377 = torch.ops.aten.view.default(permute_238, [8, 512, 512]);  permute_238 = None
        permute_239 = torch.ops.aten.permute.default(permute_237, [3, 5, 0, 1, 4, 2]);  permute_237 = None
        clone_56 = torch.ops.aten.clone.default(permute_239, memory_format = torch.contiguous_format);  permute_239 = None
        _unsafe_view_52 = torch.ops.aten._unsafe_view.default(clone_56, [8, 512, 131072]);  clone_56 = None
        bmm_39 = torch.ops.aten.bmm.default(view_377, _unsafe_view_52);  view_377 = _unsafe_view_52 = None
        view_378 = torch.ops.aten.view.default(bmm_39, [8, 512, 1, 1, 4096, 32]);  bmm_39 = None
        permute_240 = torch.ops.aten.permute.default(view_378, [3, 4, 1, 0, 5, 2]);  view_378 = None
        view_379 = torch.ops.aten.view.default(permute_240, [1, 4096, 512, 8, 32]);  permute_240 = None
        mul_36 = torch.ops.aten.mul.Tensor(sigmoid_21, view_379);  sigmoid_21 = view_379 = None
        view_380 = torch.ops.aten.view.default(mul_36, [1, 4096, 512, 256]);  mul_36 = None
        _to_copy_236 = torch.ops.aten._to_copy.default(arg68_1, dtype = torch.bfloat16)
        t_79 = torch.ops.aten.t.default(_to_copy_236);  _to_copy_236 = None
        view_381 = torch.ops.aten.view.default(view_380, [2097152, 256]);  view_380 = None
        mm_74 = torch.ops.aten.mm.default(view_381, t_79);  view_381 = t_79 = None
        view_382 = torch.ops.aten.view.default(mm_74, [1, 4096, 512, 64]);  mm_74 = None
        slice_111 = torch.ops.aten.slice.Tensor(add_19, dim = 0, start = 0, end = 9223372036854775807);  add_19 = None
        slice_112 = torch.ops.aten.slice.Tensor(slice_111, dim = 1, start = 12288, end = 16384);  slice_111 = None
        slice_113 = torch.ops.aten.slice.Tensor(slice_112, dim = 2, start = 0, end = 9223372036854775807);  slice_112 = None
        slice_114 = torch.ops.aten.slice.Tensor(slice_113, dim = 3, start = 0, end = 9223372036854775807);  slice_113 = None
        slice_115 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
        slice_116 = torch.ops.aten.slice.Tensor(slice_115, dim = 1, start = 12288, end = 16384);  slice_115 = None
        slice_117 = torch.ops.aten.slice.Tensor(slice_116, dim = 2, start = 0, end = 9223372036854775807);  slice_116 = None
        _to_copy_237 = torch.ops.aten._to_copy.default(add_28, dtype = torch.float32)
        native_layer_norm_default_51 = torch.ops.aten.native_layer_norm.default(_to_copy_237, [256], arg65_1, arg66_1, 1e-05);  _to_copy_237 = arg65_1 = arg66_1 = None
        getitem_315 = native_layer_norm_default_51[0]
        _to_copy_238 = torch.ops.aten._to_copy.default(arg67_1, dtype = torch.bfloat16);  arg67_1 = None
        _to_copy_239 = torch.ops.aten._to_copy.default(getitem_315, dtype = torch.bfloat16);  getitem_315 = None
        t_80 = torch.ops.aten.t.default(_to_copy_238);  _to_copy_238 = None
        view_383 = torch.ops.aten.view.default(_to_copy_239, [262144, 256]);  _to_copy_239 = None
        mm_75 = torch.ops.aten.mm.default(view_383, t_80);  view_383 = t_80 = None
        view_384 = torch.ops.aten.view.default(mm_75, [1, 512, 512, 8]);  mm_75 = None
        permute_241 = torch.ops.aten.permute.default(view_384, [0, 3, 1, 2]);  view_384 = None
        view_385 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_31 = torch.ops.aten.bitwise_not.default(view_385);  view_385 = None
        masked_fill_31 = torch.ops.aten.masked_fill.Scalar(permute_241, bitwise_not_31, -10000);  permute_241 = bitwise_not_31 = None
        _to_copy_240 = torch.ops.aten._to_copy.default(masked_fill_31, dtype = torch.float32, layout = torch.strided, device = self.device);  masked_fill_31 = None
        _softmax_7 = torch.ops.aten._softmax.default(_to_copy_240, -1, False);  _to_copy_240 = None
        _to_copy_241 = torch.ops.aten._to_copy.default(slice_114, dtype = torch.float32);  slice_114 = None
        native_layer_norm_default_52 = torch.ops.aten.native_layer_norm.default(_to_copy_241, [64], arg62_1, arg63_1, 1e-05);  _to_copy_241 = arg62_1 = arg63_1 = None
        getitem_318 = native_layer_norm_default_52[0]
        _to_copy_242 = torch.ops.aten._to_copy.default(arg64_1, dtype = torch.bfloat16);  arg64_1 = None
        _to_copy_243 = torch.ops.aten._to_copy.default(getitem_318, dtype = torch.bfloat16);  getitem_318 = None
        t_81 = torch.ops.aten.t.default(_to_copy_242);  _to_copy_242 = None
        view_386 = torch.ops.aten.view.default(_to_copy_243, [2097152, 64]);  _to_copy_243 = None
        mm_76 = torch.ops.aten.mm.default(view_386, t_81);  view_386 = t_81 = None
        view_387 = torch.ops.aten.view.default(mm_76, [1, 4096, 512, 512]);  mm_76 = None
        view_388 = torch.ops.aten.view.default(view_387, [1, 4096, 512, 2, 8, 32]);  view_387 = None
        permute_242 = torch.ops.aten.permute.default(view_388, [3, 0, 1, 2, 4, 5]);  view_388 = None
        unbind_int_21 = torch.ops.aten.unbind.int(permute_242);  permute_242 = None
        getitem_321 = unbind_int_21[0]
        getitem_322 = unbind_int_21[1];  unbind_int_21 = None
        sigmoid_22 = torch.ops.aten.sigmoid.default(getitem_322);  getitem_322 = None
        bitwise_not_32 = torch.ops.aten.bitwise_not.default(slice_117);  slice_117 = None
        view_389 = torch.ops.aten.view.default(bitwise_not_32, [1, 4096, 512, 1, 1]);  bitwise_not_32 = None
        masked_fill_32 = torch.ops.aten.masked_fill.Scalar(getitem_321, view_389, 0);  getitem_321 = view_389 = None
        _to_copy_244 = torch.ops.aten._to_copy.default(_softmax_7, dtype = torch.bfloat16);  _softmax_7 = None
        unsqueeze_170 = torch.ops.aten.unsqueeze.default(_to_copy_244, 4);  _to_copy_244 = None
        unsqueeze_171 = torch.ops.aten.unsqueeze.default(unsqueeze_170, 5);  unsqueeze_170 = None
        permute_243 = torch.ops.aten.permute.default(unsqueeze_171, [0, 4, 2, 1, 5, 3]);  unsqueeze_171 = None
        unsqueeze_172 = torch.ops.aten.unsqueeze.default(masked_fill_32, 5);  masked_fill_32 = None
        permute_244 = torch.ops.aten.permute.default(unsqueeze_172, [0, 1, 5, 3, 4, 2]);  unsqueeze_172 = None
        permute_245 = torch.ops.aten.permute.default(permute_243, [3, 2, 5, 0, 1, 4]);  permute_243 = None
        view_390 = torch.ops.aten.view.default(permute_245, [8, 512, 512]);  permute_245 = None
        permute_246 = torch.ops.aten.permute.default(permute_244, [3, 5, 0, 1, 4, 2]);  permute_244 = None
        clone_57 = torch.ops.aten.clone.default(permute_246, memory_format = torch.contiguous_format);  permute_246 = None
        _unsafe_view_53 = torch.ops.aten._unsafe_view.default(clone_57, [8, 512, 131072]);  clone_57 = None
        bmm_40 = torch.ops.aten.bmm.default(view_390, _unsafe_view_53);  view_390 = _unsafe_view_53 = None
        view_391 = torch.ops.aten.view.default(bmm_40, [8, 512, 1, 1, 4096, 32]);  bmm_40 = None
        permute_247 = torch.ops.aten.permute.default(view_391, [3, 4, 1, 0, 5, 2]);  view_391 = None
        view_392 = torch.ops.aten.view.default(permute_247, [1, 4096, 512, 8, 32]);  permute_247 = None
        mul_37 = torch.ops.aten.mul.Tensor(sigmoid_22, view_392);  sigmoid_22 = view_392 = None
        view_393 = torch.ops.aten.view.default(mul_37, [1, 4096, 512, 256]);  mul_37 = None
        _to_copy_245 = torch.ops.aten._to_copy.default(arg68_1, dtype = torch.bfloat16);  arg68_1 = None
        t_82 = torch.ops.aten.t.default(_to_copy_245);  _to_copy_245 = None
        view_394 = torch.ops.aten.view.default(view_393, [2097152, 256]);  view_393 = None
        mm_77 = torch.ops.aten.mm.default(view_394, t_82);  view_394 = t_82 = None
        view_395 = torch.ops.aten.view.default(mm_77, [1, 4096, 512, 64]);  mm_77 = None
        cat_6 = torch.ops.aten.cat.default([view_356, view_369, view_382, view_395], dim = 1);  view_356 = view_369 = view_382 = view_395 = None
        add_30 = torch.ops.aten.add.Tensor(add_29, cat_6);  add_29 = cat_6 = None
        _to_copy_246 = torch.ops.aten._to_copy.default(add_28, dtype = torch.float32)
        native_layer_norm_default_53 = torch.ops.aten.native_layer_norm.default(_to_copy_246, [256], arg109_1, arg110_1, 1e-05);  _to_copy_246 = arg109_1 = arg110_1 = None
        getitem_323 = native_layer_norm_default_53[0]
        split_with_sizes_default_6 = torch.ops.aten.split_with_sizes.default(arg112_1, [512, 512]);  arg112_1 = None
        getitem_326 = split_with_sizes_default_6[0]
        getitem_327 = split_with_sizes_default_6[1];  split_with_sizes_default_6 = None
        split_with_sizes_default_7 = torch.ops.aten.split_with_sizes.default(arg113_1, [512, 512, 256]);  arg113_1 = None
        getitem_328 = split_with_sizes_default_7[0]
        getitem_329 = split_with_sizes_default_7[1]
        getitem_330 = split_with_sizes_default_7[2];  split_with_sizes_default_7 = None
        _to_copy_247 = torch.ops.aten._to_copy.default(getitem_326, dtype = torch.bfloat16);  getitem_326 = None
        _to_copy_248 = torch.ops.aten._to_copy.default(getitem_323, dtype = torch.bfloat16)
        t_83 = torch.ops.aten.t.default(_to_copy_247);  _to_copy_247 = None
        view_396 = torch.ops.aten.view.default(_to_copy_248, [262144, 256]);  _to_copy_248 = None
        mm_78 = torch.ops.aten.mm.default(view_396, t_83);  view_396 = t_83 = None
        view_397 = torch.ops.aten.view.default(mm_78, [1, 512, 512, 512]);  mm_78 = None
        _to_copy_249 = torch.ops.aten._to_copy.default(getitem_328, dtype = torch.bfloat16);  getitem_328 = None
        _to_copy_250 = torch.ops.aten._to_copy.default(getitem_323, dtype = torch.bfloat16)
        t_84 = torch.ops.aten.t.default(_to_copy_249);  _to_copy_249 = None
        view_398 = torch.ops.aten.view.default(_to_copy_250, [262144, 256]);  _to_copy_250 = None
        mm_79 = torch.ops.aten.mm.default(view_398, t_84);  view_398 = t_84 = None
        view_399 = torch.ops.aten.view.default(mm_79, [1, 512, 512, 512]);  mm_79 = None
        sigmoid_23 = torch.ops.aten.sigmoid.default(view_399);  view_399 = None
        mul_38 = torch.ops.aten.mul.Tensor(view_397, sigmoid_23);  view_397 = sigmoid_23 = None
        unsqueeze_173 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_33 = torch.ops.aten.bitwise_not.default(unsqueeze_173);  unsqueeze_173 = None
        masked_fill_33 = torch.ops.aten.masked_fill.Scalar(mul_38, bitwise_not_33, 0);  mul_38 = bitwise_not_33 = None
        split_tensor_28 = torch.ops.aten.split.Tensor(masked_fill_33, 256, dim = -1)
        getitem_333 = split_tensor_28[0]
        unsqueeze_176 = torch.ops.aten.unsqueeze.default(getitem_333, 4);  getitem_333 = None
        permute_252 = torch.ops.aten.permute.default(unsqueeze_176, [0, 1, 4, 3, 2]);  unsqueeze_176 = None
        permute_253 = torch.ops.aten.permute.default(permute_252, [3, 1, 4, 0, 2]);  permute_252 = None
        view_402 = torch.ops.aten.view.default(permute_253, [256, 512, 512]);  permute_253 = None
        split_tensor_29 = torch.ops.aten.split.Tensor(masked_fill_33, 256, dim = -1);  masked_fill_33 = None
        getitem_336 = split_tensor_29[1];  split_tensor_29 = None
        unsqueeze_177 = torch.ops.aten.unsqueeze.default(getitem_336, 4);  getitem_336 = None
        permute_254 = torch.ops.aten.permute.default(unsqueeze_177, [0, 4, 1, 3, 2]);  unsqueeze_177 = None
        permute_255 = torch.ops.aten.permute.default(permute_254, [3, 4, 0, 2, 1]);  permute_254 = None
        view_403 = torch.ops.aten.view.default(permute_255, [256, 512, 512]);  permute_255 = None
        bmm_41 = torch.ops.aten.bmm.default(view_402, view_403);  view_402 = view_403 = None
        view_404 = torch.ops.aten.view.default(bmm_41, [256, 512, 1, 1, 512]);  bmm_41 = None
        permute_256 = torch.ops.aten.permute.default(view_404, [3, 1, 4, 0, 2]);  view_404 = None
        view_405 = torch.ops.aten.view.default(permute_256, [1, 512, 512, 256]);  permute_256 = None
        _to_copy_251 = torch.ops.aten._to_copy.default(getitem_327, dtype = torch.bfloat16);  getitem_327 = None
        _to_copy_252 = torch.ops.aten._to_copy.default(getitem_323, dtype = torch.bfloat16)
        t_85 = torch.ops.aten.t.default(_to_copy_251);  _to_copy_251 = None
        view_406 = torch.ops.aten.view.default(_to_copy_252, [262144, 256]);  _to_copy_252 = None
        mm_80 = torch.ops.aten.mm.default(view_406, t_85);  view_406 = t_85 = None
        view_407 = torch.ops.aten.view.default(mm_80, [1, 512, 512, 512]);  mm_80 = None
        _to_copy_253 = torch.ops.aten._to_copy.default(getitem_329, dtype = torch.bfloat16);  getitem_329 = None
        _to_copy_254 = torch.ops.aten._to_copy.default(getitem_323, dtype = torch.bfloat16)
        t_86 = torch.ops.aten.t.default(_to_copy_253);  _to_copy_253 = None
        view_408 = torch.ops.aten.view.default(_to_copy_254, [262144, 256]);  _to_copy_254 = None
        mm_81 = torch.ops.aten.mm.default(view_408, t_86);  view_408 = t_86 = None
        view_409 = torch.ops.aten.view.default(mm_81, [1, 512, 512, 512]);  mm_81 = None
        sigmoid_24 = torch.ops.aten.sigmoid.default(view_409);  view_409 = None
        mul_39 = torch.ops.aten.mul.Tensor(view_407, sigmoid_24);  view_407 = sigmoid_24 = None
        view_410 = torch.ops.aten.view.default(mul_39, [262144, 512]);  mul_39 = None
        view_411 = torch.ops.aten.view.default(view_410, [1, 512, 512, 512]);  view_410 = None
        transpose_6 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_178 = torch.ops.aten.unsqueeze.default(transpose_6, 3);  transpose_6 = None
        clone_58 = torch.ops.aten.clone.default(unsqueeze_178, memory_format = torch.contiguous_format);  unsqueeze_178 = None
        bitwise_not_34 = torch.ops.aten.bitwise_not.default(clone_58);  clone_58 = None
        masked_fill_34 = torch.ops.aten.masked_fill.Scalar(view_411, bitwise_not_34, 0);  view_411 = bitwise_not_34 = None
        view_412 = torch.ops.aten.view.default(masked_fill_34, [262144, 512]);  masked_fill_34 = None
        view_416 = torch.ops.aten.view.default(view_412, [1, 512, 512, 512])
        split_tensor_30 = torch.ops.aten.split.Tensor(view_416, 256, dim = -1);  view_416 = None
        getitem_339 = split_tensor_30[0]
        unsqueeze_181 = torch.ops.aten.unsqueeze.default(getitem_339, 4);  getitem_339 = None
        permute_261 = torch.ops.aten.permute.default(unsqueeze_181, [0, 2, 4, 3, 1]);  unsqueeze_181 = None
        permute_262 = torch.ops.aten.permute.default(permute_261, [3, 1, 4, 0, 2]);  permute_261 = None
        view_417 = torch.ops.aten.view.default(permute_262, [256, 512, 512]);  permute_262 = None
        view_418 = torch.ops.aten.view.default(view_412, [1, 512, 512, 512]);  view_412 = None
        split_tensor_31 = torch.ops.aten.split.Tensor(view_418, 256, dim = -1);  view_418 = None
        getitem_342 = split_tensor_31[1];  split_tensor_31 = None
        unsqueeze_182 = torch.ops.aten.unsqueeze.default(getitem_342, 4);  getitem_342 = None
        permute_263 = torch.ops.aten.permute.default(unsqueeze_182, [0, 4, 2, 3, 1]);  unsqueeze_182 = None
        permute_264 = torch.ops.aten.permute.default(permute_263, [3, 4, 0, 2, 1]);  permute_263 = None
        view_419 = torch.ops.aten.view.default(permute_264, [256, 512, 512]);  permute_264 = None
        bmm_42 = torch.ops.aten.bmm.default(view_417, view_419);  view_417 = view_419 = None
        view_420 = torch.ops.aten.view.default(bmm_42, [256, 512, 1, 1, 512]);  bmm_42 = None
        permute_265 = torch.ops.aten.permute.default(view_420, [3, 1, 4, 0, 2]);  view_420 = None
        view_421 = torch.ops.aten.view.default(permute_265, [1, 512, 512, 256]);  permute_265 = None
        _to_copy_255 = torch.ops.aten._to_copy.default(view_405, dtype = torch.float32);  view_405 = None
        native_layer_norm_default_54 = torch.ops.aten.native_layer_norm.default(_to_copy_255, [256], None, None, 1e-05);  _to_copy_255 = None
        getitem_343 = native_layer_norm_default_54[0]
        _to_copy_256 = torch.ops.aten._to_copy.default(view_421, dtype = torch.float32);  view_421 = None
        native_layer_norm_default_55 = torch.ops.aten.native_layer_norm.default(_to_copy_256, [256], None, None, 1e-05);  _to_copy_256 = None
        getitem_346 = native_layer_norm_default_55[0]
        add_31 = torch.ops.aten.add.Tensor(getitem_343, getitem_346);  getitem_343 = getitem_346 = None
        _to_copy_257 = torch.ops.aten._to_copy.default(arg111_1, dtype = torch.bfloat16);  arg111_1 = None
        _to_copy_258 = torch.ops.aten._to_copy.default(add_31, dtype = torch.bfloat16);  add_31 = None
        t_87 = torch.ops.aten.t.default(_to_copy_257);  _to_copy_257 = None
        view_422 = torch.ops.aten.view.default(_to_copy_258, [262144, 256]);  _to_copy_258 = None
        mm_82 = torch.ops.aten.mm.default(view_422, t_87);  view_422 = t_87 = None
        view_423 = torch.ops.aten.view.default(mm_82, [1, 512, 512, 256]);  mm_82 = None
        _to_copy_259 = torch.ops.aten._to_copy.default(getitem_330, dtype = torch.bfloat16);  getitem_330 = None
        _to_copy_260 = torch.ops.aten._to_copy.default(getitem_323, dtype = torch.bfloat16);  getitem_323 = None
        t_88 = torch.ops.aten.t.default(_to_copy_259);  _to_copy_259 = None
        view_424 = torch.ops.aten.view.default(_to_copy_260, [262144, 256]);  _to_copy_260 = None
        mm_83 = torch.ops.aten.mm.default(view_424, t_88);  view_424 = t_88 = None
        view_425 = torch.ops.aten.view.default(mm_83, [1, 512, 512, 256]);  mm_83 = None
        sigmoid_25 = torch.ops.aten.sigmoid.default(view_425);  view_425 = None
        mul_40 = torch.ops.aten.mul.Tensor(view_423, sigmoid_25);  view_423 = sigmoid_25 = None
        add_32 = torch.ops.aten.add.Tensor(add_28, mul_40);  mul_40 = None
        split_tensor_32 = torch.ops.aten.split.Tensor(add_28, 512, dim = -2);  add_28 = None
        getitem_349 = split_tensor_32[0];  split_tensor_32 = None
        _to_copy_261 = torch.ops.aten._to_copy.default(getitem_349, dtype = torch.float32);  getitem_349 = None
        native_layer_norm_default_56 = torch.ops.aten.native_layer_norm.default(_to_copy_261, [256], arg92_1, arg93_1, 1e-05);  _to_copy_261 = arg92_1 = arg93_1 = None
        getitem_350 = native_layer_norm_default_56[0]
        _to_copy_262 = torch.ops.aten._to_copy.default(arg94_1, dtype = torch.bfloat16);  arg94_1 = None
        _to_copy_263 = torch.ops.aten._to_copy.default(getitem_350, dtype = torch.bfloat16);  getitem_350 = None
        t_89 = torch.ops.aten.t.default(_to_copy_262);  _to_copy_262 = None
        view_426 = torch.ops.aten.view.default(_to_copy_263, [262144, 256]);  _to_copy_263 = None
        mm_84 = torch.ops.aten.mm.default(view_426, t_89);  view_426 = t_89 = None
        view_427 = torch.ops.aten.view.default(mm_84, [1, 512, 512, 2048]);  mm_84 = None
        split_tensor_33 = torch.ops.aten.split.Tensor(view_427, 1024, dim = -1);  view_427 = None
        getitem_353 = split_tensor_33[0]
        getitem_354 = split_tensor_33[1];  split_tensor_33 = None
        silu_11 = torch.ops.aten.silu.default(getitem_353);  getitem_353 = None
        mul_41 = torch.ops.aten.mul.Tensor(silu_11, getitem_354);  silu_11 = getitem_354 = None
        _to_copy_264 = torch.ops.aten._to_copy.default(arg95_1, dtype = torch.bfloat16);  arg95_1 = None
        t_90 = torch.ops.aten.t.default(_to_copy_264);  _to_copy_264 = None
        view_429 = torch.ops.aten.view.default(mul_41, [262144, 1024]);  mul_41 = None
        mm_85 = torch.ops.aten.mm.default(view_429, t_90);  view_429 = t_90 = None
        view_430 = torch.ops.aten.view.default(mm_85, [1, 512, 512, 256]);  mm_85 = None
        add_33 = torch.ops.aten.add.Tensor(add_32, view_430);  add_32 = view_430 = None
        _to_copy_265 = torch.ops.aten._to_copy.default(add_33, dtype = torch.float32)
        native_layer_norm_default_57 = torch.ops.aten.native_layer_norm.default(_to_copy_265, [256], None, None, 1e-05);  _to_copy_265 = None
        getitem_355 = native_layer_norm_default_57[0]
        _to_copy_266 = torch.ops.aten._to_copy.default(arg130_1, dtype = torch.bfloat16);  arg130_1 = None
        _to_copy_267 = torch.ops.aten._to_copy.default(getitem_355, dtype = torch.bfloat16)
        t_91 = torch.ops.aten.t.default(_to_copy_266);  _to_copy_266 = None
        view_431 = torch.ops.aten.view.default(_to_copy_267, [262144, 256]);  _to_copy_267 = None
        mm_86 = torch.ops.aten.mm.default(view_431, t_91);  view_431 = t_91 = None
        view_432 = torch.ops.aten.view.default(mm_86, [1, 512, 512, 8]);  mm_86 = None
        view_433 = torch.ops.aten.view.default(view_432, [1, 512, 512, 2, 4]);  view_432 = None
        permute_266 = torch.ops.aten.permute.default(view_433, [0, 3, 4, 1, 2]);  view_433 = None
        view_434 = torch.ops.aten.view.default(permute_266, [1, 2, 4, 1, 512, 512]);  permute_266 = None
        view_435 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_35 = torch.ops.aten.bitwise_not.default(view_435);  view_435 = None
        masked_fill_35 = torch.ops.aten.masked_fill.Scalar(view_434, bitwise_not_35, -10000);  view_434 = bitwise_not_35 = None
        view_436 = torch.ops.aten.view.default(masked_fill_35, [1, 2, 4, 512, 512]);  masked_fill_35 = None
        permute_267 = torch.ops.aten.permute.default(view_436, [1, 0, 2, 3, 4]);  view_436 = None
        view_437 = torch.ops.aten.view.default(permute_267, [2, 4, 1, 512, 512]);  permute_267 = None
        _to_copy_268 = torch.ops.aten._to_copy.default(arg131_1, dtype = torch.bfloat16);  arg131_1 = None
        _to_copy_269 = torch.ops.aten._to_copy.default(getitem_355, dtype = torch.bfloat16)
        t_92 = torch.ops.aten.t.default(_to_copy_268);  _to_copy_268 = None
        view_438 = torch.ops.aten.view.default(_to_copy_269, [262144, 256]);  _to_copy_269 = None
        mm_87 = torch.ops.aten.mm.default(view_438, t_92);  view_438 = t_92 = None
        view_439 = torch.ops.aten.view.default(mm_87, [1, 512, 512, 1024]);  mm_87 = None
        select_7 = torch.ops.aten.select.int(view_437, 0, 0)
        view_440 = torch.ops.aten.view.default(view_439, [1, 512, 512, 4, 4, 64]);  view_439 = None
        permute_268 = torch.ops.aten.permute.default(view_440, [4, 0, 3, 1, 2, 5]);  view_440 = None
        view_441 = torch.ops.aten.view.default(permute_268, [4, 4, 512, 512, 64]);  permute_268 = None
        unbind_int_22 = torch.ops.aten.unbind.int(view_441);  view_441 = None
        getitem_358 = unbind_int_22[0]
        getitem_359 = unbind_int_22[1]
        getitem_360 = unbind_int_22[2]
        getitem_361 = unbind_int_22[3];  unbind_int_22 = None
        expand_15 = torch.ops.aten.expand.default(select_7, [4, 512, 512, 512]);  select_7 = None
        _scaled_dot_product_efficient_attention_default_6 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_358, getitem_359, getitem_360, expand_15, False);  getitem_358 = getitem_359 = getitem_360 = expand_15 = None
        getitem_362 = _scaled_dot_product_efficient_attention_default_6[0]
        sigmoid_26 = torch.ops.aten.sigmoid.default(getitem_361);  getitem_361 = None
        mul_42 = torch.ops.aten.mul.Tensor(getitem_362, sigmoid_26);  getitem_362 = sigmoid_26 = None
        view_442 = torch.ops.aten.view.default(mul_42, [1, 4, 512, 512, 64]);  mul_42 = None
        permute_269 = torch.ops.aten.permute.default(view_442, [0, 2, 3, 1, 4]);  view_442 = None
        clone_59 = torch.ops.aten.clone.default(permute_269, memory_format = torch.contiguous_format);  permute_269 = None
        _unsafe_view_54 = torch.ops.aten._unsafe_view.default(clone_59, [1, 512, 512, 256]);  clone_59 = None
        transpose_7 = torch.ops.aten.transpose.int(getitem_355, 1, 2);  getitem_355 = None
        _to_copy_270 = torch.ops.aten._to_copy.default(arg132_1, dtype = torch.bfloat16);  arg132_1 = None
        _to_copy_271 = torch.ops.aten._to_copy.default(transpose_7, dtype = torch.bfloat16);  transpose_7 = None
        t_93 = torch.ops.aten.t.default(_to_copy_270);  _to_copy_270 = None
        expand_16 = torch.ops.aten.expand.default(_to_copy_271, [1, 512, 512, 256]);  _to_copy_271 = None
        view_443 = torch.ops.aten.view.default(expand_16, [512, 512, 256]);  expand_16 = None
        expand_17 = torch.ops.aten.expand.default(t_93, [1, 512, 256, 1024]);  t_93 = None
        view_444 = torch.ops.aten.view.default(expand_17, [512, 256, 1024]);  expand_17 = None
        bmm_43 = torch.ops.aten.bmm.default(view_443, view_444);  view_443 = view_444 = None
        view_445 = torch.ops.aten.view.default(bmm_43, [1, 512, 512, 1024]);  bmm_43 = None
        select_8 = torch.ops.aten.select.int(view_437, 0, 1);  view_437 = None
        view_446 = torch.ops.aten.view.default(view_445, [1, 512, 512, 4, 4, 64]);  view_445 = None
        permute_270 = torch.ops.aten.permute.default(view_446, [4, 0, 3, 1, 2, 5]);  view_446 = None
        view_447 = torch.ops.aten.view.default(permute_270, [4, 4, 512, 512, 64]);  permute_270 = None
        unbind_int_23 = torch.ops.aten.unbind.int(view_447);  view_447 = None
        getitem_366 = unbind_int_23[0]
        getitem_367 = unbind_int_23[1]
        getitem_368 = unbind_int_23[2]
        getitem_369 = unbind_int_23[3];  unbind_int_23 = None
        expand_18 = torch.ops.aten.expand.default(select_8, [4, 512, 512, 512]);  select_8 = None
        _scaled_dot_product_efficient_attention_default_7 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_366, getitem_367, getitem_368, expand_18, False);  getitem_366 = getitem_367 = getitem_368 = expand_18 = None
        getitem_370 = _scaled_dot_product_efficient_attention_default_7[0]
        sigmoid_27 = torch.ops.aten.sigmoid.default(getitem_369);  getitem_369 = None
        mul_43 = torch.ops.aten.mul.Tensor(getitem_370, sigmoid_27);  getitem_370 = sigmoid_27 = None
        view_448 = torch.ops.aten.view.default(mul_43, [1, 4, 512, 512, 64]);  mul_43 = None
        permute_271 = torch.ops.aten.permute.default(view_448, [0, 2, 3, 1, 4]);  view_448 = None
        clone_60 = torch.ops.aten.clone.default(permute_271, memory_format = torch.contiguous_format);  permute_271 = None
        _unsafe_view_55 = torch.ops.aten._unsafe_view.default(clone_60, [1, 512, 512, 256]);  clone_60 = None
        cat_7 = torch.ops.aten.cat.default([_unsafe_view_54, _unsafe_view_55], dim = -1);  _unsafe_view_54 = _unsafe_view_55 = None
        slice_118 = torch.ops.aten.slice.Tensor(arg129_1, dim = 0, start = 0, end = 9223372036854775807);  arg129_1 = None
        unsqueeze_183 = torch.ops.aten.unsqueeze.default(slice_118, 1);  slice_118 = None
        mul_44 = torch.ops.aten.mul.Tensor(arg133_1, unsqueeze_183);  arg133_1 = unsqueeze_183 = None
        _to_copy_272 = torch.ops.aten._to_copy.default(mul_44, dtype = torch.bfloat16);  mul_44 = None
        t_94 = torch.ops.aten.t.default(_to_copy_272);  _to_copy_272 = None
        view_449 = torch.ops.aten.view.default(cat_7, [262144, 512]);  cat_7 = None
        mm_88 = torch.ops.aten.mm.default(view_449, t_94);  view_449 = t_94 = None
        view_450 = torch.ops.aten.view.default(mm_88, [1, 512, 512, 256]);  mm_88 = None
        add_34 = torch.ops.aten.add.Tensor(add_33, view_450);  add_33 = view_450 = None
        slice_119 = torch.ops.aten.slice.Tensor(add_30, dim = 0, start = 0, end = 9223372036854775807)
        slice_120 = torch.ops.aten.slice.Tensor(slice_119, dim = 1, start = 0, end = 4096);  slice_119 = None
        slice_121 = torch.ops.aten.slice.Tensor(slice_120, dim = 2, start = 0, end = 9223372036854775807);  slice_120 = None
        slice_122 = torch.ops.aten.slice.Tensor(slice_121, dim = 3, start = 0, end = 9223372036854775807);  slice_121 = None
        slice_123 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
        slice_124 = torch.ops.aten.slice.Tensor(slice_123, dim = 1, start = 0, end = 4096);  slice_123 = None
        slice_125 = torch.ops.aten.slice.Tensor(slice_124, dim = 2, start = 0, end = 9223372036854775807);  slice_124 = None
        _to_copy_273 = torch.ops.aten._to_copy.default(slice_122, dtype = torch.float32);  slice_122 = None
        native_layer_norm_default_58 = torch.ops.aten.native_layer_norm.default(_to_copy_273, [64], None, None, 1e-05);  _to_copy_273 = None
        getitem_374 = native_layer_norm_default_58[0]
        view_451 = torch.ops.aten.view.default(slice_125, [1, 4096, 512, 1]);  slice_125 = None
        bitwise_not_36 = torch.ops.aten.bitwise_not.default(view_451);  view_451 = None
        masked_fill_36 = torch.ops.aten.masked_fill.Scalar(getitem_374, bitwise_not_36, 0);  getitem_374 = bitwise_not_36 = None
        unbind_int_24 = torch.ops.aten.unbind.int(arg45_1)
        getitem_377 = unbind_int_24[0]
        getitem_378 = unbind_int_24[1];  unbind_int_24 = None
        _to_copy_274 = torch.ops.aten._to_copy.default(getitem_377, dtype = torch.bfloat16);  getitem_377 = None
        _to_copy_275 = torch.ops.aten._to_copy.default(masked_fill_36, dtype = torch.bfloat16)
        unsqueeze_184 = torch.ops.aten.unsqueeze.default(_to_copy_274, 3);  _to_copy_274 = None
        unsqueeze_185 = torch.ops.aten.unsqueeze.default(unsqueeze_184, 4);  unsqueeze_184 = None
        unsqueeze_186 = torch.ops.aten.unsqueeze.default(unsqueeze_185, 5);  unsqueeze_185 = None
        permute_272 = torch.ops.aten.permute.default(unsqueeze_186, [0, 1, 3, 4, 5, 2]);  unsqueeze_186 = None
        unsqueeze_187 = torch.ops.aten.unsqueeze.default(_to_copy_275, 4);  _to_copy_275 = None
        unsqueeze_188 = torch.ops.aten.unsqueeze.default(unsqueeze_187, 5);  unsqueeze_187 = None
        permute_273 = torch.ops.aten.permute.default(unsqueeze_188, [4, 5, 0, 1, 2, 3]);  unsqueeze_188 = None
        permute_274 = torch.ops.aten.permute.default(permute_272, [0, 1, 5, 2, 3, 4]);  permute_272 = None
        view_452 = torch.ops.aten.view.default(permute_274, [1, 64, 64]);  permute_274 = None
        permute_275 = torch.ops.aten.permute.default(permute_273, [5, 2, 3, 4, 0, 1]);  permute_273 = None
        view_453 = torch.ops.aten.view.default(permute_275, [1, 64, 2097152]);  permute_275 = None
        bmm_44 = torch.ops.aten.bmm.default(view_452, view_453);  view_452 = view_453 = None
        view_454 = torch.ops.aten.view.default(bmm_44, [8, 8, 1, 1, 4096, 512]);  bmm_44 = None
        permute_276 = torch.ops.aten.permute.default(view_454, [0, 1, 3, 4, 5, 2]);  view_454 = None
        view_455 = torch.ops.aten.view.default(permute_276, [8, 8, 1, 4096, 512]);  permute_276 = None
        _to_copy_276 = torch.ops.aten._to_copy.default(getitem_378, dtype = torch.bfloat16);  getitem_378 = None
        _to_copy_277 = torch.ops.aten._to_copy.default(masked_fill_36, dtype = torch.bfloat16);  masked_fill_36 = None
        unsqueeze_189 = torch.ops.aten.unsqueeze.default(_to_copy_276, 3);  _to_copy_276 = None
        unsqueeze_190 = torch.ops.aten.unsqueeze.default(unsqueeze_189, 4);  unsqueeze_189 = None
        unsqueeze_191 = torch.ops.aten.unsqueeze.default(unsqueeze_190, 5);  unsqueeze_190 = None
        permute_277 = torch.ops.aten.permute.default(unsqueeze_191, [0, 1, 3, 4, 5, 2]);  unsqueeze_191 = None
        unsqueeze_192 = torch.ops.aten.unsqueeze.default(_to_copy_277, 4);  _to_copy_277 = None
        unsqueeze_193 = torch.ops.aten.unsqueeze.default(unsqueeze_192, 5);  unsqueeze_192 = None
        permute_278 = torch.ops.aten.permute.default(unsqueeze_193, [4, 5, 0, 1, 2, 3]);  unsqueeze_193 = None
        permute_279 = torch.ops.aten.permute.default(permute_277, [0, 1, 5, 2, 3, 4]);  permute_277 = None
        view_456 = torch.ops.aten.view.default(permute_279, [1, 64, 64]);  permute_279 = None
        permute_280 = torch.ops.aten.permute.default(permute_278, [5, 2, 3, 4, 0, 1]);  permute_278 = None
        view_457 = torch.ops.aten.view.default(permute_280, [1, 64, 2097152]);  permute_280 = None
        bmm_45 = torch.ops.aten.bmm.default(view_456, view_457);  view_456 = view_457 = None
        view_458 = torch.ops.aten.view.default(bmm_45, [8, 8, 1, 1, 4096, 512]);  bmm_45 = None
        permute_281 = torch.ops.aten.permute.default(view_458, [0, 1, 3, 4, 5, 2]);  view_458 = None
        view_459 = torch.ops.aten.view.default(permute_281, [8, 8, 1, 4096, 512]);  permute_281 = None
        unsqueeze_194 = torch.ops.aten.unsqueeze.default(view_455, 5);  view_455 = None
        unsqueeze_195 = torch.ops.aten.unsqueeze.default(unsqueeze_194, 6);  unsqueeze_194 = None
        permute_282 = torch.ops.aten.permute.default(unsqueeze_195, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_195 = None
        unsqueeze_196 = torch.ops.aten.unsqueeze.default(view_459, 5);  view_459 = None
        unsqueeze_197 = torch.ops.aten.unsqueeze.default(unsqueeze_196, 6);  unsqueeze_196 = None
        permute_283 = torch.ops.aten.permute.default(unsqueeze_197, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_197 = None
        permute_284 = torch.ops.aten.permute.default(permute_282, [3, 1, 4, 6, 0, 2, 5]);  permute_282 = None
        clone_61 = torch.ops.aten.clone.default(permute_284, memory_format = torch.contiguous_format);  permute_284 = None
        _unsafe_view_56 = torch.ops.aten._unsafe_view.default(clone_61, [8, 4096, 4096]);  clone_61 = None
        permute_285 = torch.ops.aten.permute.default(permute_283, [3, 6, 0, 2, 5, 1, 4]);  permute_283 = None
        clone_62 = torch.ops.aten.clone.default(permute_285, memory_format = torch.contiguous_format);  permute_285 = None
        _unsafe_view_57 = torch.ops.aten._unsafe_view.default(clone_62, [8, 4096, 4096]);  clone_62 = None
        bmm_46 = torch.ops.aten.bmm.default(_unsafe_view_56, _unsafe_view_57);  _unsafe_view_56 = _unsafe_view_57 = None
        view_460 = torch.ops.aten.view.default(bmm_46, [8, 512, 8, 1, 1, 512, 8]);  bmm_46 = None
        permute_286 = torch.ops.aten.permute.default(view_460, [4, 1, 5, 0, 2, 6, 3]);  view_460 = None
        view_461 = torch.ops.aten.view.default(permute_286, [1, 512, 512, 8, 8, 8]);  permute_286 = None
        clone_63 = torch.ops.aten.clone.default(view_461, memory_format = torch.contiguous_format);  view_461 = None
        _unsafe_view_58 = torch.ops.aten._unsafe_view.default(clone_63, [1, 512, 512, 512]);  clone_63 = None
        add_35 = torch.ops.aten.add.Tensor(_unsafe_view_58, 0);  _unsafe_view_58 = None
        slice_126 = torch.ops.aten.slice.Tensor(add_30, dim = 0, start = 0, end = 9223372036854775807)
        slice_127 = torch.ops.aten.slice.Tensor(slice_126, dim = 1, start = 4096, end = 8192);  slice_126 = None
        slice_128 = torch.ops.aten.slice.Tensor(slice_127, dim = 2, start = 0, end = 9223372036854775807);  slice_127 = None
        slice_129 = torch.ops.aten.slice.Tensor(slice_128, dim = 3, start = 0, end = 9223372036854775807);  slice_128 = None
        slice_130 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
        slice_131 = torch.ops.aten.slice.Tensor(slice_130, dim = 1, start = 4096, end = 8192);  slice_130 = None
        slice_132 = torch.ops.aten.slice.Tensor(slice_131, dim = 2, start = 0, end = 9223372036854775807);  slice_131 = None
        _to_copy_278 = torch.ops.aten._to_copy.default(slice_129, dtype = torch.float32);  slice_129 = None
        native_layer_norm_default_59 = torch.ops.aten.native_layer_norm.default(_to_copy_278, [64], None, None, 1e-05);  _to_copy_278 = None
        getitem_379 = native_layer_norm_default_59[0]
        view_462 = torch.ops.aten.view.default(slice_132, [1, 4096, 512, 1]);  slice_132 = None
        bitwise_not_37 = torch.ops.aten.bitwise_not.default(view_462);  view_462 = None
        masked_fill_37 = torch.ops.aten.masked_fill.Scalar(getitem_379, bitwise_not_37, 0);  getitem_379 = bitwise_not_37 = None
        unbind_int_25 = torch.ops.aten.unbind.int(arg45_1)
        getitem_382 = unbind_int_25[0]
        getitem_383 = unbind_int_25[1];  unbind_int_25 = None
        _to_copy_279 = torch.ops.aten._to_copy.default(getitem_382, dtype = torch.bfloat16);  getitem_382 = None
        _to_copy_280 = torch.ops.aten._to_copy.default(masked_fill_37, dtype = torch.bfloat16)
        unsqueeze_198 = torch.ops.aten.unsqueeze.default(_to_copy_279, 3);  _to_copy_279 = None
        unsqueeze_199 = torch.ops.aten.unsqueeze.default(unsqueeze_198, 4);  unsqueeze_198 = None
        unsqueeze_200 = torch.ops.aten.unsqueeze.default(unsqueeze_199, 5);  unsqueeze_199 = None
        permute_287 = torch.ops.aten.permute.default(unsqueeze_200, [0, 1, 3, 4, 5, 2]);  unsqueeze_200 = None
        unsqueeze_201 = torch.ops.aten.unsqueeze.default(_to_copy_280, 4);  _to_copy_280 = None
        unsqueeze_202 = torch.ops.aten.unsqueeze.default(unsqueeze_201, 5);  unsqueeze_201 = None
        permute_288 = torch.ops.aten.permute.default(unsqueeze_202, [4, 5, 0, 1, 2, 3]);  unsqueeze_202 = None
        permute_289 = torch.ops.aten.permute.default(permute_287, [0, 1, 5, 2, 3, 4]);  permute_287 = None
        view_463 = torch.ops.aten.view.default(permute_289, [1, 64, 64]);  permute_289 = None
        permute_290 = torch.ops.aten.permute.default(permute_288, [5, 2, 3, 4, 0, 1]);  permute_288 = None
        view_464 = torch.ops.aten.view.default(permute_290, [1, 64, 2097152]);  permute_290 = None
        bmm_47 = torch.ops.aten.bmm.default(view_463, view_464);  view_463 = view_464 = None
        view_465 = torch.ops.aten.view.default(bmm_47, [8, 8, 1, 1, 4096, 512]);  bmm_47 = None
        permute_291 = torch.ops.aten.permute.default(view_465, [0, 1, 3, 4, 5, 2]);  view_465 = None
        view_466 = torch.ops.aten.view.default(permute_291, [8, 8, 1, 4096, 512]);  permute_291 = None
        _to_copy_281 = torch.ops.aten._to_copy.default(getitem_383, dtype = torch.bfloat16);  getitem_383 = None
        _to_copy_282 = torch.ops.aten._to_copy.default(masked_fill_37, dtype = torch.bfloat16);  masked_fill_37 = None
        unsqueeze_203 = torch.ops.aten.unsqueeze.default(_to_copy_281, 3);  _to_copy_281 = None
        unsqueeze_204 = torch.ops.aten.unsqueeze.default(unsqueeze_203, 4);  unsqueeze_203 = None
        unsqueeze_205 = torch.ops.aten.unsqueeze.default(unsqueeze_204, 5);  unsqueeze_204 = None
        permute_292 = torch.ops.aten.permute.default(unsqueeze_205, [0, 1, 3, 4, 5, 2]);  unsqueeze_205 = None
        unsqueeze_206 = torch.ops.aten.unsqueeze.default(_to_copy_282, 4);  _to_copy_282 = None
        unsqueeze_207 = torch.ops.aten.unsqueeze.default(unsqueeze_206, 5);  unsqueeze_206 = None
        permute_293 = torch.ops.aten.permute.default(unsqueeze_207, [4, 5, 0, 1, 2, 3]);  unsqueeze_207 = None
        permute_294 = torch.ops.aten.permute.default(permute_292, [0, 1, 5, 2, 3, 4]);  permute_292 = None
        view_467 = torch.ops.aten.view.default(permute_294, [1, 64, 64]);  permute_294 = None
        permute_295 = torch.ops.aten.permute.default(permute_293, [5, 2, 3, 4, 0, 1]);  permute_293 = None
        view_468 = torch.ops.aten.view.default(permute_295, [1, 64, 2097152]);  permute_295 = None
        bmm_48 = torch.ops.aten.bmm.default(view_467, view_468);  view_467 = view_468 = None
        view_469 = torch.ops.aten.view.default(bmm_48, [8, 8, 1, 1, 4096, 512]);  bmm_48 = None
        permute_296 = torch.ops.aten.permute.default(view_469, [0, 1, 3, 4, 5, 2]);  view_469 = None
        view_470 = torch.ops.aten.view.default(permute_296, [8, 8, 1, 4096, 512]);  permute_296 = None
        unsqueeze_208 = torch.ops.aten.unsqueeze.default(view_466, 5);  view_466 = None
        unsqueeze_209 = torch.ops.aten.unsqueeze.default(unsqueeze_208, 6);  unsqueeze_208 = None
        permute_297 = torch.ops.aten.permute.default(unsqueeze_209, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_209 = None
        unsqueeze_210 = torch.ops.aten.unsqueeze.default(view_470, 5);  view_470 = None
        unsqueeze_211 = torch.ops.aten.unsqueeze.default(unsqueeze_210, 6);  unsqueeze_210 = None
        permute_298 = torch.ops.aten.permute.default(unsqueeze_211, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_211 = None
        permute_299 = torch.ops.aten.permute.default(permute_297, [3, 1, 4, 6, 0, 2, 5]);  permute_297 = None
        clone_64 = torch.ops.aten.clone.default(permute_299, memory_format = torch.contiguous_format);  permute_299 = None
        _unsafe_view_59 = torch.ops.aten._unsafe_view.default(clone_64, [8, 4096, 4096]);  clone_64 = None
        permute_300 = torch.ops.aten.permute.default(permute_298, [3, 6, 0, 2, 5, 1, 4]);  permute_298 = None
        clone_65 = torch.ops.aten.clone.default(permute_300, memory_format = torch.contiguous_format);  permute_300 = None
        _unsafe_view_60 = torch.ops.aten._unsafe_view.default(clone_65, [8, 4096, 4096]);  clone_65 = None
        bmm_49 = torch.ops.aten.bmm.default(_unsafe_view_59, _unsafe_view_60);  _unsafe_view_59 = _unsafe_view_60 = None
        view_471 = torch.ops.aten.view.default(bmm_49, [8, 512, 8, 1, 1, 512, 8]);  bmm_49 = None
        permute_301 = torch.ops.aten.permute.default(view_471, [4, 1, 5, 0, 2, 6, 3]);  view_471 = None
        view_472 = torch.ops.aten.view.default(permute_301, [1, 512, 512, 8, 8, 8]);  permute_301 = None
        clone_66 = torch.ops.aten.clone.default(view_472, memory_format = torch.contiguous_format);  view_472 = None
        _unsafe_view_61 = torch.ops.aten._unsafe_view.default(clone_66, [1, 512, 512, 512]);  clone_66 = None
        add_36 = torch.ops.aten.add.Tensor(add_35, _unsafe_view_61);  add_35 = _unsafe_view_61 = None
        slice_133 = torch.ops.aten.slice.Tensor(add_30, dim = 0, start = 0, end = 9223372036854775807)
        slice_134 = torch.ops.aten.slice.Tensor(slice_133, dim = 1, start = 8192, end = 12288);  slice_133 = None
        slice_135 = torch.ops.aten.slice.Tensor(slice_134, dim = 2, start = 0, end = 9223372036854775807);  slice_134 = None
        slice_136 = torch.ops.aten.slice.Tensor(slice_135, dim = 3, start = 0, end = 9223372036854775807);  slice_135 = None
        slice_137 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
        slice_138 = torch.ops.aten.slice.Tensor(slice_137, dim = 1, start = 8192, end = 12288);  slice_137 = None
        slice_139 = torch.ops.aten.slice.Tensor(slice_138, dim = 2, start = 0, end = 9223372036854775807);  slice_138 = None
        _to_copy_283 = torch.ops.aten._to_copy.default(slice_136, dtype = torch.float32);  slice_136 = None
        native_layer_norm_default_60 = torch.ops.aten.native_layer_norm.default(_to_copy_283, [64], None, None, 1e-05);  _to_copy_283 = None
        getitem_384 = native_layer_norm_default_60[0]
        view_473 = torch.ops.aten.view.default(slice_139, [1, 4096, 512, 1]);  slice_139 = None
        bitwise_not_38 = torch.ops.aten.bitwise_not.default(view_473);  view_473 = None
        masked_fill_38 = torch.ops.aten.masked_fill.Scalar(getitem_384, bitwise_not_38, 0);  getitem_384 = bitwise_not_38 = None
        unbind_int_26 = torch.ops.aten.unbind.int(arg45_1)
        getitem_387 = unbind_int_26[0]
        getitem_388 = unbind_int_26[1];  unbind_int_26 = None
        _to_copy_284 = torch.ops.aten._to_copy.default(getitem_387, dtype = torch.bfloat16);  getitem_387 = None
        _to_copy_285 = torch.ops.aten._to_copy.default(masked_fill_38, dtype = torch.bfloat16)
        unsqueeze_212 = torch.ops.aten.unsqueeze.default(_to_copy_284, 3);  _to_copy_284 = None
        unsqueeze_213 = torch.ops.aten.unsqueeze.default(unsqueeze_212, 4);  unsqueeze_212 = None
        unsqueeze_214 = torch.ops.aten.unsqueeze.default(unsqueeze_213, 5);  unsqueeze_213 = None
        permute_302 = torch.ops.aten.permute.default(unsqueeze_214, [0, 1, 3, 4, 5, 2]);  unsqueeze_214 = None
        unsqueeze_215 = torch.ops.aten.unsqueeze.default(_to_copy_285, 4);  _to_copy_285 = None
        unsqueeze_216 = torch.ops.aten.unsqueeze.default(unsqueeze_215, 5);  unsqueeze_215 = None
        permute_303 = torch.ops.aten.permute.default(unsqueeze_216, [4, 5, 0, 1, 2, 3]);  unsqueeze_216 = None
        permute_304 = torch.ops.aten.permute.default(permute_302, [0, 1, 5, 2, 3, 4]);  permute_302 = None
        view_474 = torch.ops.aten.view.default(permute_304, [1, 64, 64]);  permute_304 = None
        permute_305 = torch.ops.aten.permute.default(permute_303, [5, 2, 3, 4, 0, 1]);  permute_303 = None
        view_475 = torch.ops.aten.view.default(permute_305, [1, 64, 2097152]);  permute_305 = None
        bmm_50 = torch.ops.aten.bmm.default(view_474, view_475);  view_474 = view_475 = None
        view_476 = torch.ops.aten.view.default(bmm_50, [8, 8, 1, 1, 4096, 512]);  bmm_50 = None
        permute_306 = torch.ops.aten.permute.default(view_476, [0, 1, 3, 4, 5, 2]);  view_476 = None
        view_477 = torch.ops.aten.view.default(permute_306, [8, 8, 1, 4096, 512]);  permute_306 = None
        _to_copy_286 = torch.ops.aten._to_copy.default(getitem_388, dtype = torch.bfloat16);  getitem_388 = None
        _to_copy_287 = torch.ops.aten._to_copy.default(masked_fill_38, dtype = torch.bfloat16);  masked_fill_38 = None
        unsqueeze_217 = torch.ops.aten.unsqueeze.default(_to_copy_286, 3);  _to_copy_286 = None
        unsqueeze_218 = torch.ops.aten.unsqueeze.default(unsqueeze_217, 4);  unsqueeze_217 = None
        unsqueeze_219 = torch.ops.aten.unsqueeze.default(unsqueeze_218, 5);  unsqueeze_218 = None
        permute_307 = torch.ops.aten.permute.default(unsqueeze_219, [0, 1, 3, 4, 5, 2]);  unsqueeze_219 = None
        unsqueeze_220 = torch.ops.aten.unsqueeze.default(_to_copy_287, 4);  _to_copy_287 = None
        unsqueeze_221 = torch.ops.aten.unsqueeze.default(unsqueeze_220, 5);  unsqueeze_220 = None
        permute_308 = torch.ops.aten.permute.default(unsqueeze_221, [4, 5, 0, 1, 2, 3]);  unsqueeze_221 = None
        permute_309 = torch.ops.aten.permute.default(permute_307, [0, 1, 5, 2, 3, 4]);  permute_307 = None
        view_478 = torch.ops.aten.view.default(permute_309, [1, 64, 64]);  permute_309 = None
        permute_310 = torch.ops.aten.permute.default(permute_308, [5, 2, 3, 4, 0, 1]);  permute_308 = None
        view_479 = torch.ops.aten.view.default(permute_310, [1, 64, 2097152]);  permute_310 = None
        bmm_51 = torch.ops.aten.bmm.default(view_478, view_479);  view_478 = view_479 = None
        view_480 = torch.ops.aten.view.default(bmm_51, [8, 8, 1, 1, 4096, 512]);  bmm_51 = None
        permute_311 = torch.ops.aten.permute.default(view_480, [0, 1, 3, 4, 5, 2]);  view_480 = None
        view_481 = torch.ops.aten.view.default(permute_311, [8, 8, 1, 4096, 512]);  permute_311 = None
        unsqueeze_222 = torch.ops.aten.unsqueeze.default(view_477, 5);  view_477 = None
        unsqueeze_223 = torch.ops.aten.unsqueeze.default(unsqueeze_222, 6);  unsqueeze_222 = None
        permute_312 = torch.ops.aten.permute.default(unsqueeze_223, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_223 = None
        unsqueeze_224 = torch.ops.aten.unsqueeze.default(view_481, 5);  view_481 = None
        unsqueeze_225 = torch.ops.aten.unsqueeze.default(unsqueeze_224, 6);  unsqueeze_224 = None
        permute_313 = torch.ops.aten.permute.default(unsqueeze_225, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_225 = None
        permute_314 = torch.ops.aten.permute.default(permute_312, [3, 1, 4, 6, 0, 2, 5]);  permute_312 = None
        clone_67 = torch.ops.aten.clone.default(permute_314, memory_format = torch.contiguous_format);  permute_314 = None
        _unsafe_view_62 = torch.ops.aten._unsafe_view.default(clone_67, [8, 4096, 4096]);  clone_67 = None
        permute_315 = torch.ops.aten.permute.default(permute_313, [3, 6, 0, 2, 5, 1, 4]);  permute_313 = None
        clone_68 = torch.ops.aten.clone.default(permute_315, memory_format = torch.contiguous_format);  permute_315 = None
        _unsafe_view_63 = torch.ops.aten._unsafe_view.default(clone_68, [8, 4096, 4096]);  clone_68 = None
        bmm_52 = torch.ops.aten.bmm.default(_unsafe_view_62, _unsafe_view_63);  _unsafe_view_62 = _unsafe_view_63 = None
        view_482 = torch.ops.aten.view.default(bmm_52, [8, 512, 8, 1, 1, 512, 8]);  bmm_52 = None
        permute_316 = torch.ops.aten.permute.default(view_482, [4, 1, 5, 0, 2, 6, 3]);  view_482 = None
        view_483 = torch.ops.aten.view.default(permute_316, [1, 512, 512, 8, 8, 8]);  permute_316 = None
        clone_69 = torch.ops.aten.clone.default(view_483, memory_format = torch.contiguous_format);  view_483 = None
        _unsafe_view_64 = torch.ops.aten._unsafe_view.default(clone_69, [1, 512, 512, 512]);  clone_69 = None
        add_37 = torch.ops.aten.add.Tensor(add_36, _unsafe_view_64);  add_36 = _unsafe_view_64 = None
        slice_140 = torch.ops.aten.slice.Tensor(add_30, dim = 0, start = 0, end = 9223372036854775807)
        slice_141 = torch.ops.aten.slice.Tensor(slice_140, dim = 1, start = 12288, end = 16384);  slice_140 = None
        slice_142 = torch.ops.aten.slice.Tensor(slice_141, dim = 2, start = 0, end = 9223372036854775807);  slice_141 = None
        slice_143 = torch.ops.aten.slice.Tensor(slice_142, dim = 3, start = 0, end = 9223372036854775807);  slice_142 = None
        slice_144 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
        slice_145 = torch.ops.aten.slice.Tensor(slice_144, dim = 1, start = 12288, end = 16384);  slice_144 = None
        slice_146 = torch.ops.aten.slice.Tensor(slice_145, dim = 2, start = 0, end = 9223372036854775807);  slice_145 = None
        _to_copy_288 = torch.ops.aten._to_copy.default(slice_143, dtype = torch.float32);  slice_143 = None
        native_layer_norm_default_61 = torch.ops.aten.native_layer_norm.default(_to_copy_288, [64], None, None, 1e-05);  _to_copy_288 = None
        getitem_389 = native_layer_norm_default_61[0]
        view_484 = torch.ops.aten.view.default(slice_146, [1, 4096, 512, 1]);  slice_146 = None
        bitwise_not_39 = torch.ops.aten.bitwise_not.default(view_484);  view_484 = None
        masked_fill_39 = torch.ops.aten.masked_fill.Scalar(getitem_389, bitwise_not_39, 0);  getitem_389 = bitwise_not_39 = None
        unbind_int_27 = torch.ops.aten.unbind.int(arg45_1);  arg45_1 = None
        getitem_392 = unbind_int_27[0]
        getitem_393 = unbind_int_27[1];  unbind_int_27 = None
        _to_copy_289 = torch.ops.aten._to_copy.default(getitem_392, dtype = torch.bfloat16);  getitem_392 = None
        _to_copy_290 = torch.ops.aten._to_copy.default(masked_fill_39, dtype = torch.bfloat16)
        unsqueeze_226 = torch.ops.aten.unsqueeze.default(_to_copy_289, 3);  _to_copy_289 = None
        unsqueeze_227 = torch.ops.aten.unsqueeze.default(unsqueeze_226, 4);  unsqueeze_226 = None
        unsqueeze_228 = torch.ops.aten.unsqueeze.default(unsqueeze_227, 5);  unsqueeze_227 = None
        permute_317 = torch.ops.aten.permute.default(unsqueeze_228, [0, 1, 3, 4, 5, 2]);  unsqueeze_228 = None
        unsqueeze_229 = torch.ops.aten.unsqueeze.default(_to_copy_290, 4);  _to_copy_290 = None
        unsqueeze_230 = torch.ops.aten.unsqueeze.default(unsqueeze_229, 5);  unsqueeze_229 = None
        permute_318 = torch.ops.aten.permute.default(unsqueeze_230, [4, 5, 0, 1, 2, 3]);  unsqueeze_230 = None
        permute_319 = torch.ops.aten.permute.default(permute_317, [0, 1, 5, 2, 3, 4]);  permute_317 = None
        view_485 = torch.ops.aten.view.default(permute_319, [1, 64, 64]);  permute_319 = None
        permute_320 = torch.ops.aten.permute.default(permute_318, [5, 2, 3, 4, 0, 1]);  permute_318 = None
        view_486 = torch.ops.aten.view.default(permute_320, [1, 64, 2097152]);  permute_320 = None
        bmm_53 = torch.ops.aten.bmm.default(view_485, view_486);  view_485 = view_486 = None
        view_487 = torch.ops.aten.view.default(bmm_53, [8, 8, 1, 1, 4096, 512]);  bmm_53 = None
        permute_321 = torch.ops.aten.permute.default(view_487, [0, 1, 3, 4, 5, 2]);  view_487 = None
        view_488 = torch.ops.aten.view.default(permute_321, [8, 8, 1, 4096, 512]);  permute_321 = None
        _to_copy_291 = torch.ops.aten._to_copy.default(getitem_393, dtype = torch.bfloat16);  getitem_393 = None
        _to_copy_292 = torch.ops.aten._to_copy.default(masked_fill_39, dtype = torch.bfloat16);  masked_fill_39 = None
        unsqueeze_231 = torch.ops.aten.unsqueeze.default(_to_copy_291, 3);  _to_copy_291 = None
        unsqueeze_232 = torch.ops.aten.unsqueeze.default(unsqueeze_231, 4);  unsqueeze_231 = None
        unsqueeze_233 = torch.ops.aten.unsqueeze.default(unsqueeze_232, 5);  unsqueeze_232 = None
        permute_322 = torch.ops.aten.permute.default(unsqueeze_233, [0, 1, 3, 4, 5, 2]);  unsqueeze_233 = None
        unsqueeze_234 = torch.ops.aten.unsqueeze.default(_to_copy_292, 4);  _to_copy_292 = None
        unsqueeze_235 = torch.ops.aten.unsqueeze.default(unsqueeze_234, 5);  unsqueeze_234 = None
        permute_323 = torch.ops.aten.permute.default(unsqueeze_235, [4, 5, 0, 1, 2, 3]);  unsqueeze_235 = None
        permute_324 = torch.ops.aten.permute.default(permute_322, [0, 1, 5, 2, 3, 4]);  permute_322 = None
        view_489 = torch.ops.aten.view.default(permute_324, [1, 64, 64]);  permute_324 = None
        permute_325 = torch.ops.aten.permute.default(permute_323, [5, 2, 3, 4, 0, 1]);  permute_323 = None
        view_490 = torch.ops.aten.view.default(permute_325, [1, 64, 2097152]);  permute_325 = None
        bmm_54 = torch.ops.aten.bmm.default(view_489, view_490);  view_489 = view_490 = None
        view_491 = torch.ops.aten.view.default(bmm_54, [8, 8, 1, 1, 4096, 512]);  bmm_54 = None
        permute_326 = torch.ops.aten.permute.default(view_491, [0, 1, 3, 4, 5, 2]);  view_491 = None
        view_492 = torch.ops.aten.view.default(permute_326, [8, 8, 1, 4096, 512]);  permute_326 = None
        unsqueeze_236 = torch.ops.aten.unsqueeze.default(view_488, 5);  view_488 = None
        unsqueeze_237 = torch.ops.aten.unsqueeze.default(unsqueeze_236, 6);  unsqueeze_236 = None
        permute_327 = torch.ops.aten.permute.default(unsqueeze_237, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_237 = None
        unsqueeze_238 = torch.ops.aten.unsqueeze.default(view_492, 5);  view_492 = None
        unsqueeze_239 = torch.ops.aten.unsqueeze.default(unsqueeze_238, 6);  unsqueeze_238 = None
        permute_328 = torch.ops.aten.permute.default(unsqueeze_239, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_239 = None
        permute_329 = torch.ops.aten.permute.default(permute_327, [3, 1, 4, 6, 0, 2, 5]);  permute_327 = None
        clone_70 = torch.ops.aten.clone.default(permute_329, memory_format = torch.contiguous_format);  permute_329 = None
        _unsafe_view_65 = torch.ops.aten._unsafe_view.default(clone_70, [8, 4096, 4096]);  clone_70 = None
        permute_330 = torch.ops.aten.permute.default(permute_328, [3, 6, 0, 2, 5, 1, 4]);  permute_328 = None
        clone_71 = torch.ops.aten.clone.default(permute_330, memory_format = torch.contiguous_format);  permute_330 = None
        _unsafe_view_66 = torch.ops.aten._unsafe_view.default(clone_71, [8, 4096, 4096]);  clone_71 = None
        bmm_55 = torch.ops.aten.bmm.default(_unsafe_view_65, _unsafe_view_66);  _unsafe_view_65 = _unsafe_view_66 = None
        view_493 = torch.ops.aten.view.default(bmm_55, [8, 512, 8, 1, 1, 512, 8]);  bmm_55 = None
        permute_331 = torch.ops.aten.permute.default(view_493, [4, 1, 5, 0, 2, 6, 3]);  view_493 = None
        view_494 = torch.ops.aten.view.default(permute_331, [1, 512, 512, 8, 8, 8]);  permute_331 = None
        clone_72 = torch.ops.aten.clone.default(view_494, memory_format = torch.contiguous_format);  view_494 = None
        _unsafe_view_67 = torch.ops.aten._unsafe_view.default(clone_72, [1, 512, 512, 512]);  clone_72 = None
        add_38 = torch.ops.aten.add.Tensor(add_37, _unsafe_view_67);  add_37 = _unsafe_view_67 = None
        _to_copy_293 = torch.ops.aten._to_copy.default(add_38, dtype = torch.float32);  add_38 = None
        native_layer_norm_default_62 = torch.ops.aten.native_layer_norm.default(_to_copy_293, [512], arg46_1, arg47_1, 0.1);  _to_copy_293 = arg46_1 = arg47_1 = None
        getitem_394 = native_layer_norm_default_62[0]
        _to_copy_294 = torch.ops.aten._to_copy.default(arg49_1, dtype = torch.bfloat16);  arg49_1 = None
        _to_copy_295 = torch.ops.aten._to_copy.default(arg48_1, dtype = torch.bfloat16);  arg48_1 = None
        _to_copy_296 = torch.ops.aten._to_copy.default(getitem_394, dtype = torch.bfloat16);  getitem_394 = None
        view_495 = torch.ops.aten.view.default(_to_copy_296, [262144, 512]);  _to_copy_296 = None
        t_95 = torch.ops.aten.t.default(_to_copy_295);  _to_copy_295 = None
        addmm_2 = torch.ops.aten.addmm.default(_to_copy_294, view_495, t_95);  _to_copy_294 = view_495 = t_95 = None
        view_496 = torch.ops.aten.view.default(addmm_2, [1, 512, 512, 256]);  addmm_2 = None
        add_39 = torch.ops.aten.add.Tensor(add_34, view_496);  add_34 = view_496 = None
        split_tensor_34 = torch.ops.aten.split.Tensor(add_30, 128, dim = -2)
        getitem_397 = split_tensor_34[0]
        getitem_398 = split_tensor_34[1]
        getitem_399 = split_tensor_34[2]
        getitem_400 = split_tensor_34[3];  split_tensor_34 = None
        _to_copy_297 = torch.ops.aten._to_copy.default(getitem_397, dtype = torch.float32);  getitem_397 = None
        native_layer_norm_default_63 = torch.ops.aten.native_layer_norm.default(_to_copy_297, [64], arg84_1, arg85_1, 1e-05);  _to_copy_297 = None
        getitem_401 = native_layer_norm_default_63[0]
        _to_copy_298 = torch.ops.aten._to_copy.default(arg86_1, dtype = torch.bfloat16)
        _to_copy_299 = torch.ops.aten._to_copy.default(getitem_401, dtype = torch.bfloat16);  getitem_401 = None
        t_96 = torch.ops.aten.t.default(_to_copy_298);  _to_copy_298 = None
        view_497 = torch.ops.aten.view.default(_to_copy_299, [2097152, 64]);  _to_copy_299 = None
        mm_89 = torch.ops.aten.mm.default(view_497, t_96);  view_497 = t_96 = None
        view_498 = torch.ops.aten.view.default(mm_89, [1, 16384, 128, 512]);  mm_89 = None
        split_tensor_35 = torch.ops.aten.split.Tensor(view_498, 256, dim = -1);  view_498 = None
        getitem_404 = split_tensor_35[0]
        getitem_405 = split_tensor_35[1];  split_tensor_35 = None
        silu_12 = torch.ops.aten.silu.default(getitem_404);  getitem_404 = None
        mul_45 = torch.ops.aten.mul.Tensor(silu_12, getitem_405);  silu_12 = getitem_405 = None
        _to_copy_300 = torch.ops.aten._to_copy.default(arg87_1, dtype = torch.bfloat16)
        t_97 = torch.ops.aten.t.default(_to_copy_300);  _to_copy_300 = None
        view_500 = torch.ops.aten.view.default(mul_45, [2097152, 256]);  mul_45 = None
        mm_90 = torch.ops.aten.mm.default(view_500, t_97);  view_500 = t_97 = None
        view_501 = torch.ops.aten.view.default(mm_90, [1, 16384, 128, 64]);  mm_90 = None
        _to_copy_301 = torch.ops.aten._to_copy.default(getitem_398, dtype = torch.float32);  getitem_398 = None
        native_layer_norm_default_64 = torch.ops.aten.native_layer_norm.default(_to_copy_301, [64], arg84_1, arg85_1, 1e-05);  _to_copy_301 = None
        getitem_406 = native_layer_norm_default_64[0]
        _to_copy_302 = torch.ops.aten._to_copy.default(arg86_1, dtype = torch.bfloat16)
        _to_copy_303 = torch.ops.aten._to_copy.default(getitem_406, dtype = torch.bfloat16);  getitem_406 = None
        t_98 = torch.ops.aten.t.default(_to_copy_302);  _to_copy_302 = None
        view_502 = torch.ops.aten.view.default(_to_copy_303, [2097152, 64]);  _to_copy_303 = None
        mm_91 = torch.ops.aten.mm.default(view_502, t_98);  view_502 = t_98 = None
        view_503 = torch.ops.aten.view.default(mm_91, [1, 16384, 128, 512]);  mm_91 = None
        split_tensor_36 = torch.ops.aten.split.Tensor(view_503, 256, dim = -1);  view_503 = None
        getitem_409 = split_tensor_36[0]
        getitem_410 = split_tensor_36[1];  split_tensor_36 = None
        silu_13 = torch.ops.aten.silu.default(getitem_409);  getitem_409 = None
        mul_46 = torch.ops.aten.mul.Tensor(silu_13, getitem_410);  silu_13 = getitem_410 = None
        _to_copy_304 = torch.ops.aten._to_copy.default(arg87_1, dtype = torch.bfloat16)
        t_99 = torch.ops.aten.t.default(_to_copy_304);  _to_copy_304 = None
        view_505 = torch.ops.aten.view.default(mul_46, [2097152, 256]);  mul_46 = None
        mm_92 = torch.ops.aten.mm.default(view_505, t_99);  view_505 = t_99 = None
        view_506 = torch.ops.aten.view.default(mm_92, [1, 16384, 128, 64]);  mm_92 = None
        _to_copy_305 = torch.ops.aten._to_copy.default(getitem_399, dtype = torch.float32);  getitem_399 = None
        native_layer_norm_default_65 = torch.ops.aten.native_layer_norm.default(_to_copy_305, [64], arg84_1, arg85_1, 1e-05);  _to_copy_305 = None
        getitem_411 = native_layer_norm_default_65[0]
        _to_copy_306 = torch.ops.aten._to_copy.default(arg86_1, dtype = torch.bfloat16)
        _to_copy_307 = torch.ops.aten._to_copy.default(getitem_411, dtype = torch.bfloat16);  getitem_411 = None
        t_100 = torch.ops.aten.t.default(_to_copy_306);  _to_copy_306 = None
        view_507 = torch.ops.aten.view.default(_to_copy_307, [2097152, 64]);  _to_copy_307 = None
        mm_93 = torch.ops.aten.mm.default(view_507, t_100);  view_507 = t_100 = None
        view_508 = torch.ops.aten.view.default(mm_93, [1, 16384, 128, 512]);  mm_93 = None
        split_tensor_37 = torch.ops.aten.split.Tensor(view_508, 256, dim = -1);  view_508 = None
        getitem_414 = split_tensor_37[0]
        getitem_415 = split_tensor_37[1];  split_tensor_37 = None
        silu_14 = torch.ops.aten.silu.default(getitem_414);  getitem_414 = None
        mul_47 = torch.ops.aten.mul.Tensor(silu_14, getitem_415);  silu_14 = getitem_415 = None
        _to_copy_308 = torch.ops.aten._to_copy.default(arg87_1, dtype = torch.bfloat16)
        t_101 = torch.ops.aten.t.default(_to_copy_308);  _to_copy_308 = None
        view_510 = torch.ops.aten.view.default(mul_47, [2097152, 256]);  mul_47 = None
        mm_94 = torch.ops.aten.mm.default(view_510, t_101);  view_510 = t_101 = None
        view_511 = torch.ops.aten.view.default(mm_94, [1, 16384, 128, 64]);  mm_94 = None
        _to_copy_309 = torch.ops.aten._to_copy.default(getitem_400, dtype = torch.float32);  getitem_400 = None
        native_layer_norm_default_66 = torch.ops.aten.native_layer_norm.default(_to_copy_309, [64], arg84_1, arg85_1, 1e-05);  _to_copy_309 = arg84_1 = arg85_1 = None
        getitem_416 = native_layer_norm_default_66[0]
        _to_copy_310 = torch.ops.aten._to_copy.default(arg86_1, dtype = torch.bfloat16);  arg86_1 = None
        _to_copy_311 = torch.ops.aten._to_copy.default(getitem_416, dtype = torch.bfloat16);  getitem_416 = None
        t_102 = torch.ops.aten.t.default(_to_copy_310);  _to_copy_310 = None
        view_512 = torch.ops.aten.view.default(_to_copy_311, [2097152, 64]);  _to_copy_311 = None
        mm_95 = torch.ops.aten.mm.default(view_512, t_102);  view_512 = t_102 = None
        view_513 = torch.ops.aten.view.default(mm_95, [1, 16384, 128, 512]);  mm_95 = None
        split_tensor_38 = torch.ops.aten.split.Tensor(view_513, 256, dim = -1);  view_513 = None
        getitem_419 = split_tensor_38[0]
        getitem_420 = split_tensor_38[1];  split_tensor_38 = None
        silu_15 = torch.ops.aten.silu.default(getitem_419);  getitem_419 = None
        mul_48 = torch.ops.aten.mul.Tensor(silu_15, getitem_420);  silu_15 = getitem_420 = None
        _to_copy_312 = torch.ops.aten._to_copy.default(arg87_1, dtype = torch.bfloat16);  arg87_1 = None
        t_103 = torch.ops.aten.t.default(_to_copy_312);  _to_copy_312 = None
        view_515 = torch.ops.aten.view.default(mul_48, [2097152, 256]);  mul_48 = None
        mm_96 = torch.ops.aten.mm.default(view_515, t_103);  view_515 = t_103 = None
        view_516 = torch.ops.aten.view.default(mm_96, [1, 16384, 128, 64]);  mm_96 = None
        cat_8 = torch.ops.aten.cat.default([view_501, view_506, view_511, view_516], dim = -2);  view_501 = view_506 = view_511 = view_516 = None
        add_40 = torch.ops.aten.add.Tensor(add_30, cat_8);  cat_8 = None
        slice_147 = torch.ops.aten.slice.Tensor(add_30, dim = 0, start = 0, end = 9223372036854775807)
        slice_148 = torch.ops.aten.slice.Tensor(slice_147, dim = 1, start = 0, end = 4096);  slice_147 = None
        slice_149 = torch.ops.aten.slice.Tensor(slice_148, dim = 2, start = 0, end = 9223372036854775807);  slice_148 = None
        slice_150 = torch.ops.aten.slice.Tensor(slice_149, dim = 3, start = 0, end = 9223372036854775807);  slice_149 = None
        slice_151 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
        slice_152 = torch.ops.aten.slice.Tensor(slice_151, dim = 1, start = 0, end = 4096);  slice_151 = None
        slice_153 = torch.ops.aten.slice.Tensor(slice_152, dim = 2, start = 0, end = 9223372036854775807);  slice_152 = None
        _to_copy_313 = torch.ops.aten._to_copy.default(add_39, dtype = torch.float32)
        native_layer_norm_default_67 = torch.ops.aten.native_layer_norm.default(_to_copy_313, [256], arg72_1, arg73_1, 1e-05);  _to_copy_313 = None
        getitem_421 = native_layer_norm_default_67[0]
        _to_copy_314 = torch.ops.aten._to_copy.default(arg74_1, dtype = torch.bfloat16)
        _to_copy_315 = torch.ops.aten._to_copy.default(getitem_421, dtype = torch.bfloat16);  getitem_421 = None
        t_104 = torch.ops.aten.t.default(_to_copy_314);  _to_copy_314 = None
        view_517 = torch.ops.aten.view.default(_to_copy_315, [262144, 256]);  _to_copy_315 = None
        mm_97 = torch.ops.aten.mm.default(view_517, t_104);  view_517 = t_104 = None
        view_518 = torch.ops.aten.view.default(mm_97, [1, 512, 512, 8]);  mm_97 = None
        permute_332 = torch.ops.aten.permute.default(view_518, [0, 3, 1, 2]);  view_518 = None
        view_519 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_40 = torch.ops.aten.bitwise_not.default(view_519);  view_519 = None
        masked_fill_40 = torch.ops.aten.masked_fill.Scalar(permute_332, bitwise_not_40, -10000);  permute_332 = bitwise_not_40 = None
        _to_copy_316 = torch.ops.aten._to_copy.default(masked_fill_40, dtype = torch.float32, layout = torch.strided, device = self.device);  masked_fill_40 = None
        _softmax_8 = torch.ops.aten._softmax.default(_to_copy_316, -1, False);  _to_copy_316 = None
        _to_copy_317 = torch.ops.aten._to_copy.default(slice_150, dtype = torch.float32);  slice_150 = None
        native_layer_norm_default_68 = torch.ops.aten.native_layer_norm.default(_to_copy_317, [64], arg69_1, arg70_1, 1e-05);  _to_copy_317 = None
        getitem_424 = native_layer_norm_default_68[0]
        _to_copy_318 = torch.ops.aten._to_copy.default(arg71_1, dtype = torch.bfloat16)
        _to_copy_319 = torch.ops.aten._to_copy.default(getitem_424, dtype = torch.bfloat16);  getitem_424 = None
        t_105 = torch.ops.aten.t.default(_to_copy_318);  _to_copy_318 = None
        view_520 = torch.ops.aten.view.default(_to_copy_319, [2097152, 64]);  _to_copy_319 = None
        mm_98 = torch.ops.aten.mm.default(view_520, t_105);  view_520 = t_105 = None
        view_521 = torch.ops.aten.view.default(mm_98, [1, 4096, 512, 512]);  mm_98 = None
        view_522 = torch.ops.aten.view.default(view_521, [1, 4096, 512, 2, 8, 32]);  view_521 = None
        permute_333 = torch.ops.aten.permute.default(view_522, [3, 0, 1, 2, 4, 5]);  view_522 = None
        unbind_int_28 = torch.ops.aten.unbind.int(permute_333);  permute_333 = None
        getitem_427 = unbind_int_28[0]
        getitem_428 = unbind_int_28[1];  unbind_int_28 = None
        sigmoid_28 = torch.ops.aten.sigmoid.default(getitem_428);  getitem_428 = None
        bitwise_not_41 = torch.ops.aten.bitwise_not.default(slice_153);  slice_153 = None
        view_523 = torch.ops.aten.view.default(bitwise_not_41, [1, 4096, 512, 1, 1]);  bitwise_not_41 = None
        masked_fill_41 = torch.ops.aten.masked_fill.Scalar(getitem_427, view_523, 0);  getitem_427 = view_523 = None
        _to_copy_320 = torch.ops.aten._to_copy.default(_softmax_8, dtype = torch.bfloat16);  _softmax_8 = None
        unsqueeze_240 = torch.ops.aten.unsqueeze.default(_to_copy_320, 4);  _to_copy_320 = None
        unsqueeze_241 = torch.ops.aten.unsqueeze.default(unsqueeze_240, 5);  unsqueeze_240 = None
        permute_334 = torch.ops.aten.permute.default(unsqueeze_241, [0, 4, 2, 1, 5, 3]);  unsqueeze_241 = None
        unsqueeze_242 = torch.ops.aten.unsqueeze.default(masked_fill_41, 5);  masked_fill_41 = None
        permute_335 = torch.ops.aten.permute.default(unsqueeze_242, [0, 1, 5, 3, 4, 2]);  unsqueeze_242 = None
        permute_336 = torch.ops.aten.permute.default(permute_334, [3, 2, 5, 0, 1, 4]);  permute_334 = None
        view_524 = torch.ops.aten.view.default(permute_336, [8, 512, 512]);  permute_336 = None
        permute_337 = torch.ops.aten.permute.default(permute_335, [3, 5, 0, 1, 4, 2]);  permute_335 = None
        clone_73 = torch.ops.aten.clone.default(permute_337, memory_format = torch.contiguous_format);  permute_337 = None
        _unsafe_view_68 = torch.ops.aten._unsafe_view.default(clone_73, [8, 512, 131072]);  clone_73 = None
        bmm_56 = torch.ops.aten.bmm.default(view_524, _unsafe_view_68);  view_524 = _unsafe_view_68 = None
        view_525 = torch.ops.aten.view.default(bmm_56, [8, 512, 1, 1, 4096, 32]);  bmm_56 = None
        permute_338 = torch.ops.aten.permute.default(view_525, [3, 4, 1, 0, 5, 2]);  view_525 = None
        view_526 = torch.ops.aten.view.default(permute_338, [1, 4096, 512, 8, 32]);  permute_338 = None
        mul_49 = torch.ops.aten.mul.Tensor(sigmoid_28, view_526);  sigmoid_28 = view_526 = None
        view_527 = torch.ops.aten.view.default(mul_49, [1, 4096, 512, 256]);  mul_49 = None
        _to_copy_321 = torch.ops.aten._to_copy.default(arg75_1, dtype = torch.bfloat16)
        t_106 = torch.ops.aten.t.default(_to_copy_321);  _to_copy_321 = None
        view_528 = torch.ops.aten.view.default(view_527, [2097152, 256]);  view_527 = None
        mm_99 = torch.ops.aten.mm.default(view_528, t_106);  view_528 = t_106 = None
        view_529 = torch.ops.aten.view.default(mm_99, [1, 4096, 512, 64]);  mm_99 = None
        slice_154 = torch.ops.aten.slice.Tensor(add_30, dim = 0, start = 0, end = 9223372036854775807)
        slice_155 = torch.ops.aten.slice.Tensor(slice_154, dim = 1, start = 4096, end = 8192);  slice_154 = None
        slice_156 = torch.ops.aten.slice.Tensor(slice_155, dim = 2, start = 0, end = 9223372036854775807);  slice_155 = None
        slice_157 = torch.ops.aten.slice.Tensor(slice_156, dim = 3, start = 0, end = 9223372036854775807);  slice_156 = None
        slice_158 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
        slice_159 = torch.ops.aten.slice.Tensor(slice_158, dim = 1, start = 4096, end = 8192);  slice_158 = None
        slice_160 = torch.ops.aten.slice.Tensor(slice_159, dim = 2, start = 0, end = 9223372036854775807);  slice_159 = None
        _to_copy_322 = torch.ops.aten._to_copy.default(add_39, dtype = torch.float32)
        native_layer_norm_default_69 = torch.ops.aten.native_layer_norm.default(_to_copy_322, [256], arg72_1, arg73_1, 1e-05);  _to_copy_322 = None
        getitem_429 = native_layer_norm_default_69[0]
        _to_copy_323 = torch.ops.aten._to_copy.default(arg74_1, dtype = torch.bfloat16)
        _to_copy_324 = torch.ops.aten._to_copy.default(getitem_429, dtype = torch.bfloat16);  getitem_429 = None
        t_107 = torch.ops.aten.t.default(_to_copy_323);  _to_copy_323 = None
        view_530 = torch.ops.aten.view.default(_to_copy_324, [262144, 256]);  _to_copy_324 = None
        mm_100 = torch.ops.aten.mm.default(view_530, t_107);  view_530 = t_107 = None
        view_531 = torch.ops.aten.view.default(mm_100, [1, 512, 512, 8]);  mm_100 = None
        permute_339 = torch.ops.aten.permute.default(view_531, [0, 3, 1, 2]);  view_531 = None
        view_532 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_42 = torch.ops.aten.bitwise_not.default(view_532);  view_532 = None
        masked_fill_42 = torch.ops.aten.masked_fill.Scalar(permute_339, bitwise_not_42, -10000);  permute_339 = bitwise_not_42 = None
        _to_copy_325 = torch.ops.aten._to_copy.default(masked_fill_42, dtype = torch.float32, layout = torch.strided, device = self.device);  masked_fill_42 = None
        _softmax_9 = torch.ops.aten._softmax.default(_to_copy_325, -1, False);  _to_copy_325 = None
        _to_copy_326 = torch.ops.aten._to_copy.default(slice_157, dtype = torch.float32);  slice_157 = None
        native_layer_norm_default_70 = torch.ops.aten.native_layer_norm.default(_to_copy_326, [64], arg69_1, arg70_1, 1e-05);  _to_copy_326 = None
        getitem_432 = native_layer_norm_default_70[0]
        _to_copy_327 = torch.ops.aten._to_copy.default(arg71_1, dtype = torch.bfloat16)
        _to_copy_328 = torch.ops.aten._to_copy.default(getitem_432, dtype = torch.bfloat16);  getitem_432 = None
        t_108 = torch.ops.aten.t.default(_to_copy_327);  _to_copy_327 = None
        view_533 = torch.ops.aten.view.default(_to_copy_328, [2097152, 64]);  _to_copy_328 = None
        mm_101 = torch.ops.aten.mm.default(view_533, t_108);  view_533 = t_108 = None
        view_534 = torch.ops.aten.view.default(mm_101, [1, 4096, 512, 512]);  mm_101 = None
        view_535 = torch.ops.aten.view.default(view_534, [1, 4096, 512, 2, 8, 32]);  view_534 = None
        permute_340 = torch.ops.aten.permute.default(view_535, [3, 0, 1, 2, 4, 5]);  view_535 = None
        unbind_int_29 = torch.ops.aten.unbind.int(permute_340);  permute_340 = None
        getitem_435 = unbind_int_29[0]
        getitem_436 = unbind_int_29[1];  unbind_int_29 = None
        sigmoid_29 = torch.ops.aten.sigmoid.default(getitem_436);  getitem_436 = None
        bitwise_not_43 = torch.ops.aten.bitwise_not.default(slice_160);  slice_160 = None
        view_536 = torch.ops.aten.view.default(bitwise_not_43, [1, 4096, 512, 1, 1]);  bitwise_not_43 = None
        masked_fill_43 = torch.ops.aten.masked_fill.Scalar(getitem_435, view_536, 0);  getitem_435 = view_536 = None
        _to_copy_329 = torch.ops.aten._to_copy.default(_softmax_9, dtype = torch.bfloat16);  _softmax_9 = None
        unsqueeze_243 = torch.ops.aten.unsqueeze.default(_to_copy_329, 4);  _to_copy_329 = None
        unsqueeze_244 = torch.ops.aten.unsqueeze.default(unsqueeze_243, 5);  unsqueeze_243 = None
        permute_341 = torch.ops.aten.permute.default(unsqueeze_244, [0, 4, 2, 1, 5, 3]);  unsqueeze_244 = None
        unsqueeze_245 = torch.ops.aten.unsqueeze.default(masked_fill_43, 5);  masked_fill_43 = None
        permute_342 = torch.ops.aten.permute.default(unsqueeze_245, [0, 1, 5, 3, 4, 2]);  unsqueeze_245 = None
        permute_343 = torch.ops.aten.permute.default(permute_341, [3, 2, 5, 0, 1, 4]);  permute_341 = None
        view_537 = torch.ops.aten.view.default(permute_343, [8, 512, 512]);  permute_343 = None
        permute_344 = torch.ops.aten.permute.default(permute_342, [3, 5, 0, 1, 4, 2]);  permute_342 = None
        clone_74 = torch.ops.aten.clone.default(permute_344, memory_format = torch.contiguous_format);  permute_344 = None
        _unsafe_view_69 = torch.ops.aten._unsafe_view.default(clone_74, [8, 512, 131072]);  clone_74 = None
        bmm_57 = torch.ops.aten.bmm.default(view_537, _unsafe_view_69);  view_537 = _unsafe_view_69 = None
        view_538 = torch.ops.aten.view.default(bmm_57, [8, 512, 1, 1, 4096, 32]);  bmm_57 = None
        permute_345 = torch.ops.aten.permute.default(view_538, [3, 4, 1, 0, 5, 2]);  view_538 = None
        view_539 = torch.ops.aten.view.default(permute_345, [1, 4096, 512, 8, 32]);  permute_345 = None
        mul_50 = torch.ops.aten.mul.Tensor(sigmoid_29, view_539);  sigmoid_29 = view_539 = None
        view_540 = torch.ops.aten.view.default(mul_50, [1, 4096, 512, 256]);  mul_50 = None
        _to_copy_330 = torch.ops.aten._to_copy.default(arg75_1, dtype = torch.bfloat16)
        t_109 = torch.ops.aten.t.default(_to_copy_330);  _to_copy_330 = None
        view_541 = torch.ops.aten.view.default(view_540, [2097152, 256]);  view_540 = None
        mm_102 = torch.ops.aten.mm.default(view_541, t_109);  view_541 = t_109 = None
        view_542 = torch.ops.aten.view.default(mm_102, [1, 4096, 512, 64]);  mm_102 = None
        slice_161 = torch.ops.aten.slice.Tensor(add_30, dim = 0, start = 0, end = 9223372036854775807)
        slice_162 = torch.ops.aten.slice.Tensor(slice_161, dim = 1, start = 8192, end = 12288);  slice_161 = None
        slice_163 = torch.ops.aten.slice.Tensor(slice_162, dim = 2, start = 0, end = 9223372036854775807);  slice_162 = None
        slice_164 = torch.ops.aten.slice.Tensor(slice_163, dim = 3, start = 0, end = 9223372036854775807);  slice_163 = None
        slice_165 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
        slice_166 = torch.ops.aten.slice.Tensor(slice_165, dim = 1, start = 8192, end = 12288);  slice_165 = None
        slice_167 = torch.ops.aten.slice.Tensor(slice_166, dim = 2, start = 0, end = 9223372036854775807);  slice_166 = None
        _to_copy_331 = torch.ops.aten._to_copy.default(add_39, dtype = torch.float32)
        native_layer_norm_default_71 = torch.ops.aten.native_layer_norm.default(_to_copy_331, [256], arg72_1, arg73_1, 1e-05);  _to_copy_331 = None
        getitem_437 = native_layer_norm_default_71[0]
        _to_copy_332 = torch.ops.aten._to_copy.default(arg74_1, dtype = torch.bfloat16)
        _to_copy_333 = torch.ops.aten._to_copy.default(getitem_437, dtype = torch.bfloat16);  getitem_437 = None
        t_110 = torch.ops.aten.t.default(_to_copy_332);  _to_copy_332 = None
        view_543 = torch.ops.aten.view.default(_to_copy_333, [262144, 256]);  _to_copy_333 = None
        mm_103 = torch.ops.aten.mm.default(view_543, t_110);  view_543 = t_110 = None
        view_544 = torch.ops.aten.view.default(mm_103, [1, 512, 512, 8]);  mm_103 = None
        permute_346 = torch.ops.aten.permute.default(view_544, [0, 3, 1, 2]);  view_544 = None
        view_545 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_44 = torch.ops.aten.bitwise_not.default(view_545);  view_545 = None
        masked_fill_44 = torch.ops.aten.masked_fill.Scalar(permute_346, bitwise_not_44, -10000);  permute_346 = bitwise_not_44 = None
        _to_copy_334 = torch.ops.aten._to_copy.default(masked_fill_44, dtype = torch.float32, layout = torch.strided, device = self.device);  masked_fill_44 = None
        _softmax_10 = torch.ops.aten._softmax.default(_to_copy_334, -1, False);  _to_copy_334 = None
        _to_copy_335 = torch.ops.aten._to_copy.default(slice_164, dtype = torch.float32);  slice_164 = None
        native_layer_norm_default_72 = torch.ops.aten.native_layer_norm.default(_to_copy_335, [64], arg69_1, arg70_1, 1e-05);  _to_copy_335 = None
        getitem_440 = native_layer_norm_default_72[0]
        _to_copy_336 = torch.ops.aten._to_copy.default(arg71_1, dtype = torch.bfloat16)
        _to_copy_337 = torch.ops.aten._to_copy.default(getitem_440, dtype = torch.bfloat16);  getitem_440 = None
        t_111 = torch.ops.aten.t.default(_to_copy_336);  _to_copy_336 = None
        view_546 = torch.ops.aten.view.default(_to_copy_337, [2097152, 64]);  _to_copy_337 = None
        mm_104 = torch.ops.aten.mm.default(view_546, t_111);  view_546 = t_111 = None
        view_547 = torch.ops.aten.view.default(mm_104, [1, 4096, 512, 512]);  mm_104 = None
        view_548 = torch.ops.aten.view.default(view_547, [1, 4096, 512, 2, 8, 32]);  view_547 = None
        permute_347 = torch.ops.aten.permute.default(view_548, [3, 0, 1, 2, 4, 5]);  view_548 = None
        unbind_int_30 = torch.ops.aten.unbind.int(permute_347);  permute_347 = None
        getitem_443 = unbind_int_30[0]
        getitem_444 = unbind_int_30[1];  unbind_int_30 = None
        sigmoid_30 = torch.ops.aten.sigmoid.default(getitem_444);  getitem_444 = None
        bitwise_not_45 = torch.ops.aten.bitwise_not.default(slice_167);  slice_167 = None
        view_549 = torch.ops.aten.view.default(bitwise_not_45, [1, 4096, 512, 1, 1]);  bitwise_not_45 = None
        masked_fill_45 = torch.ops.aten.masked_fill.Scalar(getitem_443, view_549, 0);  getitem_443 = view_549 = None
        _to_copy_338 = torch.ops.aten._to_copy.default(_softmax_10, dtype = torch.bfloat16);  _softmax_10 = None
        unsqueeze_246 = torch.ops.aten.unsqueeze.default(_to_copy_338, 4);  _to_copy_338 = None
        unsqueeze_247 = torch.ops.aten.unsqueeze.default(unsqueeze_246, 5);  unsqueeze_246 = None
        permute_348 = torch.ops.aten.permute.default(unsqueeze_247, [0, 4, 2, 1, 5, 3]);  unsqueeze_247 = None
        unsqueeze_248 = torch.ops.aten.unsqueeze.default(masked_fill_45, 5);  masked_fill_45 = None
        permute_349 = torch.ops.aten.permute.default(unsqueeze_248, [0, 1, 5, 3, 4, 2]);  unsqueeze_248 = None
        permute_350 = torch.ops.aten.permute.default(permute_348, [3, 2, 5, 0, 1, 4]);  permute_348 = None
        view_550 = torch.ops.aten.view.default(permute_350, [8, 512, 512]);  permute_350 = None
        permute_351 = torch.ops.aten.permute.default(permute_349, [3, 5, 0, 1, 4, 2]);  permute_349 = None
        clone_75 = torch.ops.aten.clone.default(permute_351, memory_format = torch.contiguous_format);  permute_351 = None
        _unsafe_view_70 = torch.ops.aten._unsafe_view.default(clone_75, [8, 512, 131072]);  clone_75 = None
        bmm_58 = torch.ops.aten.bmm.default(view_550, _unsafe_view_70);  view_550 = _unsafe_view_70 = None
        view_551 = torch.ops.aten.view.default(bmm_58, [8, 512, 1, 1, 4096, 32]);  bmm_58 = None
        permute_352 = torch.ops.aten.permute.default(view_551, [3, 4, 1, 0, 5, 2]);  view_551 = None
        view_552 = torch.ops.aten.view.default(permute_352, [1, 4096, 512, 8, 32]);  permute_352 = None
        mul_51 = torch.ops.aten.mul.Tensor(sigmoid_30, view_552);  sigmoid_30 = view_552 = None
        view_553 = torch.ops.aten.view.default(mul_51, [1, 4096, 512, 256]);  mul_51 = None
        _to_copy_339 = torch.ops.aten._to_copy.default(arg75_1, dtype = torch.bfloat16)
        t_112 = torch.ops.aten.t.default(_to_copy_339);  _to_copy_339 = None
        view_554 = torch.ops.aten.view.default(view_553, [2097152, 256]);  view_553 = None
        mm_105 = torch.ops.aten.mm.default(view_554, t_112);  view_554 = t_112 = None
        view_555 = torch.ops.aten.view.default(mm_105, [1, 4096, 512, 64]);  mm_105 = None
        slice_168 = torch.ops.aten.slice.Tensor(add_30, dim = 0, start = 0, end = 9223372036854775807);  add_30 = None
        slice_169 = torch.ops.aten.slice.Tensor(slice_168, dim = 1, start = 12288, end = 16384);  slice_168 = None
        slice_170 = torch.ops.aten.slice.Tensor(slice_169, dim = 2, start = 0, end = 9223372036854775807);  slice_169 = None
        slice_171 = torch.ops.aten.slice.Tensor(slice_170, dim = 3, start = 0, end = 9223372036854775807);  slice_170 = None
        slice_172 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
        slice_173 = torch.ops.aten.slice.Tensor(slice_172, dim = 1, start = 12288, end = 16384);  slice_172 = None
        slice_174 = torch.ops.aten.slice.Tensor(slice_173, dim = 2, start = 0, end = 9223372036854775807);  slice_173 = None
        _to_copy_340 = torch.ops.aten._to_copy.default(add_39, dtype = torch.float32)
        native_layer_norm_default_73 = torch.ops.aten.native_layer_norm.default(_to_copy_340, [256], arg72_1, arg73_1, 1e-05);  _to_copy_340 = arg72_1 = arg73_1 = None
        getitem_445 = native_layer_norm_default_73[0]
        _to_copy_341 = torch.ops.aten._to_copy.default(arg74_1, dtype = torch.bfloat16);  arg74_1 = None
        _to_copy_342 = torch.ops.aten._to_copy.default(getitem_445, dtype = torch.bfloat16);  getitem_445 = None
        t_113 = torch.ops.aten.t.default(_to_copy_341);  _to_copy_341 = None
        view_556 = torch.ops.aten.view.default(_to_copy_342, [262144, 256]);  _to_copy_342 = None
        mm_106 = torch.ops.aten.mm.default(view_556, t_113);  view_556 = t_113 = None
        view_557 = torch.ops.aten.view.default(mm_106, [1, 512, 512, 8]);  mm_106 = None
        permute_353 = torch.ops.aten.permute.default(view_557, [0, 3, 1, 2]);  view_557 = None
        view_558 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_46 = torch.ops.aten.bitwise_not.default(view_558);  view_558 = None
        masked_fill_46 = torch.ops.aten.masked_fill.Scalar(permute_353, bitwise_not_46, -10000);  permute_353 = bitwise_not_46 = None
        _to_copy_343 = torch.ops.aten._to_copy.default(masked_fill_46, dtype = torch.float32, layout = torch.strided, device = self.device);  masked_fill_46 = None
        _softmax_11 = torch.ops.aten._softmax.default(_to_copy_343, -1, False);  _to_copy_343 = None
        _to_copy_344 = torch.ops.aten._to_copy.default(slice_171, dtype = torch.float32);  slice_171 = None
        native_layer_norm_default_74 = torch.ops.aten.native_layer_norm.default(_to_copy_344, [64], arg69_1, arg70_1, 1e-05);  _to_copy_344 = arg69_1 = arg70_1 = None
        getitem_448 = native_layer_norm_default_74[0]
        _to_copy_345 = torch.ops.aten._to_copy.default(arg71_1, dtype = torch.bfloat16);  arg71_1 = None
        _to_copy_346 = torch.ops.aten._to_copy.default(getitem_448, dtype = torch.bfloat16);  getitem_448 = None
        t_114 = torch.ops.aten.t.default(_to_copy_345);  _to_copy_345 = None
        view_559 = torch.ops.aten.view.default(_to_copy_346, [2097152, 64]);  _to_copy_346 = None
        mm_107 = torch.ops.aten.mm.default(view_559, t_114);  view_559 = t_114 = None
        view_560 = torch.ops.aten.view.default(mm_107, [1, 4096, 512, 512]);  mm_107 = None
        view_561 = torch.ops.aten.view.default(view_560, [1, 4096, 512, 2, 8, 32]);  view_560 = None
        permute_354 = torch.ops.aten.permute.default(view_561, [3, 0, 1, 2, 4, 5]);  view_561 = None
        unbind_int_31 = torch.ops.aten.unbind.int(permute_354);  permute_354 = None
        getitem_451 = unbind_int_31[0]
        getitem_452 = unbind_int_31[1];  unbind_int_31 = None
        sigmoid_31 = torch.ops.aten.sigmoid.default(getitem_452);  getitem_452 = None
        bitwise_not_47 = torch.ops.aten.bitwise_not.default(slice_174);  slice_174 = None
        view_562 = torch.ops.aten.view.default(bitwise_not_47, [1, 4096, 512, 1, 1]);  bitwise_not_47 = None
        masked_fill_47 = torch.ops.aten.masked_fill.Scalar(getitem_451, view_562, 0);  getitem_451 = view_562 = None
        _to_copy_347 = torch.ops.aten._to_copy.default(_softmax_11, dtype = torch.bfloat16);  _softmax_11 = None
        unsqueeze_249 = torch.ops.aten.unsqueeze.default(_to_copy_347, 4);  _to_copy_347 = None
        unsqueeze_250 = torch.ops.aten.unsqueeze.default(unsqueeze_249, 5);  unsqueeze_249 = None
        permute_355 = torch.ops.aten.permute.default(unsqueeze_250, [0, 4, 2, 1, 5, 3]);  unsqueeze_250 = None
        unsqueeze_251 = torch.ops.aten.unsqueeze.default(masked_fill_47, 5);  masked_fill_47 = None
        permute_356 = torch.ops.aten.permute.default(unsqueeze_251, [0, 1, 5, 3, 4, 2]);  unsqueeze_251 = None
        permute_357 = torch.ops.aten.permute.default(permute_355, [3, 2, 5, 0, 1, 4]);  permute_355 = None
        view_563 = torch.ops.aten.view.default(permute_357, [8, 512, 512]);  permute_357 = None
        permute_358 = torch.ops.aten.permute.default(permute_356, [3, 5, 0, 1, 4, 2]);  permute_356 = None
        clone_76 = torch.ops.aten.clone.default(permute_358, memory_format = torch.contiguous_format);  permute_358 = None
        _unsafe_view_71 = torch.ops.aten._unsafe_view.default(clone_76, [8, 512, 131072]);  clone_76 = None
        bmm_59 = torch.ops.aten.bmm.default(view_563, _unsafe_view_71);  view_563 = _unsafe_view_71 = None
        view_564 = torch.ops.aten.view.default(bmm_59, [8, 512, 1, 1, 4096, 32]);  bmm_59 = None
        permute_359 = torch.ops.aten.permute.default(view_564, [3, 4, 1, 0, 5, 2]);  view_564 = None
        view_565 = torch.ops.aten.view.default(permute_359, [1, 4096, 512, 8, 32]);  permute_359 = None
        mul_52 = torch.ops.aten.mul.Tensor(sigmoid_31, view_565);  sigmoid_31 = view_565 = None
        view_566 = torch.ops.aten.view.default(mul_52, [1, 4096, 512, 256]);  mul_52 = None
        _to_copy_348 = torch.ops.aten._to_copy.default(arg75_1, dtype = torch.bfloat16);  arg75_1 = None
        t_115 = torch.ops.aten.t.default(_to_copy_348);  _to_copy_348 = None
        view_567 = torch.ops.aten.view.default(view_566, [2097152, 256]);  view_566 = None
        mm_108 = torch.ops.aten.mm.default(view_567, t_115);  view_567 = t_115 = None
        view_568 = torch.ops.aten.view.default(mm_108, [1, 4096, 512, 64]);  mm_108 = None
        cat_9 = torch.ops.aten.cat.default([view_529, view_542, view_555, view_568], dim = 1);  view_529 = view_542 = view_555 = view_568 = None
        add_41 = torch.ops.aten.add.Tensor(add_40, cat_9);  add_40 = cat_9 = None
        _to_copy_349 = torch.ops.aten._to_copy.default(add_39, dtype = torch.float32)
        native_layer_norm_default_75 = torch.ops.aten.native_layer_norm.default(_to_copy_349, [256], arg114_1, arg115_1, 1e-05);  _to_copy_349 = arg114_1 = arg115_1 = None
        getitem_453 = native_layer_norm_default_75[0]
        split_with_sizes_default_8 = torch.ops.aten.split_with_sizes.default(arg117_1, [512, 512]);  arg117_1 = None
        getitem_456 = split_with_sizes_default_8[0]
        getitem_457 = split_with_sizes_default_8[1];  split_with_sizes_default_8 = None
        split_with_sizes_default_9 = torch.ops.aten.split_with_sizes.default(arg118_1, [512, 512, 256]);  arg118_1 = None
        getitem_458 = split_with_sizes_default_9[0]
        getitem_459 = split_with_sizes_default_9[1]
        getitem_460 = split_with_sizes_default_9[2];  split_with_sizes_default_9 = None
        _to_copy_350 = torch.ops.aten._to_copy.default(getitem_456, dtype = torch.bfloat16);  getitem_456 = None
        _to_copy_351 = torch.ops.aten._to_copy.default(getitem_453, dtype = torch.bfloat16)
        t_116 = torch.ops.aten.t.default(_to_copy_350);  _to_copy_350 = None
        view_569 = torch.ops.aten.view.default(_to_copy_351, [262144, 256]);  _to_copy_351 = None
        mm_109 = torch.ops.aten.mm.default(view_569, t_116);  view_569 = t_116 = None
        view_570 = torch.ops.aten.view.default(mm_109, [1, 512, 512, 512]);  mm_109 = None
        _to_copy_352 = torch.ops.aten._to_copy.default(getitem_458, dtype = torch.bfloat16);  getitem_458 = None
        _to_copy_353 = torch.ops.aten._to_copy.default(getitem_453, dtype = torch.bfloat16)
        t_117 = torch.ops.aten.t.default(_to_copy_352);  _to_copy_352 = None
        view_571 = torch.ops.aten.view.default(_to_copy_353, [262144, 256]);  _to_copy_353 = None
        mm_110 = torch.ops.aten.mm.default(view_571, t_117);  view_571 = t_117 = None
        view_572 = torch.ops.aten.view.default(mm_110, [1, 512, 512, 512]);  mm_110 = None
        sigmoid_32 = torch.ops.aten.sigmoid.default(view_572);  view_572 = None
        mul_53 = torch.ops.aten.mul.Tensor(view_570, sigmoid_32);  view_570 = sigmoid_32 = None
        unsqueeze_252 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_48 = torch.ops.aten.bitwise_not.default(unsqueeze_252);  unsqueeze_252 = None
        masked_fill_48 = torch.ops.aten.masked_fill.Scalar(mul_53, bitwise_not_48, 0);  mul_53 = bitwise_not_48 = None
        split_tensor_39 = torch.ops.aten.split.Tensor(masked_fill_48, 256, dim = -1)
        getitem_463 = split_tensor_39[0]
        unsqueeze_255 = torch.ops.aten.unsqueeze.default(getitem_463, 4);  getitem_463 = None
        permute_364 = torch.ops.aten.permute.default(unsqueeze_255, [0, 1, 4, 3, 2]);  unsqueeze_255 = None
        permute_365 = torch.ops.aten.permute.default(permute_364, [3, 1, 4, 0, 2]);  permute_364 = None
        view_575 = torch.ops.aten.view.default(permute_365, [256, 512, 512]);  permute_365 = None
        split_tensor_40 = torch.ops.aten.split.Tensor(masked_fill_48, 256, dim = -1);  masked_fill_48 = None
        getitem_466 = split_tensor_40[1];  split_tensor_40 = None
        unsqueeze_256 = torch.ops.aten.unsqueeze.default(getitem_466, 4);  getitem_466 = None
        permute_366 = torch.ops.aten.permute.default(unsqueeze_256, [0, 4, 1, 3, 2]);  unsqueeze_256 = None
        permute_367 = torch.ops.aten.permute.default(permute_366, [3, 4, 0, 2, 1]);  permute_366 = None
        view_576 = torch.ops.aten.view.default(permute_367, [256, 512, 512]);  permute_367 = None
        bmm_60 = torch.ops.aten.bmm.default(view_575, view_576);  view_575 = view_576 = None
        view_577 = torch.ops.aten.view.default(bmm_60, [256, 512, 1, 1, 512]);  bmm_60 = None
        permute_368 = torch.ops.aten.permute.default(view_577, [3, 1, 4, 0, 2]);  view_577 = None
        view_578 = torch.ops.aten.view.default(permute_368, [1, 512, 512, 256]);  permute_368 = None
        _to_copy_354 = torch.ops.aten._to_copy.default(getitem_457, dtype = torch.bfloat16);  getitem_457 = None
        _to_copy_355 = torch.ops.aten._to_copy.default(getitem_453, dtype = torch.bfloat16)
        t_118 = torch.ops.aten.t.default(_to_copy_354);  _to_copy_354 = None
        view_579 = torch.ops.aten.view.default(_to_copy_355, [262144, 256]);  _to_copy_355 = None
        mm_111 = torch.ops.aten.mm.default(view_579, t_118);  view_579 = t_118 = None
        view_580 = torch.ops.aten.view.default(mm_111, [1, 512, 512, 512]);  mm_111 = None
        _to_copy_356 = torch.ops.aten._to_copy.default(getitem_459, dtype = torch.bfloat16);  getitem_459 = None
        _to_copy_357 = torch.ops.aten._to_copy.default(getitem_453, dtype = torch.bfloat16)
        t_119 = torch.ops.aten.t.default(_to_copy_356);  _to_copy_356 = None
        view_581 = torch.ops.aten.view.default(_to_copy_357, [262144, 256]);  _to_copy_357 = None
        mm_112 = torch.ops.aten.mm.default(view_581, t_119);  view_581 = t_119 = None
        view_582 = torch.ops.aten.view.default(mm_112, [1, 512, 512, 512]);  mm_112 = None
        sigmoid_33 = torch.ops.aten.sigmoid.default(view_582);  view_582 = None
        mul_54 = torch.ops.aten.mul.Tensor(view_580, sigmoid_33);  view_580 = sigmoid_33 = None
        view_583 = torch.ops.aten.view.default(mul_54, [262144, 512]);  mul_54 = None
        view_584 = torch.ops.aten.view.default(view_583, [1, 512, 512, 512]);  view_583 = None
        transpose_8 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_257 = torch.ops.aten.unsqueeze.default(transpose_8, 3);  transpose_8 = None
        clone_77 = torch.ops.aten.clone.default(unsqueeze_257, memory_format = torch.contiguous_format);  unsqueeze_257 = None
        bitwise_not_49 = torch.ops.aten.bitwise_not.default(clone_77);  clone_77 = None
        masked_fill_49 = torch.ops.aten.masked_fill.Scalar(view_584, bitwise_not_49, 0);  view_584 = bitwise_not_49 = None
        view_585 = torch.ops.aten.view.default(masked_fill_49, [262144, 512]);  masked_fill_49 = None
        view_589 = torch.ops.aten.view.default(view_585, [1, 512, 512, 512])
        split_tensor_41 = torch.ops.aten.split.Tensor(view_589, 256, dim = -1);  view_589 = None
        getitem_469 = split_tensor_41[0]
        unsqueeze_260 = torch.ops.aten.unsqueeze.default(getitem_469, 4);  getitem_469 = None
        permute_373 = torch.ops.aten.permute.default(unsqueeze_260, [0, 2, 4, 3, 1]);  unsqueeze_260 = None
        permute_374 = torch.ops.aten.permute.default(permute_373, [3, 1, 4, 0, 2]);  permute_373 = None
        view_590 = torch.ops.aten.view.default(permute_374, [256, 512, 512]);  permute_374 = None
        view_591 = torch.ops.aten.view.default(view_585, [1, 512, 512, 512]);  view_585 = None
        split_tensor_42 = torch.ops.aten.split.Tensor(view_591, 256, dim = -1);  view_591 = None
        getitem_472 = split_tensor_42[1];  split_tensor_42 = None
        unsqueeze_261 = torch.ops.aten.unsqueeze.default(getitem_472, 4);  getitem_472 = None
        permute_375 = torch.ops.aten.permute.default(unsqueeze_261, [0, 4, 2, 3, 1]);  unsqueeze_261 = None
        permute_376 = torch.ops.aten.permute.default(permute_375, [3, 4, 0, 2, 1]);  permute_375 = None
        view_592 = torch.ops.aten.view.default(permute_376, [256, 512, 512]);  permute_376 = None
        bmm_61 = torch.ops.aten.bmm.default(view_590, view_592);  view_590 = view_592 = None
        view_593 = torch.ops.aten.view.default(bmm_61, [256, 512, 1, 1, 512]);  bmm_61 = None
        permute_377 = torch.ops.aten.permute.default(view_593, [3, 1, 4, 0, 2]);  view_593 = None
        view_594 = torch.ops.aten.view.default(permute_377, [1, 512, 512, 256]);  permute_377 = None
        _to_copy_358 = torch.ops.aten._to_copy.default(view_578, dtype = torch.float32);  view_578 = None
        native_layer_norm_default_76 = torch.ops.aten.native_layer_norm.default(_to_copy_358, [256], None, None, 1e-05);  _to_copy_358 = None
        getitem_473 = native_layer_norm_default_76[0]
        _to_copy_359 = torch.ops.aten._to_copy.default(view_594, dtype = torch.float32);  view_594 = None
        native_layer_norm_default_77 = torch.ops.aten.native_layer_norm.default(_to_copy_359, [256], None, None, 1e-05);  _to_copy_359 = None
        getitem_476 = native_layer_norm_default_77[0]
        add_42 = torch.ops.aten.add.Tensor(getitem_473, getitem_476);  getitem_473 = getitem_476 = None
        _to_copy_360 = torch.ops.aten._to_copy.default(arg116_1, dtype = torch.bfloat16);  arg116_1 = None
        _to_copy_361 = torch.ops.aten._to_copy.default(add_42, dtype = torch.bfloat16);  add_42 = None
        t_120 = torch.ops.aten.t.default(_to_copy_360);  _to_copy_360 = None
        view_595 = torch.ops.aten.view.default(_to_copy_361, [262144, 256]);  _to_copy_361 = None
        mm_113 = torch.ops.aten.mm.default(view_595, t_120);  view_595 = t_120 = None
        view_596 = torch.ops.aten.view.default(mm_113, [1, 512, 512, 256]);  mm_113 = None
        _to_copy_362 = torch.ops.aten._to_copy.default(getitem_460, dtype = torch.bfloat16);  getitem_460 = None
        _to_copy_363 = torch.ops.aten._to_copy.default(getitem_453, dtype = torch.bfloat16);  getitem_453 = None
        t_121 = torch.ops.aten.t.default(_to_copy_362);  _to_copy_362 = None
        view_597 = torch.ops.aten.view.default(_to_copy_363, [262144, 256]);  _to_copy_363 = None
        mm_114 = torch.ops.aten.mm.default(view_597, t_121);  view_597 = t_121 = None
        view_598 = torch.ops.aten.view.default(mm_114, [1, 512, 512, 256]);  mm_114 = None
        sigmoid_34 = torch.ops.aten.sigmoid.default(view_598);  view_598 = None
        mul_55 = torch.ops.aten.mul.Tensor(view_596, sigmoid_34);  view_596 = sigmoid_34 = None
        add_43 = torch.ops.aten.add.Tensor(add_39, mul_55);  mul_55 = None
        split_tensor_43 = torch.ops.aten.split.Tensor(add_39, 512, dim = -2);  add_39 = None
        getitem_479 = split_tensor_43[0];  split_tensor_43 = None
        _to_copy_364 = torch.ops.aten._to_copy.default(getitem_479, dtype = torch.float32);  getitem_479 = None
        native_layer_norm_default_78 = torch.ops.aten.native_layer_norm.default(_to_copy_364, [256], arg96_1, arg97_1, 1e-05);  _to_copy_364 = arg96_1 = arg97_1 = None
        getitem_480 = native_layer_norm_default_78[0]
        _to_copy_365 = torch.ops.aten._to_copy.default(arg98_1, dtype = torch.bfloat16);  arg98_1 = None
        _to_copy_366 = torch.ops.aten._to_copy.default(getitem_480, dtype = torch.bfloat16);  getitem_480 = None
        t_122 = torch.ops.aten.t.default(_to_copy_365);  _to_copy_365 = None
        view_599 = torch.ops.aten.view.default(_to_copy_366, [262144, 256]);  _to_copy_366 = None
        mm_115 = torch.ops.aten.mm.default(view_599, t_122);  view_599 = t_122 = None
        view_600 = torch.ops.aten.view.default(mm_115, [1, 512, 512, 2048]);  mm_115 = None
        split_tensor_44 = torch.ops.aten.split.Tensor(view_600, 1024, dim = -1);  view_600 = None
        getitem_483 = split_tensor_44[0]
        getitem_484 = split_tensor_44[1];  split_tensor_44 = None
        silu_16 = torch.ops.aten.silu.default(getitem_483);  getitem_483 = None
        mul_56 = torch.ops.aten.mul.Tensor(silu_16, getitem_484);  silu_16 = getitem_484 = None
        _to_copy_367 = torch.ops.aten._to_copy.default(arg99_1, dtype = torch.bfloat16);  arg99_1 = None
        t_123 = torch.ops.aten.t.default(_to_copy_367);  _to_copy_367 = None
        view_602 = torch.ops.aten.view.default(mul_56, [262144, 1024]);  mul_56 = None
        mm_116 = torch.ops.aten.mm.default(view_602, t_123);  view_602 = t_123 = None
        view_603 = torch.ops.aten.view.default(mm_116, [1, 512, 512, 256]);  mm_116 = None
        add_44 = torch.ops.aten.add.Tensor(add_43, view_603);  add_43 = view_603 = None
        _to_copy_368 = torch.ops.aten._to_copy.default(add_44, dtype = torch.float32)
        native_layer_norm_default_79 = torch.ops.aten.native_layer_norm.default(_to_copy_368, [256], None, None, 1e-05);  _to_copy_368 = None
        getitem_485 = native_layer_norm_default_79[0]
        _to_copy_369 = torch.ops.aten._to_copy.default(arg135_1, dtype = torch.bfloat16);  arg135_1 = None
        _to_copy_370 = torch.ops.aten._to_copy.default(getitem_485, dtype = torch.bfloat16)
        t_124 = torch.ops.aten.t.default(_to_copy_369);  _to_copy_369 = None
        view_604 = torch.ops.aten.view.default(_to_copy_370, [262144, 256]);  _to_copy_370 = None
        mm_117 = torch.ops.aten.mm.default(view_604, t_124);  view_604 = t_124 = None
        view_605 = torch.ops.aten.view.default(mm_117, [1, 512, 512, 8]);  mm_117 = None
        view_606 = torch.ops.aten.view.default(view_605, [1, 512, 512, 2, 4]);  view_605 = None
        permute_378 = torch.ops.aten.permute.default(view_606, [0, 3, 4, 1, 2]);  view_606 = None
        view_607 = torch.ops.aten.view.default(permute_378, [1, 2, 4, 1, 512, 512]);  permute_378 = None
        view_608 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_50 = torch.ops.aten.bitwise_not.default(view_608);  view_608 = None
        masked_fill_50 = torch.ops.aten.masked_fill.Scalar(view_607, bitwise_not_50, -10000);  view_607 = bitwise_not_50 = None
        view_609 = torch.ops.aten.view.default(masked_fill_50, [1, 2, 4, 512, 512]);  masked_fill_50 = None
        permute_379 = torch.ops.aten.permute.default(view_609, [1, 0, 2, 3, 4]);  view_609 = None
        view_610 = torch.ops.aten.view.default(permute_379, [2, 4, 1, 512, 512]);  permute_379 = None
        _to_copy_371 = torch.ops.aten._to_copy.default(arg136_1, dtype = torch.bfloat16);  arg136_1 = None
        _to_copy_372 = torch.ops.aten._to_copy.default(getitem_485, dtype = torch.bfloat16)
        t_125 = torch.ops.aten.t.default(_to_copy_371);  _to_copy_371 = None
        view_611 = torch.ops.aten.view.default(_to_copy_372, [262144, 256]);  _to_copy_372 = None
        mm_118 = torch.ops.aten.mm.default(view_611, t_125);  view_611 = t_125 = None
        view_612 = torch.ops.aten.view.default(mm_118, [1, 512, 512, 1024]);  mm_118 = None
        select_9 = torch.ops.aten.select.int(view_610, 0, 0)
        view_613 = torch.ops.aten.view.default(view_612, [1, 512, 512, 4, 4, 64]);  view_612 = None
        permute_380 = torch.ops.aten.permute.default(view_613, [4, 0, 3, 1, 2, 5]);  view_613 = None
        view_614 = torch.ops.aten.view.default(permute_380, [4, 4, 512, 512, 64]);  permute_380 = None
        unbind_int_32 = torch.ops.aten.unbind.int(view_614);  view_614 = None
        getitem_488 = unbind_int_32[0]
        getitem_489 = unbind_int_32[1]
        getitem_490 = unbind_int_32[2]
        getitem_491 = unbind_int_32[3];  unbind_int_32 = None
        expand_19 = torch.ops.aten.expand.default(select_9, [4, 512, 512, 512]);  select_9 = None
        _scaled_dot_product_efficient_attention_default_8 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_488, getitem_489, getitem_490, expand_19, False);  getitem_488 = getitem_489 = getitem_490 = expand_19 = None
        getitem_492 = _scaled_dot_product_efficient_attention_default_8[0]
        sigmoid_35 = torch.ops.aten.sigmoid.default(getitem_491);  getitem_491 = None
        mul_57 = torch.ops.aten.mul.Tensor(getitem_492, sigmoid_35);  getitem_492 = sigmoid_35 = None
        view_615 = torch.ops.aten.view.default(mul_57, [1, 4, 512, 512, 64]);  mul_57 = None
        permute_381 = torch.ops.aten.permute.default(view_615, [0, 2, 3, 1, 4]);  view_615 = None
        clone_78 = torch.ops.aten.clone.default(permute_381, memory_format = torch.contiguous_format);  permute_381 = None
        _unsafe_view_72 = torch.ops.aten._unsafe_view.default(clone_78, [1, 512, 512, 256]);  clone_78 = None
        transpose_9 = torch.ops.aten.transpose.int(getitem_485, 1, 2);  getitem_485 = None
        _to_copy_373 = torch.ops.aten._to_copy.default(arg137_1, dtype = torch.bfloat16);  arg137_1 = None
        _to_copy_374 = torch.ops.aten._to_copy.default(transpose_9, dtype = torch.bfloat16);  transpose_9 = None
        t_126 = torch.ops.aten.t.default(_to_copy_373);  _to_copy_373 = None
        expand_20 = torch.ops.aten.expand.default(_to_copy_374, [1, 512, 512, 256]);  _to_copy_374 = None
        view_616 = torch.ops.aten.view.default(expand_20, [512, 512, 256]);  expand_20 = None
        expand_21 = torch.ops.aten.expand.default(t_126, [1, 512, 256, 1024]);  t_126 = None
        view_617 = torch.ops.aten.view.default(expand_21, [512, 256, 1024]);  expand_21 = None
        bmm_62 = torch.ops.aten.bmm.default(view_616, view_617);  view_616 = view_617 = None
        view_618 = torch.ops.aten.view.default(bmm_62, [1, 512, 512, 1024]);  bmm_62 = None
        select_10 = torch.ops.aten.select.int(view_610, 0, 1);  view_610 = None
        view_619 = torch.ops.aten.view.default(view_618, [1, 512, 512, 4, 4, 64]);  view_618 = None
        permute_382 = torch.ops.aten.permute.default(view_619, [4, 0, 3, 1, 2, 5]);  view_619 = None
        view_620 = torch.ops.aten.view.default(permute_382, [4, 4, 512, 512, 64]);  permute_382 = None
        unbind_int_33 = torch.ops.aten.unbind.int(view_620);  view_620 = None
        getitem_496 = unbind_int_33[0]
        getitem_497 = unbind_int_33[1]
        getitem_498 = unbind_int_33[2]
        getitem_499 = unbind_int_33[3];  unbind_int_33 = None
        expand_22 = torch.ops.aten.expand.default(select_10, [4, 512, 512, 512]);  select_10 = None
        _scaled_dot_product_efficient_attention_default_9 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_496, getitem_497, getitem_498, expand_22, False);  getitem_496 = getitem_497 = getitem_498 = expand_22 = None
        getitem_500 = _scaled_dot_product_efficient_attention_default_9[0]
        sigmoid_36 = torch.ops.aten.sigmoid.default(getitem_499);  getitem_499 = None
        mul_58 = torch.ops.aten.mul.Tensor(getitem_500, sigmoid_36);  getitem_500 = sigmoid_36 = None
        view_621 = torch.ops.aten.view.default(mul_58, [1, 4, 512, 512, 64]);  mul_58 = None
        permute_383 = torch.ops.aten.permute.default(view_621, [0, 2, 3, 1, 4]);  view_621 = None
        clone_79 = torch.ops.aten.clone.default(permute_383, memory_format = torch.contiguous_format);  permute_383 = None
        _unsafe_view_73 = torch.ops.aten._unsafe_view.default(clone_79, [1, 512, 512, 256]);  clone_79 = None
        cat_10 = torch.ops.aten.cat.default([_unsafe_view_72, _unsafe_view_73], dim = -1);  _unsafe_view_72 = _unsafe_view_73 = None
        slice_175 = torch.ops.aten.slice.Tensor(arg134_1, dim = 0, start = 0, end = 9223372036854775807);  arg134_1 = None
        unsqueeze_262 = torch.ops.aten.unsqueeze.default(slice_175, 1);  slice_175 = None
        mul_59 = torch.ops.aten.mul.Tensor(arg138_1, unsqueeze_262);  arg138_1 = unsqueeze_262 = None
        _to_copy_375 = torch.ops.aten._to_copy.default(mul_59, dtype = torch.bfloat16);  mul_59 = None
        t_127 = torch.ops.aten.t.default(_to_copy_375);  _to_copy_375 = None
        view_622 = torch.ops.aten.view.default(cat_10, [262144, 512]);  cat_10 = None
        mm_119 = torch.ops.aten.mm.default(view_622, t_127);  view_622 = t_127 = None
        view_623 = torch.ops.aten.view.default(mm_119, [1, 512, 512, 256]);  mm_119 = None
        add_45 = torch.ops.aten.add.Tensor(add_44, view_623);  add_44 = view_623 = None
        slice_176 = torch.ops.aten.slice.Tensor(add_41, dim = 0, start = 0, end = 9223372036854775807)
        slice_177 = torch.ops.aten.slice.Tensor(slice_176, dim = 1, start = 0, end = 4096);  slice_176 = None
        slice_178 = torch.ops.aten.slice.Tensor(slice_177, dim = 2, start = 0, end = 9223372036854775807);  slice_177 = None
        slice_179 = torch.ops.aten.slice.Tensor(slice_178, dim = 3, start = 0, end = 9223372036854775807);  slice_178 = None
        slice_180 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
        slice_181 = torch.ops.aten.slice.Tensor(slice_180, dim = 1, start = 0, end = 4096);  slice_180 = None
        slice_182 = torch.ops.aten.slice.Tensor(slice_181, dim = 2, start = 0, end = 9223372036854775807);  slice_181 = None
        _to_copy_376 = torch.ops.aten._to_copy.default(slice_179, dtype = torch.float32);  slice_179 = None
        native_layer_norm_default_80 = torch.ops.aten.native_layer_norm.default(_to_copy_376, [64], None, None, 1e-05);  _to_copy_376 = None
        getitem_504 = native_layer_norm_default_80[0]
        view_624 = torch.ops.aten.view.default(slice_182, [1, 4096, 512, 1]);  slice_182 = None
        bitwise_not_51 = torch.ops.aten.bitwise_not.default(view_624);  view_624 = None
        masked_fill_51 = torch.ops.aten.masked_fill.Scalar(getitem_504, bitwise_not_51, 0);  getitem_504 = bitwise_not_51 = None
        unbind_int_34 = torch.ops.aten.unbind.int(arg50_1)
        getitem_507 = unbind_int_34[0]
        getitem_508 = unbind_int_34[1];  unbind_int_34 = None
        _to_copy_377 = torch.ops.aten._to_copy.default(getitem_507, dtype = torch.bfloat16);  getitem_507 = None
        _to_copy_378 = torch.ops.aten._to_copy.default(masked_fill_51, dtype = torch.bfloat16)
        unsqueeze_263 = torch.ops.aten.unsqueeze.default(_to_copy_377, 3);  _to_copy_377 = None
        unsqueeze_264 = torch.ops.aten.unsqueeze.default(unsqueeze_263, 4);  unsqueeze_263 = None
        unsqueeze_265 = torch.ops.aten.unsqueeze.default(unsqueeze_264, 5);  unsqueeze_264 = None
        permute_384 = torch.ops.aten.permute.default(unsqueeze_265, [0, 1, 3, 4, 5, 2]);  unsqueeze_265 = None
        unsqueeze_266 = torch.ops.aten.unsqueeze.default(_to_copy_378, 4);  _to_copy_378 = None
        unsqueeze_267 = torch.ops.aten.unsqueeze.default(unsqueeze_266, 5);  unsqueeze_266 = None
        permute_385 = torch.ops.aten.permute.default(unsqueeze_267, [4, 5, 0, 1, 2, 3]);  unsqueeze_267 = None
        permute_386 = torch.ops.aten.permute.default(permute_384, [0, 1, 5, 2, 3, 4]);  permute_384 = None
        view_625 = torch.ops.aten.view.default(permute_386, [1, 64, 64]);  permute_386 = None
        permute_387 = torch.ops.aten.permute.default(permute_385, [5, 2, 3, 4, 0, 1]);  permute_385 = None
        view_626 = torch.ops.aten.view.default(permute_387, [1, 64, 2097152]);  permute_387 = None
        bmm_63 = torch.ops.aten.bmm.default(view_625, view_626);  view_625 = view_626 = None
        view_627 = torch.ops.aten.view.default(bmm_63, [8, 8, 1, 1, 4096, 512]);  bmm_63 = None
        permute_388 = torch.ops.aten.permute.default(view_627, [0, 1, 3, 4, 5, 2]);  view_627 = None
        view_628 = torch.ops.aten.view.default(permute_388, [8, 8, 1, 4096, 512]);  permute_388 = None
        _to_copy_379 = torch.ops.aten._to_copy.default(getitem_508, dtype = torch.bfloat16);  getitem_508 = None
        _to_copy_380 = torch.ops.aten._to_copy.default(masked_fill_51, dtype = torch.bfloat16);  masked_fill_51 = None
        unsqueeze_268 = torch.ops.aten.unsqueeze.default(_to_copy_379, 3);  _to_copy_379 = None
        unsqueeze_269 = torch.ops.aten.unsqueeze.default(unsqueeze_268, 4);  unsqueeze_268 = None
        unsqueeze_270 = torch.ops.aten.unsqueeze.default(unsqueeze_269, 5);  unsqueeze_269 = None
        permute_389 = torch.ops.aten.permute.default(unsqueeze_270, [0, 1, 3, 4, 5, 2]);  unsqueeze_270 = None
        unsqueeze_271 = torch.ops.aten.unsqueeze.default(_to_copy_380, 4);  _to_copy_380 = None
        unsqueeze_272 = torch.ops.aten.unsqueeze.default(unsqueeze_271, 5);  unsqueeze_271 = None
        permute_390 = torch.ops.aten.permute.default(unsqueeze_272, [4, 5, 0, 1, 2, 3]);  unsqueeze_272 = None
        permute_391 = torch.ops.aten.permute.default(permute_389, [0, 1, 5, 2, 3, 4]);  permute_389 = None
        view_629 = torch.ops.aten.view.default(permute_391, [1, 64, 64]);  permute_391 = None
        permute_392 = torch.ops.aten.permute.default(permute_390, [5, 2, 3, 4, 0, 1]);  permute_390 = None
        view_630 = torch.ops.aten.view.default(permute_392, [1, 64, 2097152]);  permute_392 = None
        bmm_64 = torch.ops.aten.bmm.default(view_629, view_630);  view_629 = view_630 = None
        view_631 = torch.ops.aten.view.default(bmm_64, [8, 8, 1, 1, 4096, 512]);  bmm_64 = None
        permute_393 = torch.ops.aten.permute.default(view_631, [0, 1, 3, 4, 5, 2]);  view_631 = None
        view_632 = torch.ops.aten.view.default(permute_393, [8, 8, 1, 4096, 512]);  permute_393 = None
        unsqueeze_273 = torch.ops.aten.unsqueeze.default(view_628, 5);  view_628 = None
        unsqueeze_274 = torch.ops.aten.unsqueeze.default(unsqueeze_273, 6);  unsqueeze_273 = None
        permute_394 = torch.ops.aten.permute.default(unsqueeze_274, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_274 = None
        unsqueeze_275 = torch.ops.aten.unsqueeze.default(view_632, 5);  view_632 = None
        unsqueeze_276 = torch.ops.aten.unsqueeze.default(unsqueeze_275, 6);  unsqueeze_275 = None
        permute_395 = torch.ops.aten.permute.default(unsqueeze_276, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_276 = None
        permute_396 = torch.ops.aten.permute.default(permute_394, [3, 1, 4, 6, 0, 2, 5]);  permute_394 = None
        clone_80 = torch.ops.aten.clone.default(permute_396, memory_format = torch.contiguous_format);  permute_396 = None
        _unsafe_view_74 = torch.ops.aten._unsafe_view.default(clone_80, [8, 4096, 4096]);  clone_80 = None
        permute_397 = torch.ops.aten.permute.default(permute_395, [3, 6, 0, 2, 5, 1, 4]);  permute_395 = None
        clone_81 = torch.ops.aten.clone.default(permute_397, memory_format = torch.contiguous_format);  permute_397 = None
        _unsafe_view_75 = torch.ops.aten._unsafe_view.default(clone_81, [8, 4096, 4096]);  clone_81 = None
        bmm_65 = torch.ops.aten.bmm.default(_unsafe_view_74, _unsafe_view_75);  _unsafe_view_74 = _unsafe_view_75 = None
        view_633 = torch.ops.aten.view.default(bmm_65, [8, 512, 8, 1, 1, 512, 8]);  bmm_65 = None
        permute_398 = torch.ops.aten.permute.default(view_633, [4, 1, 5, 0, 2, 6, 3]);  view_633 = None
        view_634 = torch.ops.aten.view.default(permute_398, [1, 512, 512, 8, 8, 8]);  permute_398 = None
        clone_82 = torch.ops.aten.clone.default(view_634, memory_format = torch.contiguous_format);  view_634 = None
        _unsafe_view_76 = torch.ops.aten._unsafe_view.default(clone_82, [1, 512, 512, 512]);  clone_82 = None
        add_46 = torch.ops.aten.add.Tensor(_unsafe_view_76, 0);  _unsafe_view_76 = None
        slice_183 = torch.ops.aten.slice.Tensor(add_41, dim = 0, start = 0, end = 9223372036854775807)
        slice_184 = torch.ops.aten.slice.Tensor(slice_183, dim = 1, start = 4096, end = 8192);  slice_183 = None
        slice_185 = torch.ops.aten.slice.Tensor(slice_184, dim = 2, start = 0, end = 9223372036854775807);  slice_184 = None
        slice_186 = torch.ops.aten.slice.Tensor(slice_185, dim = 3, start = 0, end = 9223372036854775807);  slice_185 = None
        slice_187 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
        slice_188 = torch.ops.aten.slice.Tensor(slice_187, dim = 1, start = 4096, end = 8192);  slice_187 = None
        slice_189 = torch.ops.aten.slice.Tensor(slice_188, dim = 2, start = 0, end = 9223372036854775807);  slice_188 = None
        _to_copy_381 = torch.ops.aten._to_copy.default(slice_186, dtype = torch.float32);  slice_186 = None
        native_layer_norm_default_81 = torch.ops.aten.native_layer_norm.default(_to_copy_381, [64], None, None, 1e-05);  _to_copy_381 = None
        getitem_509 = native_layer_norm_default_81[0]
        view_635 = torch.ops.aten.view.default(slice_189, [1, 4096, 512, 1]);  slice_189 = None
        bitwise_not_52 = torch.ops.aten.bitwise_not.default(view_635);  view_635 = None
        masked_fill_52 = torch.ops.aten.masked_fill.Scalar(getitem_509, bitwise_not_52, 0);  getitem_509 = bitwise_not_52 = None
        unbind_int_35 = torch.ops.aten.unbind.int(arg50_1)
        getitem_512 = unbind_int_35[0]
        getitem_513 = unbind_int_35[1];  unbind_int_35 = None
        _to_copy_382 = torch.ops.aten._to_copy.default(getitem_512, dtype = torch.bfloat16);  getitem_512 = None
        _to_copy_383 = torch.ops.aten._to_copy.default(masked_fill_52, dtype = torch.bfloat16)
        unsqueeze_277 = torch.ops.aten.unsqueeze.default(_to_copy_382, 3);  _to_copy_382 = None
        unsqueeze_278 = torch.ops.aten.unsqueeze.default(unsqueeze_277, 4);  unsqueeze_277 = None
        unsqueeze_279 = torch.ops.aten.unsqueeze.default(unsqueeze_278, 5);  unsqueeze_278 = None
        permute_399 = torch.ops.aten.permute.default(unsqueeze_279, [0, 1, 3, 4, 5, 2]);  unsqueeze_279 = None
        unsqueeze_280 = torch.ops.aten.unsqueeze.default(_to_copy_383, 4);  _to_copy_383 = None
        unsqueeze_281 = torch.ops.aten.unsqueeze.default(unsqueeze_280, 5);  unsqueeze_280 = None
        permute_400 = torch.ops.aten.permute.default(unsqueeze_281, [4, 5, 0, 1, 2, 3]);  unsqueeze_281 = None
        permute_401 = torch.ops.aten.permute.default(permute_399, [0, 1, 5, 2, 3, 4]);  permute_399 = None
        view_636 = torch.ops.aten.view.default(permute_401, [1, 64, 64]);  permute_401 = None
        permute_402 = torch.ops.aten.permute.default(permute_400, [5, 2, 3, 4, 0, 1]);  permute_400 = None
        view_637 = torch.ops.aten.view.default(permute_402, [1, 64, 2097152]);  permute_402 = None
        bmm_66 = torch.ops.aten.bmm.default(view_636, view_637);  view_636 = view_637 = None
        view_638 = torch.ops.aten.view.default(bmm_66, [8, 8, 1, 1, 4096, 512]);  bmm_66 = None
        permute_403 = torch.ops.aten.permute.default(view_638, [0, 1, 3, 4, 5, 2]);  view_638 = None
        view_639 = torch.ops.aten.view.default(permute_403, [8, 8, 1, 4096, 512]);  permute_403 = None
        _to_copy_384 = torch.ops.aten._to_copy.default(getitem_513, dtype = torch.bfloat16);  getitem_513 = None
        _to_copy_385 = torch.ops.aten._to_copy.default(masked_fill_52, dtype = torch.bfloat16);  masked_fill_52 = None
        unsqueeze_282 = torch.ops.aten.unsqueeze.default(_to_copy_384, 3);  _to_copy_384 = None
        unsqueeze_283 = torch.ops.aten.unsqueeze.default(unsqueeze_282, 4);  unsqueeze_282 = None
        unsqueeze_284 = torch.ops.aten.unsqueeze.default(unsqueeze_283, 5);  unsqueeze_283 = None
        permute_404 = torch.ops.aten.permute.default(unsqueeze_284, [0, 1, 3, 4, 5, 2]);  unsqueeze_284 = None
        unsqueeze_285 = torch.ops.aten.unsqueeze.default(_to_copy_385, 4);  _to_copy_385 = None
        unsqueeze_286 = torch.ops.aten.unsqueeze.default(unsqueeze_285, 5);  unsqueeze_285 = None
        permute_405 = torch.ops.aten.permute.default(unsqueeze_286, [4, 5, 0, 1, 2, 3]);  unsqueeze_286 = None
        permute_406 = torch.ops.aten.permute.default(permute_404, [0, 1, 5, 2, 3, 4]);  permute_404 = None
        view_640 = torch.ops.aten.view.default(permute_406, [1, 64, 64]);  permute_406 = None
        permute_407 = torch.ops.aten.permute.default(permute_405, [5, 2, 3, 4, 0, 1]);  permute_405 = None
        view_641 = torch.ops.aten.view.default(permute_407, [1, 64, 2097152]);  permute_407 = None
        bmm_67 = torch.ops.aten.bmm.default(view_640, view_641);  view_640 = view_641 = None
        view_642 = torch.ops.aten.view.default(bmm_67, [8, 8, 1, 1, 4096, 512]);  bmm_67 = None
        permute_408 = torch.ops.aten.permute.default(view_642, [0, 1, 3, 4, 5, 2]);  view_642 = None
        view_643 = torch.ops.aten.view.default(permute_408, [8, 8, 1, 4096, 512]);  permute_408 = None
        unsqueeze_287 = torch.ops.aten.unsqueeze.default(view_639, 5);  view_639 = None
        unsqueeze_288 = torch.ops.aten.unsqueeze.default(unsqueeze_287, 6);  unsqueeze_287 = None
        permute_409 = torch.ops.aten.permute.default(unsqueeze_288, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_288 = None
        unsqueeze_289 = torch.ops.aten.unsqueeze.default(view_643, 5);  view_643 = None
        unsqueeze_290 = torch.ops.aten.unsqueeze.default(unsqueeze_289, 6);  unsqueeze_289 = None
        permute_410 = torch.ops.aten.permute.default(unsqueeze_290, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_290 = None
        permute_411 = torch.ops.aten.permute.default(permute_409, [3, 1, 4, 6, 0, 2, 5]);  permute_409 = None
        clone_83 = torch.ops.aten.clone.default(permute_411, memory_format = torch.contiguous_format);  permute_411 = None
        _unsafe_view_77 = torch.ops.aten._unsafe_view.default(clone_83, [8, 4096, 4096]);  clone_83 = None
        permute_412 = torch.ops.aten.permute.default(permute_410, [3, 6, 0, 2, 5, 1, 4]);  permute_410 = None
        clone_84 = torch.ops.aten.clone.default(permute_412, memory_format = torch.contiguous_format);  permute_412 = None
        _unsafe_view_78 = torch.ops.aten._unsafe_view.default(clone_84, [8, 4096, 4096]);  clone_84 = None
        bmm_68 = torch.ops.aten.bmm.default(_unsafe_view_77, _unsafe_view_78);  _unsafe_view_77 = _unsafe_view_78 = None
        view_644 = torch.ops.aten.view.default(bmm_68, [8, 512, 8, 1, 1, 512, 8]);  bmm_68 = None
        permute_413 = torch.ops.aten.permute.default(view_644, [4, 1, 5, 0, 2, 6, 3]);  view_644 = None
        view_645 = torch.ops.aten.view.default(permute_413, [1, 512, 512, 8, 8, 8]);  permute_413 = None
        clone_85 = torch.ops.aten.clone.default(view_645, memory_format = torch.contiguous_format);  view_645 = None
        _unsafe_view_79 = torch.ops.aten._unsafe_view.default(clone_85, [1, 512, 512, 512]);  clone_85 = None
        add_47 = torch.ops.aten.add.Tensor(add_46, _unsafe_view_79);  add_46 = _unsafe_view_79 = None
        slice_190 = torch.ops.aten.slice.Tensor(add_41, dim = 0, start = 0, end = 9223372036854775807)
        slice_191 = torch.ops.aten.slice.Tensor(slice_190, dim = 1, start = 8192, end = 12288);  slice_190 = None
        slice_192 = torch.ops.aten.slice.Tensor(slice_191, dim = 2, start = 0, end = 9223372036854775807);  slice_191 = None
        slice_193 = torch.ops.aten.slice.Tensor(slice_192, dim = 3, start = 0, end = 9223372036854775807);  slice_192 = None
        slice_194 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807)
        slice_195 = torch.ops.aten.slice.Tensor(slice_194, dim = 1, start = 8192, end = 12288);  slice_194 = None
        slice_196 = torch.ops.aten.slice.Tensor(slice_195, dim = 2, start = 0, end = 9223372036854775807);  slice_195 = None
        _to_copy_386 = torch.ops.aten._to_copy.default(slice_193, dtype = torch.float32);  slice_193 = None
        native_layer_norm_default_82 = torch.ops.aten.native_layer_norm.default(_to_copy_386, [64], None, None, 1e-05);  _to_copy_386 = None
        getitem_514 = native_layer_norm_default_82[0]
        view_646 = torch.ops.aten.view.default(slice_196, [1, 4096, 512, 1]);  slice_196 = None
        bitwise_not_53 = torch.ops.aten.bitwise_not.default(view_646);  view_646 = None
        masked_fill_53 = torch.ops.aten.masked_fill.Scalar(getitem_514, bitwise_not_53, 0);  getitem_514 = bitwise_not_53 = None
        unbind_int_36 = torch.ops.aten.unbind.int(arg50_1)
        getitem_517 = unbind_int_36[0]
        getitem_518 = unbind_int_36[1];  unbind_int_36 = None
        _to_copy_387 = torch.ops.aten._to_copy.default(getitem_517, dtype = torch.bfloat16);  getitem_517 = None
        _to_copy_388 = torch.ops.aten._to_copy.default(masked_fill_53, dtype = torch.bfloat16)
        unsqueeze_291 = torch.ops.aten.unsqueeze.default(_to_copy_387, 3);  _to_copy_387 = None
        unsqueeze_292 = torch.ops.aten.unsqueeze.default(unsqueeze_291, 4);  unsqueeze_291 = None
        unsqueeze_293 = torch.ops.aten.unsqueeze.default(unsqueeze_292, 5);  unsqueeze_292 = None
        permute_414 = torch.ops.aten.permute.default(unsqueeze_293, [0, 1, 3, 4, 5, 2]);  unsqueeze_293 = None
        unsqueeze_294 = torch.ops.aten.unsqueeze.default(_to_copy_388, 4);  _to_copy_388 = None
        unsqueeze_295 = torch.ops.aten.unsqueeze.default(unsqueeze_294, 5);  unsqueeze_294 = None
        permute_415 = torch.ops.aten.permute.default(unsqueeze_295, [4, 5, 0, 1, 2, 3]);  unsqueeze_295 = None
        permute_416 = torch.ops.aten.permute.default(permute_414, [0, 1, 5, 2, 3, 4]);  permute_414 = None
        view_647 = torch.ops.aten.view.default(permute_416, [1, 64, 64]);  permute_416 = None
        permute_417 = torch.ops.aten.permute.default(permute_415, [5, 2, 3, 4, 0, 1]);  permute_415 = None
        view_648 = torch.ops.aten.view.default(permute_417, [1, 64, 2097152]);  permute_417 = None
        bmm_69 = torch.ops.aten.bmm.default(view_647, view_648);  view_647 = view_648 = None
        view_649 = torch.ops.aten.view.default(bmm_69, [8, 8, 1, 1, 4096, 512]);  bmm_69 = None
        permute_418 = torch.ops.aten.permute.default(view_649, [0, 1, 3, 4, 5, 2]);  view_649 = None
        view_650 = torch.ops.aten.view.default(permute_418, [8, 8, 1, 4096, 512]);  permute_418 = None
        _to_copy_389 = torch.ops.aten._to_copy.default(getitem_518, dtype = torch.bfloat16);  getitem_518 = None
        _to_copy_390 = torch.ops.aten._to_copy.default(masked_fill_53, dtype = torch.bfloat16);  masked_fill_53 = None
        unsqueeze_296 = torch.ops.aten.unsqueeze.default(_to_copy_389, 3);  _to_copy_389 = None
        unsqueeze_297 = torch.ops.aten.unsqueeze.default(unsqueeze_296, 4);  unsqueeze_296 = None
        unsqueeze_298 = torch.ops.aten.unsqueeze.default(unsqueeze_297, 5);  unsqueeze_297 = None
        permute_419 = torch.ops.aten.permute.default(unsqueeze_298, [0, 1, 3, 4, 5, 2]);  unsqueeze_298 = None
        unsqueeze_299 = torch.ops.aten.unsqueeze.default(_to_copy_390, 4);  _to_copy_390 = None
        unsqueeze_300 = torch.ops.aten.unsqueeze.default(unsqueeze_299, 5);  unsqueeze_299 = None
        permute_420 = torch.ops.aten.permute.default(unsqueeze_300, [4, 5, 0, 1, 2, 3]);  unsqueeze_300 = None
        permute_421 = torch.ops.aten.permute.default(permute_419, [0, 1, 5, 2, 3, 4]);  permute_419 = None
        view_651 = torch.ops.aten.view.default(permute_421, [1, 64, 64]);  permute_421 = None
        permute_422 = torch.ops.aten.permute.default(permute_420, [5, 2, 3, 4, 0, 1]);  permute_420 = None
        view_652 = torch.ops.aten.view.default(permute_422, [1, 64, 2097152]);  permute_422 = None
        bmm_70 = torch.ops.aten.bmm.default(view_651, view_652);  view_651 = view_652 = None
        view_653 = torch.ops.aten.view.default(bmm_70, [8, 8, 1, 1, 4096, 512]);  bmm_70 = None
        permute_423 = torch.ops.aten.permute.default(view_653, [0, 1, 3, 4, 5, 2]);  view_653 = None
        view_654 = torch.ops.aten.view.default(permute_423, [8, 8, 1, 4096, 512]);  permute_423 = None
        unsqueeze_301 = torch.ops.aten.unsqueeze.default(view_650, 5);  view_650 = None
        unsqueeze_302 = torch.ops.aten.unsqueeze.default(unsqueeze_301, 6);  unsqueeze_301 = None
        permute_424 = torch.ops.aten.permute.default(unsqueeze_302, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_302 = None
        unsqueeze_303 = torch.ops.aten.unsqueeze.default(view_654, 5);  view_654 = None
        unsqueeze_304 = torch.ops.aten.unsqueeze.default(unsqueeze_303, 6);  unsqueeze_303 = None
        permute_425 = torch.ops.aten.permute.default(unsqueeze_304, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_304 = None
        permute_426 = torch.ops.aten.permute.default(permute_424, [3, 1, 4, 6, 0, 2, 5]);  permute_424 = None
        clone_86 = torch.ops.aten.clone.default(permute_426, memory_format = torch.contiguous_format);  permute_426 = None
        _unsafe_view_80 = torch.ops.aten._unsafe_view.default(clone_86, [8, 4096, 4096]);  clone_86 = None
        permute_427 = torch.ops.aten.permute.default(permute_425, [3, 6, 0, 2, 5, 1, 4]);  permute_425 = None
        clone_87 = torch.ops.aten.clone.default(permute_427, memory_format = torch.contiguous_format);  permute_427 = None
        _unsafe_view_81 = torch.ops.aten._unsafe_view.default(clone_87, [8, 4096, 4096]);  clone_87 = None
        bmm_71 = torch.ops.aten.bmm.default(_unsafe_view_80, _unsafe_view_81);  _unsafe_view_80 = _unsafe_view_81 = None
        view_655 = torch.ops.aten.view.default(bmm_71, [8, 512, 8, 1, 1, 512, 8]);  bmm_71 = None
        permute_428 = torch.ops.aten.permute.default(view_655, [4, 1, 5, 0, 2, 6, 3]);  view_655 = None
        view_656 = torch.ops.aten.view.default(permute_428, [1, 512, 512, 8, 8, 8]);  permute_428 = None
        clone_88 = torch.ops.aten.clone.default(view_656, memory_format = torch.contiguous_format);  view_656 = None
        _unsafe_view_82 = torch.ops.aten._unsafe_view.default(clone_88, [1, 512, 512, 512]);  clone_88 = None
        add_48 = torch.ops.aten.add.Tensor(add_47, _unsafe_view_82);  add_47 = _unsafe_view_82 = None
        slice_197 = torch.ops.aten.slice.Tensor(add_41, dim = 0, start = 0, end = 9223372036854775807);  add_41 = None
        slice_198 = torch.ops.aten.slice.Tensor(slice_197, dim = 1, start = 12288, end = 16384);  slice_197 = None
        slice_199 = torch.ops.aten.slice.Tensor(slice_198, dim = 2, start = 0, end = 9223372036854775807);  slice_198 = None
        slice_200 = torch.ops.aten.slice.Tensor(slice_199, dim = 3, start = 0, end = 9223372036854775807);  slice_199 = None
        slice_201 = torch.ops.aten.slice.Tensor(arg1403_1, dim = 0, start = 0, end = 9223372036854775807);  arg1403_1 = None
        slice_202 = torch.ops.aten.slice.Tensor(slice_201, dim = 1, start = 12288, end = 16384);  slice_201 = None
        slice_203 = torch.ops.aten.slice.Tensor(slice_202, dim = 2, start = 0, end = 9223372036854775807);  slice_202 = None
        _to_copy_391 = torch.ops.aten._to_copy.default(slice_200, dtype = torch.float32);  slice_200 = None
        native_layer_norm_default_83 = torch.ops.aten.native_layer_norm.default(_to_copy_391, [64], None, None, 1e-05);  _to_copy_391 = None
        getitem_519 = native_layer_norm_default_83[0]
        view_657 = torch.ops.aten.view.default(slice_203, [1, 4096, 512, 1]);  slice_203 = None
        bitwise_not_54 = torch.ops.aten.bitwise_not.default(view_657);  view_657 = None
        masked_fill_54 = torch.ops.aten.masked_fill.Scalar(getitem_519, bitwise_not_54, 0);  getitem_519 = bitwise_not_54 = None
        unbind_int_37 = torch.ops.aten.unbind.int(arg50_1);  arg50_1 = None
        getitem_522 = unbind_int_37[0]
        getitem_523 = unbind_int_37[1];  unbind_int_37 = None
        _to_copy_392 = torch.ops.aten._to_copy.default(getitem_522, dtype = torch.bfloat16);  getitem_522 = None
        _to_copy_393 = torch.ops.aten._to_copy.default(masked_fill_54, dtype = torch.bfloat16)
        unsqueeze_305 = torch.ops.aten.unsqueeze.default(_to_copy_392, 3);  _to_copy_392 = None
        unsqueeze_306 = torch.ops.aten.unsqueeze.default(unsqueeze_305, 4);  unsqueeze_305 = None
        unsqueeze_307 = torch.ops.aten.unsqueeze.default(unsqueeze_306, 5);  unsqueeze_306 = None
        permute_429 = torch.ops.aten.permute.default(unsqueeze_307, [0, 1, 3, 4, 5, 2]);  unsqueeze_307 = None
        unsqueeze_308 = torch.ops.aten.unsqueeze.default(_to_copy_393, 4);  _to_copy_393 = None
        unsqueeze_309 = torch.ops.aten.unsqueeze.default(unsqueeze_308, 5);  unsqueeze_308 = None
        permute_430 = torch.ops.aten.permute.default(unsqueeze_309, [4, 5, 0, 1, 2, 3]);  unsqueeze_309 = None
        permute_431 = torch.ops.aten.permute.default(permute_429, [0, 1, 5, 2, 3, 4]);  permute_429 = None
        view_658 = torch.ops.aten.view.default(permute_431, [1, 64, 64]);  permute_431 = None
        permute_432 = torch.ops.aten.permute.default(permute_430, [5, 2, 3, 4, 0, 1]);  permute_430 = None
        view_659 = torch.ops.aten.view.default(permute_432, [1, 64, 2097152]);  permute_432 = None
        bmm_72 = torch.ops.aten.bmm.default(view_658, view_659);  view_658 = view_659 = None
        view_660 = torch.ops.aten.view.default(bmm_72, [8, 8, 1, 1, 4096, 512]);  bmm_72 = None
        permute_433 = torch.ops.aten.permute.default(view_660, [0, 1, 3, 4, 5, 2]);  view_660 = None
        view_661 = torch.ops.aten.view.default(permute_433, [8, 8, 1, 4096, 512]);  permute_433 = None
        _to_copy_394 = torch.ops.aten._to_copy.default(getitem_523, dtype = torch.bfloat16);  getitem_523 = None
        _to_copy_395 = torch.ops.aten._to_copy.default(masked_fill_54, dtype = torch.bfloat16);  masked_fill_54 = None
        unsqueeze_310 = torch.ops.aten.unsqueeze.default(_to_copy_394, 3);  _to_copy_394 = None
        unsqueeze_311 = torch.ops.aten.unsqueeze.default(unsqueeze_310, 4);  unsqueeze_310 = None
        unsqueeze_312 = torch.ops.aten.unsqueeze.default(unsqueeze_311, 5);  unsqueeze_311 = None
        permute_434 = torch.ops.aten.permute.default(unsqueeze_312, [0, 1, 3, 4, 5, 2]);  unsqueeze_312 = None
        unsqueeze_313 = torch.ops.aten.unsqueeze.default(_to_copy_395, 4);  _to_copy_395 = None
        unsqueeze_314 = torch.ops.aten.unsqueeze.default(unsqueeze_313, 5);  unsqueeze_313 = None
        permute_435 = torch.ops.aten.permute.default(unsqueeze_314, [4, 5, 0, 1, 2, 3]);  unsqueeze_314 = None
        permute_436 = torch.ops.aten.permute.default(permute_434, [0, 1, 5, 2, 3, 4]);  permute_434 = None
        view_662 = torch.ops.aten.view.default(permute_436, [1, 64, 64]);  permute_436 = None
        permute_437 = torch.ops.aten.permute.default(permute_435, [5, 2, 3, 4, 0, 1]);  permute_435 = None
        view_663 = torch.ops.aten.view.default(permute_437, [1, 64, 2097152]);  permute_437 = None
        bmm_73 = torch.ops.aten.bmm.default(view_662, view_663);  view_662 = view_663 = None
        view_664 = torch.ops.aten.view.default(bmm_73, [8, 8, 1, 1, 4096, 512]);  bmm_73 = None
        permute_438 = torch.ops.aten.permute.default(view_664, [0, 1, 3, 4, 5, 2]);  view_664 = None
        view_665 = torch.ops.aten.view.default(permute_438, [8, 8, 1, 4096, 512]);  permute_438 = None
        unsqueeze_315 = torch.ops.aten.unsqueeze.default(view_661, 5);  view_661 = None
        unsqueeze_316 = torch.ops.aten.unsqueeze.default(unsqueeze_315, 6);  unsqueeze_315 = None
        permute_439 = torch.ops.aten.permute.default(unsqueeze_316, [2, 4, 5, 0, 1, 6, 3]);  unsqueeze_316 = None
        unsqueeze_317 = torch.ops.aten.unsqueeze.default(view_665, 5);  view_665 = None
        unsqueeze_318 = torch.ops.aten.unsqueeze.default(unsqueeze_317, 6);  unsqueeze_317 = None
        permute_440 = torch.ops.aten.permute.default(unsqueeze_318, [2, 5, 4, 0, 6, 1, 3]);  unsqueeze_318 = None
        permute_441 = torch.ops.aten.permute.default(permute_439, [3, 1, 4, 6, 0, 2, 5]);  permute_439 = None
        clone_89 = torch.ops.aten.clone.default(permute_441, memory_format = torch.contiguous_format);  permute_441 = None
        _unsafe_view_83 = torch.ops.aten._unsafe_view.default(clone_89, [8, 4096, 4096]);  clone_89 = None
        permute_442 = torch.ops.aten.permute.default(permute_440, [3, 6, 0, 2, 5, 1, 4]);  permute_440 = None
        clone_90 = torch.ops.aten.clone.default(permute_442, memory_format = torch.contiguous_format);  permute_442 = None
        _unsafe_view_84 = torch.ops.aten._unsafe_view.default(clone_90, [8, 4096, 4096]);  clone_90 = None
        bmm_74 = torch.ops.aten.bmm.default(_unsafe_view_83, _unsafe_view_84);  _unsafe_view_83 = _unsafe_view_84 = None
        view_666 = torch.ops.aten.view.default(bmm_74, [8, 512, 8, 1, 1, 512, 8]);  bmm_74 = None
        permute_443 = torch.ops.aten.permute.default(view_666, [4, 1, 5, 0, 2, 6, 3]);  view_666 = None
        view_667 = torch.ops.aten.view.default(permute_443, [1, 512, 512, 8, 8, 8]);  permute_443 = None
        clone_91 = torch.ops.aten.clone.default(view_667, memory_format = torch.contiguous_format);  view_667 = None
        _unsafe_view_85 = torch.ops.aten._unsafe_view.default(clone_91, [1, 512, 512, 512]);  clone_91 = None
        add_49 = torch.ops.aten.add.Tensor(add_48, _unsafe_view_85);  add_48 = _unsafe_view_85 = None
        _to_copy_396 = torch.ops.aten._to_copy.default(add_49, dtype = torch.float32);  add_49 = None
        native_layer_norm_default_84 = torch.ops.aten.native_layer_norm.default(_to_copy_396, [512], arg51_1, arg52_1, 0.1);  _to_copy_396 = arg51_1 = arg52_1 = None
        getitem_524 = native_layer_norm_default_84[0]
        _to_copy_397 = torch.ops.aten._to_copy.default(arg54_1, dtype = torch.bfloat16);  arg54_1 = None
        _to_copy_398 = torch.ops.aten._to_copy.default(arg53_1, dtype = torch.bfloat16);  arg53_1 = None
        _to_copy_399 = torch.ops.aten._to_copy.default(getitem_524, dtype = torch.bfloat16);  getitem_524 = None
        view_668 = torch.ops.aten.view.default(_to_copy_399, [262144, 512]);  _to_copy_399 = None
        t_128 = torch.ops.aten.t.default(_to_copy_398);  _to_copy_398 = None
        addmm_3 = torch.ops.aten.addmm.default(_to_copy_397, view_668, t_128);  _to_copy_397 = view_668 = t_128 = None
        view_669 = torch.ops.aten.view.default(addmm_3, [1, 512, 512, 256]);  addmm_3 = None
        add_50 = torch.ops.aten.add.Tensor(add_45, view_669);  add_45 = view_669 = None
        _to_copy_400 = torch.ops.aten._to_copy.default(add_50, dtype = torch.float32)
        native_layer_norm_default_85 = torch.ops.aten.native_layer_norm.default(_to_copy_400, [256], arg119_1, arg120_1, 1e-05);  _to_copy_400 = arg119_1 = arg120_1 = None
        getitem_527 = native_layer_norm_default_85[0]
        split_with_sizes_default_10 = torch.ops.aten.split_with_sizes.default(arg122_1, [512, 512]);  arg122_1 = None
        getitem_530 = split_with_sizes_default_10[0]
        getitem_531 = split_with_sizes_default_10[1];  split_with_sizes_default_10 = None
        split_with_sizes_default_11 = torch.ops.aten.split_with_sizes.default(arg123_1, [512, 512, 256]);  arg123_1 = None
        getitem_532 = split_with_sizes_default_11[0]
        getitem_533 = split_with_sizes_default_11[1]
        getitem_534 = split_with_sizes_default_11[2];  split_with_sizes_default_11 = None
        _to_copy_401 = torch.ops.aten._to_copy.default(getitem_530, dtype = torch.bfloat16);  getitem_530 = None
        _to_copy_402 = torch.ops.aten._to_copy.default(getitem_527, dtype = torch.bfloat16)
        t_129 = torch.ops.aten.t.default(_to_copy_401);  _to_copy_401 = None
        view_670 = torch.ops.aten.view.default(_to_copy_402, [262144, 256]);  _to_copy_402 = None
        mm_120 = torch.ops.aten.mm.default(view_670, t_129);  view_670 = t_129 = None
        view_671 = torch.ops.aten.view.default(mm_120, [1, 512, 512, 512]);  mm_120 = None
        _to_copy_403 = torch.ops.aten._to_copy.default(getitem_532, dtype = torch.bfloat16);  getitem_532 = None
        _to_copy_404 = torch.ops.aten._to_copy.default(getitem_527, dtype = torch.bfloat16)
        t_130 = torch.ops.aten.t.default(_to_copy_403);  _to_copy_403 = None
        view_672 = torch.ops.aten.view.default(_to_copy_404, [262144, 256]);  _to_copy_404 = None
        mm_121 = torch.ops.aten.mm.default(view_672, t_130);  view_672 = t_130 = None
        view_673 = torch.ops.aten.view.default(mm_121, [1, 512, 512, 512]);  mm_121 = None
        sigmoid_37 = torch.ops.aten.sigmoid.default(view_673);  view_673 = None
        mul_60 = torch.ops.aten.mul.Tensor(view_671, sigmoid_37);  view_671 = sigmoid_37 = None
        unsqueeze_319 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_55 = torch.ops.aten.bitwise_not.default(unsqueeze_319);  unsqueeze_319 = None
        masked_fill_55 = torch.ops.aten.masked_fill.Scalar(mul_60, bitwise_not_55, 0);  mul_60 = bitwise_not_55 = None
        split_tensor_45 = torch.ops.aten.split.Tensor(masked_fill_55, 256, dim = -1)
        getitem_537 = split_tensor_45[0]
        unsqueeze_322 = torch.ops.aten.unsqueeze.default(getitem_537, 4);  getitem_537 = None
        permute_448 = torch.ops.aten.permute.default(unsqueeze_322, [0, 1, 4, 3, 2]);  unsqueeze_322 = None
        permute_449 = torch.ops.aten.permute.default(permute_448, [3, 1, 4, 0, 2]);  permute_448 = None
        view_676 = torch.ops.aten.view.default(permute_449, [256, 512, 512]);  permute_449 = None
        split_tensor_46 = torch.ops.aten.split.Tensor(masked_fill_55, 256, dim = -1);  masked_fill_55 = None
        getitem_540 = split_tensor_46[1];  split_tensor_46 = None
        unsqueeze_323 = torch.ops.aten.unsqueeze.default(getitem_540, 4);  getitem_540 = None
        permute_450 = torch.ops.aten.permute.default(unsqueeze_323, [0, 4, 1, 3, 2]);  unsqueeze_323 = None
        permute_451 = torch.ops.aten.permute.default(permute_450, [3, 4, 0, 2, 1]);  permute_450 = None
        view_677 = torch.ops.aten.view.default(permute_451, [256, 512, 512]);  permute_451 = None
        bmm_75 = torch.ops.aten.bmm.default(view_676, view_677);  view_676 = view_677 = None
        view_678 = torch.ops.aten.view.default(bmm_75, [256, 512, 1, 1, 512]);  bmm_75 = None
        permute_452 = torch.ops.aten.permute.default(view_678, [3, 1, 4, 0, 2]);  view_678 = None
        view_679 = torch.ops.aten.view.default(permute_452, [1, 512, 512, 256]);  permute_452 = None
        _to_copy_405 = torch.ops.aten._to_copy.default(getitem_531, dtype = torch.bfloat16);  getitem_531 = None
        _to_copy_406 = torch.ops.aten._to_copy.default(getitem_527, dtype = torch.bfloat16)
        t_131 = torch.ops.aten.t.default(_to_copy_405);  _to_copy_405 = None
        view_680 = torch.ops.aten.view.default(_to_copy_406, [262144, 256]);  _to_copy_406 = None
        mm_122 = torch.ops.aten.mm.default(view_680, t_131);  view_680 = t_131 = None
        view_681 = torch.ops.aten.view.default(mm_122, [1, 512, 512, 512]);  mm_122 = None
        _to_copy_407 = torch.ops.aten._to_copy.default(getitem_533, dtype = torch.bfloat16);  getitem_533 = None
        _to_copy_408 = torch.ops.aten._to_copy.default(getitem_527, dtype = torch.bfloat16)
        t_132 = torch.ops.aten.t.default(_to_copy_407);  _to_copy_407 = None
        view_682 = torch.ops.aten.view.default(_to_copy_408, [262144, 256]);  _to_copy_408 = None
        mm_123 = torch.ops.aten.mm.default(view_682, t_132);  view_682 = t_132 = None
        view_683 = torch.ops.aten.view.default(mm_123, [1, 512, 512, 512]);  mm_123 = None
        sigmoid_38 = torch.ops.aten.sigmoid.default(view_683);  view_683 = None
        mul_61 = torch.ops.aten.mul.Tensor(view_681, sigmoid_38);  view_681 = sigmoid_38 = None
        view_684 = torch.ops.aten.view.default(mul_61, [262144, 512]);  mul_61 = None
        view_685 = torch.ops.aten.view.default(view_684, [1, 512, 512, 512]);  view_684 = None
        transpose_10 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_324 = torch.ops.aten.unsqueeze.default(transpose_10, 3);  transpose_10 = None
        clone_92 = torch.ops.aten.clone.default(unsqueeze_324, memory_format = torch.contiguous_format);  unsqueeze_324 = None
        bitwise_not_56 = torch.ops.aten.bitwise_not.default(clone_92);  clone_92 = None
        masked_fill_56 = torch.ops.aten.masked_fill.Scalar(view_685, bitwise_not_56, 0);  view_685 = bitwise_not_56 = None
        view_686 = torch.ops.aten.view.default(masked_fill_56, [262144, 512]);  masked_fill_56 = None
        view_690 = torch.ops.aten.view.default(view_686, [1, 512, 512, 512])
        split_tensor_47 = torch.ops.aten.split.Tensor(view_690, 256, dim = -1);  view_690 = None
        getitem_543 = split_tensor_47[0]
        unsqueeze_327 = torch.ops.aten.unsqueeze.default(getitem_543, 4);  getitem_543 = None
        permute_457 = torch.ops.aten.permute.default(unsqueeze_327, [0, 2, 4, 3, 1]);  unsqueeze_327 = None
        permute_458 = torch.ops.aten.permute.default(permute_457, [3, 1, 4, 0, 2]);  permute_457 = None
        view_691 = torch.ops.aten.view.default(permute_458, [256, 512, 512]);  permute_458 = None
        view_692 = torch.ops.aten.view.default(view_686, [1, 512, 512, 512]);  view_686 = None
        split_tensor_48 = torch.ops.aten.split.Tensor(view_692, 256, dim = -1);  view_692 = None
        getitem_546 = split_tensor_48[1];  split_tensor_48 = None
        unsqueeze_328 = torch.ops.aten.unsqueeze.default(getitem_546, 4);  getitem_546 = None
        permute_459 = torch.ops.aten.permute.default(unsqueeze_328, [0, 4, 2, 3, 1]);  unsqueeze_328 = None
        permute_460 = torch.ops.aten.permute.default(permute_459, [3, 4, 0, 2, 1]);  permute_459 = None
        view_693 = torch.ops.aten.view.default(permute_460, [256, 512, 512]);  permute_460 = None
        bmm_76 = torch.ops.aten.bmm.default(view_691, view_693);  view_691 = view_693 = None
        view_694 = torch.ops.aten.view.default(bmm_76, [256, 512, 1, 1, 512]);  bmm_76 = None
        permute_461 = torch.ops.aten.permute.default(view_694, [3, 1, 4, 0, 2]);  view_694 = None
        view_695 = torch.ops.aten.view.default(permute_461, [1, 512, 512, 256]);  permute_461 = None
        _to_copy_409 = torch.ops.aten._to_copy.default(view_679, dtype = torch.float32);  view_679 = None
        native_layer_norm_default_86 = torch.ops.aten.native_layer_norm.default(_to_copy_409, [256], None, None, 1e-05);  _to_copy_409 = None
        getitem_547 = native_layer_norm_default_86[0]
        _to_copy_410 = torch.ops.aten._to_copy.default(view_695, dtype = torch.float32);  view_695 = None
        native_layer_norm_default_87 = torch.ops.aten.native_layer_norm.default(_to_copy_410, [256], None, None, 1e-05);  _to_copy_410 = None
        getitem_550 = native_layer_norm_default_87[0]
        add_51 = torch.ops.aten.add.Tensor(getitem_547, getitem_550);  getitem_547 = getitem_550 = None
        _to_copy_411 = torch.ops.aten._to_copy.default(arg121_1, dtype = torch.bfloat16);  arg121_1 = None
        _to_copy_412 = torch.ops.aten._to_copy.default(add_51, dtype = torch.bfloat16);  add_51 = None
        t_133 = torch.ops.aten.t.default(_to_copy_411);  _to_copy_411 = None
        view_696 = torch.ops.aten.view.default(_to_copy_412, [262144, 256]);  _to_copy_412 = None
        mm_124 = torch.ops.aten.mm.default(view_696, t_133);  view_696 = t_133 = None
        view_697 = torch.ops.aten.view.default(mm_124, [1, 512, 512, 256]);  mm_124 = None
        _to_copy_413 = torch.ops.aten._to_copy.default(getitem_534, dtype = torch.bfloat16);  getitem_534 = None
        _to_copy_414 = torch.ops.aten._to_copy.default(getitem_527, dtype = torch.bfloat16);  getitem_527 = None
        t_134 = torch.ops.aten.t.default(_to_copy_413);  _to_copy_413 = None
        view_698 = torch.ops.aten.view.default(_to_copy_414, [262144, 256]);  _to_copy_414 = None
        mm_125 = torch.ops.aten.mm.default(view_698, t_134);  view_698 = t_134 = None
        view_699 = torch.ops.aten.view.default(mm_125, [1, 512, 512, 256]);  mm_125 = None
        sigmoid_39 = torch.ops.aten.sigmoid.default(view_699);  view_699 = None
        mul_62 = torch.ops.aten.mul.Tensor(view_697, sigmoid_39);  view_697 = sigmoid_39 = None
        add_52 = torch.ops.aten.add.Tensor(add_50, mul_62);  mul_62 = None
        split_tensor_49 = torch.ops.aten.split.Tensor(add_50, 512, dim = -2);  add_50 = None
        getitem_553 = split_tensor_49[0];  split_tensor_49 = None
        _to_copy_415 = torch.ops.aten._to_copy.default(getitem_553, dtype = torch.float32);  getitem_553 = None
        native_layer_norm_default_88 = torch.ops.aten.native_layer_norm.default(_to_copy_415, [256], arg100_1, arg101_1, 1e-05);  _to_copy_415 = arg100_1 = arg101_1 = None
        getitem_554 = native_layer_norm_default_88[0]
        _to_copy_416 = torch.ops.aten._to_copy.default(arg102_1, dtype = torch.bfloat16);  arg102_1 = None
        _to_copy_417 = torch.ops.aten._to_copy.default(getitem_554, dtype = torch.bfloat16);  getitem_554 = None
        t_135 = torch.ops.aten.t.default(_to_copy_416);  _to_copy_416 = None
        view_700 = torch.ops.aten.view.default(_to_copy_417, [262144, 256]);  _to_copy_417 = None
        mm_126 = torch.ops.aten.mm.default(view_700, t_135);  view_700 = t_135 = None
        view_701 = torch.ops.aten.view.default(mm_126, [1, 512, 512, 2048]);  mm_126 = None
        split_tensor_50 = torch.ops.aten.split.Tensor(view_701, 1024, dim = -1);  view_701 = None
        getitem_557 = split_tensor_50[0]
        getitem_558 = split_tensor_50[1];  split_tensor_50 = None
        silu_17 = torch.ops.aten.silu.default(getitem_557);  getitem_557 = None
        mul_63 = torch.ops.aten.mul.Tensor(silu_17, getitem_558);  silu_17 = getitem_558 = None
        _to_copy_418 = torch.ops.aten._to_copy.default(arg103_1, dtype = torch.bfloat16);  arg103_1 = None
        t_136 = torch.ops.aten.t.default(_to_copy_418);  _to_copy_418 = None
        view_703 = torch.ops.aten.view.default(mul_63, [262144, 1024]);  mul_63 = None
        mm_127 = torch.ops.aten.mm.default(view_703, t_136);  view_703 = t_136 = None
        view_704 = torch.ops.aten.view.default(mm_127, [1, 512, 512, 256]);  mm_127 = None
        add_53 = torch.ops.aten.add.Tensor(add_52, view_704);  add_52 = view_704 = None
        _to_copy_419 = torch.ops.aten._to_copy.default(add_53, dtype = torch.float32)
        native_layer_norm_default_89 = torch.ops.aten.native_layer_norm.default(_to_copy_419, [256], None, None, 1e-05);  _to_copy_419 = None
        getitem_559 = native_layer_norm_default_89[0]
        _to_copy_420 = torch.ops.aten._to_copy.default(arg140_1, dtype = torch.bfloat16);  arg140_1 = None
        _to_copy_421 = torch.ops.aten._to_copy.default(getitem_559, dtype = torch.bfloat16)
        t_137 = torch.ops.aten.t.default(_to_copy_420);  _to_copy_420 = None
        view_705 = torch.ops.aten.view.default(_to_copy_421, [262144, 256]);  _to_copy_421 = None
        mm_128 = torch.ops.aten.mm.default(view_705, t_137);  view_705 = t_137 = None
        view_706 = torch.ops.aten.view.default(mm_128, [1, 512, 512, 8]);  mm_128 = None
        view_707 = torch.ops.aten.view.default(view_706, [1, 512, 512, 2, 4]);  view_706 = None
        permute_462 = torch.ops.aten.permute.default(view_707, [0, 3, 4, 1, 2]);  view_707 = None
        view_708 = torch.ops.aten.view.default(permute_462, [1, 2, 4, 1, 512, 512]);  permute_462 = None
        view_709 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_57 = torch.ops.aten.bitwise_not.default(view_709);  view_709 = None
        masked_fill_57 = torch.ops.aten.masked_fill.Scalar(view_708, bitwise_not_57, -10000);  view_708 = bitwise_not_57 = None
        view_710 = torch.ops.aten.view.default(masked_fill_57, [1, 2, 4, 512, 512]);  masked_fill_57 = None
        permute_463 = torch.ops.aten.permute.default(view_710, [1, 0, 2, 3, 4]);  view_710 = None
        view_711 = torch.ops.aten.view.default(permute_463, [2, 4, 1, 512, 512]);  permute_463 = None
        _to_copy_422 = torch.ops.aten._to_copy.default(arg141_1, dtype = torch.bfloat16);  arg141_1 = None
        _to_copy_423 = torch.ops.aten._to_copy.default(getitem_559, dtype = torch.bfloat16)
        t_138 = torch.ops.aten.t.default(_to_copy_422);  _to_copy_422 = None
        view_712 = torch.ops.aten.view.default(_to_copy_423, [262144, 256]);  _to_copy_423 = None
        mm_129 = torch.ops.aten.mm.default(view_712, t_138);  view_712 = t_138 = None
        view_713 = torch.ops.aten.view.default(mm_129, [1, 512, 512, 1024]);  mm_129 = None
        select_11 = torch.ops.aten.select.int(view_711, 0, 0)
        view_714 = torch.ops.aten.view.default(view_713, [1, 512, 512, 4, 4, 64]);  view_713 = None
        permute_464 = torch.ops.aten.permute.default(view_714, [4, 0, 3, 1, 2, 5]);  view_714 = None
        view_715 = torch.ops.aten.view.default(permute_464, [4, 4, 512, 512, 64]);  permute_464 = None
        unbind_int_38 = torch.ops.aten.unbind.int(view_715);  view_715 = None
        getitem_562 = unbind_int_38[0]
        getitem_563 = unbind_int_38[1]
        getitem_564 = unbind_int_38[2]
        getitem_565 = unbind_int_38[3];  unbind_int_38 = None
        expand_23 = torch.ops.aten.expand.default(select_11, [4, 512, 512, 512]);  select_11 = None
        _scaled_dot_product_efficient_attention_default_10 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_562, getitem_563, getitem_564, expand_23, False);  getitem_562 = getitem_563 = getitem_564 = expand_23 = None
        getitem_566 = _scaled_dot_product_efficient_attention_default_10[0]
        sigmoid_40 = torch.ops.aten.sigmoid.default(getitem_565);  getitem_565 = None
        mul_64 = torch.ops.aten.mul.Tensor(getitem_566, sigmoid_40);  getitem_566 = sigmoid_40 = None
        view_716 = torch.ops.aten.view.default(mul_64, [1, 4, 512, 512, 64]);  mul_64 = None
        permute_465 = torch.ops.aten.permute.default(view_716, [0, 2, 3, 1, 4]);  view_716 = None
        clone_93 = torch.ops.aten.clone.default(permute_465, memory_format = torch.contiguous_format);  permute_465 = None
        _unsafe_view_86 = torch.ops.aten._unsafe_view.default(clone_93, [1, 512, 512, 256]);  clone_93 = None
        transpose_11 = torch.ops.aten.transpose.int(getitem_559, 1, 2);  getitem_559 = None
        _to_copy_424 = torch.ops.aten._to_copy.default(arg142_1, dtype = torch.bfloat16);  arg142_1 = None
        _to_copy_425 = torch.ops.aten._to_copy.default(transpose_11, dtype = torch.bfloat16);  transpose_11 = None
        t_139 = torch.ops.aten.t.default(_to_copy_424);  _to_copy_424 = None
        expand_24 = torch.ops.aten.expand.default(_to_copy_425, [1, 512, 512, 256]);  _to_copy_425 = None
        view_717 = torch.ops.aten.view.default(expand_24, [512, 512, 256]);  expand_24 = None
        expand_25 = torch.ops.aten.expand.default(t_139, [1, 512, 256, 1024]);  t_139 = None
        view_718 = torch.ops.aten.view.default(expand_25, [512, 256, 1024]);  expand_25 = None
        bmm_77 = torch.ops.aten.bmm.default(view_717, view_718);  view_717 = view_718 = None
        view_719 = torch.ops.aten.view.default(bmm_77, [1, 512, 512, 1024]);  bmm_77 = None
        select_12 = torch.ops.aten.select.int(view_711, 0, 1);  view_711 = None
        view_720 = torch.ops.aten.view.default(view_719, [1, 512, 512, 4, 4, 64]);  view_719 = None
        permute_466 = torch.ops.aten.permute.default(view_720, [4, 0, 3, 1, 2, 5]);  view_720 = None
        view_721 = torch.ops.aten.view.default(permute_466, [4, 4, 512, 512, 64]);  permute_466 = None
        unbind_int_39 = torch.ops.aten.unbind.int(view_721);  view_721 = None
        getitem_570 = unbind_int_39[0]
        getitem_571 = unbind_int_39[1]
        getitem_572 = unbind_int_39[2]
        getitem_573 = unbind_int_39[3];  unbind_int_39 = None
        expand_26 = torch.ops.aten.expand.default(select_12, [4, 512, 512, 512]);  select_12 = None
        _scaled_dot_product_efficient_attention_default_11 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_570, getitem_571, getitem_572, expand_26, False);  getitem_570 = getitem_571 = getitem_572 = expand_26 = None
        getitem_574 = _scaled_dot_product_efficient_attention_default_11[0]
        sigmoid_41 = torch.ops.aten.sigmoid.default(getitem_573);  getitem_573 = None
        mul_65 = torch.ops.aten.mul.Tensor(getitem_574, sigmoid_41);  getitem_574 = sigmoid_41 = None
        view_722 = torch.ops.aten.view.default(mul_65, [1, 4, 512, 512, 64]);  mul_65 = None
        permute_467 = torch.ops.aten.permute.default(view_722, [0, 2, 3, 1, 4]);  view_722 = None
        clone_94 = torch.ops.aten.clone.default(permute_467, memory_format = torch.contiguous_format);  permute_467 = None
        _unsafe_view_87 = torch.ops.aten._unsafe_view.default(clone_94, [1, 512, 512, 256]);  clone_94 = None
        cat_11 = torch.ops.aten.cat.default([_unsafe_view_86, _unsafe_view_87], dim = -1);  _unsafe_view_86 = _unsafe_view_87 = None
        slice_204 = torch.ops.aten.slice.Tensor(arg139_1, dim = 0, start = 0, end = 9223372036854775807);  arg139_1 = None
        unsqueeze_329 = torch.ops.aten.unsqueeze.default(slice_204, 1);  slice_204 = None
        mul_66 = torch.ops.aten.mul.Tensor(arg143_1, unsqueeze_329);  arg143_1 = unsqueeze_329 = None
        _to_copy_426 = torch.ops.aten._to_copy.default(mul_66, dtype = torch.bfloat16);  mul_66 = None
        t_140 = torch.ops.aten.t.default(_to_copy_426);  _to_copy_426 = None
        view_723 = torch.ops.aten.view.default(cat_11, [262144, 512]);  cat_11 = None
        mm_130 = torch.ops.aten.mm.default(view_723, t_140);  view_723 = t_140 = None
        view_724 = torch.ops.aten.view.default(mm_130, [1, 512, 512, 256]);  mm_130 = None
        add_54 = torch.ops.aten.add.Tensor(add_53, view_724);  add_53 = view_724 = None
        add_55 = torch.ops.aten.add.Tensor(add_11, add_54);  add_11 = add_54 = None
        _to_copy_427 = torch.ops.aten._to_copy.default(add_55, dtype = torch.float32)
        native_layer_norm_default_90 = torch.ops.aten.native_layer_norm.default(_to_copy_427, [256], arg148_1, arg149_1, 1e-05);  _to_copy_427 = arg148_1 = arg149_1 = None
        getitem_578 = native_layer_norm_default_90[0]
        split_with_sizes_default_12 = torch.ops.aten.split_with_sizes.default(arg151_1, [512, 512]);  arg151_1 = None
        getitem_581 = split_with_sizes_default_12[0]
        getitem_582 = split_with_sizes_default_12[1];  split_with_sizes_default_12 = None
        split_with_sizes_default_13 = torch.ops.aten.split_with_sizes.default(arg152_1, [512, 512, 256]);  arg152_1 = None
        getitem_583 = split_with_sizes_default_13[0]
        getitem_584 = split_with_sizes_default_13[1]
        getitem_585 = split_with_sizes_default_13[2];  split_with_sizes_default_13 = None
        _to_copy_428 = torch.ops.aten._to_copy.default(getitem_581, dtype = torch.bfloat16);  getitem_581 = None
        _to_copy_429 = torch.ops.aten._to_copy.default(getitem_578, dtype = torch.bfloat16)
        t_141 = torch.ops.aten.t.default(_to_copy_428);  _to_copy_428 = None
        view_725 = torch.ops.aten.view.default(_to_copy_429, [262144, 256]);  _to_copy_429 = None
        mm_131 = torch.ops.aten.mm.default(view_725, t_141);  view_725 = t_141 = None
        view_726 = torch.ops.aten.view.default(mm_131, [1, 512, 512, 512]);  mm_131 = None
        _to_copy_430 = torch.ops.aten._to_copy.default(getitem_583, dtype = torch.bfloat16);  getitem_583 = None
        _to_copy_431 = torch.ops.aten._to_copy.default(getitem_578, dtype = torch.bfloat16)
        t_142 = torch.ops.aten.t.default(_to_copy_430);  _to_copy_430 = None
        view_727 = torch.ops.aten.view.default(_to_copy_431, [262144, 256]);  _to_copy_431 = None
        mm_132 = torch.ops.aten.mm.default(view_727, t_142);  view_727 = t_142 = None
        view_728 = torch.ops.aten.view.default(mm_132, [1, 512, 512, 512]);  mm_132 = None
        sigmoid_42 = torch.ops.aten.sigmoid.default(view_728);  view_728 = None
        mul_67 = torch.ops.aten.mul.Tensor(view_726, sigmoid_42);  view_726 = sigmoid_42 = None
        unsqueeze_330 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_58 = torch.ops.aten.bitwise_not.default(unsqueeze_330);  unsqueeze_330 = None
        masked_fill_58 = torch.ops.aten.masked_fill.Scalar(mul_67, bitwise_not_58, 0);  mul_67 = bitwise_not_58 = None
        split_tensor_51 = torch.ops.aten.split.Tensor(masked_fill_58, 256, dim = -1)
        getitem_588 = split_tensor_51[0]
        unsqueeze_333 = torch.ops.aten.unsqueeze.default(getitem_588, 4);  getitem_588 = None
        permute_472 = torch.ops.aten.permute.default(unsqueeze_333, [0, 1, 4, 3, 2]);  unsqueeze_333 = None
        permute_473 = torch.ops.aten.permute.default(permute_472, [3, 1, 4, 0, 2]);  permute_472 = None
        view_731 = torch.ops.aten.view.default(permute_473, [256, 512, 512]);  permute_473 = None
        split_tensor_52 = torch.ops.aten.split.Tensor(masked_fill_58, 256, dim = -1);  masked_fill_58 = None
        getitem_591 = split_tensor_52[1];  split_tensor_52 = None
        unsqueeze_334 = torch.ops.aten.unsqueeze.default(getitem_591, 4);  getitem_591 = None
        permute_474 = torch.ops.aten.permute.default(unsqueeze_334, [0, 4, 1, 3, 2]);  unsqueeze_334 = None
        permute_475 = torch.ops.aten.permute.default(permute_474, [3, 4, 0, 2, 1]);  permute_474 = None
        view_732 = torch.ops.aten.view.default(permute_475, [256, 512, 512]);  permute_475 = None
        bmm_78 = torch.ops.aten.bmm.default(view_731, view_732);  view_731 = view_732 = None
        view_733 = torch.ops.aten.view.default(bmm_78, [256, 512, 1, 1, 512]);  bmm_78 = None
        permute_476 = torch.ops.aten.permute.default(view_733, [3, 1, 4, 0, 2]);  view_733 = None
        view_734 = torch.ops.aten.view.default(permute_476, [1, 512, 512, 256]);  permute_476 = None
        _to_copy_432 = torch.ops.aten._to_copy.default(getitem_582, dtype = torch.bfloat16);  getitem_582 = None
        _to_copy_433 = torch.ops.aten._to_copy.default(getitem_578, dtype = torch.bfloat16)
        t_143 = torch.ops.aten.t.default(_to_copy_432);  _to_copy_432 = None
        view_735 = torch.ops.aten.view.default(_to_copy_433, [262144, 256]);  _to_copy_433 = None
        mm_133 = torch.ops.aten.mm.default(view_735, t_143);  view_735 = t_143 = None
        view_736 = torch.ops.aten.view.default(mm_133, [1, 512, 512, 512]);  mm_133 = None
        _to_copy_434 = torch.ops.aten._to_copy.default(getitem_584, dtype = torch.bfloat16);  getitem_584 = None
        _to_copy_435 = torch.ops.aten._to_copy.default(getitem_578, dtype = torch.bfloat16)
        t_144 = torch.ops.aten.t.default(_to_copy_434);  _to_copy_434 = None
        view_737 = torch.ops.aten.view.default(_to_copy_435, [262144, 256]);  _to_copy_435 = None
        mm_134 = torch.ops.aten.mm.default(view_737, t_144);  view_737 = t_144 = None
        view_738 = torch.ops.aten.view.default(mm_134, [1, 512, 512, 512]);  mm_134 = None
        sigmoid_43 = torch.ops.aten.sigmoid.default(view_738);  view_738 = None
        mul_68 = torch.ops.aten.mul.Tensor(view_736, sigmoid_43);  view_736 = sigmoid_43 = None
        view_739 = torch.ops.aten.view.default(mul_68, [262144, 512]);  mul_68 = None
        view_740 = torch.ops.aten.view.default(view_739, [1, 512, 512, 512]);  view_739 = None
        transpose_12 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_335 = torch.ops.aten.unsqueeze.default(transpose_12, 3);  transpose_12 = None
        clone_95 = torch.ops.aten.clone.default(unsqueeze_335, memory_format = torch.contiguous_format);  unsqueeze_335 = None
        bitwise_not_59 = torch.ops.aten.bitwise_not.default(clone_95);  clone_95 = None
        masked_fill_59 = torch.ops.aten.masked_fill.Scalar(view_740, bitwise_not_59, 0);  view_740 = bitwise_not_59 = None
        view_741 = torch.ops.aten.view.default(masked_fill_59, [262144, 512]);  masked_fill_59 = None
        view_745 = torch.ops.aten.view.default(view_741, [1, 512, 512, 512])
        split_tensor_53 = torch.ops.aten.split.Tensor(view_745, 256, dim = -1);  view_745 = None
        getitem_594 = split_tensor_53[0]
        unsqueeze_338 = torch.ops.aten.unsqueeze.default(getitem_594, 4);  getitem_594 = None
        permute_481 = torch.ops.aten.permute.default(unsqueeze_338, [0, 2, 4, 3, 1]);  unsqueeze_338 = None
        permute_482 = torch.ops.aten.permute.default(permute_481, [3, 1, 4, 0, 2]);  permute_481 = None
        view_746 = torch.ops.aten.view.default(permute_482, [256, 512, 512]);  permute_482 = None
        view_747 = torch.ops.aten.view.default(view_741, [1, 512, 512, 512]);  view_741 = None
        split_tensor_54 = torch.ops.aten.split.Tensor(view_747, 256, dim = -1);  view_747 = None
        getitem_597 = split_tensor_54[1];  split_tensor_54 = None
        unsqueeze_339 = torch.ops.aten.unsqueeze.default(getitem_597, 4);  getitem_597 = None
        permute_483 = torch.ops.aten.permute.default(unsqueeze_339, [0, 4, 2, 3, 1]);  unsqueeze_339 = None
        permute_484 = torch.ops.aten.permute.default(permute_483, [3, 4, 0, 2, 1]);  permute_483 = None
        view_748 = torch.ops.aten.view.default(permute_484, [256, 512, 512]);  permute_484 = None
        bmm_79 = torch.ops.aten.bmm.default(view_746, view_748);  view_746 = view_748 = None
        view_749 = torch.ops.aten.view.default(bmm_79, [256, 512, 1, 1, 512]);  bmm_79 = None
        permute_485 = torch.ops.aten.permute.default(view_749, [3, 1, 4, 0, 2]);  view_749 = None
        view_750 = torch.ops.aten.view.default(permute_485, [1, 512, 512, 256]);  permute_485 = None
        _to_copy_436 = torch.ops.aten._to_copy.default(view_734, dtype = torch.float32);  view_734 = None
        native_layer_norm_default_91 = torch.ops.aten.native_layer_norm.default(_to_copy_436, [256], None, None, 1e-05);  _to_copy_436 = None
        getitem_598 = native_layer_norm_default_91[0]
        _to_copy_437 = torch.ops.aten._to_copy.default(view_750, dtype = torch.float32);  view_750 = None
        native_layer_norm_default_92 = torch.ops.aten.native_layer_norm.default(_to_copy_437, [256], None, None, 1e-05);  _to_copy_437 = None
        getitem_601 = native_layer_norm_default_92[0]
        add_56 = torch.ops.aten.add.Tensor(getitem_598, getitem_601);  getitem_598 = getitem_601 = None
        _to_copy_438 = torch.ops.aten._to_copy.default(arg150_1, dtype = torch.bfloat16);  arg150_1 = None
        _to_copy_439 = torch.ops.aten._to_copy.default(add_56, dtype = torch.bfloat16);  add_56 = None
        t_145 = torch.ops.aten.t.default(_to_copy_438);  _to_copy_438 = None
        view_751 = torch.ops.aten.view.default(_to_copy_439, [262144, 256]);  _to_copy_439 = None
        mm_135 = torch.ops.aten.mm.default(view_751, t_145);  view_751 = t_145 = None
        view_752 = torch.ops.aten.view.default(mm_135, [1, 512, 512, 256]);  mm_135 = None
        _to_copy_440 = torch.ops.aten._to_copy.default(getitem_585, dtype = torch.bfloat16);  getitem_585 = None
        _to_copy_441 = torch.ops.aten._to_copy.default(getitem_578, dtype = torch.bfloat16);  getitem_578 = None
        t_146 = torch.ops.aten.t.default(_to_copy_440);  _to_copy_440 = None
        view_753 = torch.ops.aten.view.default(_to_copy_441, [262144, 256]);  _to_copy_441 = None
        mm_136 = torch.ops.aten.mm.default(view_753, t_146);  view_753 = t_146 = None
        view_754 = torch.ops.aten.view.default(mm_136, [1, 512, 512, 256]);  mm_136 = None
        sigmoid_44 = torch.ops.aten.sigmoid.default(view_754);  view_754 = None
        mul_69 = torch.ops.aten.mul.Tensor(view_752, sigmoid_44);  view_752 = sigmoid_44 = None
        add_57 = torch.ops.aten.add.Tensor(add_55, mul_69);  mul_69 = None
        _to_copy_442 = torch.ops.aten._to_copy.default(add_55, dtype = torch.float32)
        native_layer_norm_default_93 = torch.ops.aten.native_layer_norm.default(_to_copy_442, [256], None, None, 1e-05);  _to_copy_442 = None
        getitem_604 = native_layer_norm_default_93[0]
        _to_copy_443 = torch.ops.aten._to_copy.default(arg154_1, dtype = torch.bfloat16);  arg154_1 = None
        _to_copy_444 = torch.ops.aten._to_copy.default(getitem_604, dtype = torch.bfloat16)
        t_147 = torch.ops.aten.t.default(_to_copy_443);  _to_copy_443 = None
        view_755 = torch.ops.aten.view.default(_to_copy_444, [262144, 256]);  _to_copy_444 = None
        mm_137 = torch.ops.aten.mm.default(view_755, t_147);  view_755 = t_147 = None
        view_756 = torch.ops.aten.view.default(mm_137, [1, 512, 512, 8]);  mm_137 = None
        view_757 = torch.ops.aten.view.default(view_756, [1, 512, 512, 2, 4]);  view_756 = None
        permute_486 = torch.ops.aten.permute.default(view_757, [0, 3, 4, 1, 2]);  view_757 = None
        view_758 = torch.ops.aten.view.default(permute_486, [1, 2, 4, 1, 512, 512]);  permute_486 = None
        view_759 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_60 = torch.ops.aten.bitwise_not.default(view_759);  view_759 = None
        masked_fill_60 = torch.ops.aten.masked_fill.Scalar(view_758, bitwise_not_60, -10000);  view_758 = bitwise_not_60 = None
        view_760 = torch.ops.aten.view.default(masked_fill_60, [1, 2, 4, 512, 512]);  masked_fill_60 = None
        permute_487 = torch.ops.aten.permute.default(view_760, [1, 0, 2, 3, 4]);  view_760 = None
        view_761 = torch.ops.aten.view.default(permute_487, [2, 4, 1, 512, 512]);  permute_487 = None
        _to_copy_445 = torch.ops.aten._to_copy.default(arg155_1, dtype = torch.bfloat16);  arg155_1 = None
        _to_copy_446 = torch.ops.aten._to_copy.default(getitem_604, dtype = torch.bfloat16)
        t_148 = torch.ops.aten.t.default(_to_copy_445);  _to_copy_445 = None
        view_762 = torch.ops.aten.view.default(_to_copy_446, [262144, 256]);  _to_copy_446 = None
        mm_138 = torch.ops.aten.mm.default(view_762, t_148);  view_762 = t_148 = None
        view_763 = torch.ops.aten.view.default(mm_138, [1, 512, 512, 1024]);  mm_138 = None
        select_13 = torch.ops.aten.select.int(view_761, 0, 0)
        view_764 = torch.ops.aten.view.default(view_763, [1, 512, 512, 4, 4, 64]);  view_763 = None
        permute_488 = torch.ops.aten.permute.default(view_764, [4, 0, 3, 1, 2, 5]);  view_764 = None
        view_765 = torch.ops.aten.view.default(permute_488, [4, 4, 512, 512, 64]);  permute_488 = None
        unbind_int_40 = torch.ops.aten.unbind.int(view_765);  view_765 = None
        getitem_607 = unbind_int_40[0]
        getitem_608 = unbind_int_40[1]
        getitem_609 = unbind_int_40[2]
        getitem_610 = unbind_int_40[3];  unbind_int_40 = None
        expand_27 = torch.ops.aten.expand.default(select_13, [4, 512, 512, 512]);  select_13 = None
        _scaled_dot_product_efficient_attention_default_12 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_607, getitem_608, getitem_609, expand_27, False);  getitem_607 = getitem_608 = getitem_609 = expand_27 = None
        getitem_611 = _scaled_dot_product_efficient_attention_default_12[0]
        sigmoid_45 = torch.ops.aten.sigmoid.default(getitem_610);  getitem_610 = None
        mul_70 = torch.ops.aten.mul.Tensor(getitem_611, sigmoid_45);  getitem_611 = sigmoid_45 = None
        view_766 = torch.ops.aten.view.default(mul_70, [1, 4, 512, 512, 64]);  mul_70 = None
        permute_489 = torch.ops.aten.permute.default(view_766, [0, 2, 3, 1, 4]);  view_766 = None
        clone_96 = torch.ops.aten.clone.default(permute_489, memory_format = torch.contiguous_format);  permute_489 = None
        _unsafe_view_88 = torch.ops.aten._unsafe_view.default(clone_96, [1, 512, 512, 256]);  clone_96 = None
        transpose_13 = torch.ops.aten.transpose.int(getitem_604, 1, 2);  getitem_604 = None
        _to_copy_447 = torch.ops.aten._to_copy.default(arg156_1, dtype = torch.bfloat16);  arg156_1 = None
        _to_copy_448 = torch.ops.aten._to_copy.default(transpose_13, dtype = torch.bfloat16);  transpose_13 = None
        t_149 = torch.ops.aten.t.default(_to_copy_447);  _to_copy_447 = None
        expand_28 = torch.ops.aten.expand.default(_to_copy_448, [1, 512, 512, 256]);  _to_copy_448 = None
        view_767 = torch.ops.aten.view.default(expand_28, [512, 512, 256]);  expand_28 = None
        expand_29 = torch.ops.aten.expand.default(t_149, [1, 512, 256, 1024]);  t_149 = None
        view_768 = torch.ops.aten.view.default(expand_29, [512, 256, 1024]);  expand_29 = None
        bmm_80 = torch.ops.aten.bmm.default(view_767, view_768);  view_767 = view_768 = None
        view_769 = torch.ops.aten.view.default(bmm_80, [1, 512, 512, 1024]);  bmm_80 = None
        select_14 = torch.ops.aten.select.int(view_761, 0, 1);  view_761 = None
        view_770 = torch.ops.aten.view.default(view_769, [1, 512, 512, 4, 4, 64]);  view_769 = None
        permute_490 = torch.ops.aten.permute.default(view_770, [4, 0, 3, 1, 2, 5]);  view_770 = None
        view_771 = torch.ops.aten.view.default(permute_490, [4, 4, 512, 512, 64]);  permute_490 = None
        unbind_int_41 = torch.ops.aten.unbind.int(view_771);  view_771 = None
        getitem_615 = unbind_int_41[0]
        getitem_616 = unbind_int_41[1]
        getitem_617 = unbind_int_41[2]
        getitem_618 = unbind_int_41[3];  unbind_int_41 = None
        expand_30 = torch.ops.aten.expand.default(select_14, [4, 512, 512, 512]);  select_14 = None
        _scaled_dot_product_efficient_attention_default_13 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_615, getitem_616, getitem_617, expand_30, False);  getitem_615 = getitem_616 = getitem_617 = expand_30 = None
        getitem_619 = _scaled_dot_product_efficient_attention_default_13[0]
        sigmoid_46 = torch.ops.aten.sigmoid.default(getitem_618);  getitem_618 = None
        mul_71 = torch.ops.aten.mul.Tensor(getitem_619, sigmoid_46);  getitem_619 = sigmoid_46 = None
        view_772 = torch.ops.aten.view.default(mul_71, [1, 4, 512, 512, 64]);  mul_71 = None
        permute_491 = torch.ops.aten.permute.default(view_772, [0, 2, 3, 1, 4]);  view_772 = None
        clone_97 = torch.ops.aten.clone.default(permute_491, memory_format = torch.contiguous_format);  permute_491 = None
        _unsafe_view_89 = torch.ops.aten._unsafe_view.default(clone_97, [1, 512, 512, 256]);  clone_97 = None
        cat_12 = torch.ops.aten.cat.default([_unsafe_view_88, _unsafe_view_89], dim = -1);  _unsafe_view_88 = _unsafe_view_89 = None
        slice_205 = torch.ops.aten.slice.Tensor(arg153_1, dim = 0, start = 0, end = 9223372036854775807);  arg153_1 = None
        unsqueeze_340 = torch.ops.aten.unsqueeze.default(slice_205, 1);  slice_205 = None
        mul_72 = torch.ops.aten.mul.Tensor(arg157_1, unsqueeze_340);  arg157_1 = unsqueeze_340 = None
        _to_copy_449 = torch.ops.aten._to_copy.default(mul_72, dtype = torch.bfloat16);  mul_72 = None
        t_150 = torch.ops.aten.t.default(_to_copy_449);  _to_copy_449 = None
        view_773 = torch.ops.aten.view.default(cat_12, [262144, 512]);  cat_12 = None
        mm_139 = torch.ops.aten.mm.default(view_773, t_150);  view_773 = t_150 = None
        view_774 = torch.ops.aten.view.default(mm_139, [1, 512, 512, 256]);  mm_139 = None
        add_58 = torch.ops.aten.add.Tensor(add_57, view_774);  add_57 = view_774 = None
        split_tensor_55 = torch.ops.aten.split.Tensor(add_55, 512, dim = -2)
        getitem_623 = split_tensor_55[0];  split_tensor_55 = None
        _to_copy_450 = torch.ops.aten._to_copy.default(getitem_623, dtype = torch.float32);  getitem_623 = None
        native_layer_norm_default_94 = torch.ops.aten.native_layer_norm.default(_to_copy_450, [256], arg144_1, arg145_1, 1e-05);  _to_copy_450 = arg144_1 = arg145_1 = None
        getitem_624 = native_layer_norm_default_94[0]
        _to_copy_451 = torch.ops.aten._to_copy.default(arg146_1, dtype = torch.bfloat16);  arg146_1 = None
        _to_copy_452 = torch.ops.aten._to_copy.default(getitem_624, dtype = torch.bfloat16);  getitem_624 = None
        t_151 = torch.ops.aten.t.default(_to_copy_451);  _to_copy_451 = None
        view_775 = torch.ops.aten.view.default(_to_copy_452, [262144, 256]);  _to_copy_452 = None
        mm_140 = torch.ops.aten.mm.default(view_775, t_151);  view_775 = t_151 = None
        view_776 = torch.ops.aten.view.default(mm_140, [1, 512, 512, 1024]);  mm_140 = None
        split_tensor_56 = torch.ops.aten.split.Tensor(view_776, 512, dim = -1);  view_776 = None
        getitem_627 = split_tensor_56[0]
        getitem_628 = split_tensor_56[1];  split_tensor_56 = None
        silu_18 = torch.ops.aten.silu.default(getitem_627);  getitem_627 = None
        mul_73 = torch.ops.aten.mul.Tensor(silu_18, getitem_628);  silu_18 = getitem_628 = None
        _to_copy_453 = torch.ops.aten._to_copy.default(arg147_1, dtype = torch.bfloat16);  arg147_1 = None
        t_152 = torch.ops.aten.t.default(_to_copy_453);  _to_copy_453 = None
        view_778 = torch.ops.aten.view.default(mul_73, [262144, 512]);  mul_73 = None
        mm_141 = torch.ops.aten.mm.default(view_778, t_152);  view_778 = t_152 = None
        view_779 = torch.ops.aten.view.default(mm_141, [1, 512, 512, 256]);  mm_141 = None
        add_59 = torch.ops.aten.add.Tensor(add_58, view_779);  add_58 = view_779 = None
        _to_copy_454 = torch.ops.aten._to_copy.default(add_1, dtype = torch.float32)
        native_layer_norm_default_95 = torch.ops.aten.native_layer_norm.default(_to_copy_454, [384], arg162_1, arg163_1, 1e-05);  _to_copy_454 = arg162_1 = arg163_1 = None
        getitem_629 = native_layer_norm_default_95[0]
        _to_copy_455 = torch.ops.aten._to_copy.default(add_55, dtype = torch.float32);  add_55 = None
        native_layer_norm_default_96 = torch.ops.aten.native_layer_norm.default(_to_copy_455, [256], arg164_1, arg165_1, 1e-05);  _to_copy_455 = arg164_1 = arg165_1 = None
        getitem_632 = native_layer_norm_default_96[0]
        _to_copy_456 = torch.ops.aten._to_copy.default(arg166_1, dtype = torch.bfloat16);  arg166_1 = None
        _to_copy_457 = torch.ops.aten._to_copy.default(getitem_632, dtype = torch.bfloat16);  getitem_632 = None
        t_153 = torch.ops.aten.t.default(_to_copy_456);  _to_copy_456 = None
        view_780 = torch.ops.aten.view.default(_to_copy_457, [262144, 256]);  _to_copy_457 = None
        mm_142 = torch.ops.aten.mm.default(view_780, t_153);  view_780 = t_153 = None
        view_781 = torch.ops.aten.view.default(mm_142, [1, 512, 512, 16]);  mm_142 = None
        permute_492 = torch.ops.aten.permute.default(view_781, [0, 3, 1, 2]);  view_781 = None
        view_782 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_61 = torch.ops.aten.bitwise_not.default(view_782);  view_782 = None
        masked_fill_61 = torch.ops.aten.masked_fill.Scalar(permute_492, bitwise_not_61, -10000);  permute_492 = bitwise_not_61 = None
        _to_copy_458 = torch.ops.aten._to_copy.default(getitem_629, dtype = torch.bfloat16);  getitem_629 = None
        _to_copy_459 = torch.ops.aten._to_copy.default(arg168_1, dtype = torch.bfloat16);  arg168_1 = None
        unsqueeze_341 = torch.ops.aten.unsqueeze.default(_to_copy_458, 3);  _to_copy_458 = None
        unsqueeze_342 = torch.ops.aten.unsqueeze.default(unsqueeze_341, 4);  unsqueeze_341 = None
        unsqueeze_343 = torch.ops.aten.unsqueeze.default(unsqueeze_342, 5);  unsqueeze_342 = None
        permute_493 = torch.ops.aten.permute.default(unsqueeze_343, [3, 0, 4, 1, 5, 2]);  unsqueeze_343 = None
        unsqueeze_344 = torch.ops.aten.unsqueeze.default(_to_copy_459, 4);  _to_copy_459 = None
        unsqueeze_345 = torch.ops.aten.unsqueeze.default(unsqueeze_344, 5);  unsqueeze_344 = None
        permute_494 = torch.ops.aten.permute.default(unsqueeze_345, [1, 4, 2, 5, 3, 0]);  unsqueeze_345 = None
        permute_495 = torch.ops.aten.permute.default(permute_493, [3, 5, 0, 1, 2, 4]);  permute_493 = None
        view_783 = torch.ops.aten.view.default(permute_495, [1, 512, 384]);  permute_495 = None
        permute_496 = torch.ops.aten.permute.default(permute_494, [5, 0, 1, 2, 4, 3]);  permute_494 = None
        view_784 = torch.ops.aten.view.default(permute_496, [1, 384, 1536]);  permute_496 = None
        bmm_81 = torch.ops.aten.bmm.default(view_783, view_784);  view_783 = view_784 = None
        view_785 = torch.ops.aten.view.default(bmm_81, [512, 1, 4, 1, 16, 24]);  bmm_81 = None
        permute_497 = torch.ops.aten.permute.default(view_785, [2, 3, 4, 0, 5, 1]);  view_785 = None
        view_786 = torch.ops.aten.view.default(permute_497, [4, 1, 16, 512, 24]);  permute_497 = None
        unbind_int_42 = torch.ops.aten.unbind.int(view_786);  view_786 = None
        getitem_635 = unbind_int_42[0]
        getitem_636 = unbind_int_42[1]
        getitem_637 = unbind_int_42[2]
        getitem_638 = unbind_int_42[3];  unbind_int_42 = None
        view_787 = torch.ops.aten.view.default(arg167_1, [1, 16, 1, 24]);  arg167_1 = None
        add_60 = torch.ops.aten.add.Tensor(getitem_635, view_787);  getitem_635 = view_787 = None
        _to_copy_460 = torch.ops.aten._to_copy.default(add_60, dtype = torch.bfloat16);  add_60 = None
        expand_31 = torch.ops.aten.expand.default(masked_fill_61, [1, 16, 512, 512]);  masked_fill_61 = None
        _scaled_dot_product_efficient_attention_default_14 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_460, getitem_636, getitem_637, expand_31, False);  _to_copy_460 = getitem_636 = getitem_637 = expand_31 = None
        getitem_639 = _scaled_dot_product_efficient_attention_default_14[0]
        add_61 = torch.ops.aten.add.Tensor(getitem_638, 1);  getitem_638 = None
        sigmoid_47 = torch.ops.aten.sigmoid.default(add_61);  add_61 = None
        mul_74 = torch.ops.aten.mul.Tensor(getitem_639, sigmoid_47);  getitem_639 = sigmoid_47 = None
        _to_copy_461 = torch.ops.aten._to_copy.default(arg169_1, dtype = torch.bfloat16);  arg169_1 = None
        unsqueeze_346 = torch.ops.aten.unsqueeze.default(mul_74, 4);  mul_74 = None
        permute_498 = torch.ops.aten.permute.default(unsqueeze_346, [0, 2, 4, 3, 1]);  unsqueeze_346 = None
        unsqueeze_347 = torch.ops.aten.unsqueeze.default(_to_copy_461, 3);  _to_copy_461 = None
        unsqueeze_348 = torch.ops.aten.unsqueeze.default(unsqueeze_347, 4);  unsqueeze_347 = None
        permute_499 = torch.ops.aten.permute.default(unsqueeze_348, [3, 4, 2, 1, 0]);  unsqueeze_348 = None
        permute_500 = torch.ops.aten.permute.default(permute_498, [1, 3, 4, 0, 2]);  permute_498 = None
        clone_98 = torch.ops.aten.clone.default(permute_500, memory_format = torch.contiguous_format);  permute_500 = None
        _unsafe_view_90 = torch.ops.aten._unsafe_view.default(clone_98, [1, 512, 384]);  clone_98 = None
        permute_501 = torch.ops.aten.permute.default(permute_499, [3, 4, 0, 2, 1]);  permute_499 = None
        clone_99 = torch.ops.aten.clone.default(permute_501, memory_format = torch.contiguous_format);  permute_501 = None
        _unsafe_view_91 = torch.ops.aten._unsafe_view.default(clone_99, [1, 384, 384]);  clone_99 = None
        bmm_82 = torch.ops.aten.bmm.default(_unsafe_view_90, _unsafe_view_91);  _unsafe_view_90 = _unsafe_view_91 = None
        view_788 = torch.ops.aten.view.default(bmm_82, [512, 1, 1, 1, 384]);  bmm_82 = None
        permute_502 = torch.ops.aten.permute.default(view_788, [3, 0, 4, 1, 2]);  view_788 = None
        view_789 = torch.ops.aten.view.default(permute_502, [1, 512, 384]);  permute_502 = None
        unsqueeze_349 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_75 = torch.ops.aten.mul.Tensor(view_789, unsqueeze_349);  view_789 = unsqueeze_349 = None
        add_62 = torch.ops.aten.add.Tensor(add_1, mul_75);  mul_75 = None
        split_tensor_57 = torch.ops.aten.split.Tensor(add_1, 512, dim = -2);  add_1 = None
        getitem_643 = split_tensor_57[0];  split_tensor_57 = None
        _to_copy_462 = torch.ops.aten._to_copy.default(getitem_643, dtype = torch.float32);  getitem_643 = None
        native_layer_norm_default_97 = torch.ops.aten.native_layer_norm.default(_to_copy_462, [384], arg158_1, arg159_1, 1e-05);  _to_copy_462 = arg158_1 = arg159_1 = None
        getitem_644 = native_layer_norm_default_97[0]
        _to_copy_463 = torch.ops.aten._to_copy.default(arg160_1, dtype = torch.bfloat16);  arg160_1 = None
        _to_copy_464 = torch.ops.aten._to_copy.default(getitem_644, dtype = torch.bfloat16);  getitem_644 = None
        t_154 = torch.ops.aten.t.default(_to_copy_463);  _to_copy_463 = None
        view_790 = torch.ops.aten.view.default(_to_copy_464, [512, 384]);  _to_copy_464 = None
        mm_143 = torch.ops.aten.mm.default(view_790, t_154);  view_790 = t_154 = None
        view_791 = torch.ops.aten.view.default(mm_143, [1, 512, 1536]);  mm_143 = None
        split_tensor_58 = torch.ops.aten.split.Tensor(view_791, 768, dim = -1);  view_791 = None
        getitem_647 = split_tensor_58[0]
        getitem_648 = split_tensor_58[1];  split_tensor_58 = None
        silu_19 = torch.ops.aten.silu.default(getitem_647);  getitem_647 = None
        mul_76 = torch.ops.aten.mul.Tensor(silu_19, getitem_648);  silu_19 = getitem_648 = None
        _to_copy_465 = torch.ops.aten._to_copy.default(arg161_1, dtype = torch.bfloat16);  arg161_1 = None
        t_155 = torch.ops.aten.t.default(_to_copy_465);  _to_copy_465 = None
        view_793 = torch.ops.aten.view.default(mul_76, [512, 768]);  mul_76 = None
        mm_144 = torch.ops.aten.mm.default(view_793, t_155);  view_793 = t_155 = None
        view_794 = torch.ops.aten.view.default(mm_144, [1, 512, 384]);  mm_144 = None
        add_63 = torch.ops.aten.add.Tensor(add_62, view_794);  add_62 = view_794 = None
        _to_copy_466 = torch.ops.aten._to_copy.default(add_59, dtype = torch.float32)
        native_layer_norm_default_98 = torch.ops.aten.native_layer_norm.default(_to_copy_466, [256], arg174_1, arg175_1, 1e-05);  _to_copy_466 = arg174_1 = arg175_1 = None
        getitem_649 = native_layer_norm_default_98[0]
        split_with_sizes_default_14 = torch.ops.aten.split_with_sizes.default(arg177_1, [512, 512]);  arg177_1 = None
        getitem_652 = split_with_sizes_default_14[0]
        getitem_653 = split_with_sizes_default_14[1];  split_with_sizes_default_14 = None
        split_with_sizes_default_15 = torch.ops.aten.split_with_sizes.default(arg178_1, [512, 512, 256]);  arg178_1 = None
        getitem_654 = split_with_sizes_default_15[0]
        getitem_655 = split_with_sizes_default_15[1]
        getitem_656 = split_with_sizes_default_15[2];  split_with_sizes_default_15 = None
        _to_copy_467 = torch.ops.aten._to_copy.default(getitem_652, dtype = torch.bfloat16);  getitem_652 = None
        _to_copy_468 = torch.ops.aten._to_copy.default(getitem_649, dtype = torch.bfloat16)
        t_156 = torch.ops.aten.t.default(_to_copy_467);  _to_copy_467 = None
        view_795 = torch.ops.aten.view.default(_to_copy_468, [262144, 256]);  _to_copy_468 = None
        mm_145 = torch.ops.aten.mm.default(view_795, t_156);  view_795 = t_156 = None
        view_796 = torch.ops.aten.view.default(mm_145, [1, 512, 512, 512]);  mm_145 = None
        _to_copy_469 = torch.ops.aten._to_copy.default(getitem_654, dtype = torch.bfloat16);  getitem_654 = None
        _to_copy_470 = torch.ops.aten._to_copy.default(getitem_649, dtype = torch.bfloat16)
        t_157 = torch.ops.aten.t.default(_to_copy_469);  _to_copy_469 = None
        view_797 = torch.ops.aten.view.default(_to_copy_470, [262144, 256]);  _to_copy_470 = None
        mm_146 = torch.ops.aten.mm.default(view_797, t_157);  view_797 = t_157 = None
        view_798 = torch.ops.aten.view.default(mm_146, [1, 512, 512, 512]);  mm_146 = None
        sigmoid_48 = torch.ops.aten.sigmoid.default(view_798);  view_798 = None
        mul_77 = torch.ops.aten.mul.Tensor(view_796, sigmoid_48);  view_796 = sigmoid_48 = None
        unsqueeze_350 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_62 = torch.ops.aten.bitwise_not.default(unsqueeze_350);  unsqueeze_350 = None
        masked_fill_62 = torch.ops.aten.masked_fill.Scalar(mul_77, bitwise_not_62, 0);  mul_77 = bitwise_not_62 = None
        split_tensor_59 = torch.ops.aten.split.Tensor(masked_fill_62, 256, dim = -1)
        getitem_659 = split_tensor_59[0]
        unsqueeze_353 = torch.ops.aten.unsqueeze.default(getitem_659, 4);  getitem_659 = None
        permute_507 = torch.ops.aten.permute.default(unsqueeze_353, [0, 1, 4, 3, 2]);  unsqueeze_353 = None
        permute_508 = torch.ops.aten.permute.default(permute_507, [3, 1, 4, 0, 2]);  permute_507 = None
        view_801 = torch.ops.aten.view.default(permute_508, [256, 512, 512]);  permute_508 = None
        split_tensor_60 = torch.ops.aten.split.Tensor(masked_fill_62, 256, dim = -1);  masked_fill_62 = None
        getitem_662 = split_tensor_60[1];  split_tensor_60 = None
        unsqueeze_354 = torch.ops.aten.unsqueeze.default(getitem_662, 4);  getitem_662 = None
        permute_509 = torch.ops.aten.permute.default(unsqueeze_354, [0, 4, 1, 3, 2]);  unsqueeze_354 = None
        permute_510 = torch.ops.aten.permute.default(permute_509, [3, 4, 0, 2, 1]);  permute_509 = None
        view_802 = torch.ops.aten.view.default(permute_510, [256, 512, 512]);  permute_510 = None
        bmm_83 = torch.ops.aten.bmm.default(view_801, view_802);  view_801 = view_802 = None
        view_803 = torch.ops.aten.view.default(bmm_83, [256, 512, 1, 1, 512]);  bmm_83 = None
        permute_511 = torch.ops.aten.permute.default(view_803, [3, 1, 4, 0, 2]);  view_803 = None
        view_804 = torch.ops.aten.view.default(permute_511, [1, 512, 512, 256]);  permute_511 = None
        _to_copy_471 = torch.ops.aten._to_copy.default(getitem_653, dtype = torch.bfloat16);  getitem_653 = None
        _to_copy_472 = torch.ops.aten._to_copy.default(getitem_649, dtype = torch.bfloat16)
        t_158 = torch.ops.aten.t.default(_to_copy_471);  _to_copy_471 = None
        view_805 = torch.ops.aten.view.default(_to_copy_472, [262144, 256]);  _to_copy_472 = None
        mm_147 = torch.ops.aten.mm.default(view_805, t_158);  view_805 = t_158 = None
        view_806 = torch.ops.aten.view.default(mm_147, [1, 512, 512, 512]);  mm_147 = None
        _to_copy_473 = torch.ops.aten._to_copy.default(getitem_655, dtype = torch.bfloat16);  getitem_655 = None
        _to_copy_474 = torch.ops.aten._to_copy.default(getitem_649, dtype = torch.bfloat16)
        t_159 = torch.ops.aten.t.default(_to_copy_473);  _to_copy_473 = None
        view_807 = torch.ops.aten.view.default(_to_copy_474, [262144, 256]);  _to_copy_474 = None
        mm_148 = torch.ops.aten.mm.default(view_807, t_159);  view_807 = t_159 = None
        view_808 = torch.ops.aten.view.default(mm_148, [1, 512, 512, 512]);  mm_148 = None
        sigmoid_49 = torch.ops.aten.sigmoid.default(view_808);  view_808 = None
        mul_78 = torch.ops.aten.mul.Tensor(view_806, sigmoid_49);  view_806 = sigmoid_49 = None
        view_809 = torch.ops.aten.view.default(mul_78, [262144, 512]);  mul_78 = None
        view_810 = torch.ops.aten.view.default(view_809, [1, 512, 512, 512]);  view_809 = None
        transpose_14 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_355 = torch.ops.aten.unsqueeze.default(transpose_14, 3);  transpose_14 = None
        clone_100 = torch.ops.aten.clone.default(unsqueeze_355, memory_format = torch.contiguous_format);  unsqueeze_355 = None
        bitwise_not_63 = torch.ops.aten.bitwise_not.default(clone_100);  clone_100 = None
        masked_fill_63 = torch.ops.aten.masked_fill.Scalar(view_810, bitwise_not_63, 0);  view_810 = bitwise_not_63 = None
        view_811 = torch.ops.aten.view.default(masked_fill_63, [262144, 512]);  masked_fill_63 = None
        view_815 = torch.ops.aten.view.default(view_811, [1, 512, 512, 512])
        split_tensor_61 = torch.ops.aten.split.Tensor(view_815, 256, dim = -1);  view_815 = None
        getitem_665 = split_tensor_61[0]
        unsqueeze_358 = torch.ops.aten.unsqueeze.default(getitem_665, 4);  getitem_665 = None
        permute_516 = torch.ops.aten.permute.default(unsqueeze_358, [0, 2, 4, 3, 1]);  unsqueeze_358 = None
        permute_517 = torch.ops.aten.permute.default(permute_516, [3, 1, 4, 0, 2]);  permute_516 = None
        view_816 = torch.ops.aten.view.default(permute_517, [256, 512, 512]);  permute_517 = None
        view_817 = torch.ops.aten.view.default(view_811, [1, 512, 512, 512]);  view_811 = None
        split_tensor_62 = torch.ops.aten.split.Tensor(view_817, 256, dim = -1);  view_817 = None
        getitem_668 = split_tensor_62[1];  split_tensor_62 = None
        unsqueeze_359 = torch.ops.aten.unsqueeze.default(getitem_668, 4);  getitem_668 = None
        permute_518 = torch.ops.aten.permute.default(unsqueeze_359, [0, 4, 2, 3, 1]);  unsqueeze_359 = None
        permute_519 = torch.ops.aten.permute.default(permute_518, [3, 4, 0, 2, 1]);  permute_518 = None
        view_818 = torch.ops.aten.view.default(permute_519, [256, 512, 512]);  permute_519 = None
        bmm_84 = torch.ops.aten.bmm.default(view_816, view_818);  view_816 = view_818 = None
        view_819 = torch.ops.aten.view.default(bmm_84, [256, 512, 1, 1, 512]);  bmm_84 = None
        permute_520 = torch.ops.aten.permute.default(view_819, [3, 1, 4, 0, 2]);  view_819 = None
        view_820 = torch.ops.aten.view.default(permute_520, [1, 512, 512, 256]);  permute_520 = None
        _to_copy_475 = torch.ops.aten._to_copy.default(view_804, dtype = torch.float32);  view_804 = None
        native_layer_norm_default_99 = torch.ops.aten.native_layer_norm.default(_to_copy_475, [256], None, None, 1e-05);  _to_copy_475 = None
        getitem_669 = native_layer_norm_default_99[0]
        _to_copy_476 = torch.ops.aten._to_copy.default(view_820, dtype = torch.float32);  view_820 = None
        native_layer_norm_default_100 = torch.ops.aten.native_layer_norm.default(_to_copy_476, [256], None, None, 1e-05);  _to_copy_476 = None
        getitem_672 = native_layer_norm_default_100[0]
        add_64 = torch.ops.aten.add.Tensor(getitem_669, getitem_672);  getitem_669 = getitem_672 = None
        _to_copy_477 = torch.ops.aten._to_copy.default(arg176_1, dtype = torch.bfloat16);  arg176_1 = None
        _to_copy_478 = torch.ops.aten._to_copy.default(add_64, dtype = torch.bfloat16);  add_64 = None
        t_160 = torch.ops.aten.t.default(_to_copy_477);  _to_copy_477 = None
        view_821 = torch.ops.aten.view.default(_to_copy_478, [262144, 256]);  _to_copy_478 = None
        mm_149 = torch.ops.aten.mm.default(view_821, t_160);  view_821 = t_160 = None
        view_822 = torch.ops.aten.view.default(mm_149, [1, 512, 512, 256]);  mm_149 = None
        _to_copy_479 = torch.ops.aten._to_copy.default(getitem_656, dtype = torch.bfloat16);  getitem_656 = None
        _to_copy_480 = torch.ops.aten._to_copy.default(getitem_649, dtype = torch.bfloat16);  getitem_649 = None
        t_161 = torch.ops.aten.t.default(_to_copy_479);  _to_copy_479 = None
        view_823 = torch.ops.aten.view.default(_to_copy_480, [262144, 256]);  _to_copy_480 = None
        mm_150 = torch.ops.aten.mm.default(view_823, t_161);  view_823 = t_161 = None
        view_824 = torch.ops.aten.view.default(mm_150, [1, 512, 512, 256]);  mm_150 = None
        sigmoid_50 = torch.ops.aten.sigmoid.default(view_824);  view_824 = None
        mul_79 = torch.ops.aten.mul.Tensor(view_822, sigmoid_50);  view_822 = sigmoid_50 = None
        add_65 = torch.ops.aten.add.Tensor(add_59, mul_79);  mul_79 = None
        _to_copy_481 = torch.ops.aten._to_copy.default(add_59, dtype = torch.float32)
        native_layer_norm_default_101 = torch.ops.aten.native_layer_norm.default(_to_copy_481, [256], None, None, 1e-05);  _to_copy_481 = None
        getitem_675 = native_layer_norm_default_101[0]
        _to_copy_482 = torch.ops.aten._to_copy.default(arg180_1, dtype = torch.bfloat16);  arg180_1 = None
        _to_copy_483 = torch.ops.aten._to_copy.default(getitem_675, dtype = torch.bfloat16)
        t_162 = torch.ops.aten.t.default(_to_copy_482);  _to_copy_482 = None
        view_825 = torch.ops.aten.view.default(_to_copy_483, [262144, 256]);  _to_copy_483 = None
        mm_151 = torch.ops.aten.mm.default(view_825, t_162);  view_825 = t_162 = None
        view_826 = torch.ops.aten.view.default(mm_151, [1, 512, 512, 8]);  mm_151 = None
        view_827 = torch.ops.aten.view.default(view_826, [1, 512, 512, 2, 4]);  view_826 = None
        permute_521 = torch.ops.aten.permute.default(view_827, [0, 3, 4, 1, 2]);  view_827 = None
        view_828 = torch.ops.aten.view.default(permute_521, [1, 2, 4, 1, 512, 512]);  permute_521 = None
        view_829 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_64 = torch.ops.aten.bitwise_not.default(view_829);  view_829 = None
        masked_fill_64 = torch.ops.aten.masked_fill.Scalar(view_828, bitwise_not_64, -10000);  view_828 = bitwise_not_64 = None
        view_830 = torch.ops.aten.view.default(masked_fill_64, [1, 2, 4, 512, 512]);  masked_fill_64 = None
        permute_522 = torch.ops.aten.permute.default(view_830, [1, 0, 2, 3, 4]);  view_830 = None
        view_831 = torch.ops.aten.view.default(permute_522, [2, 4, 1, 512, 512]);  permute_522 = None
        _to_copy_484 = torch.ops.aten._to_copy.default(arg181_1, dtype = torch.bfloat16);  arg181_1 = None
        _to_copy_485 = torch.ops.aten._to_copy.default(getitem_675, dtype = torch.bfloat16)
        t_163 = torch.ops.aten.t.default(_to_copy_484);  _to_copy_484 = None
        view_832 = torch.ops.aten.view.default(_to_copy_485, [262144, 256]);  _to_copy_485 = None
        mm_152 = torch.ops.aten.mm.default(view_832, t_163);  view_832 = t_163 = None
        view_833 = torch.ops.aten.view.default(mm_152, [1, 512, 512, 1024]);  mm_152 = None
        select_15 = torch.ops.aten.select.int(view_831, 0, 0)
        view_834 = torch.ops.aten.view.default(view_833, [1, 512, 512, 4, 4, 64]);  view_833 = None
        permute_523 = torch.ops.aten.permute.default(view_834, [4, 0, 3, 1, 2, 5]);  view_834 = None
        view_835 = torch.ops.aten.view.default(permute_523, [4, 4, 512, 512, 64]);  permute_523 = None
        unbind_int_43 = torch.ops.aten.unbind.int(view_835);  view_835 = None
        getitem_678 = unbind_int_43[0]
        getitem_679 = unbind_int_43[1]
        getitem_680 = unbind_int_43[2]
        getitem_681 = unbind_int_43[3];  unbind_int_43 = None
        expand_32 = torch.ops.aten.expand.default(select_15, [4, 512, 512, 512]);  select_15 = None
        _scaled_dot_product_efficient_attention_default_15 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_678, getitem_679, getitem_680, expand_32, False);  getitem_678 = getitem_679 = getitem_680 = expand_32 = None
        getitem_682 = _scaled_dot_product_efficient_attention_default_15[0]
        sigmoid_51 = torch.ops.aten.sigmoid.default(getitem_681);  getitem_681 = None
        mul_80 = torch.ops.aten.mul.Tensor(getitem_682, sigmoid_51);  getitem_682 = sigmoid_51 = None
        view_836 = torch.ops.aten.view.default(mul_80, [1, 4, 512, 512, 64]);  mul_80 = None
        permute_524 = torch.ops.aten.permute.default(view_836, [0, 2, 3, 1, 4]);  view_836 = None
        clone_101 = torch.ops.aten.clone.default(permute_524, memory_format = torch.contiguous_format);  permute_524 = None
        _unsafe_view_92 = torch.ops.aten._unsafe_view.default(clone_101, [1, 512, 512, 256]);  clone_101 = None
        transpose_15 = torch.ops.aten.transpose.int(getitem_675, 1, 2);  getitem_675 = None
        _to_copy_486 = torch.ops.aten._to_copy.default(arg182_1, dtype = torch.bfloat16);  arg182_1 = None
        _to_copy_487 = torch.ops.aten._to_copy.default(transpose_15, dtype = torch.bfloat16);  transpose_15 = None
        t_164 = torch.ops.aten.t.default(_to_copy_486);  _to_copy_486 = None
        expand_33 = torch.ops.aten.expand.default(_to_copy_487, [1, 512, 512, 256]);  _to_copy_487 = None
        view_837 = torch.ops.aten.view.default(expand_33, [512, 512, 256]);  expand_33 = None
        expand_34 = torch.ops.aten.expand.default(t_164, [1, 512, 256, 1024]);  t_164 = None
        view_838 = torch.ops.aten.view.default(expand_34, [512, 256, 1024]);  expand_34 = None
        bmm_85 = torch.ops.aten.bmm.default(view_837, view_838);  view_837 = view_838 = None
        view_839 = torch.ops.aten.view.default(bmm_85, [1, 512, 512, 1024]);  bmm_85 = None
        select_16 = torch.ops.aten.select.int(view_831, 0, 1);  view_831 = None
        view_840 = torch.ops.aten.view.default(view_839, [1, 512, 512, 4, 4, 64]);  view_839 = None
        permute_525 = torch.ops.aten.permute.default(view_840, [4, 0, 3, 1, 2, 5]);  view_840 = None
        view_841 = torch.ops.aten.view.default(permute_525, [4, 4, 512, 512, 64]);  permute_525 = None
        unbind_int_44 = torch.ops.aten.unbind.int(view_841);  view_841 = None
        getitem_686 = unbind_int_44[0]
        getitem_687 = unbind_int_44[1]
        getitem_688 = unbind_int_44[2]
        getitem_689 = unbind_int_44[3];  unbind_int_44 = None
        expand_35 = torch.ops.aten.expand.default(select_16, [4, 512, 512, 512]);  select_16 = None
        _scaled_dot_product_efficient_attention_default_16 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_686, getitem_687, getitem_688, expand_35, False);  getitem_686 = getitem_687 = getitem_688 = expand_35 = None
        getitem_690 = _scaled_dot_product_efficient_attention_default_16[0]
        sigmoid_52 = torch.ops.aten.sigmoid.default(getitem_689);  getitem_689 = None
        mul_81 = torch.ops.aten.mul.Tensor(getitem_690, sigmoid_52);  getitem_690 = sigmoid_52 = None
        view_842 = torch.ops.aten.view.default(mul_81, [1, 4, 512, 512, 64]);  mul_81 = None
        permute_526 = torch.ops.aten.permute.default(view_842, [0, 2, 3, 1, 4]);  view_842 = None
        clone_102 = torch.ops.aten.clone.default(permute_526, memory_format = torch.contiguous_format);  permute_526 = None
        _unsafe_view_93 = torch.ops.aten._unsafe_view.default(clone_102, [1, 512, 512, 256]);  clone_102 = None
        cat_13 = torch.ops.aten.cat.default([_unsafe_view_92, _unsafe_view_93], dim = -1);  _unsafe_view_92 = _unsafe_view_93 = None
        slice_206 = torch.ops.aten.slice.Tensor(arg179_1, dim = 0, start = 0, end = 9223372036854775807);  arg179_1 = None
        unsqueeze_360 = torch.ops.aten.unsqueeze.default(slice_206, 1);  slice_206 = None
        mul_82 = torch.ops.aten.mul.Tensor(arg183_1, unsqueeze_360);  arg183_1 = unsqueeze_360 = None
        _to_copy_488 = torch.ops.aten._to_copy.default(mul_82, dtype = torch.bfloat16);  mul_82 = None
        t_165 = torch.ops.aten.t.default(_to_copy_488);  _to_copy_488 = None
        view_843 = torch.ops.aten.view.default(cat_13, [262144, 512]);  cat_13 = None
        mm_153 = torch.ops.aten.mm.default(view_843, t_165);  view_843 = t_165 = None
        view_844 = torch.ops.aten.view.default(mm_153, [1, 512, 512, 256]);  mm_153 = None
        add_66 = torch.ops.aten.add.Tensor(add_65, view_844);  add_65 = view_844 = None
        split_tensor_63 = torch.ops.aten.split.Tensor(add_59, 512, dim = -2)
        getitem_694 = split_tensor_63[0];  split_tensor_63 = None
        _to_copy_489 = torch.ops.aten._to_copy.default(getitem_694, dtype = torch.float32);  getitem_694 = None
        native_layer_norm_default_102 = torch.ops.aten.native_layer_norm.default(_to_copy_489, [256], arg170_1, arg171_1, 1e-05);  _to_copy_489 = arg170_1 = arg171_1 = None
        getitem_695 = native_layer_norm_default_102[0]
        _to_copy_490 = torch.ops.aten._to_copy.default(arg172_1, dtype = torch.bfloat16);  arg172_1 = None
        _to_copy_491 = torch.ops.aten._to_copy.default(getitem_695, dtype = torch.bfloat16);  getitem_695 = None
        t_166 = torch.ops.aten.t.default(_to_copy_490);  _to_copy_490 = None
        view_845 = torch.ops.aten.view.default(_to_copy_491, [262144, 256]);  _to_copy_491 = None
        mm_154 = torch.ops.aten.mm.default(view_845, t_166);  view_845 = t_166 = None
        view_846 = torch.ops.aten.view.default(mm_154, [1, 512, 512, 1024]);  mm_154 = None
        split_tensor_64 = torch.ops.aten.split.Tensor(view_846, 512, dim = -1);  view_846 = None
        getitem_698 = split_tensor_64[0]
        getitem_699 = split_tensor_64[1];  split_tensor_64 = None
        silu_20 = torch.ops.aten.silu.default(getitem_698);  getitem_698 = None
        mul_83 = torch.ops.aten.mul.Tensor(silu_20, getitem_699);  silu_20 = getitem_699 = None
        _to_copy_492 = torch.ops.aten._to_copy.default(arg173_1, dtype = torch.bfloat16);  arg173_1 = None
        t_167 = torch.ops.aten.t.default(_to_copy_492);  _to_copy_492 = None
        view_848 = torch.ops.aten.view.default(mul_83, [262144, 512]);  mul_83 = None
        mm_155 = torch.ops.aten.mm.default(view_848, t_167);  view_848 = t_167 = None
        view_849 = torch.ops.aten.view.default(mm_155, [1, 512, 512, 256]);  mm_155 = None
        add_67 = torch.ops.aten.add.Tensor(add_66, view_849);  add_66 = view_849 = None
        _to_copy_493 = torch.ops.aten._to_copy.default(add_63, dtype = torch.float32)
        native_layer_norm_default_103 = torch.ops.aten.native_layer_norm.default(_to_copy_493, [384], arg188_1, arg189_1, 1e-05);  _to_copy_493 = arg188_1 = arg189_1 = None
        getitem_700 = native_layer_norm_default_103[0]
        _to_copy_494 = torch.ops.aten._to_copy.default(add_59, dtype = torch.float32);  add_59 = None
        native_layer_norm_default_104 = torch.ops.aten.native_layer_norm.default(_to_copy_494, [256], arg190_1, arg191_1, 1e-05);  _to_copy_494 = arg190_1 = arg191_1 = None
        getitem_703 = native_layer_norm_default_104[0]
        _to_copy_495 = torch.ops.aten._to_copy.default(arg192_1, dtype = torch.bfloat16);  arg192_1 = None
        _to_copy_496 = torch.ops.aten._to_copy.default(getitem_703, dtype = torch.bfloat16);  getitem_703 = None
        t_168 = torch.ops.aten.t.default(_to_copy_495);  _to_copy_495 = None
        view_850 = torch.ops.aten.view.default(_to_copy_496, [262144, 256]);  _to_copy_496 = None
        mm_156 = torch.ops.aten.mm.default(view_850, t_168);  view_850 = t_168 = None
        view_851 = torch.ops.aten.view.default(mm_156, [1, 512, 512, 16]);  mm_156 = None
        permute_527 = torch.ops.aten.permute.default(view_851, [0, 3, 1, 2]);  view_851 = None
        view_852 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_65 = torch.ops.aten.bitwise_not.default(view_852);  view_852 = None
        masked_fill_65 = torch.ops.aten.masked_fill.Scalar(permute_527, bitwise_not_65, -10000);  permute_527 = bitwise_not_65 = None
        _to_copy_497 = torch.ops.aten._to_copy.default(getitem_700, dtype = torch.bfloat16);  getitem_700 = None
        _to_copy_498 = torch.ops.aten._to_copy.default(arg194_1, dtype = torch.bfloat16);  arg194_1 = None
        unsqueeze_361 = torch.ops.aten.unsqueeze.default(_to_copy_497, 3);  _to_copy_497 = None
        unsqueeze_362 = torch.ops.aten.unsqueeze.default(unsqueeze_361, 4);  unsqueeze_361 = None
        unsqueeze_363 = torch.ops.aten.unsqueeze.default(unsqueeze_362, 5);  unsqueeze_362 = None
        permute_528 = torch.ops.aten.permute.default(unsqueeze_363, [3, 0, 4, 1, 5, 2]);  unsqueeze_363 = None
        unsqueeze_364 = torch.ops.aten.unsqueeze.default(_to_copy_498, 4);  _to_copy_498 = None
        unsqueeze_365 = torch.ops.aten.unsqueeze.default(unsqueeze_364, 5);  unsqueeze_364 = None
        permute_529 = torch.ops.aten.permute.default(unsqueeze_365, [1, 4, 2, 5, 3, 0]);  unsqueeze_365 = None
        permute_530 = torch.ops.aten.permute.default(permute_528, [3, 5, 0, 1, 2, 4]);  permute_528 = None
        view_853 = torch.ops.aten.view.default(permute_530, [1, 512, 384]);  permute_530 = None
        permute_531 = torch.ops.aten.permute.default(permute_529, [5, 0, 1, 2, 4, 3]);  permute_529 = None
        view_854 = torch.ops.aten.view.default(permute_531, [1, 384, 1536]);  permute_531 = None
        bmm_86 = torch.ops.aten.bmm.default(view_853, view_854);  view_853 = view_854 = None
        view_855 = torch.ops.aten.view.default(bmm_86, [512, 1, 4, 1, 16, 24]);  bmm_86 = None
        permute_532 = torch.ops.aten.permute.default(view_855, [2, 3, 4, 0, 5, 1]);  view_855 = None
        view_856 = torch.ops.aten.view.default(permute_532, [4, 1, 16, 512, 24]);  permute_532 = None
        unbind_int_45 = torch.ops.aten.unbind.int(view_856);  view_856 = None
        getitem_706 = unbind_int_45[0]
        getitem_707 = unbind_int_45[1]
        getitem_708 = unbind_int_45[2]
        getitem_709 = unbind_int_45[3];  unbind_int_45 = None
        view_857 = torch.ops.aten.view.default(arg193_1, [1, 16, 1, 24]);  arg193_1 = None
        add_68 = torch.ops.aten.add.Tensor(getitem_706, view_857);  getitem_706 = view_857 = None
        _to_copy_499 = torch.ops.aten._to_copy.default(add_68, dtype = torch.bfloat16);  add_68 = None
        expand_36 = torch.ops.aten.expand.default(masked_fill_65, [1, 16, 512, 512]);  masked_fill_65 = None
        _scaled_dot_product_efficient_attention_default_17 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_499, getitem_707, getitem_708, expand_36, False);  _to_copy_499 = getitem_707 = getitem_708 = expand_36 = None
        getitem_710 = _scaled_dot_product_efficient_attention_default_17[0]
        add_69 = torch.ops.aten.add.Tensor(getitem_709, 1);  getitem_709 = None
        sigmoid_53 = torch.ops.aten.sigmoid.default(add_69);  add_69 = None
        mul_84 = torch.ops.aten.mul.Tensor(getitem_710, sigmoid_53);  getitem_710 = sigmoid_53 = None
        _to_copy_500 = torch.ops.aten._to_copy.default(arg195_1, dtype = torch.bfloat16);  arg195_1 = None
        unsqueeze_366 = torch.ops.aten.unsqueeze.default(mul_84, 4);  mul_84 = None
        permute_533 = torch.ops.aten.permute.default(unsqueeze_366, [0, 2, 4, 3, 1]);  unsqueeze_366 = None
        unsqueeze_367 = torch.ops.aten.unsqueeze.default(_to_copy_500, 3);  _to_copy_500 = None
        unsqueeze_368 = torch.ops.aten.unsqueeze.default(unsqueeze_367, 4);  unsqueeze_367 = None
        permute_534 = torch.ops.aten.permute.default(unsqueeze_368, [3, 4, 2, 1, 0]);  unsqueeze_368 = None
        permute_535 = torch.ops.aten.permute.default(permute_533, [1, 3, 4, 0, 2]);  permute_533 = None
        clone_103 = torch.ops.aten.clone.default(permute_535, memory_format = torch.contiguous_format);  permute_535 = None
        _unsafe_view_94 = torch.ops.aten._unsafe_view.default(clone_103, [1, 512, 384]);  clone_103 = None
        permute_536 = torch.ops.aten.permute.default(permute_534, [3, 4, 0, 2, 1]);  permute_534 = None
        clone_104 = torch.ops.aten.clone.default(permute_536, memory_format = torch.contiguous_format);  permute_536 = None
        _unsafe_view_95 = torch.ops.aten._unsafe_view.default(clone_104, [1, 384, 384]);  clone_104 = None
        bmm_87 = torch.ops.aten.bmm.default(_unsafe_view_94, _unsafe_view_95);  _unsafe_view_94 = _unsafe_view_95 = None
        view_858 = torch.ops.aten.view.default(bmm_87, [512, 1, 1, 1, 384]);  bmm_87 = None
        permute_537 = torch.ops.aten.permute.default(view_858, [3, 0, 4, 1, 2]);  view_858 = None
        view_859 = torch.ops.aten.view.default(permute_537, [1, 512, 384]);  permute_537 = None
        unsqueeze_369 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_85 = torch.ops.aten.mul.Tensor(view_859, unsqueeze_369);  view_859 = unsqueeze_369 = None
        add_70 = torch.ops.aten.add.Tensor(add_63, mul_85);  mul_85 = None
        split_tensor_65 = torch.ops.aten.split.Tensor(add_63, 512, dim = -2);  add_63 = None
        getitem_714 = split_tensor_65[0];  split_tensor_65 = None
        _to_copy_501 = torch.ops.aten._to_copy.default(getitem_714, dtype = torch.float32);  getitem_714 = None
        native_layer_norm_default_105 = torch.ops.aten.native_layer_norm.default(_to_copy_501, [384], arg184_1, arg185_1, 1e-05);  _to_copy_501 = arg184_1 = arg185_1 = None
        getitem_715 = native_layer_norm_default_105[0]
        _to_copy_502 = torch.ops.aten._to_copy.default(arg186_1, dtype = torch.bfloat16);  arg186_1 = None
        _to_copy_503 = torch.ops.aten._to_copy.default(getitem_715, dtype = torch.bfloat16);  getitem_715 = None
        t_169 = torch.ops.aten.t.default(_to_copy_502);  _to_copy_502 = None
        view_860 = torch.ops.aten.view.default(_to_copy_503, [512, 384]);  _to_copy_503 = None
        mm_157 = torch.ops.aten.mm.default(view_860, t_169);  view_860 = t_169 = None
        view_861 = torch.ops.aten.view.default(mm_157, [1, 512, 1536]);  mm_157 = None
        split_tensor_66 = torch.ops.aten.split.Tensor(view_861, 768, dim = -1);  view_861 = None
        getitem_718 = split_tensor_66[0]
        getitem_719 = split_tensor_66[1];  split_tensor_66 = None
        silu_21 = torch.ops.aten.silu.default(getitem_718);  getitem_718 = None
        mul_86 = torch.ops.aten.mul.Tensor(silu_21, getitem_719);  silu_21 = getitem_719 = None
        _to_copy_504 = torch.ops.aten._to_copy.default(arg187_1, dtype = torch.bfloat16);  arg187_1 = None
        t_170 = torch.ops.aten.t.default(_to_copy_504);  _to_copy_504 = None
        view_863 = torch.ops.aten.view.default(mul_86, [512, 768]);  mul_86 = None
        mm_158 = torch.ops.aten.mm.default(view_863, t_170);  view_863 = t_170 = None
        view_864 = torch.ops.aten.view.default(mm_158, [1, 512, 384]);  mm_158 = None
        add_71 = torch.ops.aten.add.Tensor(add_70, view_864);  add_70 = view_864 = None
        _to_copy_505 = torch.ops.aten._to_copy.default(add_67, dtype = torch.float32)
        native_layer_norm_default_106 = torch.ops.aten.native_layer_norm.default(_to_copy_505, [256], arg200_1, arg201_1, 1e-05);  _to_copy_505 = arg200_1 = arg201_1 = None
        getitem_720 = native_layer_norm_default_106[0]
        split_with_sizes_default_16 = torch.ops.aten.split_with_sizes.default(arg203_1, [512, 512]);  arg203_1 = None
        getitem_723 = split_with_sizes_default_16[0]
        getitem_724 = split_with_sizes_default_16[1];  split_with_sizes_default_16 = None
        split_with_sizes_default_17 = torch.ops.aten.split_with_sizes.default(arg204_1, [512, 512, 256]);  arg204_1 = None
        getitem_725 = split_with_sizes_default_17[0]
        getitem_726 = split_with_sizes_default_17[1]
        getitem_727 = split_with_sizes_default_17[2];  split_with_sizes_default_17 = None
        _to_copy_506 = torch.ops.aten._to_copy.default(getitem_723, dtype = torch.bfloat16);  getitem_723 = None
        _to_copy_507 = torch.ops.aten._to_copy.default(getitem_720, dtype = torch.bfloat16)
        t_171 = torch.ops.aten.t.default(_to_copy_506);  _to_copy_506 = None
        view_865 = torch.ops.aten.view.default(_to_copy_507, [262144, 256]);  _to_copy_507 = None
        mm_159 = torch.ops.aten.mm.default(view_865, t_171);  view_865 = t_171 = None
        view_866 = torch.ops.aten.view.default(mm_159, [1, 512, 512, 512]);  mm_159 = None
        _to_copy_508 = torch.ops.aten._to_copy.default(getitem_725, dtype = torch.bfloat16);  getitem_725 = None
        _to_copy_509 = torch.ops.aten._to_copy.default(getitem_720, dtype = torch.bfloat16)
        t_172 = torch.ops.aten.t.default(_to_copy_508);  _to_copy_508 = None
        view_867 = torch.ops.aten.view.default(_to_copy_509, [262144, 256]);  _to_copy_509 = None
        mm_160 = torch.ops.aten.mm.default(view_867, t_172);  view_867 = t_172 = None
        view_868 = torch.ops.aten.view.default(mm_160, [1, 512, 512, 512]);  mm_160 = None
        sigmoid_54 = torch.ops.aten.sigmoid.default(view_868);  view_868 = None
        mul_87 = torch.ops.aten.mul.Tensor(view_866, sigmoid_54);  view_866 = sigmoid_54 = None
        unsqueeze_370 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_66 = torch.ops.aten.bitwise_not.default(unsqueeze_370);  unsqueeze_370 = None
        masked_fill_66 = torch.ops.aten.masked_fill.Scalar(mul_87, bitwise_not_66, 0);  mul_87 = bitwise_not_66 = None
        split_tensor_67 = torch.ops.aten.split.Tensor(masked_fill_66, 256, dim = -1)
        getitem_730 = split_tensor_67[0]
        unsqueeze_373 = torch.ops.aten.unsqueeze.default(getitem_730, 4);  getitem_730 = None
        permute_542 = torch.ops.aten.permute.default(unsqueeze_373, [0, 1, 4, 3, 2]);  unsqueeze_373 = None
        permute_543 = torch.ops.aten.permute.default(permute_542, [3, 1, 4, 0, 2]);  permute_542 = None
        view_871 = torch.ops.aten.view.default(permute_543, [256, 512, 512]);  permute_543 = None
        split_tensor_68 = torch.ops.aten.split.Tensor(masked_fill_66, 256, dim = -1);  masked_fill_66 = None
        getitem_733 = split_tensor_68[1];  split_tensor_68 = None
        unsqueeze_374 = torch.ops.aten.unsqueeze.default(getitem_733, 4);  getitem_733 = None
        permute_544 = torch.ops.aten.permute.default(unsqueeze_374, [0, 4, 1, 3, 2]);  unsqueeze_374 = None
        permute_545 = torch.ops.aten.permute.default(permute_544, [3, 4, 0, 2, 1]);  permute_544 = None
        view_872 = torch.ops.aten.view.default(permute_545, [256, 512, 512]);  permute_545 = None
        bmm_88 = torch.ops.aten.bmm.default(view_871, view_872);  view_871 = view_872 = None
        view_873 = torch.ops.aten.view.default(bmm_88, [256, 512, 1, 1, 512]);  bmm_88 = None
        permute_546 = torch.ops.aten.permute.default(view_873, [3, 1, 4, 0, 2]);  view_873 = None
        view_874 = torch.ops.aten.view.default(permute_546, [1, 512, 512, 256]);  permute_546 = None
        _to_copy_510 = torch.ops.aten._to_copy.default(getitem_724, dtype = torch.bfloat16);  getitem_724 = None
        _to_copy_511 = torch.ops.aten._to_copy.default(getitem_720, dtype = torch.bfloat16)
        t_173 = torch.ops.aten.t.default(_to_copy_510);  _to_copy_510 = None
        view_875 = torch.ops.aten.view.default(_to_copy_511, [262144, 256]);  _to_copy_511 = None
        mm_161 = torch.ops.aten.mm.default(view_875, t_173);  view_875 = t_173 = None
        view_876 = torch.ops.aten.view.default(mm_161, [1, 512, 512, 512]);  mm_161 = None
        _to_copy_512 = torch.ops.aten._to_copy.default(getitem_726, dtype = torch.bfloat16);  getitem_726 = None
        _to_copy_513 = torch.ops.aten._to_copy.default(getitem_720, dtype = torch.bfloat16)
        t_174 = torch.ops.aten.t.default(_to_copy_512);  _to_copy_512 = None
        view_877 = torch.ops.aten.view.default(_to_copy_513, [262144, 256]);  _to_copy_513 = None
        mm_162 = torch.ops.aten.mm.default(view_877, t_174);  view_877 = t_174 = None
        view_878 = torch.ops.aten.view.default(mm_162, [1, 512, 512, 512]);  mm_162 = None
        sigmoid_55 = torch.ops.aten.sigmoid.default(view_878);  view_878 = None
        mul_88 = torch.ops.aten.mul.Tensor(view_876, sigmoid_55);  view_876 = sigmoid_55 = None
        view_879 = torch.ops.aten.view.default(mul_88, [262144, 512]);  mul_88 = None
        view_880 = torch.ops.aten.view.default(view_879, [1, 512, 512, 512]);  view_879 = None
        transpose_16 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_375 = torch.ops.aten.unsqueeze.default(transpose_16, 3);  transpose_16 = None
        clone_105 = torch.ops.aten.clone.default(unsqueeze_375, memory_format = torch.contiguous_format);  unsqueeze_375 = None
        bitwise_not_67 = torch.ops.aten.bitwise_not.default(clone_105);  clone_105 = None
        masked_fill_67 = torch.ops.aten.masked_fill.Scalar(view_880, bitwise_not_67, 0);  view_880 = bitwise_not_67 = None
        view_881 = torch.ops.aten.view.default(masked_fill_67, [262144, 512]);  masked_fill_67 = None
        view_885 = torch.ops.aten.view.default(view_881, [1, 512, 512, 512])
        split_tensor_69 = torch.ops.aten.split.Tensor(view_885, 256, dim = -1);  view_885 = None
        getitem_736 = split_tensor_69[0]
        unsqueeze_378 = torch.ops.aten.unsqueeze.default(getitem_736, 4);  getitem_736 = None
        permute_551 = torch.ops.aten.permute.default(unsqueeze_378, [0, 2, 4, 3, 1]);  unsqueeze_378 = None
        permute_552 = torch.ops.aten.permute.default(permute_551, [3, 1, 4, 0, 2]);  permute_551 = None
        view_886 = torch.ops.aten.view.default(permute_552, [256, 512, 512]);  permute_552 = None
        view_887 = torch.ops.aten.view.default(view_881, [1, 512, 512, 512]);  view_881 = None
        split_tensor_70 = torch.ops.aten.split.Tensor(view_887, 256, dim = -1);  view_887 = None
        getitem_739 = split_tensor_70[1];  split_tensor_70 = None
        unsqueeze_379 = torch.ops.aten.unsqueeze.default(getitem_739, 4);  getitem_739 = None
        permute_553 = torch.ops.aten.permute.default(unsqueeze_379, [0, 4, 2, 3, 1]);  unsqueeze_379 = None
        permute_554 = torch.ops.aten.permute.default(permute_553, [3, 4, 0, 2, 1]);  permute_553 = None
        view_888 = torch.ops.aten.view.default(permute_554, [256, 512, 512]);  permute_554 = None
        bmm_89 = torch.ops.aten.bmm.default(view_886, view_888);  view_886 = view_888 = None
        view_889 = torch.ops.aten.view.default(bmm_89, [256, 512, 1, 1, 512]);  bmm_89 = None
        permute_555 = torch.ops.aten.permute.default(view_889, [3, 1, 4, 0, 2]);  view_889 = None
        view_890 = torch.ops.aten.view.default(permute_555, [1, 512, 512, 256]);  permute_555 = None
        _to_copy_514 = torch.ops.aten._to_copy.default(view_874, dtype = torch.float32);  view_874 = None
        native_layer_norm_default_107 = torch.ops.aten.native_layer_norm.default(_to_copy_514, [256], None, None, 1e-05);  _to_copy_514 = None
        getitem_740 = native_layer_norm_default_107[0]
        _to_copy_515 = torch.ops.aten._to_copy.default(view_890, dtype = torch.float32);  view_890 = None
        native_layer_norm_default_108 = torch.ops.aten.native_layer_norm.default(_to_copy_515, [256], None, None, 1e-05);  _to_copy_515 = None
        getitem_743 = native_layer_norm_default_108[0]
        add_72 = torch.ops.aten.add.Tensor(getitem_740, getitem_743);  getitem_740 = getitem_743 = None
        _to_copy_516 = torch.ops.aten._to_copy.default(arg202_1, dtype = torch.bfloat16);  arg202_1 = None
        _to_copy_517 = torch.ops.aten._to_copy.default(add_72, dtype = torch.bfloat16);  add_72 = None
        t_175 = torch.ops.aten.t.default(_to_copy_516);  _to_copy_516 = None
        view_891 = torch.ops.aten.view.default(_to_copy_517, [262144, 256]);  _to_copy_517 = None
        mm_163 = torch.ops.aten.mm.default(view_891, t_175);  view_891 = t_175 = None
        view_892 = torch.ops.aten.view.default(mm_163, [1, 512, 512, 256]);  mm_163 = None
        _to_copy_518 = torch.ops.aten._to_copy.default(getitem_727, dtype = torch.bfloat16);  getitem_727 = None
        _to_copy_519 = torch.ops.aten._to_copy.default(getitem_720, dtype = torch.bfloat16);  getitem_720 = None
        t_176 = torch.ops.aten.t.default(_to_copy_518);  _to_copy_518 = None
        view_893 = torch.ops.aten.view.default(_to_copy_519, [262144, 256]);  _to_copy_519 = None
        mm_164 = torch.ops.aten.mm.default(view_893, t_176);  view_893 = t_176 = None
        view_894 = torch.ops.aten.view.default(mm_164, [1, 512, 512, 256]);  mm_164 = None
        sigmoid_56 = torch.ops.aten.sigmoid.default(view_894);  view_894 = None
        mul_89 = torch.ops.aten.mul.Tensor(view_892, sigmoid_56);  view_892 = sigmoid_56 = None
        add_73 = torch.ops.aten.add.Tensor(add_67, mul_89);  mul_89 = None
        _to_copy_520 = torch.ops.aten._to_copy.default(add_67, dtype = torch.float32)
        native_layer_norm_default_109 = torch.ops.aten.native_layer_norm.default(_to_copy_520, [256], None, None, 1e-05);  _to_copy_520 = None
        getitem_746 = native_layer_norm_default_109[0]
        _to_copy_521 = torch.ops.aten._to_copy.default(arg206_1, dtype = torch.bfloat16);  arg206_1 = None
        _to_copy_522 = torch.ops.aten._to_copy.default(getitem_746, dtype = torch.bfloat16)
        t_177 = torch.ops.aten.t.default(_to_copy_521);  _to_copy_521 = None
        view_895 = torch.ops.aten.view.default(_to_copy_522, [262144, 256]);  _to_copy_522 = None
        mm_165 = torch.ops.aten.mm.default(view_895, t_177);  view_895 = t_177 = None
        view_896 = torch.ops.aten.view.default(mm_165, [1, 512, 512, 8]);  mm_165 = None
        view_897 = torch.ops.aten.view.default(view_896, [1, 512, 512, 2, 4]);  view_896 = None
        permute_556 = torch.ops.aten.permute.default(view_897, [0, 3, 4, 1, 2]);  view_897 = None
        view_898 = torch.ops.aten.view.default(permute_556, [1, 2, 4, 1, 512, 512]);  permute_556 = None
        view_899 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_68 = torch.ops.aten.bitwise_not.default(view_899);  view_899 = None
        masked_fill_68 = torch.ops.aten.masked_fill.Scalar(view_898, bitwise_not_68, -10000);  view_898 = bitwise_not_68 = None
        view_900 = torch.ops.aten.view.default(masked_fill_68, [1, 2, 4, 512, 512]);  masked_fill_68 = None
        permute_557 = torch.ops.aten.permute.default(view_900, [1, 0, 2, 3, 4]);  view_900 = None
        view_901 = torch.ops.aten.view.default(permute_557, [2, 4, 1, 512, 512]);  permute_557 = None
        _to_copy_523 = torch.ops.aten._to_copy.default(arg207_1, dtype = torch.bfloat16);  arg207_1 = None
        _to_copy_524 = torch.ops.aten._to_copy.default(getitem_746, dtype = torch.bfloat16)
        t_178 = torch.ops.aten.t.default(_to_copy_523);  _to_copy_523 = None
        view_902 = torch.ops.aten.view.default(_to_copy_524, [262144, 256]);  _to_copy_524 = None
        mm_166 = torch.ops.aten.mm.default(view_902, t_178);  view_902 = t_178 = None
        view_903 = torch.ops.aten.view.default(mm_166, [1, 512, 512, 1024]);  mm_166 = None
        select_17 = torch.ops.aten.select.int(view_901, 0, 0)
        view_904 = torch.ops.aten.view.default(view_903, [1, 512, 512, 4, 4, 64]);  view_903 = None
        permute_558 = torch.ops.aten.permute.default(view_904, [4, 0, 3, 1, 2, 5]);  view_904 = None
        view_905 = torch.ops.aten.view.default(permute_558, [4, 4, 512, 512, 64]);  permute_558 = None
        unbind_int_46 = torch.ops.aten.unbind.int(view_905);  view_905 = None
        getitem_749 = unbind_int_46[0]
        getitem_750 = unbind_int_46[1]
        getitem_751 = unbind_int_46[2]
        getitem_752 = unbind_int_46[3];  unbind_int_46 = None
        expand_37 = torch.ops.aten.expand.default(select_17, [4, 512, 512, 512]);  select_17 = None
        _scaled_dot_product_efficient_attention_default_18 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_749, getitem_750, getitem_751, expand_37, False);  getitem_749 = getitem_750 = getitem_751 = expand_37 = None
        getitem_753 = _scaled_dot_product_efficient_attention_default_18[0]
        sigmoid_57 = torch.ops.aten.sigmoid.default(getitem_752);  getitem_752 = None
        mul_90 = torch.ops.aten.mul.Tensor(getitem_753, sigmoid_57);  getitem_753 = sigmoid_57 = None
        view_906 = torch.ops.aten.view.default(mul_90, [1, 4, 512, 512, 64]);  mul_90 = None
        permute_559 = torch.ops.aten.permute.default(view_906, [0, 2, 3, 1, 4]);  view_906 = None
        clone_106 = torch.ops.aten.clone.default(permute_559, memory_format = torch.contiguous_format);  permute_559 = None
        _unsafe_view_96 = torch.ops.aten._unsafe_view.default(clone_106, [1, 512, 512, 256]);  clone_106 = None
        transpose_17 = torch.ops.aten.transpose.int(getitem_746, 1, 2);  getitem_746 = None
        _to_copy_525 = torch.ops.aten._to_copy.default(arg208_1, dtype = torch.bfloat16);  arg208_1 = None
        _to_copy_526 = torch.ops.aten._to_copy.default(transpose_17, dtype = torch.bfloat16);  transpose_17 = None
        t_179 = torch.ops.aten.t.default(_to_copy_525);  _to_copy_525 = None
        expand_38 = torch.ops.aten.expand.default(_to_copy_526, [1, 512, 512, 256]);  _to_copy_526 = None
        view_907 = torch.ops.aten.view.default(expand_38, [512, 512, 256]);  expand_38 = None
        expand_39 = torch.ops.aten.expand.default(t_179, [1, 512, 256, 1024]);  t_179 = None
        view_908 = torch.ops.aten.view.default(expand_39, [512, 256, 1024]);  expand_39 = None
        bmm_90 = torch.ops.aten.bmm.default(view_907, view_908);  view_907 = view_908 = None
        view_909 = torch.ops.aten.view.default(bmm_90, [1, 512, 512, 1024]);  bmm_90 = None
        select_18 = torch.ops.aten.select.int(view_901, 0, 1);  view_901 = None
        view_910 = torch.ops.aten.view.default(view_909, [1, 512, 512, 4, 4, 64]);  view_909 = None
        permute_560 = torch.ops.aten.permute.default(view_910, [4, 0, 3, 1, 2, 5]);  view_910 = None
        view_911 = torch.ops.aten.view.default(permute_560, [4, 4, 512, 512, 64]);  permute_560 = None
        unbind_int_47 = torch.ops.aten.unbind.int(view_911);  view_911 = None
        getitem_757 = unbind_int_47[0]
        getitem_758 = unbind_int_47[1]
        getitem_759 = unbind_int_47[2]
        getitem_760 = unbind_int_47[3];  unbind_int_47 = None
        expand_40 = torch.ops.aten.expand.default(select_18, [4, 512, 512, 512]);  select_18 = None
        _scaled_dot_product_efficient_attention_default_19 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_757, getitem_758, getitem_759, expand_40, False);  getitem_757 = getitem_758 = getitem_759 = expand_40 = None
        getitem_761 = _scaled_dot_product_efficient_attention_default_19[0]
        sigmoid_58 = torch.ops.aten.sigmoid.default(getitem_760);  getitem_760 = None
        mul_91 = torch.ops.aten.mul.Tensor(getitem_761, sigmoid_58);  getitem_761 = sigmoid_58 = None
        view_912 = torch.ops.aten.view.default(mul_91, [1, 4, 512, 512, 64]);  mul_91 = None
        permute_561 = torch.ops.aten.permute.default(view_912, [0, 2, 3, 1, 4]);  view_912 = None
        clone_107 = torch.ops.aten.clone.default(permute_561, memory_format = torch.contiguous_format);  permute_561 = None
        _unsafe_view_97 = torch.ops.aten._unsafe_view.default(clone_107, [1, 512, 512, 256]);  clone_107 = None
        cat_14 = torch.ops.aten.cat.default([_unsafe_view_96, _unsafe_view_97], dim = -1);  _unsafe_view_96 = _unsafe_view_97 = None
        slice_207 = torch.ops.aten.slice.Tensor(arg205_1, dim = 0, start = 0, end = 9223372036854775807);  arg205_1 = None
        unsqueeze_380 = torch.ops.aten.unsqueeze.default(slice_207, 1);  slice_207 = None
        mul_92 = torch.ops.aten.mul.Tensor(arg209_1, unsqueeze_380);  arg209_1 = unsqueeze_380 = None
        _to_copy_527 = torch.ops.aten._to_copy.default(mul_92, dtype = torch.bfloat16);  mul_92 = None
        t_180 = torch.ops.aten.t.default(_to_copy_527);  _to_copy_527 = None
        view_913 = torch.ops.aten.view.default(cat_14, [262144, 512]);  cat_14 = None
        mm_167 = torch.ops.aten.mm.default(view_913, t_180);  view_913 = t_180 = None
        view_914 = torch.ops.aten.view.default(mm_167, [1, 512, 512, 256]);  mm_167 = None
        add_74 = torch.ops.aten.add.Tensor(add_73, view_914);  add_73 = view_914 = None
        split_tensor_71 = torch.ops.aten.split.Tensor(add_67, 512, dim = -2)
        getitem_765 = split_tensor_71[0];  split_tensor_71 = None
        _to_copy_528 = torch.ops.aten._to_copy.default(getitem_765, dtype = torch.float32);  getitem_765 = None
        native_layer_norm_default_110 = torch.ops.aten.native_layer_norm.default(_to_copy_528, [256], arg196_1, arg197_1, 1e-05);  _to_copy_528 = arg196_1 = arg197_1 = None
        getitem_766 = native_layer_norm_default_110[0]
        _to_copy_529 = torch.ops.aten._to_copy.default(arg198_1, dtype = torch.bfloat16);  arg198_1 = None
        _to_copy_530 = torch.ops.aten._to_copy.default(getitem_766, dtype = torch.bfloat16);  getitem_766 = None
        t_181 = torch.ops.aten.t.default(_to_copy_529);  _to_copy_529 = None
        view_915 = torch.ops.aten.view.default(_to_copy_530, [262144, 256]);  _to_copy_530 = None
        mm_168 = torch.ops.aten.mm.default(view_915, t_181);  view_915 = t_181 = None
        view_916 = torch.ops.aten.view.default(mm_168, [1, 512, 512, 1024]);  mm_168 = None
        split_tensor_72 = torch.ops.aten.split.Tensor(view_916, 512, dim = -1);  view_916 = None
        getitem_769 = split_tensor_72[0]
        getitem_770 = split_tensor_72[1];  split_tensor_72 = None
        silu_22 = torch.ops.aten.silu.default(getitem_769);  getitem_769 = None
        mul_93 = torch.ops.aten.mul.Tensor(silu_22, getitem_770);  silu_22 = getitem_770 = None
        _to_copy_531 = torch.ops.aten._to_copy.default(arg199_1, dtype = torch.bfloat16);  arg199_1 = None
        t_182 = torch.ops.aten.t.default(_to_copy_531);  _to_copy_531 = None
        view_918 = torch.ops.aten.view.default(mul_93, [262144, 512]);  mul_93 = None
        mm_169 = torch.ops.aten.mm.default(view_918, t_182);  view_918 = t_182 = None
        view_919 = torch.ops.aten.view.default(mm_169, [1, 512, 512, 256]);  mm_169 = None
        add_75 = torch.ops.aten.add.Tensor(add_74, view_919);  add_74 = view_919 = None
        _to_copy_532 = torch.ops.aten._to_copy.default(add_71, dtype = torch.float32)
        native_layer_norm_default_111 = torch.ops.aten.native_layer_norm.default(_to_copy_532, [384], arg214_1, arg215_1, 1e-05);  _to_copy_532 = arg214_1 = arg215_1 = None
        getitem_771 = native_layer_norm_default_111[0]
        _to_copy_533 = torch.ops.aten._to_copy.default(add_67, dtype = torch.float32);  add_67 = None
        native_layer_norm_default_112 = torch.ops.aten.native_layer_norm.default(_to_copy_533, [256], arg216_1, arg217_1, 1e-05);  _to_copy_533 = arg216_1 = arg217_1 = None
        getitem_774 = native_layer_norm_default_112[0]
        _to_copy_534 = torch.ops.aten._to_copy.default(arg218_1, dtype = torch.bfloat16);  arg218_1 = None
        _to_copy_535 = torch.ops.aten._to_copy.default(getitem_774, dtype = torch.bfloat16);  getitem_774 = None
        t_183 = torch.ops.aten.t.default(_to_copy_534);  _to_copy_534 = None
        view_920 = torch.ops.aten.view.default(_to_copy_535, [262144, 256]);  _to_copy_535 = None
        mm_170 = torch.ops.aten.mm.default(view_920, t_183);  view_920 = t_183 = None
        view_921 = torch.ops.aten.view.default(mm_170, [1, 512, 512, 16]);  mm_170 = None
        permute_562 = torch.ops.aten.permute.default(view_921, [0, 3, 1, 2]);  view_921 = None
        view_922 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_69 = torch.ops.aten.bitwise_not.default(view_922);  view_922 = None
        masked_fill_69 = torch.ops.aten.masked_fill.Scalar(permute_562, bitwise_not_69, -10000);  permute_562 = bitwise_not_69 = None
        _to_copy_536 = torch.ops.aten._to_copy.default(getitem_771, dtype = torch.bfloat16);  getitem_771 = None
        _to_copy_537 = torch.ops.aten._to_copy.default(arg220_1, dtype = torch.bfloat16);  arg220_1 = None
        unsqueeze_381 = torch.ops.aten.unsqueeze.default(_to_copy_536, 3);  _to_copy_536 = None
        unsqueeze_382 = torch.ops.aten.unsqueeze.default(unsqueeze_381, 4);  unsqueeze_381 = None
        unsqueeze_383 = torch.ops.aten.unsqueeze.default(unsqueeze_382, 5);  unsqueeze_382 = None
        permute_563 = torch.ops.aten.permute.default(unsqueeze_383, [3, 0, 4, 1, 5, 2]);  unsqueeze_383 = None
        unsqueeze_384 = torch.ops.aten.unsqueeze.default(_to_copy_537, 4);  _to_copy_537 = None
        unsqueeze_385 = torch.ops.aten.unsqueeze.default(unsqueeze_384, 5);  unsqueeze_384 = None
        permute_564 = torch.ops.aten.permute.default(unsqueeze_385, [1, 4, 2, 5, 3, 0]);  unsqueeze_385 = None
        permute_565 = torch.ops.aten.permute.default(permute_563, [3, 5, 0, 1, 2, 4]);  permute_563 = None
        view_923 = torch.ops.aten.view.default(permute_565, [1, 512, 384]);  permute_565 = None
        permute_566 = torch.ops.aten.permute.default(permute_564, [5, 0, 1, 2, 4, 3]);  permute_564 = None
        view_924 = torch.ops.aten.view.default(permute_566, [1, 384, 1536]);  permute_566 = None
        bmm_91 = torch.ops.aten.bmm.default(view_923, view_924);  view_923 = view_924 = None
        view_925 = torch.ops.aten.view.default(bmm_91, [512, 1, 4, 1, 16, 24]);  bmm_91 = None
        permute_567 = torch.ops.aten.permute.default(view_925, [2, 3, 4, 0, 5, 1]);  view_925 = None
        view_926 = torch.ops.aten.view.default(permute_567, [4, 1, 16, 512, 24]);  permute_567 = None
        unbind_int_48 = torch.ops.aten.unbind.int(view_926);  view_926 = None
        getitem_777 = unbind_int_48[0]
        getitem_778 = unbind_int_48[1]
        getitem_779 = unbind_int_48[2]
        getitem_780 = unbind_int_48[3];  unbind_int_48 = None
        view_927 = torch.ops.aten.view.default(arg219_1, [1, 16, 1, 24]);  arg219_1 = None
        add_76 = torch.ops.aten.add.Tensor(getitem_777, view_927);  getitem_777 = view_927 = None
        _to_copy_538 = torch.ops.aten._to_copy.default(add_76, dtype = torch.bfloat16);  add_76 = None
        expand_41 = torch.ops.aten.expand.default(masked_fill_69, [1, 16, 512, 512]);  masked_fill_69 = None
        _scaled_dot_product_efficient_attention_default_20 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_538, getitem_778, getitem_779, expand_41, False);  _to_copy_538 = getitem_778 = getitem_779 = expand_41 = None
        getitem_781 = _scaled_dot_product_efficient_attention_default_20[0]
        add_77 = torch.ops.aten.add.Tensor(getitem_780, 1);  getitem_780 = None
        sigmoid_59 = torch.ops.aten.sigmoid.default(add_77);  add_77 = None
        mul_94 = torch.ops.aten.mul.Tensor(getitem_781, sigmoid_59);  getitem_781 = sigmoid_59 = None
        _to_copy_539 = torch.ops.aten._to_copy.default(arg221_1, dtype = torch.bfloat16);  arg221_1 = None
        unsqueeze_386 = torch.ops.aten.unsqueeze.default(mul_94, 4);  mul_94 = None
        permute_568 = torch.ops.aten.permute.default(unsqueeze_386, [0, 2, 4, 3, 1]);  unsqueeze_386 = None
        unsqueeze_387 = torch.ops.aten.unsqueeze.default(_to_copy_539, 3);  _to_copy_539 = None
        unsqueeze_388 = torch.ops.aten.unsqueeze.default(unsqueeze_387, 4);  unsqueeze_387 = None
        permute_569 = torch.ops.aten.permute.default(unsqueeze_388, [3, 4, 2, 1, 0]);  unsqueeze_388 = None
        permute_570 = torch.ops.aten.permute.default(permute_568, [1, 3, 4, 0, 2]);  permute_568 = None
        clone_108 = torch.ops.aten.clone.default(permute_570, memory_format = torch.contiguous_format);  permute_570 = None
        _unsafe_view_98 = torch.ops.aten._unsafe_view.default(clone_108, [1, 512, 384]);  clone_108 = None
        permute_571 = torch.ops.aten.permute.default(permute_569, [3, 4, 0, 2, 1]);  permute_569 = None
        clone_109 = torch.ops.aten.clone.default(permute_571, memory_format = torch.contiguous_format);  permute_571 = None
        _unsafe_view_99 = torch.ops.aten._unsafe_view.default(clone_109, [1, 384, 384]);  clone_109 = None
        bmm_92 = torch.ops.aten.bmm.default(_unsafe_view_98, _unsafe_view_99);  _unsafe_view_98 = _unsafe_view_99 = None
        view_928 = torch.ops.aten.view.default(bmm_92, [512, 1, 1, 1, 384]);  bmm_92 = None
        permute_572 = torch.ops.aten.permute.default(view_928, [3, 0, 4, 1, 2]);  view_928 = None
        view_929 = torch.ops.aten.view.default(permute_572, [1, 512, 384]);  permute_572 = None
        unsqueeze_389 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_95 = torch.ops.aten.mul.Tensor(view_929, unsqueeze_389);  view_929 = unsqueeze_389 = None
        add_78 = torch.ops.aten.add.Tensor(add_71, mul_95);  mul_95 = None
        split_tensor_73 = torch.ops.aten.split.Tensor(add_71, 512, dim = -2);  add_71 = None
        getitem_785 = split_tensor_73[0];  split_tensor_73 = None
        _to_copy_540 = torch.ops.aten._to_copy.default(getitem_785, dtype = torch.float32);  getitem_785 = None
        native_layer_norm_default_113 = torch.ops.aten.native_layer_norm.default(_to_copy_540, [384], arg210_1, arg211_1, 1e-05);  _to_copy_540 = arg210_1 = arg211_1 = None
        getitem_786 = native_layer_norm_default_113[0]
        _to_copy_541 = torch.ops.aten._to_copy.default(arg212_1, dtype = torch.bfloat16);  arg212_1 = None
        _to_copy_542 = torch.ops.aten._to_copy.default(getitem_786, dtype = torch.bfloat16);  getitem_786 = None
        t_184 = torch.ops.aten.t.default(_to_copy_541);  _to_copy_541 = None
        view_930 = torch.ops.aten.view.default(_to_copy_542, [512, 384]);  _to_copy_542 = None
        mm_171 = torch.ops.aten.mm.default(view_930, t_184);  view_930 = t_184 = None
        view_931 = torch.ops.aten.view.default(mm_171, [1, 512, 1536]);  mm_171 = None
        split_tensor_74 = torch.ops.aten.split.Tensor(view_931, 768, dim = -1);  view_931 = None
        getitem_789 = split_tensor_74[0]
        getitem_790 = split_tensor_74[1];  split_tensor_74 = None
        silu_23 = torch.ops.aten.silu.default(getitem_789);  getitem_789 = None
        mul_96 = torch.ops.aten.mul.Tensor(silu_23, getitem_790);  silu_23 = getitem_790 = None
        _to_copy_543 = torch.ops.aten._to_copy.default(arg213_1, dtype = torch.bfloat16);  arg213_1 = None
        t_185 = torch.ops.aten.t.default(_to_copy_543);  _to_copy_543 = None
        view_933 = torch.ops.aten.view.default(mul_96, [512, 768]);  mul_96 = None
        mm_172 = torch.ops.aten.mm.default(view_933, t_185);  view_933 = t_185 = None
        view_934 = torch.ops.aten.view.default(mm_172, [1, 512, 384]);  mm_172 = None
        add_79 = torch.ops.aten.add.Tensor(add_78, view_934);  add_78 = view_934 = None
        _to_copy_544 = torch.ops.aten._to_copy.default(add_75, dtype = torch.float32)
        native_layer_norm_default_114 = torch.ops.aten.native_layer_norm.default(_to_copy_544, [256], arg226_1, arg227_1, 1e-05);  _to_copy_544 = arg226_1 = arg227_1 = None
        getitem_791 = native_layer_norm_default_114[0]
        split_with_sizes_default_18 = torch.ops.aten.split_with_sizes.default(arg229_1, [512, 512]);  arg229_1 = None
        getitem_794 = split_with_sizes_default_18[0]
        getitem_795 = split_with_sizes_default_18[1];  split_with_sizes_default_18 = None
        split_with_sizes_default_19 = torch.ops.aten.split_with_sizes.default(arg230_1, [512, 512, 256]);  arg230_1 = None
        getitem_796 = split_with_sizes_default_19[0]
        getitem_797 = split_with_sizes_default_19[1]
        getitem_798 = split_with_sizes_default_19[2];  split_with_sizes_default_19 = None
        _to_copy_545 = torch.ops.aten._to_copy.default(getitem_794, dtype = torch.bfloat16);  getitem_794 = None
        _to_copy_546 = torch.ops.aten._to_copy.default(getitem_791, dtype = torch.bfloat16)
        t_186 = torch.ops.aten.t.default(_to_copy_545);  _to_copy_545 = None
        view_935 = torch.ops.aten.view.default(_to_copy_546, [262144, 256]);  _to_copy_546 = None
        mm_173 = torch.ops.aten.mm.default(view_935, t_186);  view_935 = t_186 = None
        view_936 = torch.ops.aten.view.default(mm_173, [1, 512, 512, 512]);  mm_173 = None
        _to_copy_547 = torch.ops.aten._to_copy.default(getitem_796, dtype = torch.bfloat16);  getitem_796 = None
        _to_copy_548 = torch.ops.aten._to_copy.default(getitem_791, dtype = torch.bfloat16)
        t_187 = torch.ops.aten.t.default(_to_copy_547);  _to_copy_547 = None
        view_937 = torch.ops.aten.view.default(_to_copy_548, [262144, 256]);  _to_copy_548 = None
        mm_174 = torch.ops.aten.mm.default(view_937, t_187);  view_937 = t_187 = None
        view_938 = torch.ops.aten.view.default(mm_174, [1, 512, 512, 512]);  mm_174 = None
        sigmoid_60 = torch.ops.aten.sigmoid.default(view_938);  view_938 = None
        mul_97 = torch.ops.aten.mul.Tensor(view_936, sigmoid_60);  view_936 = sigmoid_60 = None
        unsqueeze_390 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_70 = torch.ops.aten.bitwise_not.default(unsqueeze_390);  unsqueeze_390 = None
        masked_fill_70 = torch.ops.aten.masked_fill.Scalar(mul_97, bitwise_not_70, 0);  mul_97 = bitwise_not_70 = None
        split_tensor_75 = torch.ops.aten.split.Tensor(masked_fill_70, 256, dim = -1)
        getitem_801 = split_tensor_75[0]
        unsqueeze_393 = torch.ops.aten.unsqueeze.default(getitem_801, 4);  getitem_801 = None
        permute_577 = torch.ops.aten.permute.default(unsqueeze_393, [0, 1, 4, 3, 2]);  unsqueeze_393 = None
        permute_578 = torch.ops.aten.permute.default(permute_577, [3, 1, 4, 0, 2]);  permute_577 = None
        view_941 = torch.ops.aten.view.default(permute_578, [256, 512, 512]);  permute_578 = None
        split_tensor_76 = torch.ops.aten.split.Tensor(masked_fill_70, 256, dim = -1);  masked_fill_70 = None
        getitem_804 = split_tensor_76[1];  split_tensor_76 = None
        unsqueeze_394 = torch.ops.aten.unsqueeze.default(getitem_804, 4);  getitem_804 = None
        permute_579 = torch.ops.aten.permute.default(unsqueeze_394, [0, 4, 1, 3, 2]);  unsqueeze_394 = None
        permute_580 = torch.ops.aten.permute.default(permute_579, [3, 4, 0, 2, 1]);  permute_579 = None
        view_942 = torch.ops.aten.view.default(permute_580, [256, 512, 512]);  permute_580 = None
        bmm_93 = torch.ops.aten.bmm.default(view_941, view_942);  view_941 = view_942 = None
        view_943 = torch.ops.aten.view.default(bmm_93, [256, 512, 1, 1, 512]);  bmm_93 = None
        permute_581 = torch.ops.aten.permute.default(view_943, [3, 1, 4, 0, 2]);  view_943 = None
        view_944 = torch.ops.aten.view.default(permute_581, [1, 512, 512, 256]);  permute_581 = None
        _to_copy_549 = torch.ops.aten._to_copy.default(getitem_795, dtype = torch.bfloat16);  getitem_795 = None
        _to_copy_550 = torch.ops.aten._to_copy.default(getitem_791, dtype = torch.bfloat16)
        t_188 = torch.ops.aten.t.default(_to_copy_549);  _to_copy_549 = None
        view_945 = torch.ops.aten.view.default(_to_copy_550, [262144, 256]);  _to_copy_550 = None
        mm_175 = torch.ops.aten.mm.default(view_945, t_188);  view_945 = t_188 = None
        view_946 = torch.ops.aten.view.default(mm_175, [1, 512, 512, 512]);  mm_175 = None
        _to_copy_551 = torch.ops.aten._to_copy.default(getitem_797, dtype = torch.bfloat16);  getitem_797 = None
        _to_copy_552 = torch.ops.aten._to_copy.default(getitem_791, dtype = torch.bfloat16)
        t_189 = torch.ops.aten.t.default(_to_copy_551);  _to_copy_551 = None
        view_947 = torch.ops.aten.view.default(_to_copy_552, [262144, 256]);  _to_copy_552 = None
        mm_176 = torch.ops.aten.mm.default(view_947, t_189);  view_947 = t_189 = None
        view_948 = torch.ops.aten.view.default(mm_176, [1, 512, 512, 512]);  mm_176 = None
        sigmoid_61 = torch.ops.aten.sigmoid.default(view_948);  view_948 = None
        mul_98 = torch.ops.aten.mul.Tensor(view_946, sigmoid_61);  view_946 = sigmoid_61 = None
        view_949 = torch.ops.aten.view.default(mul_98, [262144, 512]);  mul_98 = None
        view_950 = torch.ops.aten.view.default(view_949, [1, 512, 512, 512]);  view_949 = None
        transpose_18 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_395 = torch.ops.aten.unsqueeze.default(transpose_18, 3);  transpose_18 = None
        clone_110 = torch.ops.aten.clone.default(unsqueeze_395, memory_format = torch.contiguous_format);  unsqueeze_395 = None
        bitwise_not_71 = torch.ops.aten.bitwise_not.default(clone_110);  clone_110 = None
        masked_fill_71 = torch.ops.aten.masked_fill.Scalar(view_950, bitwise_not_71, 0);  view_950 = bitwise_not_71 = None
        view_951 = torch.ops.aten.view.default(masked_fill_71, [262144, 512]);  masked_fill_71 = None
        view_955 = torch.ops.aten.view.default(view_951, [1, 512, 512, 512])
        split_tensor_77 = torch.ops.aten.split.Tensor(view_955, 256, dim = -1);  view_955 = None
        getitem_807 = split_tensor_77[0]
        unsqueeze_398 = torch.ops.aten.unsqueeze.default(getitem_807, 4);  getitem_807 = None
        permute_586 = torch.ops.aten.permute.default(unsqueeze_398, [0, 2, 4, 3, 1]);  unsqueeze_398 = None
        permute_587 = torch.ops.aten.permute.default(permute_586, [3, 1, 4, 0, 2]);  permute_586 = None
        view_956 = torch.ops.aten.view.default(permute_587, [256, 512, 512]);  permute_587 = None
        view_957 = torch.ops.aten.view.default(view_951, [1, 512, 512, 512]);  view_951 = None
        split_tensor_78 = torch.ops.aten.split.Tensor(view_957, 256, dim = -1);  view_957 = None
        getitem_810 = split_tensor_78[1];  split_tensor_78 = None
        unsqueeze_399 = torch.ops.aten.unsqueeze.default(getitem_810, 4);  getitem_810 = None
        permute_588 = torch.ops.aten.permute.default(unsqueeze_399, [0, 4, 2, 3, 1]);  unsqueeze_399 = None
        permute_589 = torch.ops.aten.permute.default(permute_588, [3, 4, 0, 2, 1]);  permute_588 = None
        view_958 = torch.ops.aten.view.default(permute_589, [256, 512, 512]);  permute_589 = None
        bmm_94 = torch.ops.aten.bmm.default(view_956, view_958);  view_956 = view_958 = None
        view_959 = torch.ops.aten.view.default(bmm_94, [256, 512, 1, 1, 512]);  bmm_94 = None
        permute_590 = torch.ops.aten.permute.default(view_959, [3, 1, 4, 0, 2]);  view_959 = None
        view_960 = torch.ops.aten.view.default(permute_590, [1, 512, 512, 256]);  permute_590 = None
        _to_copy_553 = torch.ops.aten._to_copy.default(view_944, dtype = torch.float32);  view_944 = None
        native_layer_norm_default_115 = torch.ops.aten.native_layer_norm.default(_to_copy_553, [256], None, None, 1e-05);  _to_copy_553 = None
        getitem_811 = native_layer_norm_default_115[0]
        _to_copy_554 = torch.ops.aten._to_copy.default(view_960, dtype = torch.float32);  view_960 = None
        native_layer_norm_default_116 = torch.ops.aten.native_layer_norm.default(_to_copy_554, [256], None, None, 1e-05);  _to_copy_554 = None
        getitem_814 = native_layer_norm_default_116[0]
        add_80 = torch.ops.aten.add.Tensor(getitem_811, getitem_814);  getitem_811 = getitem_814 = None
        _to_copy_555 = torch.ops.aten._to_copy.default(arg228_1, dtype = torch.bfloat16);  arg228_1 = None
        _to_copy_556 = torch.ops.aten._to_copy.default(add_80, dtype = torch.bfloat16);  add_80 = None
        t_190 = torch.ops.aten.t.default(_to_copy_555);  _to_copy_555 = None
        view_961 = torch.ops.aten.view.default(_to_copy_556, [262144, 256]);  _to_copy_556 = None
        mm_177 = torch.ops.aten.mm.default(view_961, t_190);  view_961 = t_190 = None
        view_962 = torch.ops.aten.view.default(mm_177, [1, 512, 512, 256]);  mm_177 = None
        _to_copy_557 = torch.ops.aten._to_copy.default(getitem_798, dtype = torch.bfloat16);  getitem_798 = None
        _to_copy_558 = torch.ops.aten._to_copy.default(getitem_791, dtype = torch.bfloat16);  getitem_791 = None
        t_191 = torch.ops.aten.t.default(_to_copy_557);  _to_copy_557 = None
        view_963 = torch.ops.aten.view.default(_to_copy_558, [262144, 256]);  _to_copy_558 = None
        mm_178 = torch.ops.aten.mm.default(view_963, t_191);  view_963 = t_191 = None
        view_964 = torch.ops.aten.view.default(mm_178, [1, 512, 512, 256]);  mm_178 = None
        sigmoid_62 = torch.ops.aten.sigmoid.default(view_964);  view_964 = None
        mul_99 = torch.ops.aten.mul.Tensor(view_962, sigmoid_62);  view_962 = sigmoid_62 = None
        add_81 = torch.ops.aten.add.Tensor(add_75, mul_99);  mul_99 = None
        _to_copy_559 = torch.ops.aten._to_copy.default(add_75, dtype = torch.float32)
        native_layer_norm_default_117 = torch.ops.aten.native_layer_norm.default(_to_copy_559, [256], None, None, 1e-05);  _to_copy_559 = None
        getitem_817 = native_layer_norm_default_117[0]
        _to_copy_560 = torch.ops.aten._to_copy.default(arg232_1, dtype = torch.bfloat16);  arg232_1 = None
        _to_copy_561 = torch.ops.aten._to_copy.default(getitem_817, dtype = torch.bfloat16)
        t_192 = torch.ops.aten.t.default(_to_copy_560);  _to_copy_560 = None
        view_965 = torch.ops.aten.view.default(_to_copy_561, [262144, 256]);  _to_copy_561 = None
        mm_179 = torch.ops.aten.mm.default(view_965, t_192);  view_965 = t_192 = None
        view_966 = torch.ops.aten.view.default(mm_179, [1, 512, 512, 8]);  mm_179 = None
        view_967 = torch.ops.aten.view.default(view_966, [1, 512, 512, 2, 4]);  view_966 = None
        permute_591 = torch.ops.aten.permute.default(view_967, [0, 3, 4, 1, 2]);  view_967 = None
        view_968 = torch.ops.aten.view.default(permute_591, [1, 2, 4, 1, 512, 512]);  permute_591 = None
        view_969 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_72 = torch.ops.aten.bitwise_not.default(view_969);  view_969 = None
        masked_fill_72 = torch.ops.aten.masked_fill.Scalar(view_968, bitwise_not_72, -10000);  view_968 = bitwise_not_72 = None
        view_970 = torch.ops.aten.view.default(masked_fill_72, [1, 2, 4, 512, 512]);  masked_fill_72 = None
        permute_592 = torch.ops.aten.permute.default(view_970, [1, 0, 2, 3, 4]);  view_970 = None
        view_971 = torch.ops.aten.view.default(permute_592, [2, 4, 1, 512, 512]);  permute_592 = None
        _to_copy_562 = torch.ops.aten._to_copy.default(arg233_1, dtype = torch.bfloat16);  arg233_1 = None
        _to_copy_563 = torch.ops.aten._to_copy.default(getitem_817, dtype = torch.bfloat16)
        t_193 = torch.ops.aten.t.default(_to_copy_562);  _to_copy_562 = None
        view_972 = torch.ops.aten.view.default(_to_copy_563, [262144, 256]);  _to_copy_563 = None
        mm_180 = torch.ops.aten.mm.default(view_972, t_193);  view_972 = t_193 = None
        view_973 = torch.ops.aten.view.default(mm_180, [1, 512, 512, 1024]);  mm_180 = None
        select_19 = torch.ops.aten.select.int(view_971, 0, 0)
        view_974 = torch.ops.aten.view.default(view_973, [1, 512, 512, 4, 4, 64]);  view_973 = None
        permute_593 = torch.ops.aten.permute.default(view_974, [4, 0, 3, 1, 2, 5]);  view_974 = None
        view_975 = torch.ops.aten.view.default(permute_593, [4, 4, 512, 512, 64]);  permute_593 = None
        unbind_int_49 = torch.ops.aten.unbind.int(view_975);  view_975 = None
        getitem_820 = unbind_int_49[0]
        getitem_821 = unbind_int_49[1]
        getitem_822 = unbind_int_49[2]
        getitem_823 = unbind_int_49[3];  unbind_int_49 = None
        expand_42 = torch.ops.aten.expand.default(select_19, [4, 512, 512, 512]);  select_19 = None
        _scaled_dot_product_efficient_attention_default_21 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_820, getitem_821, getitem_822, expand_42, False);  getitem_820 = getitem_821 = getitem_822 = expand_42 = None
        getitem_824 = _scaled_dot_product_efficient_attention_default_21[0]
        sigmoid_63 = torch.ops.aten.sigmoid.default(getitem_823);  getitem_823 = None
        mul_100 = torch.ops.aten.mul.Tensor(getitem_824, sigmoid_63);  getitem_824 = sigmoid_63 = None
        view_976 = torch.ops.aten.view.default(mul_100, [1, 4, 512, 512, 64]);  mul_100 = None
        permute_594 = torch.ops.aten.permute.default(view_976, [0, 2, 3, 1, 4]);  view_976 = None
        clone_111 = torch.ops.aten.clone.default(permute_594, memory_format = torch.contiguous_format);  permute_594 = None
        _unsafe_view_100 = torch.ops.aten._unsafe_view.default(clone_111, [1, 512, 512, 256]);  clone_111 = None
        transpose_19 = torch.ops.aten.transpose.int(getitem_817, 1, 2);  getitem_817 = None
        _to_copy_564 = torch.ops.aten._to_copy.default(arg234_1, dtype = torch.bfloat16);  arg234_1 = None
        _to_copy_565 = torch.ops.aten._to_copy.default(transpose_19, dtype = torch.bfloat16);  transpose_19 = None
        t_194 = torch.ops.aten.t.default(_to_copy_564);  _to_copy_564 = None
        expand_43 = torch.ops.aten.expand.default(_to_copy_565, [1, 512, 512, 256]);  _to_copy_565 = None
        view_977 = torch.ops.aten.view.default(expand_43, [512, 512, 256]);  expand_43 = None
        expand_44 = torch.ops.aten.expand.default(t_194, [1, 512, 256, 1024]);  t_194 = None
        view_978 = torch.ops.aten.view.default(expand_44, [512, 256, 1024]);  expand_44 = None
        bmm_95 = torch.ops.aten.bmm.default(view_977, view_978);  view_977 = view_978 = None
        view_979 = torch.ops.aten.view.default(bmm_95, [1, 512, 512, 1024]);  bmm_95 = None
        select_20 = torch.ops.aten.select.int(view_971, 0, 1);  view_971 = None
        view_980 = torch.ops.aten.view.default(view_979, [1, 512, 512, 4, 4, 64]);  view_979 = None
        permute_595 = torch.ops.aten.permute.default(view_980, [4, 0, 3, 1, 2, 5]);  view_980 = None
        view_981 = torch.ops.aten.view.default(permute_595, [4, 4, 512, 512, 64]);  permute_595 = None
        unbind_int_50 = torch.ops.aten.unbind.int(view_981);  view_981 = None
        getitem_828 = unbind_int_50[0]
        getitem_829 = unbind_int_50[1]
        getitem_830 = unbind_int_50[2]
        getitem_831 = unbind_int_50[3];  unbind_int_50 = None
        expand_45 = torch.ops.aten.expand.default(select_20, [4, 512, 512, 512]);  select_20 = None
        _scaled_dot_product_efficient_attention_default_22 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_828, getitem_829, getitem_830, expand_45, False);  getitem_828 = getitem_829 = getitem_830 = expand_45 = None
        getitem_832 = _scaled_dot_product_efficient_attention_default_22[0]
        sigmoid_64 = torch.ops.aten.sigmoid.default(getitem_831);  getitem_831 = None
        mul_101 = torch.ops.aten.mul.Tensor(getitem_832, sigmoid_64);  getitem_832 = sigmoid_64 = None
        view_982 = torch.ops.aten.view.default(mul_101, [1, 4, 512, 512, 64]);  mul_101 = None
        permute_596 = torch.ops.aten.permute.default(view_982, [0, 2, 3, 1, 4]);  view_982 = None
        clone_112 = torch.ops.aten.clone.default(permute_596, memory_format = torch.contiguous_format);  permute_596 = None
        _unsafe_view_101 = torch.ops.aten._unsafe_view.default(clone_112, [1, 512, 512, 256]);  clone_112 = None
        cat_15 = torch.ops.aten.cat.default([_unsafe_view_100, _unsafe_view_101], dim = -1);  _unsafe_view_100 = _unsafe_view_101 = None
        slice_208 = torch.ops.aten.slice.Tensor(arg231_1, dim = 0, start = 0, end = 9223372036854775807);  arg231_1 = None
        unsqueeze_400 = torch.ops.aten.unsqueeze.default(slice_208, 1);  slice_208 = None
        mul_102 = torch.ops.aten.mul.Tensor(arg235_1, unsqueeze_400);  arg235_1 = unsqueeze_400 = None
        _to_copy_566 = torch.ops.aten._to_copy.default(mul_102, dtype = torch.bfloat16);  mul_102 = None
        t_195 = torch.ops.aten.t.default(_to_copy_566);  _to_copy_566 = None
        view_983 = torch.ops.aten.view.default(cat_15, [262144, 512]);  cat_15 = None
        mm_181 = torch.ops.aten.mm.default(view_983, t_195);  view_983 = t_195 = None
        view_984 = torch.ops.aten.view.default(mm_181, [1, 512, 512, 256]);  mm_181 = None
        add_82 = torch.ops.aten.add.Tensor(add_81, view_984);  add_81 = view_984 = None
        split_tensor_79 = torch.ops.aten.split.Tensor(add_75, 512, dim = -2)
        getitem_836 = split_tensor_79[0];  split_tensor_79 = None
        _to_copy_567 = torch.ops.aten._to_copy.default(getitem_836, dtype = torch.float32);  getitem_836 = None
        native_layer_norm_default_118 = torch.ops.aten.native_layer_norm.default(_to_copy_567, [256], arg222_1, arg223_1, 1e-05);  _to_copy_567 = arg222_1 = arg223_1 = None
        getitem_837 = native_layer_norm_default_118[0]
        _to_copy_568 = torch.ops.aten._to_copy.default(arg224_1, dtype = torch.bfloat16);  arg224_1 = None
        _to_copy_569 = torch.ops.aten._to_copy.default(getitem_837, dtype = torch.bfloat16);  getitem_837 = None
        t_196 = torch.ops.aten.t.default(_to_copy_568);  _to_copy_568 = None
        view_985 = torch.ops.aten.view.default(_to_copy_569, [262144, 256]);  _to_copy_569 = None
        mm_182 = torch.ops.aten.mm.default(view_985, t_196);  view_985 = t_196 = None
        view_986 = torch.ops.aten.view.default(mm_182, [1, 512, 512, 1024]);  mm_182 = None
        split_tensor_80 = torch.ops.aten.split.Tensor(view_986, 512, dim = -1);  view_986 = None
        getitem_840 = split_tensor_80[0]
        getitem_841 = split_tensor_80[1];  split_tensor_80 = None
        silu_24 = torch.ops.aten.silu.default(getitem_840);  getitem_840 = None
        mul_103 = torch.ops.aten.mul.Tensor(silu_24, getitem_841);  silu_24 = getitem_841 = None
        _to_copy_570 = torch.ops.aten._to_copy.default(arg225_1, dtype = torch.bfloat16);  arg225_1 = None
        t_197 = torch.ops.aten.t.default(_to_copy_570);  _to_copy_570 = None
        view_988 = torch.ops.aten.view.default(mul_103, [262144, 512]);  mul_103 = None
        mm_183 = torch.ops.aten.mm.default(view_988, t_197);  view_988 = t_197 = None
        view_989 = torch.ops.aten.view.default(mm_183, [1, 512, 512, 256]);  mm_183 = None
        add_83 = torch.ops.aten.add.Tensor(add_82, view_989);  add_82 = view_989 = None
        _to_copy_571 = torch.ops.aten._to_copy.default(add_79, dtype = torch.float32)
        native_layer_norm_default_119 = torch.ops.aten.native_layer_norm.default(_to_copy_571, [384], arg240_1, arg241_1, 1e-05);  _to_copy_571 = arg240_1 = arg241_1 = None
        getitem_842 = native_layer_norm_default_119[0]
        _to_copy_572 = torch.ops.aten._to_copy.default(add_75, dtype = torch.float32);  add_75 = None
        native_layer_norm_default_120 = torch.ops.aten.native_layer_norm.default(_to_copy_572, [256], arg242_1, arg243_1, 1e-05);  _to_copy_572 = arg242_1 = arg243_1 = None
        getitem_845 = native_layer_norm_default_120[0]
        _to_copy_573 = torch.ops.aten._to_copy.default(arg244_1, dtype = torch.bfloat16);  arg244_1 = None
        _to_copy_574 = torch.ops.aten._to_copy.default(getitem_845, dtype = torch.bfloat16);  getitem_845 = None
        t_198 = torch.ops.aten.t.default(_to_copy_573);  _to_copy_573 = None
        view_990 = torch.ops.aten.view.default(_to_copy_574, [262144, 256]);  _to_copy_574 = None
        mm_184 = torch.ops.aten.mm.default(view_990, t_198);  view_990 = t_198 = None
        view_991 = torch.ops.aten.view.default(mm_184, [1, 512, 512, 16]);  mm_184 = None
        permute_597 = torch.ops.aten.permute.default(view_991, [0, 3, 1, 2]);  view_991 = None
        view_992 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_73 = torch.ops.aten.bitwise_not.default(view_992);  view_992 = None
        masked_fill_73 = torch.ops.aten.masked_fill.Scalar(permute_597, bitwise_not_73, -10000);  permute_597 = bitwise_not_73 = None
        _to_copy_575 = torch.ops.aten._to_copy.default(getitem_842, dtype = torch.bfloat16);  getitem_842 = None
        _to_copy_576 = torch.ops.aten._to_copy.default(arg246_1, dtype = torch.bfloat16);  arg246_1 = None
        unsqueeze_401 = torch.ops.aten.unsqueeze.default(_to_copy_575, 3);  _to_copy_575 = None
        unsqueeze_402 = torch.ops.aten.unsqueeze.default(unsqueeze_401, 4);  unsqueeze_401 = None
        unsqueeze_403 = torch.ops.aten.unsqueeze.default(unsqueeze_402, 5);  unsqueeze_402 = None
        permute_598 = torch.ops.aten.permute.default(unsqueeze_403, [3, 0, 4, 1, 5, 2]);  unsqueeze_403 = None
        unsqueeze_404 = torch.ops.aten.unsqueeze.default(_to_copy_576, 4);  _to_copy_576 = None
        unsqueeze_405 = torch.ops.aten.unsqueeze.default(unsqueeze_404, 5);  unsqueeze_404 = None
        permute_599 = torch.ops.aten.permute.default(unsqueeze_405, [1, 4, 2, 5, 3, 0]);  unsqueeze_405 = None
        permute_600 = torch.ops.aten.permute.default(permute_598, [3, 5, 0, 1, 2, 4]);  permute_598 = None
        view_993 = torch.ops.aten.view.default(permute_600, [1, 512, 384]);  permute_600 = None
        permute_601 = torch.ops.aten.permute.default(permute_599, [5, 0, 1, 2, 4, 3]);  permute_599 = None
        view_994 = torch.ops.aten.view.default(permute_601, [1, 384, 1536]);  permute_601 = None
        bmm_96 = torch.ops.aten.bmm.default(view_993, view_994);  view_993 = view_994 = None
        view_995 = torch.ops.aten.view.default(bmm_96, [512, 1, 4, 1, 16, 24]);  bmm_96 = None
        permute_602 = torch.ops.aten.permute.default(view_995, [2, 3, 4, 0, 5, 1]);  view_995 = None
        view_996 = torch.ops.aten.view.default(permute_602, [4, 1, 16, 512, 24]);  permute_602 = None
        unbind_int_51 = torch.ops.aten.unbind.int(view_996);  view_996 = None
        getitem_848 = unbind_int_51[0]
        getitem_849 = unbind_int_51[1]
        getitem_850 = unbind_int_51[2]
        getitem_851 = unbind_int_51[3];  unbind_int_51 = None
        view_997 = torch.ops.aten.view.default(arg245_1, [1, 16, 1, 24]);  arg245_1 = None
        add_84 = torch.ops.aten.add.Tensor(getitem_848, view_997);  getitem_848 = view_997 = None
        _to_copy_577 = torch.ops.aten._to_copy.default(add_84, dtype = torch.bfloat16);  add_84 = None
        expand_46 = torch.ops.aten.expand.default(masked_fill_73, [1, 16, 512, 512]);  masked_fill_73 = None
        _scaled_dot_product_efficient_attention_default_23 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_577, getitem_849, getitem_850, expand_46, False);  _to_copy_577 = getitem_849 = getitem_850 = expand_46 = None
        getitem_852 = _scaled_dot_product_efficient_attention_default_23[0]
        add_85 = torch.ops.aten.add.Tensor(getitem_851, 1);  getitem_851 = None
        sigmoid_65 = torch.ops.aten.sigmoid.default(add_85);  add_85 = None
        mul_104 = torch.ops.aten.mul.Tensor(getitem_852, sigmoid_65);  getitem_852 = sigmoid_65 = None
        _to_copy_578 = torch.ops.aten._to_copy.default(arg247_1, dtype = torch.bfloat16);  arg247_1 = None
        unsqueeze_406 = torch.ops.aten.unsqueeze.default(mul_104, 4);  mul_104 = None
        permute_603 = torch.ops.aten.permute.default(unsqueeze_406, [0, 2, 4, 3, 1]);  unsqueeze_406 = None
        unsqueeze_407 = torch.ops.aten.unsqueeze.default(_to_copy_578, 3);  _to_copy_578 = None
        unsqueeze_408 = torch.ops.aten.unsqueeze.default(unsqueeze_407, 4);  unsqueeze_407 = None
        permute_604 = torch.ops.aten.permute.default(unsqueeze_408, [3, 4, 2, 1, 0]);  unsqueeze_408 = None
        permute_605 = torch.ops.aten.permute.default(permute_603, [1, 3, 4, 0, 2]);  permute_603 = None
        clone_113 = torch.ops.aten.clone.default(permute_605, memory_format = torch.contiguous_format);  permute_605 = None
        _unsafe_view_102 = torch.ops.aten._unsafe_view.default(clone_113, [1, 512, 384]);  clone_113 = None
        permute_606 = torch.ops.aten.permute.default(permute_604, [3, 4, 0, 2, 1]);  permute_604 = None
        clone_114 = torch.ops.aten.clone.default(permute_606, memory_format = torch.contiguous_format);  permute_606 = None
        _unsafe_view_103 = torch.ops.aten._unsafe_view.default(clone_114, [1, 384, 384]);  clone_114 = None
        bmm_97 = torch.ops.aten.bmm.default(_unsafe_view_102, _unsafe_view_103);  _unsafe_view_102 = _unsafe_view_103 = None
        view_998 = torch.ops.aten.view.default(bmm_97, [512, 1, 1, 1, 384]);  bmm_97 = None
        permute_607 = torch.ops.aten.permute.default(view_998, [3, 0, 4, 1, 2]);  view_998 = None
        view_999 = torch.ops.aten.view.default(permute_607, [1, 512, 384]);  permute_607 = None
        unsqueeze_409 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_105 = torch.ops.aten.mul.Tensor(view_999, unsqueeze_409);  view_999 = unsqueeze_409 = None
        add_86 = torch.ops.aten.add.Tensor(add_79, mul_105);  mul_105 = None
        split_tensor_81 = torch.ops.aten.split.Tensor(add_79, 512, dim = -2);  add_79 = None
        getitem_856 = split_tensor_81[0];  split_tensor_81 = None
        _to_copy_579 = torch.ops.aten._to_copy.default(getitem_856, dtype = torch.float32);  getitem_856 = None
        native_layer_norm_default_121 = torch.ops.aten.native_layer_norm.default(_to_copy_579, [384], arg236_1, arg237_1, 1e-05);  _to_copy_579 = arg236_1 = arg237_1 = None
        getitem_857 = native_layer_norm_default_121[0]
        _to_copy_580 = torch.ops.aten._to_copy.default(arg238_1, dtype = torch.bfloat16);  arg238_1 = None
        _to_copy_581 = torch.ops.aten._to_copy.default(getitem_857, dtype = torch.bfloat16);  getitem_857 = None
        t_199 = torch.ops.aten.t.default(_to_copy_580);  _to_copy_580 = None
        view_1000 = torch.ops.aten.view.default(_to_copy_581, [512, 384]);  _to_copy_581 = None
        mm_185 = torch.ops.aten.mm.default(view_1000, t_199);  view_1000 = t_199 = None
        view_1001 = torch.ops.aten.view.default(mm_185, [1, 512, 1536]);  mm_185 = None
        split_tensor_82 = torch.ops.aten.split.Tensor(view_1001, 768, dim = -1);  view_1001 = None
        getitem_860 = split_tensor_82[0]
        getitem_861 = split_tensor_82[1];  split_tensor_82 = None
        silu_25 = torch.ops.aten.silu.default(getitem_860);  getitem_860 = None
        mul_106 = torch.ops.aten.mul.Tensor(silu_25, getitem_861);  silu_25 = getitem_861 = None
        _to_copy_582 = torch.ops.aten._to_copy.default(arg239_1, dtype = torch.bfloat16);  arg239_1 = None
        t_200 = torch.ops.aten.t.default(_to_copy_582);  _to_copy_582 = None
        view_1003 = torch.ops.aten.view.default(mul_106, [512, 768]);  mul_106 = None
        mm_186 = torch.ops.aten.mm.default(view_1003, t_200);  view_1003 = t_200 = None
        view_1004 = torch.ops.aten.view.default(mm_186, [1, 512, 384]);  mm_186 = None
        add_87 = torch.ops.aten.add.Tensor(add_86, view_1004);  add_86 = view_1004 = None
        _to_copy_583 = torch.ops.aten._to_copy.default(add_83, dtype = torch.float32)
        native_layer_norm_default_122 = torch.ops.aten.native_layer_norm.default(_to_copy_583, [256], arg252_1, arg253_1, 1e-05);  _to_copy_583 = arg252_1 = arg253_1 = None
        getitem_862 = native_layer_norm_default_122[0]
        split_with_sizes_default_20 = torch.ops.aten.split_with_sizes.default(arg255_1, [512, 512]);  arg255_1 = None
        getitem_865 = split_with_sizes_default_20[0]
        getitem_866 = split_with_sizes_default_20[1];  split_with_sizes_default_20 = None
        split_with_sizes_default_21 = torch.ops.aten.split_with_sizes.default(arg256_1, [512, 512, 256]);  arg256_1 = None
        getitem_867 = split_with_sizes_default_21[0]
        getitem_868 = split_with_sizes_default_21[1]
        getitem_869 = split_with_sizes_default_21[2];  split_with_sizes_default_21 = None
        _to_copy_584 = torch.ops.aten._to_copy.default(getitem_865, dtype = torch.bfloat16);  getitem_865 = None
        _to_copy_585 = torch.ops.aten._to_copy.default(getitem_862, dtype = torch.bfloat16)
        t_201 = torch.ops.aten.t.default(_to_copy_584);  _to_copy_584 = None
        view_1005 = torch.ops.aten.view.default(_to_copy_585, [262144, 256]);  _to_copy_585 = None
        mm_187 = torch.ops.aten.mm.default(view_1005, t_201);  view_1005 = t_201 = None
        view_1006 = torch.ops.aten.view.default(mm_187, [1, 512, 512, 512]);  mm_187 = None
        _to_copy_586 = torch.ops.aten._to_copy.default(getitem_867, dtype = torch.bfloat16);  getitem_867 = None
        _to_copy_587 = torch.ops.aten._to_copy.default(getitem_862, dtype = torch.bfloat16)
        t_202 = torch.ops.aten.t.default(_to_copy_586);  _to_copy_586 = None
        view_1007 = torch.ops.aten.view.default(_to_copy_587, [262144, 256]);  _to_copy_587 = None
        mm_188 = torch.ops.aten.mm.default(view_1007, t_202);  view_1007 = t_202 = None
        view_1008 = torch.ops.aten.view.default(mm_188, [1, 512, 512, 512]);  mm_188 = None
        sigmoid_66 = torch.ops.aten.sigmoid.default(view_1008);  view_1008 = None
        mul_107 = torch.ops.aten.mul.Tensor(view_1006, sigmoid_66);  view_1006 = sigmoid_66 = None
        unsqueeze_410 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_74 = torch.ops.aten.bitwise_not.default(unsqueeze_410);  unsqueeze_410 = None
        masked_fill_74 = torch.ops.aten.masked_fill.Scalar(mul_107, bitwise_not_74, 0);  mul_107 = bitwise_not_74 = None
        split_tensor_83 = torch.ops.aten.split.Tensor(masked_fill_74, 256, dim = -1)
        getitem_872 = split_tensor_83[0]
        unsqueeze_413 = torch.ops.aten.unsqueeze.default(getitem_872, 4);  getitem_872 = None
        permute_612 = torch.ops.aten.permute.default(unsqueeze_413, [0, 1, 4, 3, 2]);  unsqueeze_413 = None
        permute_613 = torch.ops.aten.permute.default(permute_612, [3, 1, 4, 0, 2]);  permute_612 = None
        view_1011 = torch.ops.aten.view.default(permute_613, [256, 512, 512]);  permute_613 = None
        split_tensor_84 = torch.ops.aten.split.Tensor(masked_fill_74, 256, dim = -1);  masked_fill_74 = None
        getitem_875 = split_tensor_84[1];  split_tensor_84 = None
        unsqueeze_414 = torch.ops.aten.unsqueeze.default(getitem_875, 4);  getitem_875 = None
        permute_614 = torch.ops.aten.permute.default(unsqueeze_414, [0, 4, 1, 3, 2]);  unsqueeze_414 = None
        permute_615 = torch.ops.aten.permute.default(permute_614, [3, 4, 0, 2, 1]);  permute_614 = None
        view_1012 = torch.ops.aten.view.default(permute_615, [256, 512, 512]);  permute_615 = None
        bmm_98 = torch.ops.aten.bmm.default(view_1011, view_1012);  view_1011 = view_1012 = None
        view_1013 = torch.ops.aten.view.default(bmm_98, [256, 512, 1, 1, 512]);  bmm_98 = None
        permute_616 = torch.ops.aten.permute.default(view_1013, [3, 1, 4, 0, 2]);  view_1013 = None
        view_1014 = torch.ops.aten.view.default(permute_616, [1, 512, 512, 256]);  permute_616 = None
        _to_copy_588 = torch.ops.aten._to_copy.default(getitem_866, dtype = torch.bfloat16);  getitem_866 = None
        _to_copy_589 = torch.ops.aten._to_copy.default(getitem_862, dtype = torch.bfloat16)
        t_203 = torch.ops.aten.t.default(_to_copy_588);  _to_copy_588 = None
        view_1015 = torch.ops.aten.view.default(_to_copy_589, [262144, 256]);  _to_copy_589 = None
        mm_189 = torch.ops.aten.mm.default(view_1015, t_203);  view_1015 = t_203 = None
        view_1016 = torch.ops.aten.view.default(mm_189, [1, 512, 512, 512]);  mm_189 = None
        _to_copy_590 = torch.ops.aten._to_copy.default(getitem_868, dtype = torch.bfloat16);  getitem_868 = None
        _to_copy_591 = torch.ops.aten._to_copy.default(getitem_862, dtype = torch.bfloat16)
        t_204 = torch.ops.aten.t.default(_to_copy_590);  _to_copy_590 = None
        view_1017 = torch.ops.aten.view.default(_to_copy_591, [262144, 256]);  _to_copy_591 = None
        mm_190 = torch.ops.aten.mm.default(view_1017, t_204);  view_1017 = t_204 = None
        view_1018 = torch.ops.aten.view.default(mm_190, [1, 512, 512, 512]);  mm_190 = None
        sigmoid_67 = torch.ops.aten.sigmoid.default(view_1018);  view_1018 = None
        mul_108 = torch.ops.aten.mul.Tensor(view_1016, sigmoid_67);  view_1016 = sigmoid_67 = None
        view_1019 = torch.ops.aten.view.default(mul_108, [262144, 512]);  mul_108 = None
        view_1020 = torch.ops.aten.view.default(view_1019, [1, 512, 512, 512]);  view_1019 = None
        transpose_20 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_415 = torch.ops.aten.unsqueeze.default(transpose_20, 3);  transpose_20 = None
        clone_115 = torch.ops.aten.clone.default(unsqueeze_415, memory_format = torch.contiguous_format);  unsqueeze_415 = None
        bitwise_not_75 = torch.ops.aten.bitwise_not.default(clone_115);  clone_115 = None
        masked_fill_75 = torch.ops.aten.masked_fill.Scalar(view_1020, bitwise_not_75, 0);  view_1020 = bitwise_not_75 = None
        view_1021 = torch.ops.aten.view.default(masked_fill_75, [262144, 512]);  masked_fill_75 = None
        view_1025 = torch.ops.aten.view.default(view_1021, [1, 512, 512, 512])
        split_tensor_85 = torch.ops.aten.split.Tensor(view_1025, 256, dim = -1);  view_1025 = None
        getitem_878 = split_tensor_85[0]
        unsqueeze_418 = torch.ops.aten.unsqueeze.default(getitem_878, 4);  getitem_878 = None
        permute_621 = torch.ops.aten.permute.default(unsqueeze_418, [0, 2, 4, 3, 1]);  unsqueeze_418 = None
        permute_622 = torch.ops.aten.permute.default(permute_621, [3, 1, 4, 0, 2]);  permute_621 = None
        view_1026 = torch.ops.aten.view.default(permute_622, [256, 512, 512]);  permute_622 = None
        view_1027 = torch.ops.aten.view.default(view_1021, [1, 512, 512, 512]);  view_1021 = None
        split_tensor_86 = torch.ops.aten.split.Tensor(view_1027, 256, dim = -1);  view_1027 = None
        getitem_881 = split_tensor_86[1];  split_tensor_86 = None
        unsqueeze_419 = torch.ops.aten.unsqueeze.default(getitem_881, 4);  getitem_881 = None
        permute_623 = torch.ops.aten.permute.default(unsqueeze_419, [0, 4, 2, 3, 1]);  unsqueeze_419 = None
        permute_624 = torch.ops.aten.permute.default(permute_623, [3, 4, 0, 2, 1]);  permute_623 = None
        view_1028 = torch.ops.aten.view.default(permute_624, [256, 512, 512]);  permute_624 = None
        bmm_99 = torch.ops.aten.bmm.default(view_1026, view_1028);  view_1026 = view_1028 = None
        view_1029 = torch.ops.aten.view.default(bmm_99, [256, 512, 1, 1, 512]);  bmm_99 = None
        permute_625 = torch.ops.aten.permute.default(view_1029, [3, 1, 4, 0, 2]);  view_1029 = None
        view_1030 = torch.ops.aten.view.default(permute_625, [1, 512, 512, 256]);  permute_625 = None
        _to_copy_592 = torch.ops.aten._to_copy.default(view_1014, dtype = torch.float32);  view_1014 = None
        native_layer_norm_default_123 = torch.ops.aten.native_layer_norm.default(_to_copy_592, [256], None, None, 1e-05);  _to_copy_592 = None
        getitem_882 = native_layer_norm_default_123[0]
        _to_copy_593 = torch.ops.aten._to_copy.default(view_1030, dtype = torch.float32);  view_1030 = None
        native_layer_norm_default_124 = torch.ops.aten.native_layer_norm.default(_to_copy_593, [256], None, None, 1e-05);  _to_copy_593 = None
        getitem_885 = native_layer_norm_default_124[0]
        add_88 = torch.ops.aten.add.Tensor(getitem_882, getitem_885);  getitem_882 = getitem_885 = None
        _to_copy_594 = torch.ops.aten._to_copy.default(arg254_1, dtype = torch.bfloat16);  arg254_1 = None
        _to_copy_595 = torch.ops.aten._to_copy.default(add_88, dtype = torch.bfloat16);  add_88 = None
        t_205 = torch.ops.aten.t.default(_to_copy_594);  _to_copy_594 = None
        view_1031 = torch.ops.aten.view.default(_to_copy_595, [262144, 256]);  _to_copy_595 = None
        mm_191 = torch.ops.aten.mm.default(view_1031, t_205);  view_1031 = t_205 = None
        view_1032 = torch.ops.aten.view.default(mm_191, [1, 512, 512, 256]);  mm_191 = None
        _to_copy_596 = torch.ops.aten._to_copy.default(getitem_869, dtype = torch.bfloat16);  getitem_869 = None
        _to_copy_597 = torch.ops.aten._to_copy.default(getitem_862, dtype = torch.bfloat16);  getitem_862 = None
        t_206 = torch.ops.aten.t.default(_to_copy_596);  _to_copy_596 = None
        view_1033 = torch.ops.aten.view.default(_to_copy_597, [262144, 256]);  _to_copy_597 = None
        mm_192 = torch.ops.aten.mm.default(view_1033, t_206);  view_1033 = t_206 = None
        view_1034 = torch.ops.aten.view.default(mm_192, [1, 512, 512, 256]);  mm_192 = None
        sigmoid_68 = torch.ops.aten.sigmoid.default(view_1034);  view_1034 = None
        mul_109 = torch.ops.aten.mul.Tensor(view_1032, sigmoid_68);  view_1032 = sigmoid_68 = None
        add_89 = torch.ops.aten.add.Tensor(add_83, mul_109);  mul_109 = None
        _to_copy_598 = torch.ops.aten._to_copy.default(add_83, dtype = torch.float32)
        native_layer_norm_default_125 = torch.ops.aten.native_layer_norm.default(_to_copy_598, [256], None, None, 1e-05);  _to_copy_598 = None
        getitem_888 = native_layer_norm_default_125[0]
        _to_copy_599 = torch.ops.aten._to_copy.default(arg258_1, dtype = torch.bfloat16);  arg258_1 = None
        _to_copy_600 = torch.ops.aten._to_copy.default(getitem_888, dtype = torch.bfloat16)
        t_207 = torch.ops.aten.t.default(_to_copy_599);  _to_copy_599 = None
        view_1035 = torch.ops.aten.view.default(_to_copy_600, [262144, 256]);  _to_copy_600 = None
        mm_193 = torch.ops.aten.mm.default(view_1035, t_207);  view_1035 = t_207 = None
        view_1036 = torch.ops.aten.view.default(mm_193, [1, 512, 512, 8]);  mm_193 = None
        view_1037 = torch.ops.aten.view.default(view_1036, [1, 512, 512, 2, 4]);  view_1036 = None
        permute_626 = torch.ops.aten.permute.default(view_1037, [0, 3, 4, 1, 2]);  view_1037 = None
        view_1038 = torch.ops.aten.view.default(permute_626, [1, 2, 4, 1, 512, 512]);  permute_626 = None
        view_1039 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_76 = torch.ops.aten.bitwise_not.default(view_1039);  view_1039 = None
        masked_fill_76 = torch.ops.aten.masked_fill.Scalar(view_1038, bitwise_not_76, -10000);  view_1038 = bitwise_not_76 = None
        view_1040 = torch.ops.aten.view.default(masked_fill_76, [1, 2, 4, 512, 512]);  masked_fill_76 = None
        permute_627 = torch.ops.aten.permute.default(view_1040, [1, 0, 2, 3, 4]);  view_1040 = None
        view_1041 = torch.ops.aten.view.default(permute_627, [2, 4, 1, 512, 512]);  permute_627 = None
        _to_copy_601 = torch.ops.aten._to_copy.default(arg259_1, dtype = torch.bfloat16);  arg259_1 = None
        _to_copy_602 = torch.ops.aten._to_copy.default(getitem_888, dtype = torch.bfloat16)
        t_208 = torch.ops.aten.t.default(_to_copy_601);  _to_copy_601 = None
        view_1042 = torch.ops.aten.view.default(_to_copy_602, [262144, 256]);  _to_copy_602 = None
        mm_194 = torch.ops.aten.mm.default(view_1042, t_208);  view_1042 = t_208 = None
        view_1043 = torch.ops.aten.view.default(mm_194, [1, 512, 512, 1024]);  mm_194 = None
        select_21 = torch.ops.aten.select.int(view_1041, 0, 0)
        view_1044 = torch.ops.aten.view.default(view_1043, [1, 512, 512, 4, 4, 64]);  view_1043 = None
        permute_628 = torch.ops.aten.permute.default(view_1044, [4, 0, 3, 1, 2, 5]);  view_1044 = None
        view_1045 = torch.ops.aten.view.default(permute_628, [4, 4, 512, 512, 64]);  permute_628 = None
        unbind_int_52 = torch.ops.aten.unbind.int(view_1045);  view_1045 = None
        getitem_891 = unbind_int_52[0]
        getitem_892 = unbind_int_52[1]
        getitem_893 = unbind_int_52[2]
        getitem_894 = unbind_int_52[3];  unbind_int_52 = None
        expand_47 = torch.ops.aten.expand.default(select_21, [4, 512, 512, 512]);  select_21 = None
        _scaled_dot_product_efficient_attention_default_24 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_891, getitem_892, getitem_893, expand_47, False);  getitem_891 = getitem_892 = getitem_893 = expand_47 = None
        getitem_895 = _scaled_dot_product_efficient_attention_default_24[0]
        sigmoid_69 = torch.ops.aten.sigmoid.default(getitem_894);  getitem_894 = None
        mul_110 = torch.ops.aten.mul.Tensor(getitem_895, sigmoid_69);  getitem_895 = sigmoid_69 = None
        view_1046 = torch.ops.aten.view.default(mul_110, [1, 4, 512, 512, 64]);  mul_110 = None
        permute_629 = torch.ops.aten.permute.default(view_1046, [0, 2, 3, 1, 4]);  view_1046 = None
        clone_116 = torch.ops.aten.clone.default(permute_629, memory_format = torch.contiguous_format);  permute_629 = None
        _unsafe_view_104 = torch.ops.aten._unsafe_view.default(clone_116, [1, 512, 512, 256]);  clone_116 = None
        transpose_21 = torch.ops.aten.transpose.int(getitem_888, 1, 2);  getitem_888 = None
        _to_copy_603 = torch.ops.aten._to_copy.default(arg260_1, dtype = torch.bfloat16);  arg260_1 = None
        _to_copy_604 = torch.ops.aten._to_copy.default(transpose_21, dtype = torch.bfloat16);  transpose_21 = None
        t_209 = torch.ops.aten.t.default(_to_copy_603);  _to_copy_603 = None
        expand_48 = torch.ops.aten.expand.default(_to_copy_604, [1, 512, 512, 256]);  _to_copy_604 = None
        view_1047 = torch.ops.aten.view.default(expand_48, [512, 512, 256]);  expand_48 = None
        expand_49 = torch.ops.aten.expand.default(t_209, [1, 512, 256, 1024]);  t_209 = None
        view_1048 = torch.ops.aten.view.default(expand_49, [512, 256, 1024]);  expand_49 = None
        bmm_100 = torch.ops.aten.bmm.default(view_1047, view_1048);  view_1047 = view_1048 = None
        view_1049 = torch.ops.aten.view.default(bmm_100, [1, 512, 512, 1024]);  bmm_100 = None
        select_22 = torch.ops.aten.select.int(view_1041, 0, 1);  view_1041 = None
        view_1050 = torch.ops.aten.view.default(view_1049, [1, 512, 512, 4, 4, 64]);  view_1049 = None
        permute_630 = torch.ops.aten.permute.default(view_1050, [4, 0, 3, 1, 2, 5]);  view_1050 = None
        view_1051 = torch.ops.aten.view.default(permute_630, [4, 4, 512, 512, 64]);  permute_630 = None
        unbind_int_53 = torch.ops.aten.unbind.int(view_1051);  view_1051 = None
        getitem_899 = unbind_int_53[0]
        getitem_900 = unbind_int_53[1]
        getitem_901 = unbind_int_53[2]
        getitem_902 = unbind_int_53[3];  unbind_int_53 = None
        expand_50 = torch.ops.aten.expand.default(select_22, [4, 512, 512, 512]);  select_22 = None
        _scaled_dot_product_efficient_attention_default_25 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_899, getitem_900, getitem_901, expand_50, False);  getitem_899 = getitem_900 = getitem_901 = expand_50 = None
        getitem_903 = _scaled_dot_product_efficient_attention_default_25[0]
        sigmoid_70 = torch.ops.aten.sigmoid.default(getitem_902);  getitem_902 = None
        mul_111 = torch.ops.aten.mul.Tensor(getitem_903, sigmoid_70);  getitem_903 = sigmoid_70 = None
        view_1052 = torch.ops.aten.view.default(mul_111, [1, 4, 512, 512, 64]);  mul_111 = None
        permute_631 = torch.ops.aten.permute.default(view_1052, [0, 2, 3, 1, 4]);  view_1052 = None
        clone_117 = torch.ops.aten.clone.default(permute_631, memory_format = torch.contiguous_format);  permute_631 = None
        _unsafe_view_105 = torch.ops.aten._unsafe_view.default(clone_117, [1, 512, 512, 256]);  clone_117 = None
        cat_16 = torch.ops.aten.cat.default([_unsafe_view_104, _unsafe_view_105], dim = -1);  _unsafe_view_104 = _unsafe_view_105 = None
        slice_209 = torch.ops.aten.slice.Tensor(arg257_1, dim = 0, start = 0, end = 9223372036854775807);  arg257_1 = None
        unsqueeze_420 = torch.ops.aten.unsqueeze.default(slice_209, 1);  slice_209 = None
        mul_112 = torch.ops.aten.mul.Tensor(arg261_1, unsqueeze_420);  arg261_1 = unsqueeze_420 = None
        _to_copy_605 = torch.ops.aten._to_copy.default(mul_112, dtype = torch.bfloat16);  mul_112 = None
        t_210 = torch.ops.aten.t.default(_to_copy_605);  _to_copy_605 = None
        view_1053 = torch.ops.aten.view.default(cat_16, [262144, 512]);  cat_16 = None
        mm_195 = torch.ops.aten.mm.default(view_1053, t_210);  view_1053 = t_210 = None
        view_1054 = torch.ops.aten.view.default(mm_195, [1, 512, 512, 256]);  mm_195 = None
        add_90 = torch.ops.aten.add.Tensor(add_89, view_1054);  add_89 = view_1054 = None
        split_tensor_87 = torch.ops.aten.split.Tensor(add_83, 512, dim = -2)
        getitem_907 = split_tensor_87[0];  split_tensor_87 = None
        _to_copy_606 = torch.ops.aten._to_copy.default(getitem_907, dtype = torch.float32);  getitem_907 = None
        native_layer_norm_default_126 = torch.ops.aten.native_layer_norm.default(_to_copy_606, [256], arg248_1, arg249_1, 1e-05);  _to_copy_606 = arg248_1 = arg249_1 = None
        getitem_908 = native_layer_norm_default_126[0]
        _to_copy_607 = torch.ops.aten._to_copy.default(arg250_1, dtype = torch.bfloat16);  arg250_1 = None
        _to_copy_608 = torch.ops.aten._to_copy.default(getitem_908, dtype = torch.bfloat16);  getitem_908 = None
        t_211 = torch.ops.aten.t.default(_to_copy_607);  _to_copy_607 = None
        view_1055 = torch.ops.aten.view.default(_to_copy_608, [262144, 256]);  _to_copy_608 = None
        mm_196 = torch.ops.aten.mm.default(view_1055, t_211);  view_1055 = t_211 = None
        view_1056 = torch.ops.aten.view.default(mm_196, [1, 512, 512, 1024]);  mm_196 = None
        split_tensor_88 = torch.ops.aten.split.Tensor(view_1056, 512, dim = -1);  view_1056 = None
        getitem_911 = split_tensor_88[0]
        getitem_912 = split_tensor_88[1];  split_tensor_88 = None
        silu_26 = torch.ops.aten.silu.default(getitem_911);  getitem_911 = None
        mul_113 = torch.ops.aten.mul.Tensor(silu_26, getitem_912);  silu_26 = getitem_912 = None
        _to_copy_609 = torch.ops.aten._to_copy.default(arg251_1, dtype = torch.bfloat16);  arg251_1 = None
        t_212 = torch.ops.aten.t.default(_to_copy_609);  _to_copy_609 = None
        view_1058 = torch.ops.aten.view.default(mul_113, [262144, 512]);  mul_113 = None
        mm_197 = torch.ops.aten.mm.default(view_1058, t_212);  view_1058 = t_212 = None
        view_1059 = torch.ops.aten.view.default(mm_197, [1, 512, 512, 256]);  mm_197 = None
        add_91 = torch.ops.aten.add.Tensor(add_90, view_1059);  add_90 = view_1059 = None
        _to_copy_610 = torch.ops.aten._to_copy.default(add_87, dtype = torch.float32)
        native_layer_norm_default_127 = torch.ops.aten.native_layer_norm.default(_to_copy_610, [384], arg266_1, arg267_1, 1e-05);  _to_copy_610 = arg266_1 = arg267_1 = None
        getitem_913 = native_layer_norm_default_127[0]
        _to_copy_611 = torch.ops.aten._to_copy.default(add_83, dtype = torch.float32);  add_83 = None
        native_layer_norm_default_128 = torch.ops.aten.native_layer_norm.default(_to_copy_611, [256], arg268_1, arg269_1, 1e-05);  _to_copy_611 = arg268_1 = arg269_1 = None
        getitem_916 = native_layer_norm_default_128[0]
        _to_copy_612 = torch.ops.aten._to_copy.default(arg270_1, dtype = torch.bfloat16);  arg270_1 = None
        _to_copy_613 = torch.ops.aten._to_copy.default(getitem_916, dtype = torch.bfloat16);  getitem_916 = None
        t_213 = torch.ops.aten.t.default(_to_copy_612);  _to_copy_612 = None
        view_1060 = torch.ops.aten.view.default(_to_copy_613, [262144, 256]);  _to_copy_613 = None
        mm_198 = torch.ops.aten.mm.default(view_1060, t_213);  view_1060 = t_213 = None
        view_1061 = torch.ops.aten.view.default(mm_198, [1, 512, 512, 16]);  mm_198 = None
        permute_632 = torch.ops.aten.permute.default(view_1061, [0, 3, 1, 2]);  view_1061 = None
        view_1062 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_77 = torch.ops.aten.bitwise_not.default(view_1062);  view_1062 = None
        masked_fill_77 = torch.ops.aten.masked_fill.Scalar(permute_632, bitwise_not_77, -10000);  permute_632 = bitwise_not_77 = None
        _to_copy_614 = torch.ops.aten._to_copy.default(getitem_913, dtype = torch.bfloat16);  getitem_913 = None
        _to_copy_615 = torch.ops.aten._to_copy.default(arg272_1, dtype = torch.bfloat16);  arg272_1 = None
        unsqueeze_421 = torch.ops.aten.unsqueeze.default(_to_copy_614, 3);  _to_copy_614 = None
        unsqueeze_422 = torch.ops.aten.unsqueeze.default(unsqueeze_421, 4);  unsqueeze_421 = None
        unsqueeze_423 = torch.ops.aten.unsqueeze.default(unsqueeze_422, 5);  unsqueeze_422 = None
        permute_633 = torch.ops.aten.permute.default(unsqueeze_423, [3, 0, 4, 1, 5, 2]);  unsqueeze_423 = None
        unsqueeze_424 = torch.ops.aten.unsqueeze.default(_to_copy_615, 4);  _to_copy_615 = None
        unsqueeze_425 = torch.ops.aten.unsqueeze.default(unsqueeze_424, 5);  unsqueeze_424 = None
        permute_634 = torch.ops.aten.permute.default(unsqueeze_425, [1, 4, 2, 5, 3, 0]);  unsqueeze_425 = None
        permute_635 = torch.ops.aten.permute.default(permute_633, [3, 5, 0, 1, 2, 4]);  permute_633 = None
        view_1063 = torch.ops.aten.view.default(permute_635, [1, 512, 384]);  permute_635 = None
        permute_636 = torch.ops.aten.permute.default(permute_634, [5, 0, 1, 2, 4, 3]);  permute_634 = None
        view_1064 = torch.ops.aten.view.default(permute_636, [1, 384, 1536]);  permute_636 = None
        bmm_101 = torch.ops.aten.bmm.default(view_1063, view_1064);  view_1063 = view_1064 = None
        view_1065 = torch.ops.aten.view.default(bmm_101, [512, 1, 4, 1, 16, 24]);  bmm_101 = None
        permute_637 = torch.ops.aten.permute.default(view_1065, [2, 3, 4, 0, 5, 1]);  view_1065 = None
        view_1066 = torch.ops.aten.view.default(permute_637, [4, 1, 16, 512, 24]);  permute_637 = None
        unbind_int_54 = torch.ops.aten.unbind.int(view_1066);  view_1066 = None
        getitem_919 = unbind_int_54[0]
        getitem_920 = unbind_int_54[1]
        getitem_921 = unbind_int_54[2]
        getitem_922 = unbind_int_54[3];  unbind_int_54 = None
        view_1067 = torch.ops.aten.view.default(arg271_1, [1, 16, 1, 24]);  arg271_1 = None
        add_92 = torch.ops.aten.add.Tensor(getitem_919, view_1067);  getitem_919 = view_1067 = None
        _to_copy_616 = torch.ops.aten._to_copy.default(add_92, dtype = torch.bfloat16);  add_92 = None
        expand_51 = torch.ops.aten.expand.default(masked_fill_77, [1, 16, 512, 512]);  masked_fill_77 = None
        _scaled_dot_product_efficient_attention_default_26 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_616, getitem_920, getitem_921, expand_51, False);  _to_copy_616 = getitem_920 = getitem_921 = expand_51 = None
        getitem_923 = _scaled_dot_product_efficient_attention_default_26[0]
        add_93 = torch.ops.aten.add.Tensor(getitem_922, 1);  getitem_922 = None
        sigmoid_71 = torch.ops.aten.sigmoid.default(add_93);  add_93 = None
        mul_114 = torch.ops.aten.mul.Tensor(getitem_923, sigmoid_71);  getitem_923 = sigmoid_71 = None
        _to_copy_617 = torch.ops.aten._to_copy.default(arg273_1, dtype = torch.bfloat16);  arg273_1 = None
        unsqueeze_426 = torch.ops.aten.unsqueeze.default(mul_114, 4);  mul_114 = None
        permute_638 = torch.ops.aten.permute.default(unsqueeze_426, [0, 2, 4, 3, 1]);  unsqueeze_426 = None
        unsqueeze_427 = torch.ops.aten.unsqueeze.default(_to_copy_617, 3);  _to_copy_617 = None
        unsqueeze_428 = torch.ops.aten.unsqueeze.default(unsqueeze_427, 4);  unsqueeze_427 = None
        permute_639 = torch.ops.aten.permute.default(unsqueeze_428, [3, 4, 2, 1, 0]);  unsqueeze_428 = None
        permute_640 = torch.ops.aten.permute.default(permute_638, [1, 3, 4, 0, 2]);  permute_638 = None
        clone_118 = torch.ops.aten.clone.default(permute_640, memory_format = torch.contiguous_format);  permute_640 = None
        _unsafe_view_106 = torch.ops.aten._unsafe_view.default(clone_118, [1, 512, 384]);  clone_118 = None
        permute_641 = torch.ops.aten.permute.default(permute_639, [3, 4, 0, 2, 1]);  permute_639 = None
        clone_119 = torch.ops.aten.clone.default(permute_641, memory_format = torch.contiguous_format);  permute_641 = None
        _unsafe_view_107 = torch.ops.aten._unsafe_view.default(clone_119, [1, 384, 384]);  clone_119 = None
        bmm_102 = torch.ops.aten.bmm.default(_unsafe_view_106, _unsafe_view_107);  _unsafe_view_106 = _unsafe_view_107 = None
        view_1068 = torch.ops.aten.view.default(bmm_102, [512, 1, 1, 1, 384]);  bmm_102 = None
        permute_642 = torch.ops.aten.permute.default(view_1068, [3, 0, 4, 1, 2]);  view_1068 = None
        view_1069 = torch.ops.aten.view.default(permute_642, [1, 512, 384]);  permute_642 = None
        unsqueeze_429 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_115 = torch.ops.aten.mul.Tensor(view_1069, unsqueeze_429);  view_1069 = unsqueeze_429 = None
        add_94 = torch.ops.aten.add.Tensor(add_87, mul_115);  mul_115 = None
        split_tensor_89 = torch.ops.aten.split.Tensor(add_87, 512, dim = -2);  add_87 = None
        getitem_927 = split_tensor_89[0];  split_tensor_89 = None
        _to_copy_618 = torch.ops.aten._to_copy.default(getitem_927, dtype = torch.float32);  getitem_927 = None
        native_layer_norm_default_129 = torch.ops.aten.native_layer_norm.default(_to_copy_618, [384], arg262_1, arg263_1, 1e-05);  _to_copy_618 = arg262_1 = arg263_1 = None
        getitem_928 = native_layer_norm_default_129[0]
        _to_copy_619 = torch.ops.aten._to_copy.default(arg264_1, dtype = torch.bfloat16);  arg264_1 = None
        _to_copy_620 = torch.ops.aten._to_copy.default(getitem_928, dtype = torch.bfloat16);  getitem_928 = None
        t_214 = torch.ops.aten.t.default(_to_copy_619);  _to_copy_619 = None
        view_1070 = torch.ops.aten.view.default(_to_copy_620, [512, 384]);  _to_copy_620 = None
        mm_199 = torch.ops.aten.mm.default(view_1070, t_214);  view_1070 = t_214 = None
        view_1071 = torch.ops.aten.view.default(mm_199, [1, 512, 1536]);  mm_199 = None
        split_tensor_90 = torch.ops.aten.split.Tensor(view_1071, 768, dim = -1);  view_1071 = None
        getitem_931 = split_tensor_90[0]
        getitem_932 = split_tensor_90[1];  split_tensor_90 = None
        silu_27 = torch.ops.aten.silu.default(getitem_931);  getitem_931 = None
        mul_116 = torch.ops.aten.mul.Tensor(silu_27, getitem_932);  silu_27 = getitem_932 = None
        _to_copy_621 = torch.ops.aten._to_copy.default(arg265_1, dtype = torch.bfloat16);  arg265_1 = None
        t_215 = torch.ops.aten.t.default(_to_copy_621);  _to_copy_621 = None
        view_1073 = torch.ops.aten.view.default(mul_116, [512, 768]);  mul_116 = None
        mm_200 = torch.ops.aten.mm.default(view_1073, t_215);  view_1073 = t_215 = None
        view_1074 = torch.ops.aten.view.default(mm_200, [1, 512, 384]);  mm_200 = None
        add_95 = torch.ops.aten.add.Tensor(add_94, view_1074);  add_94 = view_1074 = None
        _to_copy_622 = torch.ops.aten._to_copy.default(add_91, dtype = torch.float32)
        native_layer_norm_default_130 = torch.ops.aten.native_layer_norm.default(_to_copy_622, [256], arg278_1, arg279_1, 1e-05);  _to_copy_622 = arg278_1 = arg279_1 = None
        getitem_933 = native_layer_norm_default_130[0]
        split_with_sizes_default_22 = torch.ops.aten.split_with_sizes.default(arg281_1, [512, 512]);  arg281_1 = None
        getitem_936 = split_with_sizes_default_22[0]
        getitem_937 = split_with_sizes_default_22[1];  split_with_sizes_default_22 = None
        split_with_sizes_default_23 = torch.ops.aten.split_with_sizes.default(arg282_1, [512, 512, 256]);  arg282_1 = None
        getitem_938 = split_with_sizes_default_23[0]
        getitem_939 = split_with_sizes_default_23[1]
        getitem_940 = split_with_sizes_default_23[2];  split_with_sizes_default_23 = None
        _to_copy_623 = torch.ops.aten._to_copy.default(getitem_936, dtype = torch.bfloat16);  getitem_936 = None
        _to_copy_624 = torch.ops.aten._to_copy.default(getitem_933, dtype = torch.bfloat16)
        t_216 = torch.ops.aten.t.default(_to_copy_623);  _to_copy_623 = None
        view_1075 = torch.ops.aten.view.default(_to_copy_624, [262144, 256]);  _to_copy_624 = None
        mm_201 = torch.ops.aten.mm.default(view_1075, t_216);  view_1075 = t_216 = None
        view_1076 = torch.ops.aten.view.default(mm_201, [1, 512, 512, 512]);  mm_201 = None
        _to_copy_625 = torch.ops.aten._to_copy.default(getitem_938, dtype = torch.bfloat16);  getitem_938 = None
        _to_copy_626 = torch.ops.aten._to_copy.default(getitem_933, dtype = torch.bfloat16)
        t_217 = torch.ops.aten.t.default(_to_copy_625);  _to_copy_625 = None
        view_1077 = torch.ops.aten.view.default(_to_copy_626, [262144, 256]);  _to_copy_626 = None
        mm_202 = torch.ops.aten.mm.default(view_1077, t_217);  view_1077 = t_217 = None
        view_1078 = torch.ops.aten.view.default(mm_202, [1, 512, 512, 512]);  mm_202 = None
        sigmoid_72 = torch.ops.aten.sigmoid.default(view_1078);  view_1078 = None
        mul_117 = torch.ops.aten.mul.Tensor(view_1076, sigmoid_72);  view_1076 = sigmoid_72 = None
        unsqueeze_430 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_78 = torch.ops.aten.bitwise_not.default(unsqueeze_430);  unsqueeze_430 = None
        masked_fill_78 = torch.ops.aten.masked_fill.Scalar(mul_117, bitwise_not_78, 0);  mul_117 = bitwise_not_78 = None
        split_tensor_91 = torch.ops.aten.split.Tensor(masked_fill_78, 256, dim = -1)
        getitem_943 = split_tensor_91[0]
        unsqueeze_433 = torch.ops.aten.unsqueeze.default(getitem_943, 4);  getitem_943 = None
        permute_647 = torch.ops.aten.permute.default(unsqueeze_433, [0, 1, 4, 3, 2]);  unsqueeze_433 = None
        permute_648 = torch.ops.aten.permute.default(permute_647, [3, 1, 4, 0, 2]);  permute_647 = None
        view_1081 = torch.ops.aten.view.default(permute_648, [256, 512, 512]);  permute_648 = None
        split_tensor_92 = torch.ops.aten.split.Tensor(masked_fill_78, 256, dim = -1);  masked_fill_78 = None
        getitem_946 = split_tensor_92[1];  split_tensor_92 = None
        unsqueeze_434 = torch.ops.aten.unsqueeze.default(getitem_946, 4);  getitem_946 = None
        permute_649 = torch.ops.aten.permute.default(unsqueeze_434, [0, 4, 1, 3, 2]);  unsqueeze_434 = None
        permute_650 = torch.ops.aten.permute.default(permute_649, [3, 4, 0, 2, 1]);  permute_649 = None
        view_1082 = torch.ops.aten.view.default(permute_650, [256, 512, 512]);  permute_650 = None
        bmm_103 = torch.ops.aten.bmm.default(view_1081, view_1082);  view_1081 = view_1082 = None
        view_1083 = torch.ops.aten.view.default(bmm_103, [256, 512, 1, 1, 512]);  bmm_103 = None
        permute_651 = torch.ops.aten.permute.default(view_1083, [3, 1, 4, 0, 2]);  view_1083 = None
        view_1084 = torch.ops.aten.view.default(permute_651, [1, 512, 512, 256]);  permute_651 = None
        _to_copy_627 = torch.ops.aten._to_copy.default(getitem_937, dtype = torch.bfloat16);  getitem_937 = None
        _to_copy_628 = torch.ops.aten._to_copy.default(getitem_933, dtype = torch.bfloat16)
        t_218 = torch.ops.aten.t.default(_to_copy_627);  _to_copy_627 = None
        view_1085 = torch.ops.aten.view.default(_to_copy_628, [262144, 256]);  _to_copy_628 = None
        mm_203 = torch.ops.aten.mm.default(view_1085, t_218);  view_1085 = t_218 = None
        view_1086 = torch.ops.aten.view.default(mm_203, [1, 512, 512, 512]);  mm_203 = None
        _to_copy_629 = torch.ops.aten._to_copy.default(getitem_939, dtype = torch.bfloat16);  getitem_939 = None
        _to_copy_630 = torch.ops.aten._to_copy.default(getitem_933, dtype = torch.bfloat16)
        t_219 = torch.ops.aten.t.default(_to_copy_629);  _to_copy_629 = None
        view_1087 = torch.ops.aten.view.default(_to_copy_630, [262144, 256]);  _to_copy_630 = None
        mm_204 = torch.ops.aten.mm.default(view_1087, t_219);  view_1087 = t_219 = None
        view_1088 = torch.ops.aten.view.default(mm_204, [1, 512, 512, 512]);  mm_204 = None
        sigmoid_73 = torch.ops.aten.sigmoid.default(view_1088);  view_1088 = None
        mul_118 = torch.ops.aten.mul.Tensor(view_1086, sigmoid_73);  view_1086 = sigmoid_73 = None
        view_1089 = torch.ops.aten.view.default(mul_118, [262144, 512]);  mul_118 = None
        view_1090 = torch.ops.aten.view.default(view_1089, [1, 512, 512, 512]);  view_1089 = None
        transpose_22 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_435 = torch.ops.aten.unsqueeze.default(transpose_22, 3);  transpose_22 = None
        clone_120 = torch.ops.aten.clone.default(unsqueeze_435, memory_format = torch.contiguous_format);  unsqueeze_435 = None
        bitwise_not_79 = torch.ops.aten.bitwise_not.default(clone_120);  clone_120 = None
        masked_fill_79 = torch.ops.aten.masked_fill.Scalar(view_1090, bitwise_not_79, 0);  view_1090 = bitwise_not_79 = None
        view_1091 = torch.ops.aten.view.default(masked_fill_79, [262144, 512]);  masked_fill_79 = None
        view_1095 = torch.ops.aten.view.default(view_1091, [1, 512, 512, 512])
        split_tensor_93 = torch.ops.aten.split.Tensor(view_1095, 256, dim = -1);  view_1095 = None
        getitem_949 = split_tensor_93[0]
        unsqueeze_438 = torch.ops.aten.unsqueeze.default(getitem_949, 4);  getitem_949 = None
        permute_656 = torch.ops.aten.permute.default(unsqueeze_438, [0, 2, 4, 3, 1]);  unsqueeze_438 = None
        permute_657 = torch.ops.aten.permute.default(permute_656, [3, 1, 4, 0, 2]);  permute_656 = None
        view_1096 = torch.ops.aten.view.default(permute_657, [256, 512, 512]);  permute_657 = None
        view_1097 = torch.ops.aten.view.default(view_1091, [1, 512, 512, 512]);  view_1091 = None
        split_tensor_94 = torch.ops.aten.split.Tensor(view_1097, 256, dim = -1);  view_1097 = None
        getitem_952 = split_tensor_94[1];  split_tensor_94 = None
        unsqueeze_439 = torch.ops.aten.unsqueeze.default(getitem_952, 4);  getitem_952 = None
        permute_658 = torch.ops.aten.permute.default(unsqueeze_439, [0, 4, 2, 3, 1]);  unsqueeze_439 = None
        permute_659 = torch.ops.aten.permute.default(permute_658, [3, 4, 0, 2, 1]);  permute_658 = None
        view_1098 = torch.ops.aten.view.default(permute_659, [256, 512, 512]);  permute_659 = None
        bmm_104 = torch.ops.aten.bmm.default(view_1096, view_1098);  view_1096 = view_1098 = None
        view_1099 = torch.ops.aten.view.default(bmm_104, [256, 512, 1, 1, 512]);  bmm_104 = None
        permute_660 = torch.ops.aten.permute.default(view_1099, [3, 1, 4, 0, 2]);  view_1099 = None
        view_1100 = torch.ops.aten.view.default(permute_660, [1, 512, 512, 256]);  permute_660 = None
        _to_copy_631 = torch.ops.aten._to_copy.default(view_1084, dtype = torch.float32);  view_1084 = None
        native_layer_norm_default_131 = torch.ops.aten.native_layer_norm.default(_to_copy_631, [256], None, None, 1e-05);  _to_copy_631 = None
        getitem_953 = native_layer_norm_default_131[0]
        _to_copy_632 = torch.ops.aten._to_copy.default(view_1100, dtype = torch.float32);  view_1100 = None
        native_layer_norm_default_132 = torch.ops.aten.native_layer_norm.default(_to_copy_632, [256], None, None, 1e-05);  _to_copy_632 = None
        getitem_956 = native_layer_norm_default_132[0]
        add_96 = torch.ops.aten.add.Tensor(getitem_953, getitem_956);  getitem_953 = getitem_956 = None
        _to_copy_633 = torch.ops.aten._to_copy.default(arg280_1, dtype = torch.bfloat16);  arg280_1 = None
        _to_copy_634 = torch.ops.aten._to_copy.default(add_96, dtype = torch.bfloat16);  add_96 = None
        t_220 = torch.ops.aten.t.default(_to_copy_633);  _to_copy_633 = None
        view_1101 = torch.ops.aten.view.default(_to_copy_634, [262144, 256]);  _to_copy_634 = None
        mm_205 = torch.ops.aten.mm.default(view_1101, t_220);  view_1101 = t_220 = None
        view_1102 = torch.ops.aten.view.default(mm_205, [1, 512, 512, 256]);  mm_205 = None
        _to_copy_635 = torch.ops.aten._to_copy.default(getitem_940, dtype = torch.bfloat16);  getitem_940 = None
        _to_copy_636 = torch.ops.aten._to_copy.default(getitem_933, dtype = torch.bfloat16);  getitem_933 = None
        t_221 = torch.ops.aten.t.default(_to_copy_635);  _to_copy_635 = None
        view_1103 = torch.ops.aten.view.default(_to_copy_636, [262144, 256]);  _to_copy_636 = None
        mm_206 = torch.ops.aten.mm.default(view_1103, t_221);  view_1103 = t_221 = None
        view_1104 = torch.ops.aten.view.default(mm_206, [1, 512, 512, 256]);  mm_206 = None
        sigmoid_74 = torch.ops.aten.sigmoid.default(view_1104);  view_1104 = None
        mul_119 = torch.ops.aten.mul.Tensor(view_1102, sigmoid_74);  view_1102 = sigmoid_74 = None
        add_97 = torch.ops.aten.add.Tensor(add_91, mul_119);  mul_119 = None
        _to_copy_637 = torch.ops.aten._to_copy.default(add_91, dtype = torch.float32)
        native_layer_norm_default_133 = torch.ops.aten.native_layer_norm.default(_to_copy_637, [256], None, None, 1e-05);  _to_copy_637 = None
        getitem_959 = native_layer_norm_default_133[0]
        _to_copy_638 = torch.ops.aten._to_copy.default(arg284_1, dtype = torch.bfloat16);  arg284_1 = None
        _to_copy_639 = torch.ops.aten._to_copy.default(getitem_959, dtype = torch.bfloat16)
        t_222 = torch.ops.aten.t.default(_to_copy_638);  _to_copy_638 = None
        view_1105 = torch.ops.aten.view.default(_to_copy_639, [262144, 256]);  _to_copy_639 = None
        mm_207 = torch.ops.aten.mm.default(view_1105, t_222);  view_1105 = t_222 = None
        view_1106 = torch.ops.aten.view.default(mm_207, [1, 512, 512, 8]);  mm_207 = None
        view_1107 = torch.ops.aten.view.default(view_1106, [1, 512, 512, 2, 4]);  view_1106 = None
        permute_661 = torch.ops.aten.permute.default(view_1107, [0, 3, 4, 1, 2]);  view_1107 = None
        view_1108 = torch.ops.aten.view.default(permute_661, [1, 2, 4, 1, 512, 512]);  permute_661 = None
        view_1109 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_80 = torch.ops.aten.bitwise_not.default(view_1109);  view_1109 = None
        masked_fill_80 = torch.ops.aten.masked_fill.Scalar(view_1108, bitwise_not_80, -10000);  view_1108 = bitwise_not_80 = None
        view_1110 = torch.ops.aten.view.default(masked_fill_80, [1, 2, 4, 512, 512]);  masked_fill_80 = None
        permute_662 = torch.ops.aten.permute.default(view_1110, [1, 0, 2, 3, 4]);  view_1110 = None
        view_1111 = torch.ops.aten.view.default(permute_662, [2, 4, 1, 512, 512]);  permute_662 = None
        _to_copy_640 = torch.ops.aten._to_copy.default(arg285_1, dtype = torch.bfloat16);  arg285_1 = None
        _to_copy_641 = torch.ops.aten._to_copy.default(getitem_959, dtype = torch.bfloat16)
        t_223 = torch.ops.aten.t.default(_to_copy_640);  _to_copy_640 = None
        view_1112 = torch.ops.aten.view.default(_to_copy_641, [262144, 256]);  _to_copy_641 = None
        mm_208 = torch.ops.aten.mm.default(view_1112, t_223);  view_1112 = t_223 = None
        view_1113 = torch.ops.aten.view.default(mm_208, [1, 512, 512, 1024]);  mm_208 = None
        select_23 = torch.ops.aten.select.int(view_1111, 0, 0)
        view_1114 = torch.ops.aten.view.default(view_1113, [1, 512, 512, 4, 4, 64]);  view_1113 = None
        permute_663 = torch.ops.aten.permute.default(view_1114, [4, 0, 3, 1, 2, 5]);  view_1114 = None
        view_1115 = torch.ops.aten.view.default(permute_663, [4, 4, 512, 512, 64]);  permute_663 = None
        unbind_int_55 = torch.ops.aten.unbind.int(view_1115);  view_1115 = None
        getitem_962 = unbind_int_55[0]
        getitem_963 = unbind_int_55[1]
        getitem_964 = unbind_int_55[2]
        getitem_965 = unbind_int_55[3];  unbind_int_55 = None
        expand_52 = torch.ops.aten.expand.default(select_23, [4, 512, 512, 512]);  select_23 = None
        _scaled_dot_product_efficient_attention_default_27 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_962, getitem_963, getitem_964, expand_52, False);  getitem_962 = getitem_963 = getitem_964 = expand_52 = None
        getitem_966 = _scaled_dot_product_efficient_attention_default_27[0]
        sigmoid_75 = torch.ops.aten.sigmoid.default(getitem_965);  getitem_965 = None
        mul_120 = torch.ops.aten.mul.Tensor(getitem_966, sigmoid_75);  getitem_966 = sigmoid_75 = None
        view_1116 = torch.ops.aten.view.default(mul_120, [1, 4, 512, 512, 64]);  mul_120 = None
        permute_664 = torch.ops.aten.permute.default(view_1116, [0, 2, 3, 1, 4]);  view_1116 = None
        clone_121 = torch.ops.aten.clone.default(permute_664, memory_format = torch.contiguous_format);  permute_664 = None
        _unsafe_view_108 = torch.ops.aten._unsafe_view.default(clone_121, [1, 512, 512, 256]);  clone_121 = None
        transpose_23 = torch.ops.aten.transpose.int(getitem_959, 1, 2);  getitem_959 = None
        _to_copy_642 = torch.ops.aten._to_copy.default(arg286_1, dtype = torch.bfloat16);  arg286_1 = None
        _to_copy_643 = torch.ops.aten._to_copy.default(transpose_23, dtype = torch.bfloat16);  transpose_23 = None
        t_224 = torch.ops.aten.t.default(_to_copy_642);  _to_copy_642 = None
        expand_53 = torch.ops.aten.expand.default(_to_copy_643, [1, 512, 512, 256]);  _to_copy_643 = None
        view_1117 = torch.ops.aten.view.default(expand_53, [512, 512, 256]);  expand_53 = None
        expand_54 = torch.ops.aten.expand.default(t_224, [1, 512, 256, 1024]);  t_224 = None
        view_1118 = torch.ops.aten.view.default(expand_54, [512, 256, 1024]);  expand_54 = None
        bmm_105 = torch.ops.aten.bmm.default(view_1117, view_1118);  view_1117 = view_1118 = None
        view_1119 = torch.ops.aten.view.default(bmm_105, [1, 512, 512, 1024]);  bmm_105 = None
        select_24 = torch.ops.aten.select.int(view_1111, 0, 1);  view_1111 = None
        view_1120 = torch.ops.aten.view.default(view_1119, [1, 512, 512, 4, 4, 64]);  view_1119 = None
        permute_665 = torch.ops.aten.permute.default(view_1120, [4, 0, 3, 1, 2, 5]);  view_1120 = None
        view_1121 = torch.ops.aten.view.default(permute_665, [4, 4, 512, 512, 64]);  permute_665 = None
        unbind_int_56 = torch.ops.aten.unbind.int(view_1121);  view_1121 = None
        getitem_970 = unbind_int_56[0]
        getitem_971 = unbind_int_56[1]
        getitem_972 = unbind_int_56[2]
        getitem_973 = unbind_int_56[3];  unbind_int_56 = None
        expand_55 = torch.ops.aten.expand.default(select_24, [4, 512, 512, 512]);  select_24 = None
        _scaled_dot_product_efficient_attention_default_28 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_970, getitem_971, getitem_972, expand_55, False);  getitem_970 = getitem_971 = getitem_972 = expand_55 = None
        getitem_974 = _scaled_dot_product_efficient_attention_default_28[0]
        sigmoid_76 = torch.ops.aten.sigmoid.default(getitem_973);  getitem_973 = None
        mul_121 = torch.ops.aten.mul.Tensor(getitem_974, sigmoid_76);  getitem_974 = sigmoid_76 = None
        view_1122 = torch.ops.aten.view.default(mul_121, [1, 4, 512, 512, 64]);  mul_121 = None
        permute_666 = torch.ops.aten.permute.default(view_1122, [0, 2, 3, 1, 4]);  view_1122 = None
        clone_122 = torch.ops.aten.clone.default(permute_666, memory_format = torch.contiguous_format);  permute_666 = None
        _unsafe_view_109 = torch.ops.aten._unsafe_view.default(clone_122, [1, 512, 512, 256]);  clone_122 = None
        cat_17 = torch.ops.aten.cat.default([_unsafe_view_108, _unsafe_view_109], dim = -1);  _unsafe_view_108 = _unsafe_view_109 = None
        slice_210 = torch.ops.aten.slice.Tensor(arg283_1, dim = 0, start = 0, end = 9223372036854775807);  arg283_1 = None
        unsqueeze_440 = torch.ops.aten.unsqueeze.default(slice_210, 1);  slice_210 = None
        mul_122 = torch.ops.aten.mul.Tensor(arg287_1, unsqueeze_440);  arg287_1 = unsqueeze_440 = None
        _to_copy_644 = torch.ops.aten._to_copy.default(mul_122, dtype = torch.bfloat16);  mul_122 = None
        t_225 = torch.ops.aten.t.default(_to_copy_644);  _to_copy_644 = None
        view_1123 = torch.ops.aten.view.default(cat_17, [262144, 512]);  cat_17 = None
        mm_209 = torch.ops.aten.mm.default(view_1123, t_225);  view_1123 = t_225 = None
        view_1124 = torch.ops.aten.view.default(mm_209, [1, 512, 512, 256]);  mm_209 = None
        add_98 = torch.ops.aten.add.Tensor(add_97, view_1124);  add_97 = view_1124 = None
        split_tensor_95 = torch.ops.aten.split.Tensor(add_91, 512, dim = -2)
        getitem_978 = split_tensor_95[0];  split_tensor_95 = None
        _to_copy_645 = torch.ops.aten._to_copy.default(getitem_978, dtype = torch.float32);  getitem_978 = None
        native_layer_norm_default_134 = torch.ops.aten.native_layer_norm.default(_to_copy_645, [256], arg274_1, arg275_1, 1e-05);  _to_copy_645 = arg274_1 = arg275_1 = None
        getitem_979 = native_layer_norm_default_134[0]
        _to_copy_646 = torch.ops.aten._to_copy.default(arg276_1, dtype = torch.bfloat16);  arg276_1 = None
        _to_copy_647 = torch.ops.aten._to_copy.default(getitem_979, dtype = torch.bfloat16);  getitem_979 = None
        t_226 = torch.ops.aten.t.default(_to_copy_646);  _to_copy_646 = None
        view_1125 = torch.ops.aten.view.default(_to_copy_647, [262144, 256]);  _to_copy_647 = None
        mm_210 = torch.ops.aten.mm.default(view_1125, t_226);  view_1125 = t_226 = None
        view_1126 = torch.ops.aten.view.default(mm_210, [1, 512, 512, 1024]);  mm_210 = None
        split_tensor_96 = torch.ops.aten.split.Tensor(view_1126, 512, dim = -1);  view_1126 = None
        getitem_982 = split_tensor_96[0]
        getitem_983 = split_tensor_96[1];  split_tensor_96 = None
        silu_28 = torch.ops.aten.silu.default(getitem_982);  getitem_982 = None
        mul_123 = torch.ops.aten.mul.Tensor(silu_28, getitem_983);  silu_28 = getitem_983 = None
        _to_copy_648 = torch.ops.aten._to_copy.default(arg277_1, dtype = torch.bfloat16);  arg277_1 = None
        t_227 = torch.ops.aten.t.default(_to_copy_648);  _to_copy_648 = None
        view_1128 = torch.ops.aten.view.default(mul_123, [262144, 512]);  mul_123 = None
        mm_211 = torch.ops.aten.mm.default(view_1128, t_227);  view_1128 = t_227 = None
        view_1129 = torch.ops.aten.view.default(mm_211, [1, 512, 512, 256]);  mm_211 = None
        add_99 = torch.ops.aten.add.Tensor(add_98, view_1129);  add_98 = view_1129 = None
        _to_copy_649 = torch.ops.aten._to_copy.default(add_95, dtype = torch.float32)
        native_layer_norm_default_135 = torch.ops.aten.native_layer_norm.default(_to_copy_649, [384], arg292_1, arg293_1, 1e-05);  _to_copy_649 = arg292_1 = arg293_1 = None
        getitem_984 = native_layer_norm_default_135[0]
        _to_copy_650 = torch.ops.aten._to_copy.default(add_91, dtype = torch.float32);  add_91 = None
        native_layer_norm_default_136 = torch.ops.aten.native_layer_norm.default(_to_copy_650, [256], arg294_1, arg295_1, 1e-05);  _to_copy_650 = arg294_1 = arg295_1 = None
        getitem_987 = native_layer_norm_default_136[0]
        _to_copy_651 = torch.ops.aten._to_copy.default(arg296_1, dtype = torch.bfloat16);  arg296_1 = None
        _to_copy_652 = torch.ops.aten._to_copy.default(getitem_987, dtype = torch.bfloat16);  getitem_987 = None
        t_228 = torch.ops.aten.t.default(_to_copy_651);  _to_copy_651 = None
        view_1130 = torch.ops.aten.view.default(_to_copy_652, [262144, 256]);  _to_copy_652 = None
        mm_212 = torch.ops.aten.mm.default(view_1130, t_228);  view_1130 = t_228 = None
        view_1131 = torch.ops.aten.view.default(mm_212, [1, 512, 512, 16]);  mm_212 = None
        permute_667 = torch.ops.aten.permute.default(view_1131, [0, 3, 1, 2]);  view_1131 = None
        view_1132 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_81 = torch.ops.aten.bitwise_not.default(view_1132);  view_1132 = None
        masked_fill_81 = torch.ops.aten.masked_fill.Scalar(permute_667, bitwise_not_81, -10000);  permute_667 = bitwise_not_81 = None
        _to_copy_653 = torch.ops.aten._to_copy.default(getitem_984, dtype = torch.bfloat16);  getitem_984 = None
        _to_copy_654 = torch.ops.aten._to_copy.default(arg298_1, dtype = torch.bfloat16);  arg298_1 = None
        unsqueeze_441 = torch.ops.aten.unsqueeze.default(_to_copy_653, 3);  _to_copy_653 = None
        unsqueeze_442 = torch.ops.aten.unsqueeze.default(unsqueeze_441, 4);  unsqueeze_441 = None
        unsqueeze_443 = torch.ops.aten.unsqueeze.default(unsqueeze_442, 5);  unsqueeze_442 = None
        permute_668 = torch.ops.aten.permute.default(unsqueeze_443, [3, 0, 4, 1, 5, 2]);  unsqueeze_443 = None
        unsqueeze_444 = torch.ops.aten.unsqueeze.default(_to_copy_654, 4);  _to_copy_654 = None
        unsqueeze_445 = torch.ops.aten.unsqueeze.default(unsqueeze_444, 5);  unsqueeze_444 = None
        permute_669 = torch.ops.aten.permute.default(unsqueeze_445, [1, 4, 2, 5, 3, 0]);  unsqueeze_445 = None
        permute_670 = torch.ops.aten.permute.default(permute_668, [3, 5, 0, 1, 2, 4]);  permute_668 = None
        view_1133 = torch.ops.aten.view.default(permute_670, [1, 512, 384]);  permute_670 = None
        permute_671 = torch.ops.aten.permute.default(permute_669, [5, 0, 1, 2, 4, 3]);  permute_669 = None
        view_1134 = torch.ops.aten.view.default(permute_671, [1, 384, 1536]);  permute_671 = None
        bmm_106 = torch.ops.aten.bmm.default(view_1133, view_1134);  view_1133 = view_1134 = None
        view_1135 = torch.ops.aten.view.default(bmm_106, [512, 1, 4, 1, 16, 24]);  bmm_106 = None
        permute_672 = torch.ops.aten.permute.default(view_1135, [2, 3, 4, 0, 5, 1]);  view_1135 = None
        view_1136 = torch.ops.aten.view.default(permute_672, [4, 1, 16, 512, 24]);  permute_672 = None
        unbind_int_57 = torch.ops.aten.unbind.int(view_1136);  view_1136 = None
        getitem_990 = unbind_int_57[0]
        getitem_991 = unbind_int_57[1]
        getitem_992 = unbind_int_57[2]
        getitem_993 = unbind_int_57[3];  unbind_int_57 = None
        view_1137 = torch.ops.aten.view.default(arg297_1, [1, 16, 1, 24]);  arg297_1 = None
        add_100 = torch.ops.aten.add.Tensor(getitem_990, view_1137);  getitem_990 = view_1137 = None
        _to_copy_655 = torch.ops.aten._to_copy.default(add_100, dtype = torch.bfloat16);  add_100 = None
        expand_56 = torch.ops.aten.expand.default(masked_fill_81, [1, 16, 512, 512]);  masked_fill_81 = None
        _scaled_dot_product_efficient_attention_default_29 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_655, getitem_991, getitem_992, expand_56, False);  _to_copy_655 = getitem_991 = getitem_992 = expand_56 = None
        getitem_994 = _scaled_dot_product_efficient_attention_default_29[0]
        add_101 = torch.ops.aten.add.Tensor(getitem_993, 1);  getitem_993 = None
        sigmoid_77 = torch.ops.aten.sigmoid.default(add_101);  add_101 = None
        mul_124 = torch.ops.aten.mul.Tensor(getitem_994, sigmoid_77);  getitem_994 = sigmoid_77 = None
        _to_copy_656 = torch.ops.aten._to_copy.default(arg299_1, dtype = torch.bfloat16);  arg299_1 = None
        unsqueeze_446 = torch.ops.aten.unsqueeze.default(mul_124, 4);  mul_124 = None
        permute_673 = torch.ops.aten.permute.default(unsqueeze_446, [0, 2, 4, 3, 1]);  unsqueeze_446 = None
        unsqueeze_447 = torch.ops.aten.unsqueeze.default(_to_copy_656, 3);  _to_copy_656 = None
        unsqueeze_448 = torch.ops.aten.unsqueeze.default(unsqueeze_447, 4);  unsqueeze_447 = None
        permute_674 = torch.ops.aten.permute.default(unsqueeze_448, [3, 4, 2, 1, 0]);  unsqueeze_448 = None
        permute_675 = torch.ops.aten.permute.default(permute_673, [1, 3, 4, 0, 2]);  permute_673 = None
        clone_123 = torch.ops.aten.clone.default(permute_675, memory_format = torch.contiguous_format);  permute_675 = None
        _unsafe_view_110 = torch.ops.aten._unsafe_view.default(clone_123, [1, 512, 384]);  clone_123 = None
        permute_676 = torch.ops.aten.permute.default(permute_674, [3, 4, 0, 2, 1]);  permute_674 = None
        clone_124 = torch.ops.aten.clone.default(permute_676, memory_format = torch.contiguous_format);  permute_676 = None
        _unsafe_view_111 = torch.ops.aten._unsafe_view.default(clone_124, [1, 384, 384]);  clone_124 = None
        bmm_107 = torch.ops.aten.bmm.default(_unsafe_view_110, _unsafe_view_111);  _unsafe_view_110 = _unsafe_view_111 = None
        view_1138 = torch.ops.aten.view.default(bmm_107, [512, 1, 1, 1, 384]);  bmm_107 = None
        permute_677 = torch.ops.aten.permute.default(view_1138, [3, 0, 4, 1, 2]);  view_1138 = None
        view_1139 = torch.ops.aten.view.default(permute_677, [1, 512, 384]);  permute_677 = None
        unsqueeze_449 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_125 = torch.ops.aten.mul.Tensor(view_1139, unsqueeze_449);  view_1139 = unsqueeze_449 = None
        add_102 = torch.ops.aten.add.Tensor(add_95, mul_125);  mul_125 = None
        split_tensor_97 = torch.ops.aten.split.Tensor(add_95, 512, dim = -2);  add_95 = None
        getitem_998 = split_tensor_97[0];  split_tensor_97 = None
        _to_copy_657 = torch.ops.aten._to_copy.default(getitem_998, dtype = torch.float32);  getitem_998 = None
        native_layer_norm_default_137 = torch.ops.aten.native_layer_norm.default(_to_copy_657, [384], arg288_1, arg289_1, 1e-05);  _to_copy_657 = arg288_1 = arg289_1 = None
        getitem_999 = native_layer_norm_default_137[0]
        _to_copy_658 = torch.ops.aten._to_copy.default(arg290_1, dtype = torch.bfloat16);  arg290_1 = None
        _to_copy_659 = torch.ops.aten._to_copy.default(getitem_999, dtype = torch.bfloat16);  getitem_999 = None
        t_229 = torch.ops.aten.t.default(_to_copy_658);  _to_copy_658 = None
        view_1140 = torch.ops.aten.view.default(_to_copy_659, [512, 384]);  _to_copy_659 = None
        mm_213 = torch.ops.aten.mm.default(view_1140, t_229);  view_1140 = t_229 = None
        view_1141 = torch.ops.aten.view.default(mm_213, [1, 512, 1536]);  mm_213 = None
        split_tensor_98 = torch.ops.aten.split.Tensor(view_1141, 768, dim = -1);  view_1141 = None
        getitem_1002 = split_tensor_98[0]
        getitem_1003 = split_tensor_98[1];  split_tensor_98 = None
        silu_29 = torch.ops.aten.silu.default(getitem_1002);  getitem_1002 = None
        mul_126 = torch.ops.aten.mul.Tensor(silu_29, getitem_1003);  silu_29 = getitem_1003 = None
        _to_copy_660 = torch.ops.aten._to_copy.default(arg291_1, dtype = torch.bfloat16);  arg291_1 = None
        t_230 = torch.ops.aten.t.default(_to_copy_660);  _to_copy_660 = None
        view_1143 = torch.ops.aten.view.default(mul_126, [512, 768]);  mul_126 = None
        mm_214 = torch.ops.aten.mm.default(view_1143, t_230);  view_1143 = t_230 = None
        view_1144 = torch.ops.aten.view.default(mm_214, [1, 512, 384]);  mm_214 = None
        add_103 = torch.ops.aten.add.Tensor(add_102, view_1144);  add_102 = view_1144 = None
        _to_copy_661 = torch.ops.aten._to_copy.default(add_99, dtype = torch.float32)
        native_layer_norm_default_138 = torch.ops.aten.native_layer_norm.default(_to_copy_661, [256], arg304_1, arg305_1, 1e-05);  _to_copy_661 = arg304_1 = arg305_1 = None
        getitem_1004 = native_layer_norm_default_138[0]
        split_with_sizes_default_24 = torch.ops.aten.split_with_sizes.default(arg307_1, [512, 512]);  arg307_1 = None
        getitem_1007 = split_with_sizes_default_24[0]
        getitem_1008 = split_with_sizes_default_24[1];  split_with_sizes_default_24 = None
        split_with_sizes_default_25 = torch.ops.aten.split_with_sizes.default(arg308_1, [512, 512, 256]);  arg308_1 = None
        getitem_1009 = split_with_sizes_default_25[0]
        getitem_1010 = split_with_sizes_default_25[1]
        getitem_1011 = split_with_sizes_default_25[2];  split_with_sizes_default_25 = None
        _to_copy_662 = torch.ops.aten._to_copy.default(getitem_1007, dtype = torch.bfloat16);  getitem_1007 = None
        _to_copy_663 = torch.ops.aten._to_copy.default(getitem_1004, dtype = torch.bfloat16)
        t_231 = torch.ops.aten.t.default(_to_copy_662);  _to_copy_662 = None
        view_1145 = torch.ops.aten.view.default(_to_copy_663, [262144, 256]);  _to_copy_663 = None
        mm_215 = torch.ops.aten.mm.default(view_1145, t_231);  view_1145 = t_231 = None
        view_1146 = torch.ops.aten.view.default(mm_215, [1, 512, 512, 512]);  mm_215 = None
        _to_copy_664 = torch.ops.aten._to_copy.default(getitem_1009, dtype = torch.bfloat16);  getitem_1009 = None
        _to_copy_665 = torch.ops.aten._to_copy.default(getitem_1004, dtype = torch.bfloat16)
        t_232 = torch.ops.aten.t.default(_to_copy_664);  _to_copy_664 = None
        view_1147 = torch.ops.aten.view.default(_to_copy_665, [262144, 256]);  _to_copy_665 = None
        mm_216 = torch.ops.aten.mm.default(view_1147, t_232);  view_1147 = t_232 = None
        view_1148 = torch.ops.aten.view.default(mm_216, [1, 512, 512, 512]);  mm_216 = None
        sigmoid_78 = torch.ops.aten.sigmoid.default(view_1148);  view_1148 = None
        mul_127 = torch.ops.aten.mul.Tensor(view_1146, sigmoid_78);  view_1146 = sigmoid_78 = None
        unsqueeze_450 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_82 = torch.ops.aten.bitwise_not.default(unsqueeze_450);  unsqueeze_450 = None
        masked_fill_82 = torch.ops.aten.masked_fill.Scalar(mul_127, bitwise_not_82, 0);  mul_127 = bitwise_not_82 = None
        split_tensor_99 = torch.ops.aten.split.Tensor(masked_fill_82, 256, dim = -1)
        getitem_1014 = split_tensor_99[0]
        unsqueeze_453 = torch.ops.aten.unsqueeze.default(getitem_1014, 4);  getitem_1014 = None
        permute_682 = torch.ops.aten.permute.default(unsqueeze_453, [0, 1, 4, 3, 2]);  unsqueeze_453 = None
        permute_683 = torch.ops.aten.permute.default(permute_682, [3, 1, 4, 0, 2]);  permute_682 = None
        view_1151 = torch.ops.aten.view.default(permute_683, [256, 512, 512]);  permute_683 = None
        split_tensor_100 = torch.ops.aten.split.Tensor(masked_fill_82, 256, dim = -1);  masked_fill_82 = None
        getitem_1017 = split_tensor_100[1];  split_tensor_100 = None
        unsqueeze_454 = torch.ops.aten.unsqueeze.default(getitem_1017, 4);  getitem_1017 = None
        permute_684 = torch.ops.aten.permute.default(unsqueeze_454, [0, 4, 1, 3, 2]);  unsqueeze_454 = None
        permute_685 = torch.ops.aten.permute.default(permute_684, [3, 4, 0, 2, 1]);  permute_684 = None
        view_1152 = torch.ops.aten.view.default(permute_685, [256, 512, 512]);  permute_685 = None
        bmm_108 = torch.ops.aten.bmm.default(view_1151, view_1152);  view_1151 = view_1152 = None
        view_1153 = torch.ops.aten.view.default(bmm_108, [256, 512, 1, 1, 512]);  bmm_108 = None
        permute_686 = torch.ops.aten.permute.default(view_1153, [3, 1, 4, 0, 2]);  view_1153 = None
        view_1154 = torch.ops.aten.view.default(permute_686, [1, 512, 512, 256]);  permute_686 = None
        _to_copy_666 = torch.ops.aten._to_copy.default(getitem_1008, dtype = torch.bfloat16);  getitem_1008 = None
        _to_copy_667 = torch.ops.aten._to_copy.default(getitem_1004, dtype = torch.bfloat16)
        t_233 = torch.ops.aten.t.default(_to_copy_666);  _to_copy_666 = None
        view_1155 = torch.ops.aten.view.default(_to_copy_667, [262144, 256]);  _to_copy_667 = None
        mm_217 = torch.ops.aten.mm.default(view_1155, t_233);  view_1155 = t_233 = None
        view_1156 = torch.ops.aten.view.default(mm_217, [1, 512, 512, 512]);  mm_217 = None
        _to_copy_668 = torch.ops.aten._to_copy.default(getitem_1010, dtype = torch.bfloat16);  getitem_1010 = None
        _to_copy_669 = torch.ops.aten._to_copy.default(getitem_1004, dtype = torch.bfloat16)
        t_234 = torch.ops.aten.t.default(_to_copy_668);  _to_copy_668 = None
        view_1157 = torch.ops.aten.view.default(_to_copy_669, [262144, 256]);  _to_copy_669 = None
        mm_218 = torch.ops.aten.mm.default(view_1157, t_234);  view_1157 = t_234 = None
        view_1158 = torch.ops.aten.view.default(mm_218, [1, 512, 512, 512]);  mm_218 = None
        sigmoid_79 = torch.ops.aten.sigmoid.default(view_1158);  view_1158 = None
        mul_128 = torch.ops.aten.mul.Tensor(view_1156, sigmoid_79);  view_1156 = sigmoid_79 = None
        view_1159 = torch.ops.aten.view.default(mul_128, [262144, 512]);  mul_128 = None
        view_1160 = torch.ops.aten.view.default(view_1159, [1, 512, 512, 512]);  view_1159 = None
        transpose_24 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_455 = torch.ops.aten.unsqueeze.default(transpose_24, 3);  transpose_24 = None
        clone_125 = torch.ops.aten.clone.default(unsqueeze_455, memory_format = torch.contiguous_format);  unsqueeze_455 = None
        bitwise_not_83 = torch.ops.aten.bitwise_not.default(clone_125);  clone_125 = None
        masked_fill_83 = torch.ops.aten.masked_fill.Scalar(view_1160, bitwise_not_83, 0);  view_1160 = bitwise_not_83 = None
        view_1161 = torch.ops.aten.view.default(masked_fill_83, [262144, 512]);  masked_fill_83 = None
        view_1165 = torch.ops.aten.view.default(view_1161, [1, 512, 512, 512])
        split_tensor_101 = torch.ops.aten.split.Tensor(view_1165, 256, dim = -1);  view_1165 = None
        getitem_1020 = split_tensor_101[0]
        unsqueeze_458 = torch.ops.aten.unsqueeze.default(getitem_1020, 4);  getitem_1020 = None
        permute_691 = torch.ops.aten.permute.default(unsqueeze_458, [0, 2, 4, 3, 1]);  unsqueeze_458 = None
        permute_692 = torch.ops.aten.permute.default(permute_691, [3, 1, 4, 0, 2]);  permute_691 = None
        view_1166 = torch.ops.aten.view.default(permute_692, [256, 512, 512]);  permute_692 = None
        view_1167 = torch.ops.aten.view.default(view_1161, [1, 512, 512, 512]);  view_1161 = None
        split_tensor_102 = torch.ops.aten.split.Tensor(view_1167, 256, dim = -1);  view_1167 = None
        getitem_1023 = split_tensor_102[1];  split_tensor_102 = None
        unsqueeze_459 = torch.ops.aten.unsqueeze.default(getitem_1023, 4);  getitem_1023 = None
        permute_693 = torch.ops.aten.permute.default(unsqueeze_459, [0, 4, 2, 3, 1]);  unsqueeze_459 = None
        permute_694 = torch.ops.aten.permute.default(permute_693, [3, 4, 0, 2, 1]);  permute_693 = None
        view_1168 = torch.ops.aten.view.default(permute_694, [256, 512, 512]);  permute_694 = None
        bmm_109 = torch.ops.aten.bmm.default(view_1166, view_1168);  view_1166 = view_1168 = None
        view_1169 = torch.ops.aten.view.default(bmm_109, [256, 512, 1, 1, 512]);  bmm_109 = None
        permute_695 = torch.ops.aten.permute.default(view_1169, [3, 1, 4, 0, 2]);  view_1169 = None
        view_1170 = torch.ops.aten.view.default(permute_695, [1, 512, 512, 256]);  permute_695 = None
        _to_copy_670 = torch.ops.aten._to_copy.default(view_1154, dtype = torch.float32);  view_1154 = None
        native_layer_norm_default_139 = torch.ops.aten.native_layer_norm.default(_to_copy_670, [256], None, None, 1e-05);  _to_copy_670 = None
        getitem_1024 = native_layer_norm_default_139[0]
        _to_copy_671 = torch.ops.aten._to_copy.default(view_1170, dtype = torch.float32);  view_1170 = None
        native_layer_norm_default_140 = torch.ops.aten.native_layer_norm.default(_to_copy_671, [256], None, None, 1e-05);  _to_copy_671 = None
        getitem_1027 = native_layer_norm_default_140[0]
        add_104 = torch.ops.aten.add.Tensor(getitem_1024, getitem_1027);  getitem_1024 = getitem_1027 = None
        _to_copy_672 = torch.ops.aten._to_copy.default(arg306_1, dtype = torch.bfloat16);  arg306_1 = None
        _to_copy_673 = torch.ops.aten._to_copy.default(add_104, dtype = torch.bfloat16);  add_104 = None
        t_235 = torch.ops.aten.t.default(_to_copy_672);  _to_copy_672 = None
        view_1171 = torch.ops.aten.view.default(_to_copy_673, [262144, 256]);  _to_copy_673 = None
        mm_219 = torch.ops.aten.mm.default(view_1171, t_235);  view_1171 = t_235 = None
        view_1172 = torch.ops.aten.view.default(mm_219, [1, 512, 512, 256]);  mm_219 = None
        _to_copy_674 = torch.ops.aten._to_copy.default(getitem_1011, dtype = torch.bfloat16);  getitem_1011 = None
        _to_copy_675 = torch.ops.aten._to_copy.default(getitem_1004, dtype = torch.bfloat16);  getitem_1004 = None
        t_236 = torch.ops.aten.t.default(_to_copy_674);  _to_copy_674 = None
        view_1173 = torch.ops.aten.view.default(_to_copy_675, [262144, 256]);  _to_copy_675 = None
        mm_220 = torch.ops.aten.mm.default(view_1173, t_236);  view_1173 = t_236 = None
        view_1174 = torch.ops.aten.view.default(mm_220, [1, 512, 512, 256]);  mm_220 = None
        sigmoid_80 = torch.ops.aten.sigmoid.default(view_1174);  view_1174 = None
        mul_129 = torch.ops.aten.mul.Tensor(view_1172, sigmoid_80);  view_1172 = sigmoid_80 = None
        add_105 = torch.ops.aten.add.Tensor(add_99, mul_129);  mul_129 = None
        _to_copy_676 = torch.ops.aten._to_copy.default(add_99, dtype = torch.float32)
        native_layer_norm_default_141 = torch.ops.aten.native_layer_norm.default(_to_copy_676, [256], None, None, 1e-05);  _to_copy_676 = None
        getitem_1030 = native_layer_norm_default_141[0]
        _to_copy_677 = torch.ops.aten._to_copy.default(arg310_1, dtype = torch.bfloat16);  arg310_1 = None
        _to_copy_678 = torch.ops.aten._to_copy.default(getitem_1030, dtype = torch.bfloat16)
        t_237 = torch.ops.aten.t.default(_to_copy_677);  _to_copy_677 = None
        view_1175 = torch.ops.aten.view.default(_to_copy_678, [262144, 256]);  _to_copy_678 = None
        mm_221 = torch.ops.aten.mm.default(view_1175, t_237);  view_1175 = t_237 = None
        view_1176 = torch.ops.aten.view.default(mm_221, [1, 512, 512, 8]);  mm_221 = None
        view_1177 = torch.ops.aten.view.default(view_1176, [1, 512, 512, 2, 4]);  view_1176 = None
        permute_696 = torch.ops.aten.permute.default(view_1177, [0, 3, 4, 1, 2]);  view_1177 = None
        view_1178 = torch.ops.aten.view.default(permute_696, [1, 2, 4, 1, 512, 512]);  permute_696 = None
        view_1179 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_84 = torch.ops.aten.bitwise_not.default(view_1179);  view_1179 = None
        masked_fill_84 = torch.ops.aten.masked_fill.Scalar(view_1178, bitwise_not_84, -10000);  view_1178 = bitwise_not_84 = None
        view_1180 = torch.ops.aten.view.default(masked_fill_84, [1, 2, 4, 512, 512]);  masked_fill_84 = None
        permute_697 = torch.ops.aten.permute.default(view_1180, [1, 0, 2, 3, 4]);  view_1180 = None
        view_1181 = torch.ops.aten.view.default(permute_697, [2, 4, 1, 512, 512]);  permute_697 = None
        _to_copy_679 = torch.ops.aten._to_copy.default(arg311_1, dtype = torch.bfloat16);  arg311_1 = None
        _to_copy_680 = torch.ops.aten._to_copy.default(getitem_1030, dtype = torch.bfloat16)
        t_238 = torch.ops.aten.t.default(_to_copy_679);  _to_copy_679 = None
        view_1182 = torch.ops.aten.view.default(_to_copy_680, [262144, 256]);  _to_copy_680 = None
        mm_222 = torch.ops.aten.mm.default(view_1182, t_238);  view_1182 = t_238 = None
        view_1183 = torch.ops.aten.view.default(mm_222, [1, 512, 512, 1024]);  mm_222 = None
        select_25 = torch.ops.aten.select.int(view_1181, 0, 0)
        view_1184 = torch.ops.aten.view.default(view_1183, [1, 512, 512, 4, 4, 64]);  view_1183 = None
        permute_698 = torch.ops.aten.permute.default(view_1184, [4, 0, 3, 1, 2, 5]);  view_1184 = None
        view_1185 = torch.ops.aten.view.default(permute_698, [4, 4, 512, 512, 64]);  permute_698 = None
        unbind_int_58 = torch.ops.aten.unbind.int(view_1185);  view_1185 = None
        getitem_1033 = unbind_int_58[0]
        getitem_1034 = unbind_int_58[1]
        getitem_1035 = unbind_int_58[2]
        getitem_1036 = unbind_int_58[3];  unbind_int_58 = None
        expand_57 = torch.ops.aten.expand.default(select_25, [4, 512, 512, 512]);  select_25 = None
        _scaled_dot_product_efficient_attention_default_30 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1033, getitem_1034, getitem_1035, expand_57, False);  getitem_1033 = getitem_1034 = getitem_1035 = expand_57 = None
        getitem_1037 = _scaled_dot_product_efficient_attention_default_30[0]
        sigmoid_81 = torch.ops.aten.sigmoid.default(getitem_1036);  getitem_1036 = None
        mul_130 = torch.ops.aten.mul.Tensor(getitem_1037, sigmoid_81);  getitem_1037 = sigmoid_81 = None
        view_1186 = torch.ops.aten.view.default(mul_130, [1, 4, 512, 512, 64]);  mul_130 = None
        permute_699 = torch.ops.aten.permute.default(view_1186, [0, 2, 3, 1, 4]);  view_1186 = None
        clone_126 = torch.ops.aten.clone.default(permute_699, memory_format = torch.contiguous_format);  permute_699 = None
        _unsafe_view_112 = torch.ops.aten._unsafe_view.default(clone_126, [1, 512, 512, 256]);  clone_126 = None
        transpose_25 = torch.ops.aten.transpose.int(getitem_1030, 1, 2);  getitem_1030 = None
        _to_copy_681 = torch.ops.aten._to_copy.default(arg312_1, dtype = torch.bfloat16);  arg312_1 = None
        _to_copy_682 = torch.ops.aten._to_copy.default(transpose_25, dtype = torch.bfloat16);  transpose_25 = None
        t_239 = torch.ops.aten.t.default(_to_copy_681);  _to_copy_681 = None
        expand_58 = torch.ops.aten.expand.default(_to_copy_682, [1, 512, 512, 256]);  _to_copy_682 = None
        view_1187 = torch.ops.aten.view.default(expand_58, [512, 512, 256]);  expand_58 = None
        expand_59 = torch.ops.aten.expand.default(t_239, [1, 512, 256, 1024]);  t_239 = None
        view_1188 = torch.ops.aten.view.default(expand_59, [512, 256, 1024]);  expand_59 = None
        bmm_110 = torch.ops.aten.bmm.default(view_1187, view_1188);  view_1187 = view_1188 = None
        view_1189 = torch.ops.aten.view.default(bmm_110, [1, 512, 512, 1024]);  bmm_110 = None
        select_26 = torch.ops.aten.select.int(view_1181, 0, 1);  view_1181 = None
        view_1190 = torch.ops.aten.view.default(view_1189, [1, 512, 512, 4, 4, 64]);  view_1189 = None
        permute_700 = torch.ops.aten.permute.default(view_1190, [4, 0, 3, 1, 2, 5]);  view_1190 = None
        view_1191 = torch.ops.aten.view.default(permute_700, [4, 4, 512, 512, 64]);  permute_700 = None
        unbind_int_59 = torch.ops.aten.unbind.int(view_1191);  view_1191 = None
        getitem_1041 = unbind_int_59[0]
        getitem_1042 = unbind_int_59[1]
        getitem_1043 = unbind_int_59[2]
        getitem_1044 = unbind_int_59[3];  unbind_int_59 = None
        expand_60 = torch.ops.aten.expand.default(select_26, [4, 512, 512, 512]);  select_26 = None
        _scaled_dot_product_efficient_attention_default_31 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1041, getitem_1042, getitem_1043, expand_60, False);  getitem_1041 = getitem_1042 = getitem_1043 = expand_60 = None
        getitem_1045 = _scaled_dot_product_efficient_attention_default_31[0]
        sigmoid_82 = torch.ops.aten.sigmoid.default(getitem_1044);  getitem_1044 = None
        mul_131 = torch.ops.aten.mul.Tensor(getitem_1045, sigmoid_82);  getitem_1045 = sigmoid_82 = None
        view_1192 = torch.ops.aten.view.default(mul_131, [1, 4, 512, 512, 64]);  mul_131 = None
        permute_701 = torch.ops.aten.permute.default(view_1192, [0, 2, 3, 1, 4]);  view_1192 = None
        clone_127 = torch.ops.aten.clone.default(permute_701, memory_format = torch.contiguous_format);  permute_701 = None
        _unsafe_view_113 = torch.ops.aten._unsafe_view.default(clone_127, [1, 512, 512, 256]);  clone_127 = None
        cat_18 = torch.ops.aten.cat.default([_unsafe_view_112, _unsafe_view_113], dim = -1);  _unsafe_view_112 = _unsafe_view_113 = None
        slice_211 = torch.ops.aten.slice.Tensor(arg309_1, dim = 0, start = 0, end = 9223372036854775807);  arg309_1 = None
        unsqueeze_460 = torch.ops.aten.unsqueeze.default(slice_211, 1);  slice_211 = None
        mul_132 = torch.ops.aten.mul.Tensor(arg313_1, unsqueeze_460);  arg313_1 = unsqueeze_460 = None
        _to_copy_683 = torch.ops.aten._to_copy.default(mul_132, dtype = torch.bfloat16);  mul_132 = None
        t_240 = torch.ops.aten.t.default(_to_copy_683);  _to_copy_683 = None
        view_1193 = torch.ops.aten.view.default(cat_18, [262144, 512]);  cat_18 = None
        mm_223 = torch.ops.aten.mm.default(view_1193, t_240);  view_1193 = t_240 = None
        view_1194 = torch.ops.aten.view.default(mm_223, [1, 512, 512, 256]);  mm_223 = None
        add_106 = torch.ops.aten.add.Tensor(add_105, view_1194);  add_105 = view_1194 = None
        split_tensor_103 = torch.ops.aten.split.Tensor(add_99, 512, dim = -2)
        getitem_1049 = split_tensor_103[0];  split_tensor_103 = None
        _to_copy_684 = torch.ops.aten._to_copy.default(getitem_1049, dtype = torch.float32);  getitem_1049 = None
        native_layer_norm_default_142 = torch.ops.aten.native_layer_norm.default(_to_copy_684, [256], arg300_1, arg301_1, 1e-05);  _to_copy_684 = arg300_1 = arg301_1 = None
        getitem_1050 = native_layer_norm_default_142[0]
        _to_copy_685 = torch.ops.aten._to_copy.default(arg302_1, dtype = torch.bfloat16);  arg302_1 = None
        _to_copy_686 = torch.ops.aten._to_copy.default(getitem_1050, dtype = torch.bfloat16);  getitem_1050 = None
        t_241 = torch.ops.aten.t.default(_to_copy_685);  _to_copy_685 = None
        view_1195 = torch.ops.aten.view.default(_to_copy_686, [262144, 256]);  _to_copy_686 = None
        mm_224 = torch.ops.aten.mm.default(view_1195, t_241);  view_1195 = t_241 = None
        view_1196 = torch.ops.aten.view.default(mm_224, [1, 512, 512, 1024]);  mm_224 = None
        split_tensor_104 = torch.ops.aten.split.Tensor(view_1196, 512, dim = -1);  view_1196 = None
        getitem_1053 = split_tensor_104[0]
        getitem_1054 = split_tensor_104[1];  split_tensor_104 = None
        silu_30 = torch.ops.aten.silu.default(getitem_1053);  getitem_1053 = None
        mul_133 = torch.ops.aten.mul.Tensor(silu_30, getitem_1054);  silu_30 = getitem_1054 = None
        _to_copy_687 = torch.ops.aten._to_copy.default(arg303_1, dtype = torch.bfloat16);  arg303_1 = None
        t_242 = torch.ops.aten.t.default(_to_copy_687);  _to_copy_687 = None
        view_1198 = torch.ops.aten.view.default(mul_133, [262144, 512]);  mul_133 = None
        mm_225 = torch.ops.aten.mm.default(view_1198, t_242);  view_1198 = t_242 = None
        view_1199 = torch.ops.aten.view.default(mm_225, [1, 512, 512, 256]);  mm_225 = None
        add_107 = torch.ops.aten.add.Tensor(add_106, view_1199);  add_106 = view_1199 = None
        _to_copy_688 = torch.ops.aten._to_copy.default(add_103, dtype = torch.float32)
        native_layer_norm_default_143 = torch.ops.aten.native_layer_norm.default(_to_copy_688, [384], arg318_1, arg319_1, 1e-05);  _to_copy_688 = arg318_1 = arg319_1 = None
        getitem_1055 = native_layer_norm_default_143[0]
        _to_copy_689 = torch.ops.aten._to_copy.default(add_99, dtype = torch.float32);  add_99 = None
        native_layer_norm_default_144 = torch.ops.aten.native_layer_norm.default(_to_copy_689, [256], arg320_1, arg321_1, 1e-05);  _to_copy_689 = arg320_1 = arg321_1 = None
        getitem_1058 = native_layer_norm_default_144[0]
        _to_copy_690 = torch.ops.aten._to_copy.default(arg322_1, dtype = torch.bfloat16);  arg322_1 = None
        _to_copy_691 = torch.ops.aten._to_copy.default(getitem_1058, dtype = torch.bfloat16);  getitem_1058 = None
        t_243 = torch.ops.aten.t.default(_to_copy_690);  _to_copy_690 = None
        view_1200 = torch.ops.aten.view.default(_to_copy_691, [262144, 256]);  _to_copy_691 = None
        mm_226 = torch.ops.aten.mm.default(view_1200, t_243);  view_1200 = t_243 = None
        view_1201 = torch.ops.aten.view.default(mm_226, [1, 512, 512, 16]);  mm_226 = None
        permute_702 = torch.ops.aten.permute.default(view_1201, [0, 3, 1, 2]);  view_1201 = None
        view_1202 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_85 = torch.ops.aten.bitwise_not.default(view_1202);  view_1202 = None
        masked_fill_85 = torch.ops.aten.masked_fill.Scalar(permute_702, bitwise_not_85, -10000);  permute_702 = bitwise_not_85 = None
        _to_copy_692 = torch.ops.aten._to_copy.default(getitem_1055, dtype = torch.bfloat16);  getitem_1055 = None
        _to_copy_693 = torch.ops.aten._to_copy.default(arg324_1, dtype = torch.bfloat16);  arg324_1 = None
        unsqueeze_461 = torch.ops.aten.unsqueeze.default(_to_copy_692, 3);  _to_copy_692 = None
        unsqueeze_462 = torch.ops.aten.unsqueeze.default(unsqueeze_461, 4);  unsqueeze_461 = None
        unsqueeze_463 = torch.ops.aten.unsqueeze.default(unsqueeze_462, 5);  unsqueeze_462 = None
        permute_703 = torch.ops.aten.permute.default(unsqueeze_463, [3, 0, 4, 1, 5, 2]);  unsqueeze_463 = None
        unsqueeze_464 = torch.ops.aten.unsqueeze.default(_to_copy_693, 4);  _to_copy_693 = None
        unsqueeze_465 = torch.ops.aten.unsqueeze.default(unsqueeze_464, 5);  unsqueeze_464 = None
        permute_704 = torch.ops.aten.permute.default(unsqueeze_465, [1, 4, 2, 5, 3, 0]);  unsqueeze_465 = None
        permute_705 = torch.ops.aten.permute.default(permute_703, [3, 5, 0, 1, 2, 4]);  permute_703 = None
        view_1203 = torch.ops.aten.view.default(permute_705, [1, 512, 384]);  permute_705 = None
        permute_706 = torch.ops.aten.permute.default(permute_704, [5, 0, 1, 2, 4, 3]);  permute_704 = None
        view_1204 = torch.ops.aten.view.default(permute_706, [1, 384, 1536]);  permute_706 = None
        bmm_111 = torch.ops.aten.bmm.default(view_1203, view_1204);  view_1203 = view_1204 = None
        view_1205 = torch.ops.aten.view.default(bmm_111, [512, 1, 4, 1, 16, 24]);  bmm_111 = None
        permute_707 = torch.ops.aten.permute.default(view_1205, [2, 3, 4, 0, 5, 1]);  view_1205 = None
        view_1206 = torch.ops.aten.view.default(permute_707, [4, 1, 16, 512, 24]);  permute_707 = None
        unbind_int_60 = torch.ops.aten.unbind.int(view_1206);  view_1206 = None
        getitem_1061 = unbind_int_60[0]
        getitem_1062 = unbind_int_60[1]
        getitem_1063 = unbind_int_60[2]
        getitem_1064 = unbind_int_60[3];  unbind_int_60 = None
        view_1207 = torch.ops.aten.view.default(arg323_1, [1, 16, 1, 24]);  arg323_1 = None
        add_108 = torch.ops.aten.add.Tensor(getitem_1061, view_1207);  getitem_1061 = view_1207 = None
        _to_copy_694 = torch.ops.aten._to_copy.default(add_108, dtype = torch.bfloat16);  add_108 = None
        expand_61 = torch.ops.aten.expand.default(masked_fill_85, [1, 16, 512, 512]);  masked_fill_85 = None
        _scaled_dot_product_efficient_attention_default_32 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_694, getitem_1062, getitem_1063, expand_61, False);  _to_copy_694 = getitem_1062 = getitem_1063 = expand_61 = None
        getitem_1065 = _scaled_dot_product_efficient_attention_default_32[0]
        add_109 = torch.ops.aten.add.Tensor(getitem_1064, 1);  getitem_1064 = None
        sigmoid_83 = torch.ops.aten.sigmoid.default(add_109);  add_109 = None
        mul_134 = torch.ops.aten.mul.Tensor(getitem_1065, sigmoid_83);  getitem_1065 = sigmoid_83 = None
        _to_copy_695 = torch.ops.aten._to_copy.default(arg325_1, dtype = torch.bfloat16);  arg325_1 = None
        unsqueeze_466 = torch.ops.aten.unsqueeze.default(mul_134, 4);  mul_134 = None
        permute_708 = torch.ops.aten.permute.default(unsqueeze_466, [0, 2, 4, 3, 1]);  unsqueeze_466 = None
        unsqueeze_467 = torch.ops.aten.unsqueeze.default(_to_copy_695, 3);  _to_copy_695 = None
        unsqueeze_468 = torch.ops.aten.unsqueeze.default(unsqueeze_467, 4);  unsqueeze_467 = None
        permute_709 = torch.ops.aten.permute.default(unsqueeze_468, [3, 4, 2, 1, 0]);  unsqueeze_468 = None
        permute_710 = torch.ops.aten.permute.default(permute_708, [1, 3, 4, 0, 2]);  permute_708 = None
        clone_128 = torch.ops.aten.clone.default(permute_710, memory_format = torch.contiguous_format);  permute_710 = None
        _unsafe_view_114 = torch.ops.aten._unsafe_view.default(clone_128, [1, 512, 384]);  clone_128 = None
        permute_711 = torch.ops.aten.permute.default(permute_709, [3, 4, 0, 2, 1]);  permute_709 = None
        clone_129 = torch.ops.aten.clone.default(permute_711, memory_format = torch.contiguous_format);  permute_711 = None
        _unsafe_view_115 = torch.ops.aten._unsafe_view.default(clone_129, [1, 384, 384]);  clone_129 = None
        bmm_112 = torch.ops.aten.bmm.default(_unsafe_view_114, _unsafe_view_115);  _unsafe_view_114 = _unsafe_view_115 = None
        view_1208 = torch.ops.aten.view.default(bmm_112, [512, 1, 1, 1, 384]);  bmm_112 = None
        permute_712 = torch.ops.aten.permute.default(view_1208, [3, 0, 4, 1, 2]);  view_1208 = None
        view_1209 = torch.ops.aten.view.default(permute_712, [1, 512, 384]);  permute_712 = None
        unsqueeze_469 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_135 = torch.ops.aten.mul.Tensor(view_1209, unsqueeze_469);  view_1209 = unsqueeze_469 = None
        add_110 = torch.ops.aten.add.Tensor(add_103, mul_135);  mul_135 = None
        split_tensor_105 = torch.ops.aten.split.Tensor(add_103, 512, dim = -2);  add_103 = None
        getitem_1069 = split_tensor_105[0];  split_tensor_105 = None
        _to_copy_696 = torch.ops.aten._to_copy.default(getitem_1069, dtype = torch.float32);  getitem_1069 = None
        native_layer_norm_default_145 = torch.ops.aten.native_layer_norm.default(_to_copy_696, [384], arg314_1, arg315_1, 1e-05);  _to_copy_696 = arg314_1 = arg315_1 = None
        getitem_1070 = native_layer_norm_default_145[0]
        _to_copy_697 = torch.ops.aten._to_copy.default(arg316_1, dtype = torch.bfloat16);  arg316_1 = None
        _to_copy_698 = torch.ops.aten._to_copy.default(getitem_1070, dtype = torch.bfloat16);  getitem_1070 = None
        t_244 = torch.ops.aten.t.default(_to_copy_697);  _to_copy_697 = None
        view_1210 = torch.ops.aten.view.default(_to_copy_698, [512, 384]);  _to_copy_698 = None
        mm_227 = torch.ops.aten.mm.default(view_1210, t_244);  view_1210 = t_244 = None
        view_1211 = torch.ops.aten.view.default(mm_227, [1, 512, 1536]);  mm_227 = None
        split_tensor_106 = torch.ops.aten.split.Tensor(view_1211, 768, dim = -1);  view_1211 = None
        getitem_1073 = split_tensor_106[0]
        getitem_1074 = split_tensor_106[1];  split_tensor_106 = None
        silu_31 = torch.ops.aten.silu.default(getitem_1073);  getitem_1073 = None
        mul_136 = torch.ops.aten.mul.Tensor(silu_31, getitem_1074);  silu_31 = getitem_1074 = None
        _to_copy_699 = torch.ops.aten._to_copy.default(arg317_1, dtype = torch.bfloat16);  arg317_1 = None
        t_245 = torch.ops.aten.t.default(_to_copy_699);  _to_copy_699 = None
        view_1213 = torch.ops.aten.view.default(mul_136, [512, 768]);  mul_136 = None
        mm_228 = torch.ops.aten.mm.default(view_1213, t_245);  view_1213 = t_245 = None
        view_1214 = torch.ops.aten.view.default(mm_228, [1, 512, 384]);  mm_228 = None
        add_111 = torch.ops.aten.add.Tensor(add_110, view_1214);  add_110 = view_1214 = None
        _to_copy_700 = torch.ops.aten._to_copy.default(add_107, dtype = torch.float32)
        native_layer_norm_default_146 = torch.ops.aten.native_layer_norm.default(_to_copy_700, [256], arg330_1, arg331_1, 1e-05);  _to_copy_700 = arg330_1 = arg331_1 = None
        getitem_1075 = native_layer_norm_default_146[0]
        split_with_sizes_default_26 = torch.ops.aten.split_with_sizes.default(arg333_1, [512, 512]);  arg333_1 = None
        getitem_1078 = split_with_sizes_default_26[0]
        getitem_1079 = split_with_sizes_default_26[1];  split_with_sizes_default_26 = None
        split_with_sizes_default_27 = torch.ops.aten.split_with_sizes.default(arg334_1, [512, 512, 256]);  arg334_1 = None
        getitem_1080 = split_with_sizes_default_27[0]
        getitem_1081 = split_with_sizes_default_27[1]
        getitem_1082 = split_with_sizes_default_27[2];  split_with_sizes_default_27 = None
        _to_copy_701 = torch.ops.aten._to_copy.default(getitem_1078, dtype = torch.bfloat16);  getitem_1078 = None
        _to_copy_702 = torch.ops.aten._to_copy.default(getitem_1075, dtype = torch.bfloat16)
        t_246 = torch.ops.aten.t.default(_to_copy_701);  _to_copy_701 = None
        view_1215 = torch.ops.aten.view.default(_to_copy_702, [262144, 256]);  _to_copy_702 = None
        mm_229 = torch.ops.aten.mm.default(view_1215, t_246);  view_1215 = t_246 = None
        view_1216 = torch.ops.aten.view.default(mm_229, [1, 512, 512, 512]);  mm_229 = None
        _to_copy_703 = torch.ops.aten._to_copy.default(getitem_1080, dtype = torch.bfloat16);  getitem_1080 = None
        _to_copy_704 = torch.ops.aten._to_copy.default(getitem_1075, dtype = torch.bfloat16)
        t_247 = torch.ops.aten.t.default(_to_copy_703);  _to_copy_703 = None
        view_1217 = torch.ops.aten.view.default(_to_copy_704, [262144, 256]);  _to_copy_704 = None
        mm_230 = torch.ops.aten.mm.default(view_1217, t_247);  view_1217 = t_247 = None
        view_1218 = torch.ops.aten.view.default(mm_230, [1, 512, 512, 512]);  mm_230 = None
        sigmoid_84 = torch.ops.aten.sigmoid.default(view_1218);  view_1218 = None
        mul_137 = torch.ops.aten.mul.Tensor(view_1216, sigmoid_84);  view_1216 = sigmoid_84 = None
        unsqueeze_470 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_86 = torch.ops.aten.bitwise_not.default(unsqueeze_470);  unsqueeze_470 = None
        masked_fill_86 = torch.ops.aten.masked_fill.Scalar(mul_137, bitwise_not_86, 0);  mul_137 = bitwise_not_86 = None
        split_tensor_107 = torch.ops.aten.split.Tensor(masked_fill_86, 256, dim = -1)
        getitem_1085 = split_tensor_107[0]
        unsqueeze_473 = torch.ops.aten.unsqueeze.default(getitem_1085, 4);  getitem_1085 = None
        permute_717 = torch.ops.aten.permute.default(unsqueeze_473, [0, 1, 4, 3, 2]);  unsqueeze_473 = None
        permute_718 = torch.ops.aten.permute.default(permute_717, [3, 1, 4, 0, 2]);  permute_717 = None
        view_1221 = torch.ops.aten.view.default(permute_718, [256, 512, 512]);  permute_718 = None
        split_tensor_108 = torch.ops.aten.split.Tensor(masked_fill_86, 256, dim = -1);  masked_fill_86 = None
        getitem_1088 = split_tensor_108[1];  split_tensor_108 = None
        unsqueeze_474 = torch.ops.aten.unsqueeze.default(getitem_1088, 4);  getitem_1088 = None
        permute_719 = torch.ops.aten.permute.default(unsqueeze_474, [0, 4, 1, 3, 2]);  unsqueeze_474 = None
        permute_720 = torch.ops.aten.permute.default(permute_719, [3, 4, 0, 2, 1]);  permute_719 = None
        view_1222 = torch.ops.aten.view.default(permute_720, [256, 512, 512]);  permute_720 = None
        bmm_113 = torch.ops.aten.bmm.default(view_1221, view_1222);  view_1221 = view_1222 = None
        view_1223 = torch.ops.aten.view.default(bmm_113, [256, 512, 1, 1, 512]);  bmm_113 = None
        permute_721 = torch.ops.aten.permute.default(view_1223, [3, 1, 4, 0, 2]);  view_1223 = None
        view_1224 = torch.ops.aten.view.default(permute_721, [1, 512, 512, 256]);  permute_721 = None
        _to_copy_705 = torch.ops.aten._to_copy.default(getitem_1079, dtype = torch.bfloat16);  getitem_1079 = None
        _to_copy_706 = torch.ops.aten._to_copy.default(getitem_1075, dtype = torch.bfloat16)
        t_248 = torch.ops.aten.t.default(_to_copy_705);  _to_copy_705 = None
        view_1225 = torch.ops.aten.view.default(_to_copy_706, [262144, 256]);  _to_copy_706 = None
        mm_231 = torch.ops.aten.mm.default(view_1225, t_248);  view_1225 = t_248 = None
        view_1226 = torch.ops.aten.view.default(mm_231, [1, 512, 512, 512]);  mm_231 = None
        _to_copy_707 = torch.ops.aten._to_copy.default(getitem_1081, dtype = torch.bfloat16);  getitem_1081 = None
        _to_copy_708 = torch.ops.aten._to_copy.default(getitem_1075, dtype = torch.bfloat16)
        t_249 = torch.ops.aten.t.default(_to_copy_707);  _to_copy_707 = None
        view_1227 = torch.ops.aten.view.default(_to_copy_708, [262144, 256]);  _to_copy_708 = None
        mm_232 = torch.ops.aten.mm.default(view_1227, t_249);  view_1227 = t_249 = None
        view_1228 = torch.ops.aten.view.default(mm_232, [1, 512, 512, 512]);  mm_232 = None
        sigmoid_85 = torch.ops.aten.sigmoid.default(view_1228);  view_1228 = None
        mul_138 = torch.ops.aten.mul.Tensor(view_1226, sigmoid_85);  view_1226 = sigmoid_85 = None
        view_1229 = torch.ops.aten.view.default(mul_138, [262144, 512]);  mul_138 = None
        view_1230 = torch.ops.aten.view.default(view_1229, [1, 512, 512, 512]);  view_1229 = None
        transpose_26 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_475 = torch.ops.aten.unsqueeze.default(transpose_26, 3);  transpose_26 = None
        clone_130 = torch.ops.aten.clone.default(unsqueeze_475, memory_format = torch.contiguous_format);  unsqueeze_475 = None
        bitwise_not_87 = torch.ops.aten.bitwise_not.default(clone_130);  clone_130 = None
        masked_fill_87 = torch.ops.aten.masked_fill.Scalar(view_1230, bitwise_not_87, 0);  view_1230 = bitwise_not_87 = None
        view_1231 = torch.ops.aten.view.default(masked_fill_87, [262144, 512]);  masked_fill_87 = None
        view_1235 = torch.ops.aten.view.default(view_1231, [1, 512, 512, 512])
        split_tensor_109 = torch.ops.aten.split.Tensor(view_1235, 256, dim = -1);  view_1235 = None
        getitem_1091 = split_tensor_109[0]
        unsqueeze_478 = torch.ops.aten.unsqueeze.default(getitem_1091, 4);  getitem_1091 = None
        permute_726 = torch.ops.aten.permute.default(unsqueeze_478, [0, 2, 4, 3, 1]);  unsqueeze_478 = None
        permute_727 = torch.ops.aten.permute.default(permute_726, [3, 1, 4, 0, 2]);  permute_726 = None
        view_1236 = torch.ops.aten.view.default(permute_727, [256, 512, 512]);  permute_727 = None
        view_1237 = torch.ops.aten.view.default(view_1231, [1, 512, 512, 512]);  view_1231 = None
        split_tensor_110 = torch.ops.aten.split.Tensor(view_1237, 256, dim = -1);  view_1237 = None
        getitem_1094 = split_tensor_110[1];  split_tensor_110 = None
        unsqueeze_479 = torch.ops.aten.unsqueeze.default(getitem_1094, 4);  getitem_1094 = None
        permute_728 = torch.ops.aten.permute.default(unsqueeze_479, [0, 4, 2, 3, 1]);  unsqueeze_479 = None
        permute_729 = torch.ops.aten.permute.default(permute_728, [3, 4, 0, 2, 1]);  permute_728 = None
        view_1238 = torch.ops.aten.view.default(permute_729, [256, 512, 512]);  permute_729 = None
        bmm_114 = torch.ops.aten.bmm.default(view_1236, view_1238);  view_1236 = view_1238 = None
        view_1239 = torch.ops.aten.view.default(bmm_114, [256, 512, 1, 1, 512]);  bmm_114 = None
        permute_730 = torch.ops.aten.permute.default(view_1239, [3, 1, 4, 0, 2]);  view_1239 = None
        view_1240 = torch.ops.aten.view.default(permute_730, [1, 512, 512, 256]);  permute_730 = None
        _to_copy_709 = torch.ops.aten._to_copy.default(view_1224, dtype = torch.float32);  view_1224 = None
        native_layer_norm_default_147 = torch.ops.aten.native_layer_norm.default(_to_copy_709, [256], None, None, 1e-05);  _to_copy_709 = None
        getitem_1095 = native_layer_norm_default_147[0]
        _to_copy_710 = torch.ops.aten._to_copy.default(view_1240, dtype = torch.float32);  view_1240 = None
        native_layer_norm_default_148 = torch.ops.aten.native_layer_norm.default(_to_copy_710, [256], None, None, 1e-05);  _to_copy_710 = None
        getitem_1098 = native_layer_norm_default_148[0]
        add_112 = torch.ops.aten.add.Tensor(getitem_1095, getitem_1098);  getitem_1095 = getitem_1098 = None
        _to_copy_711 = torch.ops.aten._to_copy.default(arg332_1, dtype = torch.bfloat16);  arg332_1 = None
        _to_copy_712 = torch.ops.aten._to_copy.default(add_112, dtype = torch.bfloat16);  add_112 = None
        t_250 = torch.ops.aten.t.default(_to_copy_711);  _to_copy_711 = None
        view_1241 = torch.ops.aten.view.default(_to_copy_712, [262144, 256]);  _to_copy_712 = None
        mm_233 = torch.ops.aten.mm.default(view_1241, t_250);  view_1241 = t_250 = None
        view_1242 = torch.ops.aten.view.default(mm_233, [1, 512, 512, 256]);  mm_233 = None
        _to_copy_713 = torch.ops.aten._to_copy.default(getitem_1082, dtype = torch.bfloat16);  getitem_1082 = None
        _to_copy_714 = torch.ops.aten._to_copy.default(getitem_1075, dtype = torch.bfloat16);  getitem_1075 = None
        t_251 = torch.ops.aten.t.default(_to_copy_713);  _to_copy_713 = None
        view_1243 = torch.ops.aten.view.default(_to_copy_714, [262144, 256]);  _to_copy_714 = None
        mm_234 = torch.ops.aten.mm.default(view_1243, t_251);  view_1243 = t_251 = None
        view_1244 = torch.ops.aten.view.default(mm_234, [1, 512, 512, 256]);  mm_234 = None
        sigmoid_86 = torch.ops.aten.sigmoid.default(view_1244);  view_1244 = None
        mul_139 = torch.ops.aten.mul.Tensor(view_1242, sigmoid_86);  view_1242 = sigmoid_86 = None
        add_113 = torch.ops.aten.add.Tensor(add_107, mul_139);  mul_139 = None
        _to_copy_715 = torch.ops.aten._to_copy.default(add_107, dtype = torch.float32)
        native_layer_norm_default_149 = torch.ops.aten.native_layer_norm.default(_to_copy_715, [256], None, None, 1e-05);  _to_copy_715 = None
        getitem_1101 = native_layer_norm_default_149[0]
        _to_copy_716 = torch.ops.aten._to_copy.default(arg336_1, dtype = torch.bfloat16);  arg336_1 = None
        _to_copy_717 = torch.ops.aten._to_copy.default(getitem_1101, dtype = torch.bfloat16)
        t_252 = torch.ops.aten.t.default(_to_copy_716);  _to_copy_716 = None
        view_1245 = torch.ops.aten.view.default(_to_copy_717, [262144, 256]);  _to_copy_717 = None
        mm_235 = torch.ops.aten.mm.default(view_1245, t_252);  view_1245 = t_252 = None
        view_1246 = torch.ops.aten.view.default(mm_235, [1, 512, 512, 8]);  mm_235 = None
        view_1247 = torch.ops.aten.view.default(view_1246, [1, 512, 512, 2, 4]);  view_1246 = None
        permute_731 = torch.ops.aten.permute.default(view_1247, [0, 3, 4, 1, 2]);  view_1247 = None
        view_1248 = torch.ops.aten.view.default(permute_731, [1, 2, 4, 1, 512, 512]);  permute_731 = None
        view_1249 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_88 = torch.ops.aten.bitwise_not.default(view_1249);  view_1249 = None
        masked_fill_88 = torch.ops.aten.masked_fill.Scalar(view_1248, bitwise_not_88, -10000);  view_1248 = bitwise_not_88 = None
        view_1250 = torch.ops.aten.view.default(masked_fill_88, [1, 2, 4, 512, 512]);  masked_fill_88 = None
        permute_732 = torch.ops.aten.permute.default(view_1250, [1, 0, 2, 3, 4]);  view_1250 = None
        view_1251 = torch.ops.aten.view.default(permute_732, [2, 4, 1, 512, 512]);  permute_732 = None
        _to_copy_718 = torch.ops.aten._to_copy.default(arg337_1, dtype = torch.bfloat16);  arg337_1 = None
        _to_copy_719 = torch.ops.aten._to_copy.default(getitem_1101, dtype = torch.bfloat16)
        t_253 = torch.ops.aten.t.default(_to_copy_718);  _to_copy_718 = None
        view_1252 = torch.ops.aten.view.default(_to_copy_719, [262144, 256]);  _to_copy_719 = None
        mm_236 = torch.ops.aten.mm.default(view_1252, t_253);  view_1252 = t_253 = None
        view_1253 = torch.ops.aten.view.default(mm_236, [1, 512, 512, 1024]);  mm_236 = None
        select_27 = torch.ops.aten.select.int(view_1251, 0, 0)
        view_1254 = torch.ops.aten.view.default(view_1253, [1, 512, 512, 4, 4, 64]);  view_1253 = None
        permute_733 = torch.ops.aten.permute.default(view_1254, [4, 0, 3, 1, 2, 5]);  view_1254 = None
        view_1255 = torch.ops.aten.view.default(permute_733, [4, 4, 512, 512, 64]);  permute_733 = None
        unbind_int_61 = torch.ops.aten.unbind.int(view_1255);  view_1255 = None
        getitem_1104 = unbind_int_61[0]
        getitem_1105 = unbind_int_61[1]
        getitem_1106 = unbind_int_61[2]
        getitem_1107 = unbind_int_61[3];  unbind_int_61 = None
        expand_62 = torch.ops.aten.expand.default(select_27, [4, 512, 512, 512]);  select_27 = None
        _scaled_dot_product_efficient_attention_default_33 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1104, getitem_1105, getitem_1106, expand_62, False);  getitem_1104 = getitem_1105 = getitem_1106 = expand_62 = None
        getitem_1108 = _scaled_dot_product_efficient_attention_default_33[0]
        sigmoid_87 = torch.ops.aten.sigmoid.default(getitem_1107);  getitem_1107 = None
        mul_140 = torch.ops.aten.mul.Tensor(getitem_1108, sigmoid_87);  getitem_1108 = sigmoid_87 = None
        view_1256 = torch.ops.aten.view.default(mul_140, [1, 4, 512, 512, 64]);  mul_140 = None
        permute_734 = torch.ops.aten.permute.default(view_1256, [0, 2, 3, 1, 4]);  view_1256 = None
        clone_131 = torch.ops.aten.clone.default(permute_734, memory_format = torch.contiguous_format);  permute_734 = None
        _unsafe_view_116 = torch.ops.aten._unsafe_view.default(clone_131, [1, 512, 512, 256]);  clone_131 = None
        transpose_27 = torch.ops.aten.transpose.int(getitem_1101, 1, 2);  getitem_1101 = None
        _to_copy_720 = torch.ops.aten._to_copy.default(arg338_1, dtype = torch.bfloat16);  arg338_1 = None
        _to_copy_721 = torch.ops.aten._to_copy.default(transpose_27, dtype = torch.bfloat16);  transpose_27 = None
        t_254 = torch.ops.aten.t.default(_to_copy_720);  _to_copy_720 = None
        expand_63 = torch.ops.aten.expand.default(_to_copy_721, [1, 512, 512, 256]);  _to_copy_721 = None
        view_1257 = torch.ops.aten.view.default(expand_63, [512, 512, 256]);  expand_63 = None
        expand_64 = torch.ops.aten.expand.default(t_254, [1, 512, 256, 1024]);  t_254 = None
        view_1258 = torch.ops.aten.view.default(expand_64, [512, 256, 1024]);  expand_64 = None
        bmm_115 = torch.ops.aten.bmm.default(view_1257, view_1258);  view_1257 = view_1258 = None
        view_1259 = torch.ops.aten.view.default(bmm_115, [1, 512, 512, 1024]);  bmm_115 = None
        select_28 = torch.ops.aten.select.int(view_1251, 0, 1);  view_1251 = None
        view_1260 = torch.ops.aten.view.default(view_1259, [1, 512, 512, 4, 4, 64]);  view_1259 = None
        permute_735 = torch.ops.aten.permute.default(view_1260, [4, 0, 3, 1, 2, 5]);  view_1260 = None
        view_1261 = torch.ops.aten.view.default(permute_735, [4, 4, 512, 512, 64]);  permute_735 = None
        unbind_int_62 = torch.ops.aten.unbind.int(view_1261);  view_1261 = None
        getitem_1112 = unbind_int_62[0]
        getitem_1113 = unbind_int_62[1]
        getitem_1114 = unbind_int_62[2]
        getitem_1115 = unbind_int_62[3];  unbind_int_62 = None
        expand_65 = torch.ops.aten.expand.default(select_28, [4, 512, 512, 512]);  select_28 = None
        _scaled_dot_product_efficient_attention_default_34 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1112, getitem_1113, getitem_1114, expand_65, False);  getitem_1112 = getitem_1113 = getitem_1114 = expand_65 = None
        getitem_1116 = _scaled_dot_product_efficient_attention_default_34[0]
        sigmoid_88 = torch.ops.aten.sigmoid.default(getitem_1115);  getitem_1115 = None
        mul_141 = torch.ops.aten.mul.Tensor(getitem_1116, sigmoid_88);  getitem_1116 = sigmoid_88 = None
        view_1262 = torch.ops.aten.view.default(mul_141, [1, 4, 512, 512, 64]);  mul_141 = None
        permute_736 = torch.ops.aten.permute.default(view_1262, [0, 2, 3, 1, 4]);  view_1262 = None
        clone_132 = torch.ops.aten.clone.default(permute_736, memory_format = torch.contiguous_format);  permute_736 = None
        _unsafe_view_117 = torch.ops.aten._unsafe_view.default(clone_132, [1, 512, 512, 256]);  clone_132 = None
        cat_19 = torch.ops.aten.cat.default([_unsafe_view_116, _unsafe_view_117], dim = -1);  _unsafe_view_116 = _unsafe_view_117 = None
        slice_212 = torch.ops.aten.slice.Tensor(arg335_1, dim = 0, start = 0, end = 9223372036854775807);  arg335_1 = None
        unsqueeze_480 = torch.ops.aten.unsqueeze.default(slice_212, 1);  slice_212 = None
        mul_142 = torch.ops.aten.mul.Tensor(arg339_1, unsqueeze_480);  arg339_1 = unsqueeze_480 = None
        _to_copy_722 = torch.ops.aten._to_copy.default(mul_142, dtype = torch.bfloat16);  mul_142 = None
        t_255 = torch.ops.aten.t.default(_to_copy_722);  _to_copy_722 = None
        view_1263 = torch.ops.aten.view.default(cat_19, [262144, 512]);  cat_19 = None
        mm_237 = torch.ops.aten.mm.default(view_1263, t_255);  view_1263 = t_255 = None
        view_1264 = torch.ops.aten.view.default(mm_237, [1, 512, 512, 256]);  mm_237 = None
        add_114 = torch.ops.aten.add.Tensor(add_113, view_1264);  add_113 = view_1264 = None
        split_tensor_111 = torch.ops.aten.split.Tensor(add_107, 512, dim = -2)
        getitem_1120 = split_tensor_111[0];  split_tensor_111 = None
        _to_copy_723 = torch.ops.aten._to_copy.default(getitem_1120, dtype = torch.float32);  getitem_1120 = None
        native_layer_norm_default_150 = torch.ops.aten.native_layer_norm.default(_to_copy_723, [256], arg326_1, arg327_1, 1e-05);  _to_copy_723 = arg326_1 = arg327_1 = None
        getitem_1121 = native_layer_norm_default_150[0]
        _to_copy_724 = torch.ops.aten._to_copy.default(arg328_1, dtype = torch.bfloat16);  arg328_1 = None
        _to_copy_725 = torch.ops.aten._to_copy.default(getitem_1121, dtype = torch.bfloat16);  getitem_1121 = None
        t_256 = torch.ops.aten.t.default(_to_copy_724);  _to_copy_724 = None
        view_1265 = torch.ops.aten.view.default(_to_copy_725, [262144, 256]);  _to_copy_725 = None
        mm_238 = torch.ops.aten.mm.default(view_1265, t_256);  view_1265 = t_256 = None
        view_1266 = torch.ops.aten.view.default(mm_238, [1, 512, 512, 1024]);  mm_238 = None
        split_tensor_112 = torch.ops.aten.split.Tensor(view_1266, 512, dim = -1);  view_1266 = None
        getitem_1124 = split_tensor_112[0]
        getitem_1125 = split_tensor_112[1];  split_tensor_112 = None
        silu_32 = torch.ops.aten.silu.default(getitem_1124);  getitem_1124 = None
        mul_143 = torch.ops.aten.mul.Tensor(silu_32, getitem_1125);  silu_32 = getitem_1125 = None
        _to_copy_726 = torch.ops.aten._to_copy.default(arg329_1, dtype = torch.bfloat16);  arg329_1 = None
        t_257 = torch.ops.aten.t.default(_to_copy_726);  _to_copy_726 = None
        view_1268 = torch.ops.aten.view.default(mul_143, [262144, 512]);  mul_143 = None
        mm_239 = torch.ops.aten.mm.default(view_1268, t_257);  view_1268 = t_257 = None
        view_1269 = torch.ops.aten.view.default(mm_239, [1, 512, 512, 256]);  mm_239 = None
        add_115 = torch.ops.aten.add.Tensor(add_114, view_1269);  add_114 = view_1269 = None
        _to_copy_727 = torch.ops.aten._to_copy.default(add_111, dtype = torch.float32)
        native_layer_norm_default_151 = torch.ops.aten.native_layer_norm.default(_to_copy_727, [384], arg344_1, arg345_1, 1e-05);  _to_copy_727 = arg344_1 = arg345_1 = None
        getitem_1126 = native_layer_norm_default_151[0]
        _to_copy_728 = torch.ops.aten._to_copy.default(add_107, dtype = torch.float32);  add_107 = None
        native_layer_norm_default_152 = torch.ops.aten.native_layer_norm.default(_to_copy_728, [256], arg346_1, arg347_1, 1e-05);  _to_copy_728 = arg346_1 = arg347_1 = None
        getitem_1129 = native_layer_norm_default_152[0]
        _to_copy_729 = torch.ops.aten._to_copy.default(arg348_1, dtype = torch.bfloat16);  arg348_1 = None
        _to_copy_730 = torch.ops.aten._to_copy.default(getitem_1129, dtype = torch.bfloat16);  getitem_1129 = None
        t_258 = torch.ops.aten.t.default(_to_copy_729);  _to_copy_729 = None
        view_1270 = torch.ops.aten.view.default(_to_copy_730, [262144, 256]);  _to_copy_730 = None
        mm_240 = torch.ops.aten.mm.default(view_1270, t_258);  view_1270 = t_258 = None
        view_1271 = torch.ops.aten.view.default(mm_240, [1, 512, 512, 16]);  mm_240 = None
        permute_737 = torch.ops.aten.permute.default(view_1271, [0, 3, 1, 2]);  view_1271 = None
        view_1272 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_89 = torch.ops.aten.bitwise_not.default(view_1272);  view_1272 = None
        masked_fill_89 = torch.ops.aten.masked_fill.Scalar(permute_737, bitwise_not_89, -10000);  permute_737 = bitwise_not_89 = None
        _to_copy_731 = torch.ops.aten._to_copy.default(getitem_1126, dtype = torch.bfloat16);  getitem_1126 = None
        _to_copy_732 = torch.ops.aten._to_copy.default(arg350_1, dtype = torch.bfloat16);  arg350_1 = None
        unsqueeze_481 = torch.ops.aten.unsqueeze.default(_to_copy_731, 3);  _to_copy_731 = None
        unsqueeze_482 = torch.ops.aten.unsqueeze.default(unsqueeze_481, 4);  unsqueeze_481 = None
        unsqueeze_483 = torch.ops.aten.unsqueeze.default(unsqueeze_482, 5);  unsqueeze_482 = None
        permute_738 = torch.ops.aten.permute.default(unsqueeze_483, [3, 0, 4, 1, 5, 2]);  unsqueeze_483 = None
        unsqueeze_484 = torch.ops.aten.unsqueeze.default(_to_copy_732, 4);  _to_copy_732 = None
        unsqueeze_485 = torch.ops.aten.unsqueeze.default(unsqueeze_484, 5);  unsqueeze_484 = None
        permute_739 = torch.ops.aten.permute.default(unsqueeze_485, [1, 4, 2, 5, 3, 0]);  unsqueeze_485 = None
        permute_740 = torch.ops.aten.permute.default(permute_738, [3, 5, 0, 1, 2, 4]);  permute_738 = None
        view_1273 = torch.ops.aten.view.default(permute_740, [1, 512, 384]);  permute_740 = None
        permute_741 = torch.ops.aten.permute.default(permute_739, [5, 0, 1, 2, 4, 3]);  permute_739 = None
        view_1274 = torch.ops.aten.view.default(permute_741, [1, 384, 1536]);  permute_741 = None
        bmm_116 = torch.ops.aten.bmm.default(view_1273, view_1274);  view_1273 = view_1274 = None
        view_1275 = torch.ops.aten.view.default(bmm_116, [512, 1, 4, 1, 16, 24]);  bmm_116 = None
        permute_742 = torch.ops.aten.permute.default(view_1275, [2, 3, 4, 0, 5, 1]);  view_1275 = None
        view_1276 = torch.ops.aten.view.default(permute_742, [4, 1, 16, 512, 24]);  permute_742 = None
        unbind_int_63 = torch.ops.aten.unbind.int(view_1276);  view_1276 = None
        getitem_1132 = unbind_int_63[0]
        getitem_1133 = unbind_int_63[1]
        getitem_1134 = unbind_int_63[2]
        getitem_1135 = unbind_int_63[3];  unbind_int_63 = None
        view_1277 = torch.ops.aten.view.default(arg349_1, [1, 16, 1, 24]);  arg349_1 = None
        add_116 = torch.ops.aten.add.Tensor(getitem_1132, view_1277);  getitem_1132 = view_1277 = None
        _to_copy_733 = torch.ops.aten._to_copy.default(add_116, dtype = torch.bfloat16);  add_116 = None
        expand_66 = torch.ops.aten.expand.default(masked_fill_89, [1, 16, 512, 512]);  masked_fill_89 = None
        _scaled_dot_product_efficient_attention_default_35 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_733, getitem_1133, getitem_1134, expand_66, False);  _to_copy_733 = getitem_1133 = getitem_1134 = expand_66 = None
        getitem_1136 = _scaled_dot_product_efficient_attention_default_35[0]
        add_117 = torch.ops.aten.add.Tensor(getitem_1135, 1);  getitem_1135 = None
        sigmoid_89 = torch.ops.aten.sigmoid.default(add_117);  add_117 = None
        mul_144 = torch.ops.aten.mul.Tensor(getitem_1136, sigmoid_89);  getitem_1136 = sigmoid_89 = None
        _to_copy_734 = torch.ops.aten._to_copy.default(arg351_1, dtype = torch.bfloat16);  arg351_1 = None
        unsqueeze_486 = torch.ops.aten.unsqueeze.default(mul_144, 4);  mul_144 = None
        permute_743 = torch.ops.aten.permute.default(unsqueeze_486, [0, 2, 4, 3, 1]);  unsqueeze_486 = None
        unsqueeze_487 = torch.ops.aten.unsqueeze.default(_to_copy_734, 3);  _to_copy_734 = None
        unsqueeze_488 = torch.ops.aten.unsqueeze.default(unsqueeze_487, 4);  unsqueeze_487 = None
        permute_744 = torch.ops.aten.permute.default(unsqueeze_488, [3, 4, 2, 1, 0]);  unsqueeze_488 = None
        permute_745 = torch.ops.aten.permute.default(permute_743, [1, 3, 4, 0, 2]);  permute_743 = None
        clone_133 = torch.ops.aten.clone.default(permute_745, memory_format = torch.contiguous_format);  permute_745 = None
        _unsafe_view_118 = torch.ops.aten._unsafe_view.default(clone_133, [1, 512, 384]);  clone_133 = None
        permute_746 = torch.ops.aten.permute.default(permute_744, [3, 4, 0, 2, 1]);  permute_744 = None
        clone_134 = torch.ops.aten.clone.default(permute_746, memory_format = torch.contiguous_format);  permute_746 = None
        _unsafe_view_119 = torch.ops.aten._unsafe_view.default(clone_134, [1, 384, 384]);  clone_134 = None
        bmm_117 = torch.ops.aten.bmm.default(_unsafe_view_118, _unsafe_view_119);  _unsafe_view_118 = _unsafe_view_119 = None
        view_1278 = torch.ops.aten.view.default(bmm_117, [512, 1, 1, 1, 384]);  bmm_117 = None
        permute_747 = torch.ops.aten.permute.default(view_1278, [3, 0, 4, 1, 2]);  view_1278 = None
        view_1279 = torch.ops.aten.view.default(permute_747, [1, 512, 384]);  permute_747 = None
        unsqueeze_489 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_145 = torch.ops.aten.mul.Tensor(view_1279, unsqueeze_489);  view_1279 = unsqueeze_489 = None
        add_118 = torch.ops.aten.add.Tensor(add_111, mul_145);  mul_145 = None
        split_tensor_113 = torch.ops.aten.split.Tensor(add_111, 512, dim = -2);  add_111 = None
        getitem_1140 = split_tensor_113[0];  split_tensor_113 = None
        _to_copy_735 = torch.ops.aten._to_copy.default(getitem_1140, dtype = torch.float32);  getitem_1140 = None
        native_layer_norm_default_153 = torch.ops.aten.native_layer_norm.default(_to_copy_735, [384], arg340_1, arg341_1, 1e-05);  _to_copy_735 = arg340_1 = arg341_1 = None
        getitem_1141 = native_layer_norm_default_153[0]
        _to_copy_736 = torch.ops.aten._to_copy.default(arg342_1, dtype = torch.bfloat16);  arg342_1 = None
        _to_copy_737 = torch.ops.aten._to_copy.default(getitem_1141, dtype = torch.bfloat16);  getitem_1141 = None
        t_259 = torch.ops.aten.t.default(_to_copy_736);  _to_copy_736 = None
        view_1280 = torch.ops.aten.view.default(_to_copy_737, [512, 384]);  _to_copy_737 = None
        mm_241 = torch.ops.aten.mm.default(view_1280, t_259);  view_1280 = t_259 = None
        view_1281 = torch.ops.aten.view.default(mm_241, [1, 512, 1536]);  mm_241 = None
        split_tensor_114 = torch.ops.aten.split.Tensor(view_1281, 768, dim = -1);  view_1281 = None
        getitem_1144 = split_tensor_114[0]
        getitem_1145 = split_tensor_114[1];  split_tensor_114 = None
        silu_33 = torch.ops.aten.silu.default(getitem_1144);  getitem_1144 = None
        mul_146 = torch.ops.aten.mul.Tensor(silu_33, getitem_1145);  silu_33 = getitem_1145 = None
        _to_copy_738 = torch.ops.aten._to_copy.default(arg343_1, dtype = torch.bfloat16);  arg343_1 = None
        t_260 = torch.ops.aten.t.default(_to_copy_738);  _to_copy_738 = None
        view_1283 = torch.ops.aten.view.default(mul_146, [512, 768]);  mul_146 = None
        mm_242 = torch.ops.aten.mm.default(view_1283, t_260);  view_1283 = t_260 = None
        view_1284 = torch.ops.aten.view.default(mm_242, [1, 512, 384]);  mm_242 = None
        add_119 = torch.ops.aten.add.Tensor(add_118, view_1284);  add_118 = view_1284 = None
        _to_copy_739 = torch.ops.aten._to_copy.default(add_115, dtype = torch.float32)
        native_layer_norm_default_154 = torch.ops.aten.native_layer_norm.default(_to_copy_739, [256], arg356_1, arg357_1, 1e-05);  _to_copy_739 = arg356_1 = arg357_1 = None
        getitem_1146 = native_layer_norm_default_154[0]
        split_with_sizes_default_28 = torch.ops.aten.split_with_sizes.default(arg359_1, [512, 512]);  arg359_1 = None
        getitem_1149 = split_with_sizes_default_28[0]
        getitem_1150 = split_with_sizes_default_28[1];  split_with_sizes_default_28 = None
        split_with_sizes_default_29 = torch.ops.aten.split_with_sizes.default(arg360_1, [512, 512, 256]);  arg360_1 = None
        getitem_1151 = split_with_sizes_default_29[0]
        getitem_1152 = split_with_sizes_default_29[1]
        getitem_1153 = split_with_sizes_default_29[2];  split_with_sizes_default_29 = None
        _to_copy_740 = torch.ops.aten._to_copy.default(getitem_1149, dtype = torch.bfloat16);  getitem_1149 = None
        _to_copy_741 = torch.ops.aten._to_copy.default(getitem_1146, dtype = torch.bfloat16)
        t_261 = torch.ops.aten.t.default(_to_copy_740);  _to_copy_740 = None
        view_1285 = torch.ops.aten.view.default(_to_copy_741, [262144, 256]);  _to_copy_741 = None
        mm_243 = torch.ops.aten.mm.default(view_1285, t_261);  view_1285 = t_261 = None
        view_1286 = torch.ops.aten.view.default(mm_243, [1, 512, 512, 512]);  mm_243 = None
        _to_copy_742 = torch.ops.aten._to_copy.default(getitem_1151, dtype = torch.bfloat16);  getitem_1151 = None
        _to_copy_743 = torch.ops.aten._to_copy.default(getitem_1146, dtype = torch.bfloat16)
        t_262 = torch.ops.aten.t.default(_to_copy_742);  _to_copy_742 = None
        view_1287 = torch.ops.aten.view.default(_to_copy_743, [262144, 256]);  _to_copy_743 = None
        mm_244 = torch.ops.aten.mm.default(view_1287, t_262);  view_1287 = t_262 = None
        view_1288 = torch.ops.aten.view.default(mm_244, [1, 512, 512, 512]);  mm_244 = None
        sigmoid_90 = torch.ops.aten.sigmoid.default(view_1288);  view_1288 = None
        mul_147 = torch.ops.aten.mul.Tensor(view_1286, sigmoid_90);  view_1286 = sigmoid_90 = None
        unsqueeze_490 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_90 = torch.ops.aten.bitwise_not.default(unsqueeze_490);  unsqueeze_490 = None
        masked_fill_90 = torch.ops.aten.masked_fill.Scalar(mul_147, bitwise_not_90, 0);  mul_147 = bitwise_not_90 = None
        split_tensor_115 = torch.ops.aten.split.Tensor(masked_fill_90, 256, dim = -1)
        getitem_1156 = split_tensor_115[0]
        unsqueeze_493 = torch.ops.aten.unsqueeze.default(getitem_1156, 4);  getitem_1156 = None
        permute_752 = torch.ops.aten.permute.default(unsqueeze_493, [0, 1, 4, 3, 2]);  unsqueeze_493 = None
        permute_753 = torch.ops.aten.permute.default(permute_752, [3, 1, 4, 0, 2]);  permute_752 = None
        view_1291 = torch.ops.aten.view.default(permute_753, [256, 512, 512]);  permute_753 = None
        split_tensor_116 = torch.ops.aten.split.Tensor(masked_fill_90, 256, dim = -1);  masked_fill_90 = None
        getitem_1159 = split_tensor_116[1];  split_tensor_116 = None
        unsqueeze_494 = torch.ops.aten.unsqueeze.default(getitem_1159, 4);  getitem_1159 = None
        permute_754 = torch.ops.aten.permute.default(unsqueeze_494, [0, 4, 1, 3, 2]);  unsqueeze_494 = None
        permute_755 = torch.ops.aten.permute.default(permute_754, [3, 4, 0, 2, 1]);  permute_754 = None
        view_1292 = torch.ops.aten.view.default(permute_755, [256, 512, 512]);  permute_755 = None
        bmm_118 = torch.ops.aten.bmm.default(view_1291, view_1292);  view_1291 = view_1292 = None
        view_1293 = torch.ops.aten.view.default(bmm_118, [256, 512, 1, 1, 512]);  bmm_118 = None
        permute_756 = torch.ops.aten.permute.default(view_1293, [3, 1, 4, 0, 2]);  view_1293 = None
        view_1294 = torch.ops.aten.view.default(permute_756, [1, 512, 512, 256]);  permute_756 = None
        _to_copy_744 = torch.ops.aten._to_copy.default(getitem_1150, dtype = torch.bfloat16);  getitem_1150 = None
        _to_copy_745 = torch.ops.aten._to_copy.default(getitem_1146, dtype = torch.bfloat16)
        t_263 = torch.ops.aten.t.default(_to_copy_744);  _to_copy_744 = None
        view_1295 = torch.ops.aten.view.default(_to_copy_745, [262144, 256]);  _to_copy_745 = None
        mm_245 = torch.ops.aten.mm.default(view_1295, t_263);  view_1295 = t_263 = None
        view_1296 = torch.ops.aten.view.default(mm_245, [1, 512, 512, 512]);  mm_245 = None
        _to_copy_746 = torch.ops.aten._to_copy.default(getitem_1152, dtype = torch.bfloat16);  getitem_1152 = None
        _to_copy_747 = torch.ops.aten._to_copy.default(getitem_1146, dtype = torch.bfloat16)
        t_264 = torch.ops.aten.t.default(_to_copy_746);  _to_copy_746 = None
        view_1297 = torch.ops.aten.view.default(_to_copy_747, [262144, 256]);  _to_copy_747 = None
        mm_246 = torch.ops.aten.mm.default(view_1297, t_264);  view_1297 = t_264 = None
        view_1298 = torch.ops.aten.view.default(mm_246, [1, 512, 512, 512]);  mm_246 = None
        sigmoid_91 = torch.ops.aten.sigmoid.default(view_1298);  view_1298 = None
        mul_148 = torch.ops.aten.mul.Tensor(view_1296, sigmoid_91);  view_1296 = sigmoid_91 = None
        view_1299 = torch.ops.aten.view.default(mul_148, [262144, 512]);  mul_148 = None
        view_1300 = torch.ops.aten.view.default(view_1299, [1, 512, 512, 512]);  view_1299 = None
        transpose_28 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_495 = torch.ops.aten.unsqueeze.default(transpose_28, 3);  transpose_28 = None
        clone_135 = torch.ops.aten.clone.default(unsqueeze_495, memory_format = torch.contiguous_format);  unsqueeze_495 = None
        bitwise_not_91 = torch.ops.aten.bitwise_not.default(clone_135);  clone_135 = None
        masked_fill_91 = torch.ops.aten.masked_fill.Scalar(view_1300, bitwise_not_91, 0);  view_1300 = bitwise_not_91 = None
        view_1301 = torch.ops.aten.view.default(masked_fill_91, [262144, 512]);  masked_fill_91 = None
        view_1305 = torch.ops.aten.view.default(view_1301, [1, 512, 512, 512])
        split_tensor_117 = torch.ops.aten.split.Tensor(view_1305, 256, dim = -1);  view_1305 = None
        getitem_1162 = split_tensor_117[0]
        unsqueeze_498 = torch.ops.aten.unsqueeze.default(getitem_1162, 4);  getitem_1162 = None
        permute_761 = torch.ops.aten.permute.default(unsqueeze_498, [0, 2, 4, 3, 1]);  unsqueeze_498 = None
        permute_762 = torch.ops.aten.permute.default(permute_761, [3, 1, 4, 0, 2]);  permute_761 = None
        view_1306 = torch.ops.aten.view.default(permute_762, [256, 512, 512]);  permute_762 = None
        view_1307 = torch.ops.aten.view.default(view_1301, [1, 512, 512, 512]);  view_1301 = None
        split_tensor_118 = torch.ops.aten.split.Tensor(view_1307, 256, dim = -1);  view_1307 = None
        getitem_1165 = split_tensor_118[1];  split_tensor_118 = None
        unsqueeze_499 = torch.ops.aten.unsqueeze.default(getitem_1165, 4);  getitem_1165 = None
        permute_763 = torch.ops.aten.permute.default(unsqueeze_499, [0, 4, 2, 3, 1]);  unsqueeze_499 = None
        permute_764 = torch.ops.aten.permute.default(permute_763, [3, 4, 0, 2, 1]);  permute_763 = None
        view_1308 = torch.ops.aten.view.default(permute_764, [256, 512, 512]);  permute_764 = None
        bmm_119 = torch.ops.aten.bmm.default(view_1306, view_1308);  view_1306 = view_1308 = None
        view_1309 = torch.ops.aten.view.default(bmm_119, [256, 512, 1, 1, 512]);  bmm_119 = None
        permute_765 = torch.ops.aten.permute.default(view_1309, [3, 1, 4, 0, 2]);  view_1309 = None
        view_1310 = torch.ops.aten.view.default(permute_765, [1, 512, 512, 256]);  permute_765 = None
        _to_copy_748 = torch.ops.aten._to_copy.default(view_1294, dtype = torch.float32);  view_1294 = None
        native_layer_norm_default_155 = torch.ops.aten.native_layer_norm.default(_to_copy_748, [256], None, None, 1e-05);  _to_copy_748 = None
        getitem_1166 = native_layer_norm_default_155[0]
        _to_copy_749 = torch.ops.aten._to_copy.default(view_1310, dtype = torch.float32);  view_1310 = None
        native_layer_norm_default_156 = torch.ops.aten.native_layer_norm.default(_to_copy_749, [256], None, None, 1e-05);  _to_copy_749 = None
        getitem_1169 = native_layer_norm_default_156[0]
        add_120 = torch.ops.aten.add.Tensor(getitem_1166, getitem_1169);  getitem_1166 = getitem_1169 = None
        _to_copy_750 = torch.ops.aten._to_copy.default(arg358_1, dtype = torch.bfloat16);  arg358_1 = None
        _to_copy_751 = torch.ops.aten._to_copy.default(add_120, dtype = torch.bfloat16);  add_120 = None
        t_265 = torch.ops.aten.t.default(_to_copy_750);  _to_copy_750 = None
        view_1311 = torch.ops.aten.view.default(_to_copy_751, [262144, 256]);  _to_copy_751 = None
        mm_247 = torch.ops.aten.mm.default(view_1311, t_265);  view_1311 = t_265 = None
        view_1312 = torch.ops.aten.view.default(mm_247, [1, 512, 512, 256]);  mm_247 = None
        _to_copy_752 = torch.ops.aten._to_copy.default(getitem_1153, dtype = torch.bfloat16);  getitem_1153 = None
        _to_copy_753 = torch.ops.aten._to_copy.default(getitem_1146, dtype = torch.bfloat16);  getitem_1146 = None
        t_266 = torch.ops.aten.t.default(_to_copy_752);  _to_copy_752 = None
        view_1313 = torch.ops.aten.view.default(_to_copy_753, [262144, 256]);  _to_copy_753 = None
        mm_248 = torch.ops.aten.mm.default(view_1313, t_266);  view_1313 = t_266 = None
        view_1314 = torch.ops.aten.view.default(mm_248, [1, 512, 512, 256]);  mm_248 = None
        sigmoid_92 = torch.ops.aten.sigmoid.default(view_1314);  view_1314 = None
        mul_149 = torch.ops.aten.mul.Tensor(view_1312, sigmoid_92);  view_1312 = sigmoid_92 = None
        add_121 = torch.ops.aten.add.Tensor(add_115, mul_149);  mul_149 = None
        _to_copy_754 = torch.ops.aten._to_copy.default(add_115, dtype = torch.float32)
        native_layer_norm_default_157 = torch.ops.aten.native_layer_norm.default(_to_copy_754, [256], None, None, 1e-05);  _to_copy_754 = None
        getitem_1172 = native_layer_norm_default_157[0]
        _to_copy_755 = torch.ops.aten._to_copy.default(arg362_1, dtype = torch.bfloat16);  arg362_1 = None
        _to_copy_756 = torch.ops.aten._to_copy.default(getitem_1172, dtype = torch.bfloat16)
        t_267 = torch.ops.aten.t.default(_to_copy_755);  _to_copy_755 = None
        view_1315 = torch.ops.aten.view.default(_to_copy_756, [262144, 256]);  _to_copy_756 = None
        mm_249 = torch.ops.aten.mm.default(view_1315, t_267);  view_1315 = t_267 = None
        view_1316 = torch.ops.aten.view.default(mm_249, [1, 512, 512, 8]);  mm_249 = None
        view_1317 = torch.ops.aten.view.default(view_1316, [1, 512, 512, 2, 4]);  view_1316 = None
        permute_766 = torch.ops.aten.permute.default(view_1317, [0, 3, 4, 1, 2]);  view_1317 = None
        view_1318 = torch.ops.aten.view.default(permute_766, [1, 2, 4, 1, 512, 512]);  permute_766 = None
        view_1319 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_92 = torch.ops.aten.bitwise_not.default(view_1319);  view_1319 = None
        masked_fill_92 = torch.ops.aten.masked_fill.Scalar(view_1318, bitwise_not_92, -10000);  view_1318 = bitwise_not_92 = None
        view_1320 = torch.ops.aten.view.default(masked_fill_92, [1, 2, 4, 512, 512]);  masked_fill_92 = None
        permute_767 = torch.ops.aten.permute.default(view_1320, [1, 0, 2, 3, 4]);  view_1320 = None
        view_1321 = torch.ops.aten.view.default(permute_767, [2, 4, 1, 512, 512]);  permute_767 = None
        _to_copy_757 = torch.ops.aten._to_copy.default(arg363_1, dtype = torch.bfloat16);  arg363_1 = None
        _to_copy_758 = torch.ops.aten._to_copy.default(getitem_1172, dtype = torch.bfloat16)
        t_268 = torch.ops.aten.t.default(_to_copy_757);  _to_copy_757 = None
        view_1322 = torch.ops.aten.view.default(_to_copy_758, [262144, 256]);  _to_copy_758 = None
        mm_250 = torch.ops.aten.mm.default(view_1322, t_268);  view_1322 = t_268 = None
        view_1323 = torch.ops.aten.view.default(mm_250, [1, 512, 512, 1024]);  mm_250 = None
        select_29 = torch.ops.aten.select.int(view_1321, 0, 0)
        view_1324 = torch.ops.aten.view.default(view_1323, [1, 512, 512, 4, 4, 64]);  view_1323 = None
        permute_768 = torch.ops.aten.permute.default(view_1324, [4, 0, 3, 1, 2, 5]);  view_1324 = None
        view_1325 = torch.ops.aten.view.default(permute_768, [4, 4, 512, 512, 64]);  permute_768 = None
        unbind_int_64 = torch.ops.aten.unbind.int(view_1325);  view_1325 = None
        getitem_1175 = unbind_int_64[0]
        getitem_1176 = unbind_int_64[1]
        getitem_1177 = unbind_int_64[2]
        getitem_1178 = unbind_int_64[3];  unbind_int_64 = None
        expand_67 = torch.ops.aten.expand.default(select_29, [4, 512, 512, 512]);  select_29 = None
        _scaled_dot_product_efficient_attention_default_36 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1175, getitem_1176, getitem_1177, expand_67, False);  getitem_1175 = getitem_1176 = getitem_1177 = expand_67 = None
        getitem_1179 = _scaled_dot_product_efficient_attention_default_36[0]
        sigmoid_93 = torch.ops.aten.sigmoid.default(getitem_1178);  getitem_1178 = None
        mul_150 = torch.ops.aten.mul.Tensor(getitem_1179, sigmoid_93);  getitem_1179 = sigmoid_93 = None
        view_1326 = torch.ops.aten.view.default(mul_150, [1, 4, 512, 512, 64]);  mul_150 = None
        permute_769 = torch.ops.aten.permute.default(view_1326, [0, 2, 3, 1, 4]);  view_1326 = None
        clone_136 = torch.ops.aten.clone.default(permute_769, memory_format = torch.contiguous_format);  permute_769 = None
        _unsafe_view_120 = torch.ops.aten._unsafe_view.default(clone_136, [1, 512, 512, 256]);  clone_136 = None
        transpose_29 = torch.ops.aten.transpose.int(getitem_1172, 1, 2);  getitem_1172 = None
        _to_copy_759 = torch.ops.aten._to_copy.default(arg364_1, dtype = torch.bfloat16);  arg364_1 = None
        _to_copy_760 = torch.ops.aten._to_copy.default(transpose_29, dtype = torch.bfloat16);  transpose_29 = None
        t_269 = torch.ops.aten.t.default(_to_copy_759);  _to_copy_759 = None
        expand_68 = torch.ops.aten.expand.default(_to_copy_760, [1, 512, 512, 256]);  _to_copy_760 = None
        view_1327 = torch.ops.aten.view.default(expand_68, [512, 512, 256]);  expand_68 = None
        expand_69 = torch.ops.aten.expand.default(t_269, [1, 512, 256, 1024]);  t_269 = None
        view_1328 = torch.ops.aten.view.default(expand_69, [512, 256, 1024]);  expand_69 = None
        bmm_120 = torch.ops.aten.bmm.default(view_1327, view_1328);  view_1327 = view_1328 = None
        view_1329 = torch.ops.aten.view.default(bmm_120, [1, 512, 512, 1024]);  bmm_120 = None
        select_30 = torch.ops.aten.select.int(view_1321, 0, 1);  view_1321 = None
        view_1330 = torch.ops.aten.view.default(view_1329, [1, 512, 512, 4, 4, 64]);  view_1329 = None
        permute_770 = torch.ops.aten.permute.default(view_1330, [4, 0, 3, 1, 2, 5]);  view_1330 = None
        view_1331 = torch.ops.aten.view.default(permute_770, [4, 4, 512, 512, 64]);  permute_770 = None
        unbind_int_65 = torch.ops.aten.unbind.int(view_1331);  view_1331 = None
        getitem_1183 = unbind_int_65[0]
        getitem_1184 = unbind_int_65[1]
        getitem_1185 = unbind_int_65[2]
        getitem_1186 = unbind_int_65[3];  unbind_int_65 = None
        expand_70 = torch.ops.aten.expand.default(select_30, [4, 512, 512, 512]);  select_30 = None
        _scaled_dot_product_efficient_attention_default_37 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1183, getitem_1184, getitem_1185, expand_70, False);  getitem_1183 = getitem_1184 = getitem_1185 = expand_70 = None
        getitem_1187 = _scaled_dot_product_efficient_attention_default_37[0]
        sigmoid_94 = torch.ops.aten.sigmoid.default(getitem_1186);  getitem_1186 = None
        mul_151 = torch.ops.aten.mul.Tensor(getitem_1187, sigmoid_94);  getitem_1187 = sigmoid_94 = None
        view_1332 = torch.ops.aten.view.default(mul_151, [1, 4, 512, 512, 64]);  mul_151 = None
        permute_771 = torch.ops.aten.permute.default(view_1332, [0, 2, 3, 1, 4]);  view_1332 = None
        clone_137 = torch.ops.aten.clone.default(permute_771, memory_format = torch.contiguous_format);  permute_771 = None
        _unsafe_view_121 = torch.ops.aten._unsafe_view.default(clone_137, [1, 512, 512, 256]);  clone_137 = None
        cat_20 = torch.ops.aten.cat.default([_unsafe_view_120, _unsafe_view_121], dim = -1);  _unsafe_view_120 = _unsafe_view_121 = None
        slice_213 = torch.ops.aten.slice.Tensor(arg361_1, dim = 0, start = 0, end = 9223372036854775807);  arg361_1 = None
        unsqueeze_500 = torch.ops.aten.unsqueeze.default(slice_213, 1);  slice_213 = None
        mul_152 = torch.ops.aten.mul.Tensor(arg365_1, unsqueeze_500);  arg365_1 = unsqueeze_500 = None
        _to_copy_761 = torch.ops.aten._to_copy.default(mul_152, dtype = torch.bfloat16);  mul_152 = None
        t_270 = torch.ops.aten.t.default(_to_copy_761);  _to_copy_761 = None
        view_1333 = torch.ops.aten.view.default(cat_20, [262144, 512]);  cat_20 = None
        mm_251 = torch.ops.aten.mm.default(view_1333, t_270);  view_1333 = t_270 = None
        view_1334 = torch.ops.aten.view.default(mm_251, [1, 512, 512, 256]);  mm_251 = None
        add_122 = torch.ops.aten.add.Tensor(add_121, view_1334);  add_121 = view_1334 = None
        split_tensor_119 = torch.ops.aten.split.Tensor(add_115, 512, dim = -2)
        getitem_1191 = split_tensor_119[0];  split_tensor_119 = None
        _to_copy_762 = torch.ops.aten._to_copy.default(getitem_1191, dtype = torch.float32);  getitem_1191 = None
        native_layer_norm_default_158 = torch.ops.aten.native_layer_norm.default(_to_copy_762, [256], arg352_1, arg353_1, 1e-05);  _to_copy_762 = arg352_1 = arg353_1 = None
        getitem_1192 = native_layer_norm_default_158[0]
        _to_copy_763 = torch.ops.aten._to_copy.default(arg354_1, dtype = torch.bfloat16);  arg354_1 = None
        _to_copy_764 = torch.ops.aten._to_copy.default(getitem_1192, dtype = torch.bfloat16);  getitem_1192 = None
        t_271 = torch.ops.aten.t.default(_to_copy_763);  _to_copy_763 = None
        view_1335 = torch.ops.aten.view.default(_to_copy_764, [262144, 256]);  _to_copy_764 = None
        mm_252 = torch.ops.aten.mm.default(view_1335, t_271);  view_1335 = t_271 = None
        view_1336 = torch.ops.aten.view.default(mm_252, [1, 512, 512, 1024]);  mm_252 = None
        split_tensor_120 = torch.ops.aten.split.Tensor(view_1336, 512, dim = -1);  view_1336 = None
        getitem_1195 = split_tensor_120[0]
        getitem_1196 = split_tensor_120[1];  split_tensor_120 = None
        silu_34 = torch.ops.aten.silu.default(getitem_1195);  getitem_1195 = None
        mul_153 = torch.ops.aten.mul.Tensor(silu_34, getitem_1196);  silu_34 = getitem_1196 = None
        _to_copy_765 = torch.ops.aten._to_copy.default(arg355_1, dtype = torch.bfloat16);  arg355_1 = None
        t_272 = torch.ops.aten.t.default(_to_copy_765);  _to_copy_765 = None
        view_1338 = torch.ops.aten.view.default(mul_153, [262144, 512]);  mul_153 = None
        mm_253 = torch.ops.aten.mm.default(view_1338, t_272);  view_1338 = t_272 = None
        view_1339 = torch.ops.aten.view.default(mm_253, [1, 512, 512, 256]);  mm_253 = None
        add_123 = torch.ops.aten.add.Tensor(add_122, view_1339);  add_122 = view_1339 = None
        _to_copy_766 = torch.ops.aten._to_copy.default(add_119, dtype = torch.float32)
        native_layer_norm_default_159 = torch.ops.aten.native_layer_norm.default(_to_copy_766, [384], arg370_1, arg371_1, 1e-05);  _to_copy_766 = arg370_1 = arg371_1 = None
        getitem_1197 = native_layer_norm_default_159[0]
        _to_copy_767 = torch.ops.aten._to_copy.default(add_115, dtype = torch.float32);  add_115 = None
        native_layer_norm_default_160 = torch.ops.aten.native_layer_norm.default(_to_copy_767, [256], arg372_1, arg373_1, 1e-05);  _to_copy_767 = arg372_1 = arg373_1 = None
        getitem_1200 = native_layer_norm_default_160[0]
        _to_copy_768 = torch.ops.aten._to_copy.default(arg374_1, dtype = torch.bfloat16);  arg374_1 = None
        _to_copy_769 = torch.ops.aten._to_copy.default(getitem_1200, dtype = torch.bfloat16);  getitem_1200 = None
        t_273 = torch.ops.aten.t.default(_to_copy_768);  _to_copy_768 = None
        view_1340 = torch.ops.aten.view.default(_to_copy_769, [262144, 256]);  _to_copy_769 = None
        mm_254 = torch.ops.aten.mm.default(view_1340, t_273);  view_1340 = t_273 = None
        view_1341 = torch.ops.aten.view.default(mm_254, [1, 512, 512, 16]);  mm_254 = None
        permute_772 = torch.ops.aten.permute.default(view_1341, [0, 3, 1, 2]);  view_1341 = None
        view_1342 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_93 = torch.ops.aten.bitwise_not.default(view_1342);  view_1342 = None
        masked_fill_93 = torch.ops.aten.masked_fill.Scalar(permute_772, bitwise_not_93, -10000);  permute_772 = bitwise_not_93 = None
        _to_copy_770 = torch.ops.aten._to_copy.default(getitem_1197, dtype = torch.bfloat16);  getitem_1197 = None
        _to_copy_771 = torch.ops.aten._to_copy.default(arg376_1, dtype = torch.bfloat16);  arg376_1 = None
        unsqueeze_501 = torch.ops.aten.unsqueeze.default(_to_copy_770, 3);  _to_copy_770 = None
        unsqueeze_502 = torch.ops.aten.unsqueeze.default(unsqueeze_501, 4);  unsqueeze_501 = None
        unsqueeze_503 = torch.ops.aten.unsqueeze.default(unsqueeze_502, 5);  unsqueeze_502 = None
        permute_773 = torch.ops.aten.permute.default(unsqueeze_503, [3, 0, 4, 1, 5, 2]);  unsqueeze_503 = None
        unsqueeze_504 = torch.ops.aten.unsqueeze.default(_to_copy_771, 4);  _to_copy_771 = None
        unsqueeze_505 = torch.ops.aten.unsqueeze.default(unsqueeze_504, 5);  unsqueeze_504 = None
        permute_774 = torch.ops.aten.permute.default(unsqueeze_505, [1, 4, 2, 5, 3, 0]);  unsqueeze_505 = None
        permute_775 = torch.ops.aten.permute.default(permute_773, [3, 5, 0, 1, 2, 4]);  permute_773 = None
        view_1343 = torch.ops.aten.view.default(permute_775, [1, 512, 384]);  permute_775 = None
        permute_776 = torch.ops.aten.permute.default(permute_774, [5, 0, 1, 2, 4, 3]);  permute_774 = None
        view_1344 = torch.ops.aten.view.default(permute_776, [1, 384, 1536]);  permute_776 = None
        bmm_121 = torch.ops.aten.bmm.default(view_1343, view_1344);  view_1343 = view_1344 = None
        view_1345 = torch.ops.aten.view.default(bmm_121, [512, 1, 4, 1, 16, 24]);  bmm_121 = None
        permute_777 = torch.ops.aten.permute.default(view_1345, [2, 3, 4, 0, 5, 1]);  view_1345 = None
        view_1346 = torch.ops.aten.view.default(permute_777, [4, 1, 16, 512, 24]);  permute_777 = None
        unbind_int_66 = torch.ops.aten.unbind.int(view_1346);  view_1346 = None
        getitem_1203 = unbind_int_66[0]
        getitem_1204 = unbind_int_66[1]
        getitem_1205 = unbind_int_66[2]
        getitem_1206 = unbind_int_66[3];  unbind_int_66 = None
        view_1347 = torch.ops.aten.view.default(arg375_1, [1, 16, 1, 24]);  arg375_1 = None
        add_124 = torch.ops.aten.add.Tensor(getitem_1203, view_1347);  getitem_1203 = view_1347 = None
        _to_copy_772 = torch.ops.aten._to_copy.default(add_124, dtype = torch.bfloat16);  add_124 = None
        expand_71 = torch.ops.aten.expand.default(masked_fill_93, [1, 16, 512, 512]);  masked_fill_93 = None
        _scaled_dot_product_efficient_attention_default_38 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_772, getitem_1204, getitem_1205, expand_71, False);  _to_copy_772 = getitem_1204 = getitem_1205 = expand_71 = None
        getitem_1207 = _scaled_dot_product_efficient_attention_default_38[0]
        add_125 = torch.ops.aten.add.Tensor(getitem_1206, 1);  getitem_1206 = None
        sigmoid_95 = torch.ops.aten.sigmoid.default(add_125);  add_125 = None
        mul_154 = torch.ops.aten.mul.Tensor(getitem_1207, sigmoid_95);  getitem_1207 = sigmoid_95 = None
        _to_copy_773 = torch.ops.aten._to_copy.default(arg377_1, dtype = torch.bfloat16);  arg377_1 = None
        unsqueeze_506 = torch.ops.aten.unsqueeze.default(mul_154, 4);  mul_154 = None
        permute_778 = torch.ops.aten.permute.default(unsqueeze_506, [0, 2, 4, 3, 1]);  unsqueeze_506 = None
        unsqueeze_507 = torch.ops.aten.unsqueeze.default(_to_copy_773, 3);  _to_copy_773 = None
        unsqueeze_508 = torch.ops.aten.unsqueeze.default(unsqueeze_507, 4);  unsqueeze_507 = None
        permute_779 = torch.ops.aten.permute.default(unsqueeze_508, [3, 4, 2, 1, 0]);  unsqueeze_508 = None
        permute_780 = torch.ops.aten.permute.default(permute_778, [1, 3, 4, 0, 2]);  permute_778 = None
        clone_138 = torch.ops.aten.clone.default(permute_780, memory_format = torch.contiguous_format);  permute_780 = None
        _unsafe_view_122 = torch.ops.aten._unsafe_view.default(clone_138, [1, 512, 384]);  clone_138 = None
        permute_781 = torch.ops.aten.permute.default(permute_779, [3, 4, 0, 2, 1]);  permute_779 = None
        clone_139 = torch.ops.aten.clone.default(permute_781, memory_format = torch.contiguous_format);  permute_781 = None
        _unsafe_view_123 = torch.ops.aten._unsafe_view.default(clone_139, [1, 384, 384]);  clone_139 = None
        bmm_122 = torch.ops.aten.bmm.default(_unsafe_view_122, _unsafe_view_123);  _unsafe_view_122 = _unsafe_view_123 = None
        view_1348 = torch.ops.aten.view.default(bmm_122, [512, 1, 1, 1, 384]);  bmm_122 = None
        permute_782 = torch.ops.aten.permute.default(view_1348, [3, 0, 4, 1, 2]);  view_1348 = None
        view_1349 = torch.ops.aten.view.default(permute_782, [1, 512, 384]);  permute_782 = None
        unsqueeze_509 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_155 = torch.ops.aten.mul.Tensor(view_1349, unsqueeze_509);  view_1349 = unsqueeze_509 = None
        add_126 = torch.ops.aten.add.Tensor(add_119, mul_155);  mul_155 = None
        split_tensor_121 = torch.ops.aten.split.Tensor(add_119, 512, dim = -2);  add_119 = None
        getitem_1211 = split_tensor_121[0];  split_tensor_121 = None
        _to_copy_774 = torch.ops.aten._to_copy.default(getitem_1211, dtype = torch.float32);  getitem_1211 = None
        native_layer_norm_default_161 = torch.ops.aten.native_layer_norm.default(_to_copy_774, [384], arg366_1, arg367_1, 1e-05);  _to_copy_774 = arg366_1 = arg367_1 = None
        getitem_1212 = native_layer_norm_default_161[0]
        _to_copy_775 = torch.ops.aten._to_copy.default(arg368_1, dtype = torch.bfloat16);  arg368_1 = None
        _to_copy_776 = torch.ops.aten._to_copy.default(getitem_1212, dtype = torch.bfloat16);  getitem_1212 = None
        t_274 = torch.ops.aten.t.default(_to_copy_775);  _to_copy_775 = None
        view_1350 = torch.ops.aten.view.default(_to_copy_776, [512, 384]);  _to_copy_776 = None
        mm_255 = torch.ops.aten.mm.default(view_1350, t_274);  view_1350 = t_274 = None
        view_1351 = torch.ops.aten.view.default(mm_255, [1, 512, 1536]);  mm_255 = None
        split_tensor_122 = torch.ops.aten.split.Tensor(view_1351, 768, dim = -1);  view_1351 = None
        getitem_1215 = split_tensor_122[0]
        getitem_1216 = split_tensor_122[1];  split_tensor_122 = None
        silu_35 = torch.ops.aten.silu.default(getitem_1215);  getitem_1215 = None
        mul_156 = torch.ops.aten.mul.Tensor(silu_35, getitem_1216);  silu_35 = getitem_1216 = None
        _to_copy_777 = torch.ops.aten._to_copy.default(arg369_1, dtype = torch.bfloat16);  arg369_1 = None
        t_275 = torch.ops.aten.t.default(_to_copy_777);  _to_copy_777 = None
        view_1353 = torch.ops.aten.view.default(mul_156, [512, 768]);  mul_156 = None
        mm_256 = torch.ops.aten.mm.default(view_1353, t_275);  view_1353 = t_275 = None
        view_1354 = torch.ops.aten.view.default(mm_256, [1, 512, 384]);  mm_256 = None
        add_127 = torch.ops.aten.add.Tensor(add_126, view_1354);  add_126 = view_1354 = None
        _to_copy_778 = torch.ops.aten._to_copy.default(add_123, dtype = torch.float32)
        native_layer_norm_default_162 = torch.ops.aten.native_layer_norm.default(_to_copy_778, [256], arg382_1, arg383_1, 1e-05);  _to_copy_778 = arg382_1 = arg383_1 = None
        getitem_1217 = native_layer_norm_default_162[0]
        split_with_sizes_default_30 = torch.ops.aten.split_with_sizes.default(arg385_1, [512, 512]);  arg385_1 = None
        getitem_1220 = split_with_sizes_default_30[0]
        getitem_1221 = split_with_sizes_default_30[1];  split_with_sizes_default_30 = None
        split_with_sizes_default_31 = torch.ops.aten.split_with_sizes.default(arg386_1, [512, 512, 256]);  arg386_1 = None
        getitem_1222 = split_with_sizes_default_31[0]
        getitem_1223 = split_with_sizes_default_31[1]
        getitem_1224 = split_with_sizes_default_31[2];  split_with_sizes_default_31 = None
        _to_copy_779 = torch.ops.aten._to_copy.default(getitem_1220, dtype = torch.bfloat16);  getitem_1220 = None
        _to_copy_780 = torch.ops.aten._to_copy.default(getitem_1217, dtype = torch.bfloat16)
        t_276 = torch.ops.aten.t.default(_to_copy_779);  _to_copy_779 = None
        view_1355 = torch.ops.aten.view.default(_to_copy_780, [262144, 256]);  _to_copy_780 = None
        mm_257 = torch.ops.aten.mm.default(view_1355, t_276);  view_1355 = t_276 = None
        view_1356 = torch.ops.aten.view.default(mm_257, [1, 512, 512, 512]);  mm_257 = None
        _to_copy_781 = torch.ops.aten._to_copy.default(getitem_1222, dtype = torch.bfloat16);  getitem_1222 = None
        _to_copy_782 = torch.ops.aten._to_copy.default(getitem_1217, dtype = torch.bfloat16)
        t_277 = torch.ops.aten.t.default(_to_copy_781);  _to_copy_781 = None
        view_1357 = torch.ops.aten.view.default(_to_copy_782, [262144, 256]);  _to_copy_782 = None
        mm_258 = torch.ops.aten.mm.default(view_1357, t_277);  view_1357 = t_277 = None
        view_1358 = torch.ops.aten.view.default(mm_258, [1, 512, 512, 512]);  mm_258 = None
        sigmoid_96 = torch.ops.aten.sigmoid.default(view_1358);  view_1358 = None
        mul_157 = torch.ops.aten.mul.Tensor(view_1356, sigmoid_96);  view_1356 = sigmoid_96 = None
        unsqueeze_510 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_94 = torch.ops.aten.bitwise_not.default(unsqueeze_510);  unsqueeze_510 = None
        masked_fill_94 = torch.ops.aten.masked_fill.Scalar(mul_157, bitwise_not_94, 0);  mul_157 = bitwise_not_94 = None
        split_tensor_123 = torch.ops.aten.split.Tensor(masked_fill_94, 256, dim = -1)
        getitem_1227 = split_tensor_123[0]
        unsqueeze_513 = torch.ops.aten.unsqueeze.default(getitem_1227, 4);  getitem_1227 = None
        permute_787 = torch.ops.aten.permute.default(unsqueeze_513, [0, 1, 4, 3, 2]);  unsqueeze_513 = None
        permute_788 = torch.ops.aten.permute.default(permute_787, [3, 1, 4, 0, 2]);  permute_787 = None
        view_1361 = torch.ops.aten.view.default(permute_788, [256, 512, 512]);  permute_788 = None
        split_tensor_124 = torch.ops.aten.split.Tensor(masked_fill_94, 256, dim = -1);  masked_fill_94 = None
        getitem_1230 = split_tensor_124[1];  split_tensor_124 = None
        unsqueeze_514 = torch.ops.aten.unsqueeze.default(getitem_1230, 4);  getitem_1230 = None
        permute_789 = torch.ops.aten.permute.default(unsqueeze_514, [0, 4, 1, 3, 2]);  unsqueeze_514 = None
        permute_790 = torch.ops.aten.permute.default(permute_789, [3, 4, 0, 2, 1]);  permute_789 = None
        view_1362 = torch.ops.aten.view.default(permute_790, [256, 512, 512]);  permute_790 = None
        bmm_123 = torch.ops.aten.bmm.default(view_1361, view_1362);  view_1361 = view_1362 = None
        view_1363 = torch.ops.aten.view.default(bmm_123, [256, 512, 1, 1, 512]);  bmm_123 = None
        permute_791 = torch.ops.aten.permute.default(view_1363, [3, 1, 4, 0, 2]);  view_1363 = None
        view_1364 = torch.ops.aten.view.default(permute_791, [1, 512, 512, 256]);  permute_791 = None
        _to_copy_783 = torch.ops.aten._to_copy.default(getitem_1221, dtype = torch.bfloat16);  getitem_1221 = None
        _to_copy_784 = torch.ops.aten._to_copy.default(getitem_1217, dtype = torch.bfloat16)
        t_278 = torch.ops.aten.t.default(_to_copy_783);  _to_copy_783 = None
        view_1365 = torch.ops.aten.view.default(_to_copy_784, [262144, 256]);  _to_copy_784 = None
        mm_259 = torch.ops.aten.mm.default(view_1365, t_278);  view_1365 = t_278 = None
        view_1366 = torch.ops.aten.view.default(mm_259, [1, 512, 512, 512]);  mm_259 = None
        _to_copy_785 = torch.ops.aten._to_copy.default(getitem_1223, dtype = torch.bfloat16);  getitem_1223 = None
        _to_copy_786 = torch.ops.aten._to_copy.default(getitem_1217, dtype = torch.bfloat16)
        t_279 = torch.ops.aten.t.default(_to_copy_785);  _to_copy_785 = None
        view_1367 = torch.ops.aten.view.default(_to_copy_786, [262144, 256]);  _to_copy_786 = None
        mm_260 = torch.ops.aten.mm.default(view_1367, t_279);  view_1367 = t_279 = None
        view_1368 = torch.ops.aten.view.default(mm_260, [1, 512, 512, 512]);  mm_260 = None
        sigmoid_97 = torch.ops.aten.sigmoid.default(view_1368);  view_1368 = None
        mul_158 = torch.ops.aten.mul.Tensor(view_1366, sigmoid_97);  view_1366 = sigmoid_97 = None
        view_1369 = torch.ops.aten.view.default(mul_158, [262144, 512]);  mul_158 = None
        view_1370 = torch.ops.aten.view.default(view_1369, [1, 512, 512, 512]);  view_1369 = None
        transpose_30 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_515 = torch.ops.aten.unsqueeze.default(transpose_30, 3);  transpose_30 = None
        clone_140 = torch.ops.aten.clone.default(unsqueeze_515, memory_format = torch.contiguous_format);  unsqueeze_515 = None
        bitwise_not_95 = torch.ops.aten.bitwise_not.default(clone_140);  clone_140 = None
        masked_fill_95 = torch.ops.aten.masked_fill.Scalar(view_1370, bitwise_not_95, 0);  view_1370 = bitwise_not_95 = None
        view_1371 = torch.ops.aten.view.default(masked_fill_95, [262144, 512]);  masked_fill_95 = None
        view_1375 = torch.ops.aten.view.default(view_1371, [1, 512, 512, 512])
        split_tensor_125 = torch.ops.aten.split.Tensor(view_1375, 256, dim = -1);  view_1375 = None
        getitem_1233 = split_tensor_125[0]
        unsqueeze_518 = torch.ops.aten.unsqueeze.default(getitem_1233, 4);  getitem_1233 = None
        permute_796 = torch.ops.aten.permute.default(unsqueeze_518, [0, 2, 4, 3, 1]);  unsqueeze_518 = None
        permute_797 = torch.ops.aten.permute.default(permute_796, [3, 1, 4, 0, 2]);  permute_796 = None
        view_1376 = torch.ops.aten.view.default(permute_797, [256, 512, 512]);  permute_797 = None
        view_1377 = torch.ops.aten.view.default(view_1371, [1, 512, 512, 512]);  view_1371 = None
        split_tensor_126 = torch.ops.aten.split.Tensor(view_1377, 256, dim = -1);  view_1377 = None
        getitem_1236 = split_tensor_126[1];  split_tensor_126 = None
        unsqueeze_519 = torch.ops.aten.unsqueeze.default(getitem_1236, 4);  getitem_1236 = None
        permute_798 = torch.ops.aten.permute.default(unsqueeze_519, [0, 4, 2, 3, 1]);  unsqueeze_519 = None
        permute_799 = torch.ops.aten.permute.default(permute_798, [3, 4, 0, 2, 1]);  permute_798 = None
        view_1378 = torch.ops.aten.view.default(permute_799, [256, 512, 512]);  permute_799 = None
        bmm_124 = torch.ops.aten.bmm.default(view_1376, view_1378);  view_1376 = view_1378 = None
        view_1379 = torch.ops.aten.view.default(bmm_124, [256, 512, 1, 1, 512]);  bmm_124 = None
        permute_800 = torch.ops.aten.permute.default(view_1379, [3, 1, 4, 0, 2]);  view_1379 = None
        view_1380 = torch.ops.aten.view.default(permute_800, [1, 512, 512, 256]);  permute_800 = None
        _to_copy_787 = torch.ops.aten._to_copy.default(view_1364, dtype = torch.float32);  view_1364 = None
        native_layer_norm_default_163 = torch.ops.aten.native_layer_norm.default(_to_copy_787, [256], None, None, 1e-05);  _to_copy_787 = None
        getitem_1237 = native_layer_norm_default_163[0]
        _to_copy_788 = torch.ops.aten._to_copy.default(view_1380, dtype = torch.float32);  view_1380 = None
        native_layer_norm_default_164 = torch.ops.aten.native_layer_norm.default(_to_copy_788, [256], None, None, 1e-05);  _to_copy_788 = None
        getitem_1240 = native_layer_norm_default_164[0]
        add_128 = torch.ops.aten.add.Tensor(getitem_1237, getitem_1240);  getitem_1237 = getitem_1240 = None
        _to_copy_789 = torch.ops.aten._to_copy.default(arg384_1, dtype = torch.bfloat16);  arg384_1 = None
        _to_copy_790 = torch.ops.aten._to_copy.default(add_128, dtype = torch.bfloat16);  add_128 = None
        t_280 = torch.ops.aten.t.default(_to_copy_789);  _to_copy_789 = None
        view_1381 = torch.ops.aten.view.default(_to_copy_790, [262144, 256]);  _to_copy_790 = None
        mm_261 = torch.ops.aten.mm.default(view_1381, t_280);  view_1381 = t_280 = None
        view_1382 = torch.ops.aten.view.default(mm_261, [1, 512, 512, 256]);  mm_261 = None
        _to_copy_791 = torch.ops.aten._to_copy.default(getitem_1224, dtype = torch.bfloat16);  getitem_1224 = None
        _to_copy_792 = torch.ops.aten._to_copy.default(getitem_1217, dtype = torch.bfloat16);  getitem_1217 = None
        t_281 = torch.ops.aten.t.default(_to_copy_791);  _to_copy_791 = None
        view_1383 = torch.ops.aten.view.default(_to_copy_792, [262144, 256]);  _to_copy_792 = None
        mm_262 = torch.ops.aten.mm.default(view_1383, t_281);  view_1383 = t_281 = None
        view_1384 = torch.ops.aten.view.default(mm_262, [1, 512, 512, 256]);  mm_262 = None
        sigmoid_98 = torch.ops.aten.sigmoid.default(view_1384);  view_1384 = None
        mul_159 = torch.ops.aten.mul.Tensor(view_1382, sigmoid_98);  view_1382 = sigmoid_98 = None
        add_129 = torch.ops.aten.add.Tensor(add_123, mul_159);  mul_159 = None
        _to_copy_793 = torch.ops.aten._to_copy.default(add_123, dtype = torch.float32)
        native_layer_norm_default_165 = torch.ops.aten.native_layer_norm.default(_to_copy_793, [256], None, None, 1e-05);  _to_copy_793 = None
        getitem_1243 = native_layer_norm_default_165[0]
        _to_copy_794 = torch.ops.aten._to_copy.default(arg388_1, dtype = torch.bfloat16);  arg388_1 = None
        _to_copy_795 = torch.ops.aten._to_copy.default(getitem_1243, dtype = torch.bfloat16)
        t_282 = torch.ops.aten.t.default(_to_copy_794);  _to_copy_794 = None
        view_1385 = torch.ops.aten.view.default(_to_copy_795, [262144, 256]);  _to_copy_795 = None
        mm_263 = torch.ops.aten.mm.default(view_1385, t_282);  view_1385 = t_282 = None
        view_1386 = torch.ops.aten.view.default(mm_263, [1, 512, 512, 8]);  mm_263 = None
        view_1387 = torch.ops.aten.view.default(view_1386, [1, 512, 512, 2, 4]);  view_1386 = None
        permute_801 = torch.ops.aten.permute.default(view_1387, [0, 3, 4, 1, 2]);  view_1387 = None
        view_1388 = torch.ops.aten.view.default(permute_801, [1, 2, 4, 1, 512, 512]);  permute_801 = None
        view_1389 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_96 = torch.ops.aten.bitwise_not.default(view_1389);  view_1389 = None
        masked_fill_96 = torch.ops.aten.masked_fill.Scalar(view_1388, bitwise_not_96, -10000);  view_1388 = bitwise_not_96 = None
        view_1390 = torch.ops.aten.view.default(masked_fill_96, [1, 2, 4, 512, 512]);  masked_fill_96 = None
        permute_802 = torch.ops.aten.permute.default(view_1390, [1, 0, 2, 3, 4]);  view_1390 = None
        view_1391 = torch.ops.aten.view.default(permute_802, [2, 4, 1, 512, 512]);  permute_802 = None
        _to_copy_796 = torch.ops.aten._to_copy.default(arg389_1, dtype = torch.bfloat16);  arg389_1 = None
        _to_copy_797 = torch.ops.aten._to_copy.default(getitem_1243, dtype = torch.bfloat16)
        t_283 = torch.ops.aten.t.default(_to_copy_796);  _to_copy_796 = None
        view_1392 = torch.ops.aten.view.default(_to_copy_797, [262144, 256]);  _to_copy_797 = None
        mm_264 = torch.ops.aten.mm.default(view_1392, t_283);  view_1392 = t_283 = None
        view_1393 = torch.ops.aten.view.default(mm_264, [1, 512, 512, 1024]);  mm_264 = None
        select_31 = torch.ops.aten.select.int(view_1391, 0, 0)
        view_1394 = torch.ops.aten.view.default(view_1393, [1, 512, 512, 4, 4, 64]);  view_1393 = None
        permute_803 = torch.ops.aten.permute.default(view_1394, [4, 0, 3, 1, 2, 5]);  view_1394 = None
        view_1395 = torch.ops.aten.view.default(permute_803, [4, 4, 512, 512, 64]);  permute_803 = None
        unbind_int_67 = torch.ops.aten.unbind.int(view_1395);  view_1395 = None
        getitem_1246 = unbind_int_67[0]
        getitem_1247 = unbind_int_67[1]
        getitem_1248 = unbind_int_67[2]
        getitem_1249 = unbind_int_67[3];  unbind_int_67 = None
        expand_72 = torch.ops.aten.expand.default(select_31, [4, 512, 512, 512]);  select_31 = None
        _scaled_dot_product_efficient_attention_default_39 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1246, getitem_1247, getitem_1248, expand_72, False);  getitem_1246 = getitem_1247 = getitem_1248 = expand_72 = None
        getitem_1250 = _scaled_dot_product_efficient_attention_default_39[0]
        sigmoid_99 = torch.ops.aten.sigmoid.default(getitem_1249);  getitem_1249 = None
        mul_160 = torch.ops.aten.mul.Tensor(getitem_1250, sigmoid_99);  getitem_1250 = sigmoid_99 = None
        view_1396 = torch.ops.aten.view.default(mul_160, [1, 4, 512, 512, 64]);  mul_160 = None
        permute_804 = torch.ops.aten.permute.default(view_1396, [0, 2, 3, 1, 4]);  view_1396 = None
        clone_141 = torch.ops.aten.clone.default(permute_804, memory_format = torch.contiguous_format);  permute_804 = None
        _unsafe_view_124 = torch.ops.aten._unsafe_view.default(clone_141, [1, 512, 512, 256]);  clone_141 = None
        transpose_31 = torch.ops.aten.transpose.int(getitem_1243, 1, 2);  getitem_1243 = None
        _to_copy_798 = torch.ops.aten._to_copy.default(arg390_1, dtype = torch.bfloat16);  arg390_1 = None
        _to_copy_799 = torch.ops.aten._to_copy.default(transpose_31, dtype = torch.bfloat16);  transpose_31 = None
        t_284 = torch.ops.aten.t.default(_to_copy_798);  _to_copy_798 = None
        expand_73 = torch.ops.aten.expand.default(_to_copy_799, [1, 512, 512, 256]);  _to_copy_799 = None
        view_1397 = torch.ops.aten.view.default(expand_73, [512, 512, 256]);  expand_73 = None
        expand_74 = torch.ops.aten.expand.default(t_284, [1, 512, 256, 1024]);  t_284 = None
        view_1398 = torch.ops.aten.view.default(expand_74, [512, 256, 1024]);  expand_74 = None
        bmm_125 = torch.ops.aten.bmm.default(view_1397, view_1398);  view_1397 = view_1398 = None
        view_1399 = torch.ops.aten.view.default(bmm_125, [1, 512, 512, 1024]);  bmm_125 = None
        select_32 = torch.ops.aten.select.int(view_1391, 0, 1);  view_1391 = None
        view_1400 = torch.ops.aten.view.default(view_1399, [1, 512, 512, 4, 4, 64]);  view_1399 = None
        permute_805 = torch.ops.aten.permute.default(view_1400, [4, 0, 3, 1, 2, 5]);  view_1400 = None
        view_1401 = torch.ops.aten.view.default(permute_805, [4, 4, 512, 512, 64]);  permute_805 = None
        unbind_int_68 = torch.ops.aten.unbind.int(view_1401);  view_1401 = None
        getitem_1254 = unbind_int_68[0]
        getitem_1255 = unbind_int_68[1]
        getitem_1256 = unbind_int_68[2]
        getitem_1257 = unbind_int_68[3];  unbind_int_68 = None
        expand_75 = torch.ops.aten.expand.default(select_32, [4, 512, 512, 512]);  select_32 = None
        _scaled_dot_product_efficient_attention_default_40 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1254, getitem_1255, getitem_1256, expand_75, False);  getitem_1254 = getitem_1255 = getitem_1256 = expand_75 = None
        getitem_1258 = _scaled_dot_product_efficient_attention_default_40[0]
        sigmoid_100 = torch.ops.aten.sigmoid.default(getitem_1257);  getitem_1257 = None
        mul_161 = torch.ops.aten.mul.Tensor(getitem_1258, sigmoid_100);  getitem_1258 = sigmoid_100 = None
        view_1402 = torch.ops.aten.view.default(mul_161, [1, 4, 512, 512, 64]);  mul_161 = None
        permute_806 = torch.ops.aten.permute.default(view_1402, [0, 2, 3, 1, 4]);  view_1402 = None
        clone_142 = torch.ops.aten.clone.default(permute_806, memory_format = torch.contiguous_format);  permute_806 = None
        _unsafe_view_125 = torch.ops.aten._unsafe_view.default(clone_142, [1, 512, 512, 256]);  clone_142 = None
        cat_21 = torch.ops.aten.cat.default([_unsafe_view_124, _unsafe_view_125], dim = -1);  _unsafe_view_124 = _unsafe_view_125 = None
        slice_214 = torch.ops.aten.slice.Tensor(arg387_1, dim = 0, start = 0, end = 9223372036854775807);  arg387_1 = None
        unsqueeze_520 = torch.ops.aten.unsqueeze.default(slice_214, 1);  slice_214 = None
        mul_162 = torch.ops.aten.mul.Tensor(arg391_1, unsqueeze_520);  arg391_1 = unsqueeze_520 = None
        _to_copy_800 = torch.ops.aten._to_copy.default(mul_162, dtype = torch.bfloat16);  mul_162 = None
        t_285 = torch.ops.aten.t.default(_to_copy_800);  _to_copy_800 = None
        view_1403 = torch.ops.aten.view.default(cat_21, [262144, 512]);  cat_21 = None
        mm_265 = torch.ops.aten.mm.default(view_1403, t_285);  view_1403 = t_285 = None
        view_1404 = torch.ops.aten.view.default(mm_265, [1, 512, 512, 256]);  mm_265 = None
        add_130 = torch.ops.aten.add.Tensor(add_129, view_1404);  add_129 = view_1404 = None
        split_tensor_127 = torch.ops.aten.split.Tensor(add_123, 512, dim = -2)
        getitem_1262 = split_tensor_127[0];  split_tensor_127 = None
        _to_copy_801 = torch.ops.aten._to_copy.default(getitem_1262, dtype = torch.float32);  getitem_1262 = None
        native_layer_norm_default_166 = torch.ops.aten.native_layer_norm.default(_to_copy_801, [256], arg378_1, arg379_1, 1e-05);  _to_copy_801 = arg378_1 = arg379_1 = None
        getitem_1263 = native_layer_norm_default_166[0]
        _to_copy_802 = torch.ops.aten._to_copy.default(arg380_1, dtype = torch.bfloat16);  arg380_1 = None
        _to_copy_803 = torch.ops.aten._to_copy.default(getitem_1263, dtype = torch.bfloat16);  getitem_1263 = None
        t_286 = torch.ops.aten.t.default(_to_copy_802);  _to_copy_802 = None
        view_1405 = torch.ops.aten.view.default(_to_copy_803, [262144, 256]);  _to_copy_803 = None
        mm_266 = torch.ops.aten.mm.default(view_1405, t_286);  view_1405 = t_286 = None
        view_1406 = torch.ops.aten.view.default(mm_266, [1, 512, 512, 1024]);  mm_266 = None
        split_tensor_128 = torch.ops.aten.split.Tensor(view_1406, 512, dim = -1);  view_1406 = None
        getitem_1266 = split_tensor_128[0]
        getitem_1267 = split_tensor_128[1];  split_tensor_128 = None
        silu_36 = torch.ops.aten.silu.default(getitem_1266);  getitem_1266 = None
        mul_163 = torch.ops.aten.mul.Tensor(silu_36, getitem_1267);  silu_36 = getitem_1267 = None
        _to_copy_804 = torch.ops.aten._to_copy.default(arg381_1, dtype = torch.bfloat16);  arg381_1 = None
        t_287 = torch.ops.aten.t.default(_to_copy_804);  _to_copy_804 = None
        view_1408 = torch.ops.aten.view.default(mul_163, [262144, 512]);  mul_163 = None
        mm_267 = torch.ops.aten.mm.default(view_1408, t_287);  view_1408 = t_287 = None
        view_1409 = torch.ops.aten.view.default(mm_267, [1, 512, 512, 256]);  mm_267 = None
        add_131 = torch.ops.aten.add.Tensor(add_130, view_1409);  add_130 = view_1409 = None
        _to_copy_805 = torch.ops.aten._to_copy.default(add_127, dtype = torch.float32)
        native_layer_norm_default_167 = torch.ops.aten.native_layer_norm.default(_to_copy_805, [384], arg396_1, arg397_1, 1e-05);  _to_copy_805 = arg396_1 = arg397_1 = None
        getitem_1268 = native_layer_norm_default_167[0]
        _to_copy_806 = torch.ops.aten._to_copy.default(add_123, dtype = torch.float32);  add_123 = None
        native_layer_norm_default_168 = torch.ops.aten.native_layer_norm.default(_to_copy_806, [256], arg398_1, arg399_1, 1e-05);  _to_copy_806 = arg398_1 = arg399_1 = None
        getitem_1271 = native_layer_norm_default_168[0]
        _to_copy_807 = torch.ops.aten._to_copy.default(arg400_1, dtype = torch.bfloat16);  arg400_1 = None
        _to_copy_808 = torch.ops.aten._to_copy.default(getitem_1271, dtype = torch.bfloat16);  getitem_1271 = None
        t_288 = torch.ops.aten.t.default(_to_copy_807);  _to_copy_807 = None
        view_1410 = torch.ops.aten.view.default(_to_copy_808, [262144, 256]);  _to_copy_808 = None
        mm_268 = torch.ops.aten.mm.default(view_1410, t_288);  view_1410 = t_288 = None
        view_1411 = torch.ops.aten.view.default(mm_268, [1, 512, 512, 16]);  mm_268 = None
        permute_807 = torch.ops.aten.permute.default(view_1411, [0, 3, 1, 2]);  view_1411 = None
        view_1412 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_97 = torch.ops.aten.bitwise_not.default(view_1412);  view_1412 = None
        masked_fill_97 = torch.ops.aten.masked_fill.Scalar(permute_807, bitwise_not_97, -10000);  permute_807 = bitwise_not_97 = None
        _to_copy_809 = torch.ops.aten._to_copy.default(getitem_1268, dtype = torch.bfloat16);  getitem_1268 = None
        _to_copy_810 = torch.ops.aten._to_copy.default(arg402_1, dtype = torch.bfloat16);  arg402_1 = None
        unsqueeze_521 = torch.ops.aten.unsqueeze.default(_to_copy_809, 3);  _to_copy_809 = None
        unsqueeze_522 = torch.ops.aten.unsqueeze.default(unsqueeze_521, 4);  unsqueeze_521 = None
        unsqueeze_523 = torch.ops.aten.unsqueeze.default(unsqueeze_522, 5);  unsqueeze_522 = None
        permute_808 = torch.ops.aten.permute.default(unsqueeze_523, [3, 0, 4, 1, 5, 2]);  unsqueeze_523 = None
        unsqueeze_524 = torch.ops.aten.unsqueeze.default(_to_copy_810, 4);  _to_copy_810 = None
        unsqueeze_525 = torch.ops.aten.unsqueeze.default(unsqueeze_524, 5);  unsqueeze_524 = None
        permute_809 = torch.ops.aten.permute.default(unsqueeze_525, [1, 4, 2, 5, 3, 0]);  unsqueeze_525 = None
        permute_810 = torch.ops.aten.permute.default(permute_808, [3, 5, 0, 1, 2, 4]);  permute_808 = None
        view_1413 = torch.ops.aten.view.default(permute_810, [1, 512, 384]);  permute_810 = None
        permute_811 = torch.ops.aten.permute.default(permute_809, [5, 0, 1, 2, 4, 3]);  permute_809 = None
        view_1414 = torch.ops.aten.view.default(permute_811, [1, 384, 1536]);  permute_811 = None
        bmm_126 = torch.ops.aten.bmm.default(view_1413, view_1414);  view_1413 = view_1414 = None
        view_1415 = torch.ops.aten.view.default(bmm_126, [512, 1, 4, 1, 16, 24]);  bmm_126 = None
        permute_812 = torch.ops.aten.permute.default(view_1415, [2, 3, 4, 0, 5, 1]);  view_1415 = None
        view_1416 = torch.ops.aten.view.default(permute_812, [4, 1, 16, 512, 24]);  permute_812 = None
        unbind_int_69 = torch.ops.aten.unbind.int(view_1416);  view_1416 = None
        getitem_1274 = unbind_int_69[0]
        getitem_1275 = unbind_int_69[1]
        getitem_1276 = unbind_int_69[2]
        getitem_1277 = unbind_int_69[3];  unbind_int_69 = None
        view_1417 = torch.ops.aten.view.default(arg401_1, [1, 16, 1, 24]);  arg401_1 = None
        add_132 = torch.ops.aten.add.Tensor(getitem_1274, view_1417);  getitem_1274 = view_1417 = None
        _to_copy_811 = torch.ops.aten._to_copy.default(add_132, dtype = torch.bfloat16);  add_132 = None
        expand_76 = torch.ops.aten.expand.default(masked_fill_97, [1, 16, 512, 512]);  masked_fill_97 = None
        _scaled_dot_product_efficient_attention_default_41 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_811, getitem_1275, getitem_1276, expand_76, False);  _to_copy_811 = getitem_1275 = getitem_1276 = expand_76 = None
        getitem_1278 = _scaled_dot_product_efficient_attention_default_41[0]
        add_133 = torch.ops.aten.add.Tensor(getitem_1277, 1);  getitem_1277 = None
        sigmoid_101 = torch.ops.aten.sigmoid.default(add_133);  add_133 = None
        mul_164 = torch.ops.aten.mul.Tensor(getitem_1278, sigmoid_101);  getitem_1278 = sigmoid_101 = None
        _to_copy_812 = torch.ops.aten._to_copy.default(arg403_1, dtype = torch.bfloat16);  arg403_1 = None
        unsqueeze_526 = torch.ops.aten.unsqueeze.default(mul_164, 4);  mul_164 = None
        permute_813 = torch.ops.aten.permute.default(unsqueeze_526, [0, 2, 4, 3, 1]);  unsqueeze_526 = None
        unsqueeze_527 = torch.ops.aten.unsqueeze.default(_to_copy_812, 3);  _to_copy_812 = None
        unsqueeze_528 = torch.ops.aten.unsqueeze.default(unsqueeze_527, 4);  unsqueeze_527 = None
        permute_814 = torch.ops.aten.permute.default(unsqueeze_528, [3, 4, 2, 1, 0]);  unsqueeze_528 = None
        permute_815 = torch.ops.aten.permute.default(permute_813, [1, 3, 4, 0, 2]);  permute_813 = None
        clone_143 = torch.ops.aten.clone.default(permute_815, memory_format = torch.contiguous_format);  permute_815 = None
        _unsafe_view_126 = torch.ops.aten._unsafe_view.default(clone_143, [1, 512, 384]);  clone_143 = None
        permute_816 = torch.ops.aten.permute.default(permute_814, [3, 4, 0, 2, 1]);  permute_814 = None
        clone_144 = torch.ops.aten.clone.default(permute_816, memory_format = torch.contiguous_format);  permute_816 = None
        _unsafe_view_127 = torch.ops.aten._unsafe_view.default(clone_144, [1, 384, 384]);  clone_144 = None
        bmm_127 = torch.ops.aten.bmm.default(_unsafe_view_126, _unsafe_view_127);  _unsafe_view_126 = _unsafe_view_127 = None
        view_1418 = torch.ops.aten.view.default(bmm_127, [512, 1, 1, 1, 384]);  bmm_127 = None
        permute_817 = torch.ops.aten.permute.default(view_1418, [3, 0, 4, 1, 2]);  view_1418 = None
        view_1419 = torch.ops.aten.view.default(permute_817, [1, 512, 384]);  permute_817 = None
        unsqueeze_529 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_165 = torch.ops.aten.mul.Tensor(view_1419, unsqueeze_529);  view_1419 = unsqueeze_529 = None
        add_134 = torch.ops.aten.add.Tensor(add_127, mul_165);  mul_165 = None
        split_tensor_129 = torch.ops.aten.split.Tensor(add_127, 512, dim = -2);  add_127 = None
        getitem_1282 = split_tensor_129[0];  split_tensor_129 = None
        _to_copy_813 = torch.ops.aten._to_copy.default(getitem_1282, dtype = torch.float32);  getitem_1282 = None
        native_layer_norm_default_169 = torch.ops.aten.native_layer_norm.default(_to_copy_813, [384], arg392_1, arg393_1, 1e-05);  _to_copy_813 = arg392_1 = arg393_1 = None
        getitem_1283 = native_layer_norm_default_169[0]
        _to_copy_814 = torch.ops.aten._to_copy.default(arg394_1, dtype = torch.bfloat16);  arg394_1 = None
        _to_copy_815 = torch.ops.aten._to_copy.default(getitem_1283, dtype = torch.bfloat16);  getitem_1283 = None
        t_289 = torch.ops.aten.t.default(_to_copy_814);  _to_copy_814 = None
        view_1420 = torch.ops.aten.view.default(_to_copy_815, [512, 384]);  _to_copy_815 = None
        mm_269 = torch.ops.aten.mm.default(view_1420, t_289);  view_1420 = t_289 = None
        view_1421 = torch.ops.aten.view.default(mm_269, [1, 512, 1536]);  mm_269 = None
        split_tensor_130 = torch.ops.aten.split.Tensor(view_1421, 768, dim = -1);  view_1421 = None
        getitem_1286 = split_tensor_130[0]
        getitem_1287 = split_tensor_130[1];  split_tensor_130 = None
        silu_37 = torch.ops.aten.silu.default(getitem_1286);  getitem_1286 = None
        mul_166 = torch.ops.aten.mul.Tensor(silu_37, getitem_1287);  silu_37 = getitem_1287 = None
        _to_copy_816 = torch.ops.aten._to_copy.default(arg395_1, dtype = torch.bfloat16);  arg395_1 = None
        t_290 = torch.ops.aten.t.default(_to_copy_816);  _to_copy_816 = None
        view_1423 = torch.ops.aten.view.default(mul_166, [512, 768]);  mul_166 = None
        mm_270 = torch.ops.aten.mm.default(view_1423, t_290);  view_1423 = t_290 = None
        view_1424 = torch.ops.aten.view.default(mm_270, [1, 512, 384]);  mm_270 = None
        add_135 = torch.ops.aten.add.Tensor(add_134, view_1424);  add_134 = view_1424 = None
        _to_copy_817 = torch.ops.aten._to_copy.default(add_131, dtype = torch.float32)
        native_layer_norm_default_170 = torch.ops.aten.native_layer_norm.default(_to_copy_817, [256], arg408_1, arg409_1, 1e-05);  _to_copy_817 = arg408_1 = arg409_1 = None
        getitem_1288 = native_layer_norm_default_170[0]
        split_with_sizes_default_32 = torch.ops.aten.split_with_sizes.default(arg411_1, [512, 512]);  arg411_1 = None
        getitem_1291 = split_with_sizes_default_32[0]
        getitem_1292 = split_with_sizes_default_32[1];  split_with_sizes_default_32 = None
        split_with_sizes_default_33 = torch.ops.aten.split_with_sizes.default(arg412_1, [512, 512, 256]);  arg412_1 = None
        getitem_1293 = split_with_sizes_default_33[0]
        getitem_1294 = split_with_sizes_default_33[1]
        getitem_1295 = split_with_sizes_default_33[2];  split_with_sizes_default_33 = None
        _to_copy_818 = torch.ops.aten._to_copy.default(getitem_1291, dtype = torch.bfloat16);  getitem_1291 = None
        _to_copy_819 = torch.ops.aten._to_copy.default(getitem_1288, dtype = torch.bfloat16)
        t_291 = torch.ops.aten.t.default(_to_copy_818);  _to_copy_818 = None
        view_1425 = torch.ops.aten.view.default(_to_copy_819, [262144, 256]);  _to_copy_819 = None
        mm_271 = torch.ops.aten.mm.default(view_1425, t_291);  view_1425 = t_291 = None
        view_1426 = torch.ops.aten.view.default(mm_271, [1, 512, 512, 512]);  mm_271 = None
        _to_copy_820 = torch.ops.aten._to_copy.default(getitem_1293, dtype = torch.bfloat16);  getitem_1293 = None
        _to_copy_821 = torch.ops.aten._to_copy.default(getitem_1288, dtype = torch.bfloat16)
        t_292 = torch.ops.aten.t.default(_to_copy_820);  _to_copy_820 = None
        view_1427 = torch.ops.aten.view.default(_to_copy_821, [262144, 256]);  _to_copy_821 = None
        mm_272 = torch.ops.aten.mm.default(view_1427, t_292);  view_1427 = t_292 = None
        view_1428 = torch.ops.aten.view.default(mm_272, [1, 512, 512, 512]);  mm_272 = None
        sigmoid_102 = torch.ops.aten.sigmoid.default(view_1428);  view_1428 = None
        mul_167 = torch.ops.aten.mul.Tensor(view_1426, sigmoid_102);  view_1426 = sigmoid_102 = None
        unsqueeze_530 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_98 = torch.ops.aten.bitwise_not.default(unsqueeze_530);  unsqueeze_530 = None
        masked_fill_98 = torch.ops.aten.masked_fill.Scalar(mul_167, bitwise_not_98, 0);  mul_167 = bitwise_not_98 = None
        split_tensor_131 = torch.ops.aten.split.Tensor(masked_fill_98, 256, dim = -1)
        getitem_1298 = split_tensor_131[0]
        unsqueeze_533 = torch.ops.aten.unsqueeze.default(getitem_1298, 4);  getitem_1298 = None
        permute_822 = torch.ops.aten.permute.default(unsqueeze_533, [0, 1, 4, 3, 2]);  unsqueeze_533 = None
        permute_823 = torch.ops.aten.permute.default(permute_822, [3, 1, 4, 0, 2]);  permute_822 = None
        view_1431 = torch.ops.aten.view.default(permute_823, [256, 512, 512]);  permute_823 = None
        split_tensor_132 = torch.ops.aten.split.Tensor(masked_fill_98, 256, dim = -1);  masked_fill_98 = None
        getitem_1301 = split_tensor_132[1];  split_tensor_132 = None
        unsqueeze_534 = torch.ops.aten.unsqueeze.default(getitem_1301, 4);  getitem_1301 = None
        permute_824 = torch.ops.aten.permute.default(unsqueeze_534, [0, 4, 1, 3, 2]);  unsqueeze_534 = None
        permute_825 = torch.ops.aten.permute.default(permute_824, [3, 4, 0, 2, 1]);  permute_824 = None
        view_1432 = torch.ops.aten.view.default(permute_825, [256, 512, 512]);  permute_825 = None
        bmm_128 = torch.ops.aten.bmm.default(view_1431, view_1432);  view_1431 = view_1432 = None
        view_1433 = torch.ops.aten.view.default(bmm_128, [256, 512, 1, 1, 512]);  bmm_128 = None
        permute_826 = torch.ops.aten.permute.default(view_1433, [3, 1, 4, 0, 2]);  view_1433 = None
        view_1434 = torch.ops.aten.view.default(permute_826, [1, 512, 512, 256]);  permute_826 = None
        _to_copy_822 = torch.ops.aten._to_copy.default(getitem_1292, dtype = torch.bfloat16);  getitem_1292 = None
        _to_copy_823 = torch.ops.aten._to_copy.default(getitem_1288, dtype = torch.bfloat16)
        t_293 = torch.ops.aten.t.default(_to_copy_822);  _to_copy_822 = None
        view_1435 = torch.ops.aten.view.default(_to_copy_823, [262144, 256]);  _to_copy_823 = None
        mm_273 = torch.ops.aten.mm.default(view_1435, t_293);  view_1435 = t_293 = None
        view_1436 = torch.ops.aten.view.default(mm_273, [1, 512, 512, 512]);  mm_273 = None
        _to_copy_824 = torch.ops.aten._to_copy.default(getitem_1294, dtype = torch.bfloat16);  getitem_1294 = None
        _to_copy_825 = torch.ops.aten._to_copy.default(getitem_1288, dtype = torch.bfloat16)
        t_294 = torch.ops.aten.t.default(_to_copy_824);  _to_copy_824 = None
        view_1437 = torch.ops.aten.view.default(_to_copy_825, [262144, 256]);  _to_copy_825 = None
        mm_274 = torch.ops.aten.mm.default(view_1437, t_294);  view_1437 = t_294 = None
        view_1438 = torch.ops.aten.view.default(mm_274, [1, 512, 512, 512]);  mm_274 = None
        sigmoid_103 = torch.ops.aten.sigmoid.default(view_1438);  view_1438 = None
        mul_168 = torch.ops.aten.mul.Tensor(view_1436, sigmoid_103);  view_1436 = sigmoid_103 = None
        view_1439 = torch.ops.aten.view.default(mul_168, [262144, 512]);  mul_168 = None
        view_1440 = torch.ops.aten.view.default(view_1439, [1, 512, 512, 512]);  view_1439 = None
        transpose_32 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_535 = torch.ops.aten.unsqueeze.default(transpose_32, 3);  transpose_32 = None
        clone_145 = torch.ops.aten.clone.default(unsqueeze_535, memory_format = torch.contiguous_format);  unsqueeze_535 = None
        bitwise_not_99 = torch.ops.aten.bitwise_not.default(clone_145);  clone_145 = None
        masked_fill_99 = torch.ops.aten.masked_fill.Scalar(view_1440, bitwise_not_99, 0);  view_1440 = bitwise_not_99 = None
        view_1441 = torch.ops.aten.view.default(masked_fill_99, [262144, 512]);  masked_fill_99 = None
        view_1445 = torch.ops.aten.view.default(view_1441, [1, 512, 512, 512])
        split_tensor_133 = torch.ops.aten.split.Tensor(view_1445, 256, dim = -1);  view_1445 = None
        getitem_1304 = split_tensor_133[0]
        unsqueeze_538 = torch.ops.aten.unsqueeze.default(getitem_1304, 4);  getitem_1304 = None
        permute_831 = torch.ops.aten.permute.default(unsqueeze_538, [0, 2, 4, 3, 1]);  unsqueeze_538 = None
        permute_832 = torch.ops.aten.permute.default(permute_831, [3, 1, 4, 0, 2]);  permute_831 = None
        view_1446 = torch.ops.aten.view.default(permute_832, [256, 512, 512]);  permute_832 = None
        view_1447 = torch.ops.aten.view.default(view_1441, [1, 512, 512, 512]);  view_1441 = None
        split_tensor_134 = torch.ops.aten.split.Tensor(view_1447, 256, dim = -1);  view_1447 = None
        getitem_1307 = split_tensor_134[1];  split_tensor_134 = None
        unsqueeze_539 = torch.ops.aten.unsqueeze.default(getitem_1307, 4);  getitem_1307 = None
        permute_833 = torch.ops.aten.permute.default(unsqueeze_539, [0, 4, 2, 3, 1]);  unsqueeze_539 = None
        permute_834 = torch.ops.aten.permute.default(permute_833, [3, 4, 0, 2, 1]);  permute_833 = None
        view_1448 = torch.ops.aten.view.default(permute_834, [256, 512, 512]);  permute_834 = None
        bmm_129 = torch.ops.aten.bmm.default(view_1446, view_1448);  view_1446 = view_1448 = None
        view_1449 = torch.ops.aten.view.default(bmm_129, [256, 512, 1, 1, 512]);  bmm_129 = None
        permute_835 = torch.ops.aten.permute.default(view_1449, [3, 1, 4, 0, 2]);  view_1449 = None
        view_1450 = torch.ops.aten.view.default(permute_835, [1, 512, 512, 256]);  permute_835 = None
        _to_copy_826 = torch.ops.aten._to_copy.default(view_1434, dtype = torch.float32);  view_1434 = None
        native_layer_norm_default_171 = torch.ops.aten.native_layer_norm.default(_to_copy_826, [256], None, None, 1e-05);  _to_copy_826 = None
        getitem_1308 = native_layer_norm_default_171[0]
        _to_copy_827 = torch.ops.aten._to_copy.default(view_1450, dtype = torch.float32);  view_1450 = None
        native_layer_norm_default_172 = torch.ops.aten.native_layer_norm.default(_to_copy_827, [256], None, None, 1e-05);  _to_copy_827 = None
        getitem_1311 = native_layer_norm_default_172[0]
        add_136 = torch.ops.aten.add.Tensor(getitem_1308, getitem_1311);  getitem_1308 = getitem_1311 = None
        _to_copy_828 = torch.ops.aten._to_copy.default(arg410_1, dtype = torch.bfloat16);  arg410_1 = None
        _to_copy_829 = torch.ops.aten._to_copy.default(add_136, dtype = torch.bfloat16);  add_136 = None
        t_295 = torch.ops.aten.t.default(_to_copy_828);  _to_copy_828 = None
        view_1451 = torch.ops.aten.view.default(_to_copy_829, [262144, 256]);  _to_copy_829 = None
        mm_275 = torch.ops.aten.mm.default(view_1451, t_295);  view_1451 = t_295 = None
        view_1452 = torch.ops.aten.view.default(mm_275, [1, 512, 512, 256]);  mm_275 = None
        _to_copy_830 = torch.ops.aten._to_copy.default(getitem_1295, dtype = torch.bfloat16);  getitem_1295 = None
        _to_copy_831 = torch.ops.aten._to_copy.default(getitem_1288, dtype = torch.bfloat16);  getitem_1288 = None
        t_296 = torch.ops.aten.t.default(_to_copy_830);  _to_copy_830 = None
        view_1453 = torch.ops.aten.view.default(_to_copy_831, [262144, 256]);  _to_copy_831 = None
        mm_276 = torch.ops.aten.mm.default(view_1453, t_296);  view_1453 = t_296 = None
        view_1454 = torch.ops.aten.view.default(mm_276, [1, 512, 512, 256]);  mm_276 = None
        sigmoid_104 = torch.ops.aten.sigmoid.default(view_1454);  view_1454 = None
        mul_169 = torch.ops.aten.mul.Tensor(view_1452, sigmoid_104);  view_1452 = sigmoid_104 = None
        add_137 = torch.ops.aten.add.Tensor(add_131, mul_169);  mul_169 = None
        _to_copy_832 = torch.ops.aten._to_copy.default(add_131, dtype = torch.float32)
        native_layer_norm_default_173 = torch.ops.aten.native_layer_norm.default(_to_copy_832, [256], None, None, 1e-05);  _to_copy_832 = None
        getitem_1314 = native_layer_norm_default_173[0]
        _to_copy_833 = torch.ops.aten._to_copy.default(arg414_1, dtype = torch.bfloat16);  arg414_1 = None
        _to_copy_834 = torch.ops.aten._to_copy.default(getitem_1314, dtype = torch.bfloat16)
        t_297 = torch.ops.aten.t.default(_to_copy_833);  _to_copy_833 = None
        view_1455 = torch.ops.aten.view.default(_to_copy_834, [262144, 256]);  _to_copy_834 = None
        mm_277 = torch.ops.aten.mm.default(view_1455, t_297);  view_1455 = t_297 = None
        view_1456 = torch.ops.aten.view.default(mm_277, [1, 512, 512, 8]);  mm_277 = None
        view_1457 = torch.ops.aten.view.default(view_1456, [1, 512, 512, 2, 4]);  view_1456 = None
        permute_836 = torch.ops.aten.permute.default(view_1457, [0, 3, 4, 1, 2]);  view_1457 = None
        view_1458 = torch.ops.aten.view.default(permute_836, [1, 2, 4, 1, 512, 512]);  permute_836 = None
        view_1459 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_100 = torch.ops.aten.bitwise_not.default(view_1459);  view_1459 = None
        masked_fill_100 = torch.ops.aten.masked_fill.Scalar(view_1458, bitwise_not_100, -10000);  view_1458 = bitwise_not_100 = None
        view_1460 = torch.ops.aten.view.default(masked_fill_100, [1, 2, 4, 512, 512]);  masked_fill_100 = None
        permute_837 = torch.ops.aten.permute.default(view_1460, [1, 0, 2, 3, 4]);  view_1460 = None
        view_1461 = torch.ops.aten.view.default(permute_837, [2, 4, 1, 512, 512]);  permute_837 = None
        _to_copy_835 = torch.ops.aten._to_copy.default(arg415_1, dtype = torch.bfloat16);  arg415_1 = None
        _to_copy_836 = torch.ops.aten._to_copy.default(getitem_1314, dtype = torch.bfloat16)
        t_298 = torch.ops.aten.t.default(_to_copy_835);  _to_copy_835 = None
        view_1462 = torch.ops.aten.view.default(_to_copy_836, [262144, 256]);  _to_copy_836 = None
        mm_278 = torch.ops.aten.mm.default(view_1462, t_298);  view_1462 = t_298 = None
        view_1463 = torch.ops.aten.view.default(mm_278, [1, 512, 512, 1024]);  mm_278 = None
        select_33 = torch.ops.aten.select.int(view_1461, 0, 0)
        view_1464 = torch.ops.aten.view.default(view_1463, [1, 512, 512, 4, 4, 64]);  view_1463 = None
        permute_838 = torch.ops.aten.permute.default(view_1464, [4, 0, 3, 1, 2, 5]);  view_1464 = None
        view_1465 = torch.ops.aten.view.default(permute_838, [4, 4, 512, 512, 64]);  permute_838 = None
        unbind_int_70 = torch.ops.aten.unbind.int(view_1465);  view_1465 = None
        getitem_1317 = unbind_int_70[0]
        getitem_1318 = unbind_int_70[1]
        getitem_1319 = unbind_int_70[2]
        getitem_1320 = unbind_int_70[3];  unbind_int_70 = None
        expand_77 = torch.ops.aten.expand.default(select_33, [4, 512, 512, 512]);  select_33 = None
        _scaled_dot_product_efficient_attention_default_42 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1317, getitem_1318, getitem_1319, expand_77, False);  getitem_1317 = getitem_1318 = getitem_1319 = expand_77 = None
        getitem_1321 = _scaled_dot_product_efficient_attention_default_42[0]
        sigmoid_105 = torch.ops.aten.sigmoid.default(getitem_1320);  getitem_1320 = None
        mul_170 = torch.ops.aten.mul.Tensor(getitem_1321, sigmoid_105);  getitem_1321 = sigmoid_105 = None
        view_1466 = torch.ops.aten.view.default(mul_170, [1, 4, 512, 512, 64]);  mul_170 = None
        permute_839 = torch.ops.aten.permute.default(view_1466, [0, 2, 3, 1, 4]);  view_1466 = None
        clone_146 = torch.ops.aten.clone.default(permute_839, memory_format = torch.contiguous_format);  permute_839 = None
        _unsafe_view_128 = torch.ops.aten._unsafe_view.default(clone_146, [1, 512, 512, 256]);  clone_146 = None
        transpose_33 = torch.ops.aten.transpose.int(getitem_1314, 1, 2);  getitem_1314 = None
        _to_copy_837 = torch.ops.aten._to_copy.default(arg416_1, dtype = torch.bfloat16);  arg416_1 = None
        _to_copy_838 = torch.ops.aten._to_copy.default(transpose_33, dtype = torch.bfloat16);  transpose_33 = None
        t_299 = torch.ops.aten.t.default(_to_copy_837);  _to_copy_837 = None
        expand_78 = torch.ops.aten.expand.default(_to_copy_838, [1, 512, 512, 256]);  _to_copy_838 = None
        view_1467 = torch.ops.aten.view.default(expand_78, [512, 512, 256]);  expand_78 = None
        expand_79 = torch.ops.aten.expand.default(t_299, [1, 512, 256, 1024]);  t_299 = None
        view_1468 = torch.ops.aten.view.default(expand_79, [512, 256, 1024]);  expand_79 = None
        bmm_130 = torch.ops.aten.bmm.default(view_1467, view_1468);  view_1467 = view_1468 = None
        view_1469 = torch.ops.aten.view.default(bmm_130, [1, 512, 512, 1024]);  bmm_130 = None
        select_34 = torch.ops.aten.select.int(view_1461, 0, 1);  view_1461 = None
        view_1470 = torch.ops.aten.view.default(view_1469, [1, 512, 512, 4, 4, 64]);  view_1469 = None
        permute_840 = torch.ops.aten.permute.default(view_1470, [4, 0, 3, 1, 2, 5]);  view_1470 = None
        view_1471 = torch.ops.aten.view.default(permute_840, [4, 4, 512, 512, 64]);  permute_840 = None
        unbind_int_71 = torch.ops.aten.unbind.int(view_1471);  view_1471 = None
        getitem_1325 = unbind_int_71[0]
        getitem_1326 = unbind_int_71[1]
        getitem_1327 = unbind_int_71[2]
        getitem_1328 = unbind_int_71[3];  unbind_int_71 = None
        expand_80 = torch.ops.aten.expand.default(select_34, [4, 512, 512, 512]);  select_34 = None
        _scaled_dot_product_efficient_attention_default_43 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1325, getitem_1326, getitem_1327, expand_80, False);  getitem_1325 = getitem_1326 = getitem_1327 = expand_80 = None
        getitem_1329 = _scaled_dot_product_efficient_attention_default_43[0]
        sigmoid_106 = torch.ops.aten.sigmoid.default(getitem_1328);  getitem_1328 = None
        mul_171 = torch.ops.aten.mul.Tensor(getitem_1329, sigmoid_106);  getitem_1329 = sigmoid_106 = None
        view_1472 = torch.ops.aten.view.default(mul_171, [1, 4, 512, 512, 64]);  mul_171 = None
        permute_841 = torch.ops.aten.permute.default(view_1472, [0, 2, 3, 1, 4]);  view_1472 = None
        clone_147 = torch.ops.aten.clone.default(permute_841, memory_format = torch.contiguous_format);  permute_841 = None
        _unsafe_view_129 = torch.ops.aten._unsafe_view.default(clone_147, [1, 512, 512, 256]);  clone_147 = None
        cat_22 = torch.ops.aten.cat.default([_unsafe_view_128, _unsafe_view_129], dim = -1);  _unsafe_view_128 = _unsafe_view_129 = None
        slice_215 = torch.ops.aten.slice.Tensor(arg413_1, dim = 0, start = 0, end = 9223372036854775807);  arg413_1 = None
        unsqueeze_540 = torch.ops.aten.unsqueeze.default(slice_215, 1);  slice_215 = None
        mul_172 = torch.ops.aten.mul.Tensor(arg417_1, unsqueeze_540);  arg417_1 = unsqueeze_540 = None
        _to_copy_839 = torch.ops.aten._to_copy.default(mul_172, dtype = torch.bfloat16);  mul_172 = None
        t_300 = torch.ops.aten.t.default(_to_copy_839);  _to_copy_839 = None
        view_1473 = torch.ops.aten.view.default(cat_22, [262144, 512]);  cat_22 = None
        mm_279 = torch.ops.aten.mm.default(view_1473, t_300);  view_1473 = t_300 = None
        view_1474 = torch.ops.aten.view.default(mm_279, [1, 512, 512, 256]);  mm_279 = None
        add_138 = torch.ops.aten.add.Tensor(add_137, view_1474);  add_137 = view_1474 = None
        split_tensor_135 = torch.ops.aten.split.Tensor(add_131, 512, dim = -2)
        getitem_1333 = split_tensor_135[0];  split_tensor_135 = None
        _to_copy_840 = torch.ops.aten._to_copy.default(getitem_1333, dtype = torch.float32);  getitem_1333 = None
        native_layer_norm_default_174 = torch.ops.aten.native_layer_norm.default(_to_copy_840, [256], arg404_1, arg405_1, 1e-05);  _to_copy_840 = arg404_1 = arg405_1 = None
        getitem_1334 = native_layer_norm_default_174[0]
        _to_copy_841 = torch.ops.aten._to_copy.default(arg406_1, dtype = torch.bfloat16);  arg406_1 = None
        _to_copy_842 = torch.ops.aten._to_copy.default(getitem_1334, dtype = torch.bfloat16);  getitem_1334 = None
        t_301 = torch.ops.aten.t.default(_to_copy_841);  _to_copy_841 = None
        view_1475 = torch.ops.aten.view.default(_to_copy_842, [262144, 256]);  _to_copy_842 = None
        mm_280 = torch.ops.aten.mm.default(view_1475, t_301);  view_1475 = t_301 = None
        view_1476 = torch.ops.aten.view.default(mm_280, [1, 512, 512, 1024]);  mm_280 = None
        split_tensor_136 = torch.ops.aten.split.Tensor(view_1476, 512, dim = -1);  view_1476 = None
        getitem_1337 = split_tensor_136[0]
        getitem_1338 = split_tensor_136[1];  split_tensor_136 = None
        silu_38 = torch.ops.aten.silu.default(getitem_1337);  getitem_1337 = None
        mul_173 = torch.ops.aten.mul.Tensor(silu_38, getitem_1338);  silu_38 = getitem_1338 = None
        _to_copy_843 = torch.ops.aten._to_copy.default(arg407_1, dtype = torch.bfloat16);  arg407_1 = None
        t_302 = torch.ops.aten.t.default(_to_copy_843);  _to_copy_843 = None
        view_1478 = torch.ops.aten.view.default(mul_173, [262144, 512]);  mul_173 = None
        mm_281 = torch.ops.aten.mm.default(view_1478, t_302);  view_1478 = t_302 = None
        view_1479 = torch.ops.aten.view.default(mm_281, [1, 512, 512, 256]);  mm_281 = None
        add_139 = torch.ops.aten.add.Tensor(add_138, view_1479);  add_138 = view_1479 = None
        _to_copy_844 = torch.ops.aten._to_copy.default(add_135, dtype = torch.float32)
        native_layer_norm_default_175 = torch.ops.aten.native_layer_norm.default(_to_copy_844, [384], arg422_1, arg423_1, 1e-05);  _to_copy_844 = arg422_1 = arg423_1 = None
        getitem_1339 = native_layer_norm_default_175[0]
        _to_copy_845 = torch.ops.aten._to_copy.default(add_131, dtype = torch.float32);  add_131 = None
        native_layer_norm_default_176 = torch.ops.aten.native_layer_norm.default(_to_copy_845, [256], arg424_1, arg425_1, 1e-05);  _to_copy_845 = arg424_1 = arg425_1 = None
        getitem_1342 = native_layer_norm_default_176[0]
        _to_copy_846 = torch.ops.aten._to_copy.default(arg426_1, dtype = torch.bfloat16);  arg426_1 = None
        _to_copy_847 = torch.ops.aten._to_copy.default(getitem_1342, dtype = torch.bfloat16);  getitem_1342 = None
        t_303 = torch.ops.aten.t.default(_to_copy_846);  _to_copy_846 = None
        view_1480 = torch.ops.aten.view.default(_to_copy_847, [262144, 256]);  _to_copy_847 = None
        mm_282 = torch.ops.aten.mm.default(view_1480, t_303);  view_1480 = t_303 = None
        view_1481 = torch.ops.aten.view.default(mm_282, [1, 512, 512, 16]);  mm_282 = None
        permute_842 = torch.ops.aten.permute.default(view_1481, [0, 3, 1, 2]);  view_1481 = None
        view_1482 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_101 = torch.ops.aten.bitwise_not.default(view_1482);  view_1482 = None
        masked_fill_101 = torch.ops.aten.masked_fill.Scalar(permute_842, bitwise_not_101, -10000);  permute_842 = bitwise_not_101 = None
        _to_copy_848 = torch.ops.aten._to_copy.default(getitem_1339, dtype = torch.bfloat16);  getitem_1339 = None
        _to_copy_849 = torch.ops.aten._to_copy.default(arg428_1, dtype = torch.bfloat16);  arg428_1 = None
        unsqueeze_541 = torch.ops.aten.unsqueeze.default(_to_copy_848, 3);  _to_copy_848 = None
        unsqueeze_542 = torch.ops.aten.unsqueeze.default(unsqueeze_541, 4);  unsqueeze_541 = None
        unsqueeze_543 = torch.ops.aten.unsqueeze.default(unsqueeze_542, 5);  unsqueeze_542 = None
        permute_843 = torch.ops.aten.permute.default(unsqueeze_543, [3, 0, 4, 1, 5, 2]);  unsqueeze_543 = None
        unsqueeze_544 = torch.ops.aten.unsqueeze.default(_to_copy_849, 4);  _to_copy_849 = None
        unsqueeze_545 = torch.ops.aten.unsqueeze.default(unsqueeze_544, 5);  unsqueeze_544 = None
        permute_844 = torch.ops.aten.permute.default(unsqueeze_545, [1, 4, 2, 5, 3, 0]);  unsqueeze_545 = None
        permute_845 = torch.ops.aten.permute.default(permute_843, [3, 5, 0, 1, 2, 4]);  permute_843 = None
        view_1483 = torch.ops.aten.view.default(permute_845, [1, 512, 384]);  permute_845 = None
        permute_846 = torch.ops.aten.permute.default(permute_844, [5, 0, 1, 2, 4, 3]);  permute_844 = None
        view_1484 = torch.ops.aten.view.default(permute_846, [1, 384, 1536]);  permute_846 = None
        bmm_131 = torch.ops.aten.bmm.default(view_1483, view_1484);  view_1483 = view_1484 = None
        view_1485 = torch.ops.aten.view.default(bmm_131, [512, 1, 4, 1, 16, 24]);  bmm_131 = None
        permute_847 = torch.ops.aten.permute.default(view_1485, [2, 3, 4, 0, 5, 1]);  view_1485 = None
        view_1486 = torch.ops.aten.view.default(permute_847, [4, 1, 16, 512, 24]);  permute_847 = None
        unbind_int_72 = torch.ops.aten.unbind.int(view_1486);  view_1486 = None
        getitem_1345 = unbind_int_72[0]
        getitem_1346 = unbind_int_72[1]
        getitem_1347 = unbind_int_72[2]
        getitem_1348 = unbind_int_72[3];  unbind_int_72 = None
        view_1487 = torch.ops.aten.view.default(arg427_1, [1, 16, 1, 24]);  arg427_1 = None
        add_140 = torch.ops.aten.add.Tensor(getitem_1345, view_1487);  getitem_1345 = view_1487 = None
        _to_copy_850 = torch.ops.aten._to_copy.default(add_140, dtype = torch.bfloat16);  add_140 = None
        expand_81 = torch.ops.aten.expand.default(masked_fill_101, [1, 16, 512, 512]);  masked_fill_101 = None
        _scaled_dot_product_efficient_attention_default_44 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_850, getitem_1346, getitem_1347, expand_81, False);  _to_copy_850 = getitem_1346 = getitem_1347 = expand_81 = None
        getitem_1349 = _scaled_dot_product_efficient_attention_default_44[0]
        add_141 = torch.ops.aten.add.Tensor(getitem_1348, 1);  getitem_1348 = None
        sigmoid_107 = torch.ops.aten.sigmoid.default(add_141);  add_141 = None
        mul_174 = torch.ops.aten.mul.Tensor(getitem_1349, sigmoid_107);  getitem_1349 = sigmoid_107 = None
        _to_copy_851 = torch.ops.aten._to_copy.default(arg429_1, dtype = torch.bfloat16);  arg429_1 = None
        unsqueeze_546 = torch.ops.aten.unsqueeze.default(mul_174, 4);  mul_174 = None
        permute_848 = torch.ops.aten.permute.default(unsqueeze_546, [0, 2, 4, 3, 1]);  unsqueeze_546 = None
        unsqueeze_547 = torch.ops.aten.unsqueeze.default(_to_copy_851, 3);  _to_copy_851 = None
        unsqueeze_548 = torch.ops.aten.unsqueeze.default(unsqueeze_547, 4);  unsqueeze_547 = None
        permute_849 = torch.ops.aten.permute.default(unsqueeze_548, [3, 4, 2, 1, 0]);  unsqueeze_548 = None
        permute_850 = torch.ops.aten.permute.default(permute_848, [1, 3, 4, 0, 2]);  permute_848 = None
        clone_148 = torch.ops.aten.clone.default(permute_850, memory_format = torch.contiguous_format);  permute_850 = None
        _unsafe_view_130 = torch.ops.aten._unsafe_view.default(clone_148, [1, 512, 384]);  clone_148 = None
        permute_851 = torch.ops.aten.permute.default(permute_849, [3, 4, 0, 2, 1]);  permute_849 = None
        clone_149 = torch.ops.aten.clone.default(permute_851, memory_format = torch.contiguous_format);  permute_851 = None
        _unsafe_view_131 = torch.ops.aten._unsafe_view.default(clone_149, [1, 384, 384]);  clone_149 = None
        bmm_132 = torch.ops.aten.bmm.default(_unsafe_view_130, _unsafe_view_131);  _unsafe_view_130 = _unsafe_view_131 = None
        view_1488 = torch.ops.aten.view.default(bmm_132, [512, 1, 1, 1, 384]);  bmm_132 = None
        permute_852 = torch.ops.aten.permute.default(view_1488, [3, 0, 4, 1, 2]);  view_1488 = None
        view_1489 = torch.ops.aten.view.default(permute_852, [1, 512, 384]);  permute_852 = None
        unsqueeze_549 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_175 = torch.ops.aten.mul.Tensor(view_1489, unsqueeze_549);  view_1489 = unsqueeze_549 = None
        add_142 = torch.ops.aten.add.Tensor(add_135, mul_175);  mul_175 = None
        split_tensor_137 = torch.ops.aten.split.Tensor(add_135, 512, dim = -2);  add_135 = None
        getitem_1353 = split_tensor_137[0];  split_tensor_137 = None
        _to_copy_852 = torch.ops.aten._to_copy.default(getitem_1353, dtype = torch.float32);  getitem_1353 = None
        native_layer_norm_default_177 = torch.ops.aten.native_layer_norm.default(_to_copy_852, [384], arg418_1, arg419_1, 1e-05);  _to_copy_852 = arg418_1 = arg419_1 = None
        getitem_1354 = native_layer_norm_default_177[0]
        _to_copy_853 = torch.ops.aten._to_copy.default(arg420_1, dtype = torch.bfloat16);  arg420_1 = None
        _to_copy_854 = torch.ops.aten._to_copy.default(getitem_1354, dtype = torch.bfloat16);  getitem_1354 = None
        t_304 = torch.ops.aten.t.default(_to_copy_853);  _to_copy_853 = None
        view_1490 = torch.ops.aten.view.default(_to_copy_854, [512, 384]);  _to_copy_854 = None
        mm_283 = torch.ops.aten.mm.default(view_1490, t_304);  view_1490 = t_304 = None
        view_1491 = torch.ops.aten.view.default(mm_283, [1, 512, 1536]);  mm_283 = None
        split_tensor_138 = torch.ops.aten.split.Tensor(view_1491, 768, dim = -1);  view_1491 = None
        getitem_1357 = split_tensor_138[0]
        getitem_1358 = split_tensor_138[1];  split_tensor_138 = None
        silu_39 = torch.ops.aten.silu.default(getitem_1357);  getitem_1357 = None
        mul_176 = torch.ops.aten.mul.Tensor(silu_39, getitem_1358);  silu_39 = getitem_1358 = None
        _to_copy_855 = torch.ops.aten._to_copy.default(arg421_1, dtype = torch.bfloat16);  arg421_1 = None
        t_305 = torch.ops.aten.t.default(_to_copy_855);  _to_copy_855 = None
        view_1493 = torch.ops.aten.view.default(mul_176, [512, 768]);  mul_176 = None
        mm_284 = torch.ops.aten.mm.default(view_1493, t_305);  view_1493 = t_305 = None
        view_1494 = torch.ops.aten.view.default(mm_284, [1, 512, 384]);  mm_284 = None
        add_143 = torch.ops.aten.add.Tensor(add_142, view_1494);  add_142 = view_1494 = None
        _to_copy_856 = torch.ops.aten._to_copy.default(add_139, dtype = torch.float32)
        native_layer_norm_default_178 = torch.ops.aten.native_layer_norm.default(_to_copy_856, [256], arg434_1, arg435_1, 1e-05);  _to_copy_856 = arg434_1 = arg435_1 = None
        getitem_1359 = native_layer_norm_default_178[0]
        split_with_sizes_default_34 = torch.ops.aten.split_with_sizes.default(arg437_1, [512, 512]);  arg437_1 = None
        getitem_1362 = split_with_sizes_default_34[0]
        getitem_1363 = split_with_sizes_default_34[1];  split_with_sizes_default_34 = None
        split_with_sizes_default_35 = torch.ops.aten.split_with_sizes.default(arg438_1, [512, 512, 256]);  arg438_1 = None
        getitem_1364 = split_with_sizes_default_35[0]
        getitem_1365 = split_with_sizes_default_35[1]
        getitem_1366 = split_with_sizes_default_35[2];  split_with_sizes_default_35 = None
        _to_copy_857 = torch.ops.aten._to_copy.default(getitem_1362, dtype = torch.bfloat16);  getitem_1362 = None
        _to_copy_858 = torch.ops.aten._to_copy.default(getitem_1359, dtype = torch.bfloat16)
        t_306 = torch.ops.aten.t.default(_to_copy_857);  _to_copy_857 = None
        view_1495 = torch.ops.aten.view.default(_to_copy_858, [262144, 256]);  _to_copy_858 = None
        mm_285 = torch.ops.aten.mm.default(view_1495, t_306);  view_1495 = t_306 = None
        view_1496 = torch.ops.aten.view.default(mm_285, [1, 512, 512, 512]);  mm_285 = None
        _to_copy_859 = torch.ops.aten._to_copy.default(getitem_1364, dtype = torch.bfloat16);  getitem_1364 = None
        _to_copy_860 = torch.ops.aten._to_copy.default(getitem_1359, dtype = torch.bfloat16)
        t_307 = torch.ops.aten.t.default(_to_copy_859);  _to_copy_859 = None
        view_1497 = torch.ops.aten.view.default(_to_copy_860, [262144, 256]);  _to_copy_860 = None
        mm_286 = torch.ops.aten.mm.default(view_1497, t_307);  view_1497 = t_307 = None
        view_1498 = torch.ops.aten.view.default(mm_286, [1, 512, 512, 512]);  mm_286 = None
        sigmoid_108 = torch.ops.aten.sigmoid.default(view_1498);  view_1498 = None
        mul_177 = torch.ops.aten.mul.Tensor(view_1496, sigmoid_108);  view_1496 = sigmoid_108 = None
        unsqueeze_550 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_102 = torch.ops.aten.bitwise_not.default(unsqueeze_550);  unsqueeze_550 = None
        masked_fill_102 = torch.ops.aten.masked_fill.Scalar(mul_177, bitwise_not_102, 0);  mul_177 = bitwise_not_102 = None
        split_tensor_139 = torch.ops.aten.split.Tensor(masked_fill_102, 256, dim = -1)
        getitem_1369 = split_tensor_139[0]
        unsqueeze_553 = torch.ops.aten.unsqueeze.default(getitem_1369, 4);  getitem_1369 = None
        permute_857 = torch.ops.aten.permute.default(unsqueeze_553, [0, 1, 4, 3, 2]);  unsqueeze_553 = None
        permute_858 = torch.ops.aten.permute.default(permute_857, [3, 1, 4, 0, 2]);  permute_857 = None
        view_1501 = torch.ops.aten.view.default(permute_858, [256, 512, 512]);  permute_858 = None
        split_tensor_140 = torch.ops.aten.split.Tensor(masked_fill_102, 256, dim = -1);  masked_fill_102 = None
        getitem_1372 = split_tensor_140[1];  split_tensor_140 = None
        unsqueeze_554 = torch.ops.aten.unsqueeze.default(getitem_1372, 4);  getitem_1372 = None
        permute_859 = torch.ops.aten.permute.default(unsqueeze_554, [0, 4, 1, 3, 2]);  unsqueeze_554 = None
        permute_860 = torch.ops.aten.permute.default(permute_859, [3, 4, 0, 2, 1]);  permute_859 = None
        view_1502 = torch.ops.aten.view.default(permute_860, [256, 512, 512]);  permute_860 = None
        bmm_133 = torch.ops.aten.bmm.default(view_1501, view_1502);  view_1501 = view_1502 = None
        view_1503 = torch.ops.aten.view.default(bmm_133, [256, 512, 1, 1, 512]);  bmm_133 = None
        permute_861 = torch.ops.aten.permute.default(view_1503, [3, 1, 4, 0, 2]);  view_1503 = None
        view_1504 = torch.ops.aten.view.default(permute_861, [1, 512, 512, 256]);  permute_861 = None
        _to_copy_861 = torch.ops.aten._to_copy.default(getitem_1363, dtype = torch.bfloat16);  getitem_1363 = None
        _to_copy_862 = torch.ops.aten._to_copy.default(getitem_1359, dtype = torch.bfloat16)
        t_308 = torch.ops.aten.t.default(_to_copy_861);  _to_copy_861 = None
        view_1505 = torch.ops.aten.view.default(_to_copy_862, [262144, 256]);  _to_copy_862 = None
        mm_287 = torch.ops.aten.mm.default(view_1505, t_308);  view_1505 = t_308 = None
        view_1506 = torch.ops.aten.view.default(mm_287, [1, 512, 512, 512]);  mm_287 = None
        _to_copy_863 = torch.ops.aten._to_copy.default(getitem_1365, dtype = torch.bfloat16);  getitem_1365 = None
        _to_copy_864 = torch.ops.aten._to_copy.default(getitem_1359, dtype = torch.bfloat16)
        t_309 = torch.ops.aten.t.default(_to_copy_863);  _to_copy_863 = None
        view_1507 = torch.ops.aten.view.default(_to_copy_864, [262144, 256]);  _to_copy_864 = None
        mm_288 = torch.ops.aten.mm.default(view_1507, t_309);  view_1507 = t_309 = None
        view_1508 = torch.ops.aten.view.default(mm_288, [1, 512, 512, 512]);  mm_288 = None
        sigmoid_109 = torch.ops.aten.sigmoid.default(view_1508);  view_1508 = None
        mul_178 = torch.ops.aten.mul.Tensor(view_1506, sigmoid_109);  view_1506 = sigmoid_109 = None
        view_1509 = torch.ops.aten.view.default(mul_178, [262144, 512]);  mul_178 = None
        view_1510 = torch.ops.aten.view.default(view_1509, [1, 512, 512, 512]);  view_1509 = None
        transpose_34 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_555 = torch.ops.aten.unsqueeze.default(transpose_34, 3);  transpose_34 = None
        clone_150 = torch.ops.aten.clone.default(unsqueeze_555, memory_format = torch.contiguous_format);  unsqueeze_555 = None
        bitwise_not_103 = torch.ops.aten.bitwise_not.default(clone_150);  clone_150 = None
        masked_fill_103 = torch.ops.aten.masked_fill.Scalar(view_1510, bitwise_not_103, 0);  view_1510 = bitwise_not_103 = None
        view_1511 = torch.ops.aten.view.default(masked_fill_103, [262144, 512]);  masked_fill_103 = None
        view_1515 = torch.ops.aten.view.default(view_1511, [1, 512, 512, 512])
        split_tensor_141 = torch.ops.aten.split.Tensor(view_1515, 256, dim = -1);  view_1515 = None
        getitem_1375 = split_tensor_141[0]
        unsqueeze_558 = torch.ops.aten.unsqueeze.default(getitem_1375, 4);  getitem_1375 = None
        permute_866 = torch.ops.aten.permute.default(unsqueeze_558, [0, 2, 4, 3, 1]);  unsqueeze_558 = None
        permute_867 = torch.ops.aten.permute.default(permute_866, [3, 1, 4, 0, 2]);  permute_866 = None
        view_1516 = torch.ops.aten.view.default(permute_867, [256, 512, 512]);  permute_867 = None
        view_1517 = torch.ops.aten.view.default(view_1511, [1, 512, 512, 512]);  view_1511 = None
        split_tensor_142 = torch.ops.aten.split.Tensor(view_1517, 256, dim = -1);  view_1517 = None
        getitem_1378 = split_tensor_142[1];  split_tensor_142 = None
        unsqueeze_559 = torch.ops.aten.unsqueeze.default(getitem_1378, 4);  getitem_1378 = None
        permute_868 = torch.ops.aten.permute.default(unsqueeze_559, [0, 4, 2, 3, 1]);  unsqueeze_559 = None
        permute_869 = torch.ops.aten.permute.default(permute_868, [3, 4, 0, 2, 1]);  permute_868 = None
        view_1518 = torch.ops.aten.view.default(permute_869, [256, 512, 512]);  permute_869 = None
        bmm_134 = torch.ops.aten.bmm.default(view_1516, view_1518);  view_1516 = view_1518 = None
        view_1519 = torch.ops.aten.view.default(bmm_134, [256, 512, 1, 1, 512]);  bmm_134 = None
        permute_870 = torch.ops.aten.permute.default(view_1519, [3, 1, 4, 0, 2]);  view_1519 = None
        view_1520 = torch.ops.aten.view.default(permute_870, [1, 512, 512, 256]);  permute_870 = None
        _to_copy_865 = torch.ops.aten._to_copy.default(view_1504, dtype = torch.float32);  view_1504 = None
        native_layer_norm_default_179 = torch.ops.aten.native_layer_norm.default(_to_copy_865, [256], None, None, 1e-05);  _to_copy_865 = None
        getitem_1379 = native_layer_norm_default_179[0]
        _to_copy_866 = torch.ops.aten._to_copy.default(view_1520, dtype = torch.float32);  view_1520 = None
        native_layer_norm_default_180 = torch.ops.aten.native_layer_norm.default(_to_copy_866, [256], None, None, 1e-05);  _to_copy_866 = None
        getitem_1382 = native_layer_norm_default_180[0]
        add_144 = torch.ops.aten.add.Tensor(getitem_1379, getitem_1382);  getitem_1379 = getitem_1382 = None
        _to_copy_867 = torch.ops.aten._to_copy.default(arg436_1, dtype = torch.bfloat16);  arg436_1 = None
        _to_copy_868 = torch.ops.aten._to_copy.default(add_144, dtype = torch.bfloat16);  add_144 = None
        t_310 = torch.ops.aten.t.default(_to_copy_867);  _to_copy_867 = None
        view_1521 = torch.ops.aten.view.default(_to_copy_868, [262144, 256]);  _to_copy_868 = None
        mm_289 = torch.ops.aten.mm.default(view_1521, t_310);  view_1521 = t_310 = None
        view_1522 = torch.ops.aten.view.default(mm_289, [1, 512, 512, 256]);  mm_289 = None
        _to_copy_869 = torch.ops.aten._to_copy.default(getitem_1366, dtype = torch.bfloat16);  getitem_1366 = None
        _to_copy_870 = torch.ops.aten._to_copy.default(getitem_1359, dtype = torch.bfloat16);  getitem_1359 = None
        t_311 = torch.ops.aten.t.default(_to_copy_869);  _to_copy_869 = None
        view_1523 = torch.ops.aten.view.default(_to_copy_870, [262144, 256]);  _to_copy_870 = None
        mm_290 = torch.ops.aten.mm.default(view_1523, t_311);  view_1523 = t_311 = None
        view_1524 = torch.ops.aten.view.default(mm_290, [1, 512, 512, 256]);  mm_290 = None
        sigmoid_110 = torch.ops.aten.sigmoid.default(view_1524);  view_1524 = None
        mul_179 = torch.ops.aten.mul.Tensor(view_1522, sigmoid_110);  view_1522 = sigmoid_110 = None
        add_145 = torch.ops.aten.add.Tensor(add_139, mul_179);  mul_179 = None
        _to_copy_871 = torch.ops.aten._to_copy.default(add_139, dtype = torch.float32)
        native_layer_norm_default_181 = torch.ops.aten.native_layer_norm.default(_to_copy_871, [256], None, None, 1e-05);  _to_copy_871 = None
        getitem_1385 = native_layer_norm_default_181[0]
        _to_copy_872 = torch.ops.aten._to_copy.default(arg440_1, dtype = torch.bfloat16);  arg440_1 = None
        _to_copy_873 = torch.ops.aten._to_copy.default(getitem_1385, dtype = torch.bfloat16)
        t_312 = torch.ops.aten.t.default(_to_copy_872);  _to_copy_872 = None
        view_1525 = torch.ops.aten.view.default(_to_copy_873, [262144, 256]);  _to_copy_873 = None
        mm_291 = torch.ops.aten.mm.default(view_1525, t_312);  view_1525 = t_312 = None
        view_1526 = torch.ops.aten.view.default(mm_291, [1, 512, 512, 8]);  mm_291 = None
        view_1527 = torch.ops.aten.view.default(view_1526, [1, 512, 512, 2, 4]);  view_1526 = None
        permute_871 = torch.ops.aten.permute.default(view_1527, [0, 3, 4, 1, 2]);  view_1527 = None
        view_1528 = torch.ops.aten.view.default(permute_871, [1, 2, 4, 1, 512, 512]);  permute_871 = None
        view_1529 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_104 = torch.ops.aten.bitwise_not.default(view_1529);  view_1529 = None
        masked_fill_104 = torch.ops.aten.masked_fill.Scalar(view_1528, bitwise_not_104, -10000);  view_1528 = bitwise_not_104 = None
        view_1530 = torch.ops.aten.view.default(masked_fill_104, [1, 2, 4, 512, 512]);  masked_fill_104 = None
        permute_872 = torch.ops.aten.permute.default(view_1530, [1, 0, 2, 3, 4]);  view_1530 = None
        view_1531 = torch.ops.aten.view.default(permute_872, [2, 4, 1, 512, 512]);  permute_872 = None
        _to_copy_874 = torch.ops.aten._to_copy.default(arg441_1, dtype = torch.bfloat16);  arg441_1 = None
        _to_copy_875 = torch.ops.aten._to_copy.default(getitem_1385, dtype = torch.bfloat16)
        t_313 = torch.ops.aten.t.default(_to_copy_874);  _to_copy_874 = None
        view_1532 = torch.ops.aten.view.default(_to_copy_875, [262144, 256]);  _to_copy_875 = None
        mm_292 = torch.ops.aten.mm.default(view_1532, t_313);  view_1532 = t_313 = None
        view_1533 = torch.ops.aten.view.default(mm_292, [1, 512, 512, 1024]);  mm_292 = None
        select_35 = torch.ops.aten.select.int(view_1531, 0, 0)
        view_1534 = torch.ops.aten.view.default(view_1533, [1, 512, 512, 4, 4, 64]);  view_1533 = None
        permute_873 = torch.ops.aten.permute.default(view_1534, [4, 0, 3, 1, 2, 5]);  view_1534 = None
        view_1535 = torch.ops.aten.view.default(permute_873, [4, 4, 512, 512, 64]);  permute_873 = None
        unbind_int_73 = torch.ops.aten.unbind.int(view_1535);  view_1535 = None
        getitem_1388 = unbind_int_73[0]
        getitem_1389 = unbind_int_73[1]
        getitem_1390 = unbind_int_73[2]
        getitem_1391 = unbind_int_73[3];  unbind_int_73 = None
        expand_82 = torch.ops.aten.expand.default(select_35, [4, 512, 512, 512]);  select_35 = None
        _scaled_dot_product_efficient_attention_default_45 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1388, getitem_1389, getitem_1390, expand_82, False);  getitem_1388 = getitem_1389 = getitem_1390 = expand_82 = None
        getitem_1392 = _scaled_dot_product_efficient_attention_default_45[0]
        sigmoid_111 = torch.ops.aten.sigmoid.default(getitem_1391);  getitem_1391 = None
        mul_180 = torch.ops.aten.mul.Tensor(getitem_1392, sigmoid_111);  getitem_1392 = sigmoid_111 = None
        view_1536 = torch.ops.aten.view.default(mul_180, [1, 4, 512, 512, 64]);  mul_180 = None
        permute_874 = torch.ops.aten.permute.default(view_1536, [0, 2, 3, 1, 4]);  view_1536 = None
        clone_151 = torch.ops.aten.clone.default(permute_874, memory_format = torch.contiguous_format);  permute_874 = None
        _unsafe_view_132 = torch.ops.aten._unsafe_view.default(clone_151, [1, 512, 512, 256]);  clone_151 = None
        transpose_35 = torch.ops.aten.transpose.int(getitem_1385, 1, 2);  getitem_1385 = None
        _to_copy_876 = torch.ops.aten._to_copy.default(arg442_1, dtype = torch.bfloat16);  arg442_1 = None
        _to_copy_877 = torch.ops.aten._to_copy.default(transpose_35, dtype = torch.bfloat16);  transpose_35 = None
        t_314 = torch.ops.aten.t.default(_to_copy_876);  _to_copy_876 = None
        expand_83 = torch.ops.aten.expand.default(_to_copy_877, [1, 512, 512, 256]);  _to_copy_877 = None
        view_1537 = torch.ops.aten.view.default(expand_83, [512, 512, 256]);  expand_83 = None
        expand_84 = torch.ops.aten.expand.default(t_314, [1, 512, 256, 1024]);  t_314 = None
        view_1538 = torch.ops.aten.view.default(expand_84, [512, 256, 1024]);  expand_84 = None
        bmm_135 = torch.ops.aten.bmm.default(view_1537, view_1538);  view_1537 = view_1538 = None
        view_1539 = torch.ops.aten.view.default(bmm_135, [1, 512, 512, 1024]);  bmm_135 = None
        select_36 = torch.ops.aten.select.int(view_1531, 0, 1);  view_1531 = None
        view_1540 = torch.ops.aten.view.default(view_1539, [1, 512, 512, 4, 4, 64]);  view_1539 = None
        permute_875 = torch.ops.aten.permute.default(view_1540, [4, 0, 3, 1, 2, 5]);  view_1540 = None
        view_1541 = torch.ops.aten.view.default(permute_875, [4, 4, 512, 512, 64]);  permute_875 = None
        unbind_int_74 = torch.ops.aten.unbind.int(view_1541);  view_1541 = None
        getitem_1396 = unbind_int_74[0]
        getitem_1397 = unbind_int_74[1]
        getitem_1398 = unbind_int_74[2]
        getitem_1399 = unbind_int_74[3];  unbind_int_74 = None
        expand_85 = torch.ops.aten.expand.default(select_36, [4, 512, 512, 512]);  select_36 = None
        _scaled_dot_product_efficient_attention_default_46 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1396, getitem_1397, getitem_1398, expand_85, False);  getitem_1396 = getitem_1397 = getitem_1398 = expand_85 = None
        getitem_1400 = _scaled_dot_product_efficient_attention_default_46[0]
        sigmoid_112 = torch.ops.aten.sigmoid.default(getitem_1399);  getitem_1399 = None
        mul_181 = torch.ops.aten.mul.Tensor(getitem_1400, sigmoid_112);  getitem_1400 = sigmoid_112 = None
        view_1542 = torch.ops.aten.view.default(mul_181, [1, 4, 512, 512, 64]);  mul_181 = None
        permute_876 = torch.ops.aten.permute.default(view_1542, [0, 2, 3, 1, 4]);  view_1542 = None
        clone_152 = torch.ops.aten.clone.default(permute_876, memory_format = torch.contiguous_format);  permute_876 = None
        _unsafe_view_133 = torch.ops.aten._unsafe_view.default(clone_152, [1, 512, 512, 256]);  clone_152 = None
        cat_23 = torch.ops.aten.cat.default([_unsafe_view_132, _unsafe_view_133], dim = -1);  _unsafe_view_132 = _unsafe_view_133 = None
        slice_216 = torch.ops.aten.slice.Tensor(arg439_1, dim = 0, start = 0, end = 9223372036854775807);  arg439_1 = None
        unsqueeze_560 = torch.ops.aten.unsqueeze.default(slice_216, 1);  slice_216 = None
        mul_182 = torch.ops.aten.mul.Tensor(arg443_1, unsqueeze_560);  arg443_1 = unsqueeze_560 = None
        _to_copy_878 = torch.ops.aten._to_copy.default(mul_182, dtype = torch.bfloat16);  mul_182 = None
        t_315 = torch.ops.aten.t.default(_to_copy_878);  _to_copy_878 = None
        view_1543 = torch.ops.aten.view.default(cat_23, [262144, 512]);  cat_23 = None
        mm_293 = torch.ops.aten.mm.default(view_1543, t_315);  view_1543 = t_315 = None
        view_1544 = torch.ops.aten.view.default(mm_293, [1, 512, 512, 256]);  mm_293 = None
        add_146 = torch.ops.aten.add.Tensor(add_145, view_1544);  add_145 = view_1544 = None
        split_tensor_143 = torch.ops.aten.split.Tensor(add_139, 512, dim = -2)
        getitem_1404 = split_tensor_143[0];  split_tensor_143 = None
        _to_copy_879 = torch.ops.aten._to_copy.default(getitem_1404, dtype = torch.float32);  getitem_1404 = None
        native_layer_norm_default_182 = torch.ops.aten.native_layer_norm.default(_to_copy_879, [256], arg430_1, arg431_1, 1e-05);  _to_copy_879 = arg430_1 = arg431_1 = None
        getitem_1405 = native_layer_norm_default_182[0]
        _to_copy_880 = torch.ops.aten._to_copy.default(arg432_1, dtype = torch.bfloat16);  arg432_1 = None
        _to_copy_881 = torch.ops.aten._to_copy.default(getitem_1405, dtype = torch.bfloat16);  getitem_1405 = None
        t_316 = torch.ops.aten.t.default(_to_copy_880);  _to_copy_880 = None
        view_1545 = torch.ops.aten.view.default(_to_copy_881, [262144, 256]);  _to_copy_881 = None
        mm_294 = torch.ops.aten.mm.default(view_1545, t_316);  view_1545 = t_316 = None
        view_1546 = torch.ops.aten.view.default(mm_294, [1, 512, 512, 1024]);  mm_294 = None
        split_tensor_144 = torch.ops.aten.split.Tensor(view_1546, 512, dim = -1);  view_1546 = None
        getitem_1408 = split_tensor_144[0]
        getitem_1409 = split_tensor_144[1];  split_tensor_144 = None
        silu_40 = torch.ops.aten.silu.default(getitem_1408);  getitem_1408 = None
        mul_183 = torch.ops.aten.mul.Tensor(silu_40, getitem_1409);  silu_40 = getitem_1409 = None
        _to_copy_882 = torch.ops.aten._to_copy.default(arg433_1, dtype = torch.bfloat16);  arg433_1 = None
        t_317 = torch.ops.aten.t.default(_to_copy_882);  _to_copy_882 = None
        view_1548 = torch.ops.aten.view.default(mul_183, [262144, 512]);  mul_183 = None
        mm_295 = torch.ops.aten.mm.default(view_1548, t_317);  view_1548 = t_317 = None
        view_1549 = torch.ops.aten.view.default(mm_295, [1, 512, 512, 256]);  mm_295 = None
        add_147 = torch.ops.aten.add.Tensor(add_146, view_1549);  add_146 = view_1549 = None
        _to_copy_883 = torch.ops.aten._to_copy.default(add_143, dtype = torch.float32)
        native_layer_norm_default_183 = torch.ops.aten.native_layer_norm.default(_to_copy_883, [384], arg448_1, arg449_1, 1e-05);  _to_copy_883 = arg448_1 = arg449_1 = None
        getitem_1410 = native_layer_norm_default_183[0]
        _to_copy_884 = torch.ops.aten._to_copy.default(add_139, dtype = torch.float32);  add_139 = None
        native_layer_norm_default_184 = torch.ops.aten.native_layer_norm.default(_to_copy_884, [256], arg450_1, arg451_1, 1e-05);  _to_copy_884 = arg450_1 = arg451_1 = None
        getitem_1413 = native_layer_norm_default_184[0]
        _to_copy_885 = torch.ops.aten._to_copy.default(arg452_1, dtype = torch.bfloat16);  arg452_1 = None
        _to_copy_886 = torch.ops.aten._to_copy.default(getitem_1413, dtype = torch.bfloat16);  getitem_1413 = None
        t_318 = torch.ops.aten.t.default(_to_copy_885);  _to_copy_885 = None
        view_1550 = torch.ops.aten.view.default(_to_copy_886, [262144, 256]);  _to_copy_886 = None
        mm_296 = torch.ops.aten.mm.default(view_1550, t_318);  view_1550 = t_318 = None
        view_1551 = torch.ops.aten.view.default(mm_296, [1, 512, 512, 16]);  mm_296 = None
        permute_877 = torch.ops.aten.permute.default(view_1551, [0, 3, 1, 2]);  view_1551 = None
        view_1552 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_105 = torch.ops.aten.bitwise_not.default(view_1552);  view_1552 = None
        masked_fill_105 = torch.ops.aten.masked_fill.Scalar(permute_877, bitwise_not_105, -10000);  permute_877 = bitwise_not_105 = None
        _to_copy_887 = torch.ops.aten._to_copy.default(getitem_1410, dtype = torch.bfloat16);  getitem_1410 = None
        _to_copy_888 = torch.ops.aten._to_copy.default(arg454_1, dtype = torch.bfloat16);  arg454_1 = None
        unsqueeze_561 = torch.ops.aten.unsqueeze.default(_to_copy_887, 3);  _to_copy_887 = None
        unsqueeze_562 = torch.ops.aten.unsqueeze.default(unsqueeze_561, 4);  unsqueeze_561 = None
        unsqueeze_563 = torch.ops.aten.unsqueeze.default(unsqueeze_562, 5);  unsqueeze_562 = None
        permute_878 = torch.ops.aten.permute.default(unsqueeze_563, [3, 0, 4, 1, 5, 2]);  unsqueeze_563 = None
        unsqueeze_564 = torch.ops.aten.unsqueeze.default(_to_copy_888, 4);  _to_copy_888 = None
        unsqueeze_565 = torch.ops.aten.unsqueeze.default(unsqueeze_564, 5);  unsqueeze_564 = None
        permute_879 = torch.ops.aten.permute.default(unsqueeze_565, [1, 4, 2, 5, 3, 0]);  unsqueeze_565 = None
        permute_880 = torch.ops.aten.permute.default(permute_878, [3, 5, 0, 1, 2, 4]);  permute_878 = None
        view_1553 = torch.ops.aten.view.default(permute_880, [1, 512, 384]);  permute_880 = None
        permute_881 = torch.ops.aten.permute.default(permute_879, [5, 0, 1, 2, 4, 3]);  permute_879 = None
        view_1554 = torch.ops.aten.view.default(permute_881, [1, 384, 1536]);  permute_881 = None
        bmm_136 = torch.ops.aten.bmm.default(view_1553, view_1554);  view_1553 = view_1554 = None
        view_1555 = torch.ops.aten.view.default(bmm_136, [512, 1, 4, 1, 16, 24]);  bmm_136 = None
        permute_882 = torch.ops.aten.permute.default(view_1555, [2, 3, 4, 0, 5, 1]);  view_1555 = None
        view_1556 = torch.ops.aten.view.default(permute_882, [4, 1, 16, 512, 24]);  permute_882 = None
        unbind_int_75 = torch.ops.aten.unbind.int(view_1556);  view_1556 = None
        getitem_1416 = unbind_int_75[0]
        getitem_1417 = unbind_int_75[1]
        getitem_1418 = unbind_int_75[2]
        getitem_1419 = unbind_int_75[3];  unbind_int_75 = None
        view_1557 = torch.ops.aten.view.default(arg453_1, [1, 16, 1, 24]);  arg453_1 = None
        add_148 = torch.ops.aten.add.Tensor(getitem_1416, view_1557);  getitem_1416 = view_1557 = None
        _to_copy_889 = torch.ops.aten._to_copy.default(add_148, dtype = torch.bfloat16);  add_148 = None
        expand_86 = torch.ops.aten.expand.default(masked_fill_105, [1, 16, 512, 512]);  masked_fill_105 = None
        _scaled_dot_product_efficient_attention_default_47 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_889, getitem_1417, getitem_1418, expand_86, False);  _to_copy_889 = getitem_1417 = getitem_1418 = expand_86 = None
        getitem_1420 = _scaled_dot_product_efficient_attention_default_47[0]
        add_149 = torch.ops.aten.add.Tensor(getitem_1419, 1);  getitem_1419 = None
        sigmoid_113 = torch.ops.aten.sigmoid.default(add_149);  add_149 = None
        mul_184 = torch.ops.aten.mul.Tensor(getitem_1420, sigmoid_113);  getitem_1420 = sigmoid_113 = None
        _to_copy_890 = torch.ops.aten._to_copy.default(arg455_1, dtype = torch.bfloat16);  arg455_1 = None
        unsqueeze_566 = torch.ops.aten.unsqueeze.default(mul_184, 4);  mul_184 = None
        permute_883 = torch.ops.aten.permute.default(unsqueeze_566, [0, 2, 4, 3, 1]);  unsqueeze_566 = None
        unsqueeze_567 = torch.ops.aten.unsqueeze.default(_to_copy_890, 3);  _to_copy_890 = None
        unsqueeze_568 = torch.ops.aten.unsqueeze.default(unsqueeze_567, 4);  unsqueeze_567 = None
        permute_884 = torch.ops.aten.permute.default(unsqueeze_568, [3, 4, 2, 1, 0]);  unsqueeze_568 = None
        permute_885 = torch.ops.aten.permute.default(permute_883, [1, 3, 4, 0, 2]);  permute_883 = None
        clone_153 = torch.ops.aten.clone.default(permute_885, memory_format = torch.contiguous_format);  permute_885 = None
        _unsafe_view_134 = torch.ops.aten._unsafe_view.default(clone_153, [1, 512, 384]);  clone_153 = None
        permute_886 = torch.ops.aten.permute.default(permute_884, [3, 4, 0, 2, 1]);  permute_884 = None
        clone_154 = torch.ops.aten.clone.default(permute_886, memory_format = torch.contiguous_format);  permute_886 = None
        _unsafe_view_135 = torch.ops.aten._unsafe_view.default(clone_154, [1, 384, 384]);  clone_154 = None
        bmm_137 = torch.ops.aten.bmm.default(_unsafe_view_134, _unsafe_view_135);  _unsafe_view_134 = _unsafe_view_135 = None
        view_1558 = torch.ops.aten.view.default(bmm_137, [512, 1, 1, 1, 384]);  bmm_137 = None
        permute_887 = torch.ops.aten.permute.default(view_1558, [3, 0, 4, 1, 2]);  view_1558 = None
        view_1559 = torch.ops.aten.view.default(permute_887, [1, 512, 384]);  permute_887 = None
        unsqueeze_569 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_185 = torch.ops.aten.mul.Tensor(view_1559, unsqueeze_569);  view_1559 = unsqueeze_569 = None
        add_150 = torch.ops.aten.add.Tensor(add_143, mul_185);  mul_185 = None
        split_tensor_145 = torch.ops.aten.split.Tensor(add_143, 512, dim = -2);  add_143 = None
        getitem_1424 = split_tensor_145[0];  split_tensor_145 = None
        _to_copy_891 = torch.ops.aten._to_copy.default(getitem_1424, dtype = torch.float32);  getitem_1424 = None
        native_layer_norm_default_185 = torch.ops.aten.native_layer_norm.default(_to_copy_891, [384], arg444_1, arg445_1, 1e-05);  _to_copy_891 = arg444_1 = arg445_1 = None
        getitem_1425 = native_layer_norm_default_185[0]
        _to_copy_892 = torch.ops.aten._to_copy.default(arg446_1, dtype = torch.bfloat16);  arg446_1 = None
        _to_copy_893 = torch.ops.aten._to_copy.default(getitem_1425, dtype = torch.bfloat16);  getitem_1425 = None
        t_319 = torch.ops.aten.t.default(_to_copy_892);  _to_copy_892 = None
        view_1560 = torch.ops.aten.view.default(_to_copy_893, [512, 384]);  _to_copy_893 = None
        mm_297 = torch.ops.aten.mm.default(view_1560, t_319);  view_1560 = t_319 = None
        view_1561 = torch.ops.aten.view.default(mm_297, [1, 512, 1536]);  mm_297 = None
        split_tensor_146 = torch.ops.aten.split.Tensor(view_1561, 768, dim = -1);  view_1561 = None
        getitem_1428 = split_tensor_146[0]
        getitem_1429 = split_tensor_146[1];  split_tensor_146 = None
        silu_41 = torch.ops.aten.silu.default(getitem_1428);  getitem_1428 = None
        mul_186 = torch.ops.aten.mul.Tensor(silu_41, getitem_1429);  silu_41 = getitem_1429 = None
        _to_copy_894 = torch.ops.aten._to_copy.default(arg447_1, dtype = torch.bfloat16);  arg447_1 = None
        t_320 = torch.ops.aten.t.default(_to_copy_894);  _to_copy_894 = None
        view_1563 = torch.ops.aten.view.default(mul_186, [512, 768]);  mul_186 = None
        mm_298 = torch.ops.aten.mm.default(view_1563, t_320);  view_1563 = t_320 = None
        view_1564 = torch.ops.aten.view.default(mm_298, [1, 512, 384]);  mm_298 = None
        add_151 = torch.ops.aten.add.Tensor(add_150, view_1564);  add_150 = view_1564 = None
        _to_copy_895 = torch.ops.aten._to_copy.default(add_147, dtype = torch.float32)
        native_layer_norm_default_186 = torch.ops.aten.native_layer_norm.default(_to_copy_895, [256], arg460_1, arg461_1, 1e-05);  _to_copy_895 = arg460_1 = arg461_1 = None
        getitem_1430 = native_layer_norm_default_186[0]
        split_with_sizes_default_36 = torch.ops.aten.split_with_sizes.default(arg463_1, [512, 512]);  arg463_1 = None
        getitem_1433 = split_with_sizes_default_36[0]
        getitem_1434 = split_with_sizes_default_36[1];  split_with_sizes_default_36 = None
        split_with_sizes_default_37 = torch.ops.aten.split_with_sizes.default(arg464_1, [512, 512, 256]);  arg464_1 = None
        getitem_1435 = split_with_sizes_default_37[0]
        getitem_1436 = split_with_sizes_default_37[1]
        getitem_1437 = split_with_sizes_default_37[2];  split_with_sizes_default_37 = None
        _to_copy_896 = torch.ops.aten._to_copy.default(getitem_1433, dtype = torch.bfloat16);  getitem_1433 = None
        _to_copy_897 = torch.ops.aten._to_copy.default(getitem_1430, dtype = torch.bfloat16)
        t_321 = torch.ops.aten.t.default(_to_copy_896);  _to_copy_896 = None
        view_1565 = torch.ops.aten.view.default(_to_copy_897, [262144, 256]);  _to_copy_897 = None
        mm_299 = torch.ops.aten.mm.default(view_1565, t_321);  view_1565 = t_321 = None
        view_1566 = torch.ops.aten.view.default(mm_299, [1, 512, 512, 512]);  mm_299 = None
        _to_copy_898 = torch.ops.aten._to_copy.default(getitem_1435, dtype = torch.bfloat16);  getitem_1435 = None
        _to_copy_899 = torch.ops.aten._to_copy.default(getitem_1430, dtype = torch.bfloat16)
        t_322 = torch.ops.aten.t.default(_to_copy_898);  _to_copy_898 = None
        view_1567 = torch.ops.aten.view.default(_to_copy_899, [262144, 256]);  _to_copy_899 = None
        mm_300 = torch.ops.aten.mm.default(view_1567, t_322);  view_1567 = t_322 = None
        view_1568 = torch.ops.aten.view.default(mm_300, [1, 512, 512, 512]);  mm_300 = None
        sigmoid_114 = torch.ops.aten.sigmoid.default(view_1568);  view_1568 = None
        mul_187 = torch.ops.aten.mul.Tensor(view_1566, sigmoid_114);  view_1566 = sigmoid_114 = None
        unsqueeze_570 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_106 = torch.ops.aten.bitwise_not.default(unsqueeze_570);  unsqueeze_570 = None
        masked_fill_106 = torch.ops.aten.masked_fill.Scalar(mul_187, bitwise_not_106, 0);  mul_187 = bitwise_not_106 = None
        split_tensor_147 = torch.ops.aten.split.Tensor(masked_fill_106, 256, dim = -1)
        getitem_1440 = split_tensor_147[0]
        unsqueeze_573 = torch.ops.aten.unsqueeze.default(getitem_1440, 4);  getitem_1440 = None
        permute_892 = torch.ops.aten.permute.default(unsqueeze_573, [0, 1, 4, 3, 2]);  unsqueeze_573 = None
        permute_893 = torch.ops.aten.permute.default(permute_892, [3, 1, 4, 0, 2]);  permute_892 = None
        view_1571 = torch.ops.aten.view.default(permute_893, [256, 512, 512]);  permute_893 = None
        split_tensor_148 = torch.ops.aten.split.Tensor(masked_fill_106, 256, dim = -1);  masked_fill_106 = None
        getitem_1443 = split_tensor_148[1];  split_tensor_148 = None
        unsqueeze_574 = torch.ops.aten.unsqueeze.default(getitem_1443, 4);  getitem_1443 = None
        permute_894 = torch.ops.aten.permute.default(unsqueeze_574, [0, 4, 1, 3, 2]);  unsqueeze_574 = None
        permute_895 = torch.ops.aten.permute.default(permute_894, [3, 4, 0, 2, 1]);  permute_894 = None
        view_1572 = torch.ops.aten.view.default(permute_895, [256, 512, 512]);  permute_895 = None
        bmm_138 = torch.ops.aten.bmm.default(view_1571, view_1572);  view_1571 = view_1572 = None
        view_1573 = torch.ops.aten.view.default(bmm_138, [256, 512, 1, 1, 512]);  bmm_138 = None
        permute_896 = torch.ops.aten.permute.default(view_1573, [3, 1, 4, 0, 2]);  view_1573 = None
        view_1574 = torch.ops.aten.view.default(permute_896, [1, 512, 512, 256]);  permute_896 = None
        _to_copy_900 = torch.ops.aten._to_copy.default(getitem_1434, dtype = torch.bfloat16);  getitem_1434 = None
        _to_copy_901 = torch.ops.aten._to_copy.default(getitem_1430, dtype = torch.bfloat16)
        t_323 = torch.ops.aten.t.default(_to_copy_900);  _to_copy_900 = None
        view_1575 = torch.ops.aten.view.default(_to_copy_901, [262144, 256]);  _to_copy_901 = None
        mm_301 = torch.ops.aten.mm.default(view_1575, t_323);  view_1575 = t_323 = None
        view_1576 = torch.ops.aten.view.default(mm_301, [1, 512, 512, 512]);  mm_301 = None
        _to_copy_902 = torch.ops.aten._to_copy.default(getitem_1436, dtype = torch.bfloat16);  getitem_1436 = None
        _to_copy_903 = torch.ops.aten._to_copy.default(getitem_1430, dtype = torch.bfloat16)
        t_324 = torch.ops.aten.t.default(_to_copy_902);  _to_copy_902 = None
        view_1577 = torch.ops.aten.view.default(_to_copy_903, [262144, 256]);  _to_copy_903 = None
        mm_302 = torch.ops.aten.mm.default(view_1577, t_324);  view_1577 = t_324 = None
        view_1578 = torch.ops.aten.view.default(mm_302, [1, 512, 512, 512]);  mm_302 = None
        sigmoid_115 = torch.ops.aten.sigmoid.default(view_1578);  view_1578 = None
        mul_188 = torch.ops.aten.mul.Tensor(view_1576, sigmoid_115);  view_1576 = sigmoid_115 = None
        view_1579 = torch.ops.aten.view.default(mul_188, [262144, 512]);  mul_188 = None
        view_1580 = torch.ops.aten.view.default(view_1579, [1, 512, 512, 512]);  view_1579 = None
        transpose_36 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_575 = torch.ops.aten.unsqueeze.default(transpose_36, 3);  transpose_36 = None
        clone_155 = torch.ops.aten.clone.default(unsqueeze_575, memory_format = torch.contiguous_format);  unsqueeze_575 = None
        bitwise_not_107 = torch.ops.aten.bitwise_not.default(clone_155);  clone_155 = None
        masked_fill_107 = torch.ops.aten.masked_fill.Scalar(view_1580, bitwise_not_107, 0);  view_1580 = bitwise_not_107 = None
        view_1581 = torch.ops.aten.view.default(masked_fill_107, [262144, 512]);  masked_fill_107 = None
        view_1585 = torch.ops.aten.view.default(view_1581, [1, 512, 512, 512])
        split_tensor_149 = torch.ops.aten.split.Tensor(view_1585, 256, dim = -1);  view_1585 = None
        getitem_1446 = split_tensor_149[0]
        unsqueeze_578 = torch.ops.aten.unsqueeze.default(getitem_1446, 4);  getitem_1446 = None
        permute_901 = torch.ops.aten.permute.default(unsqueeze_578, [0, 2, 4, 3, 1]);  unsqueeze_578 = None
        permute_902 = torch.ops.aten.permute.default(permute_901, [3, 1, 4, 0, 2]);  permute_901 = None
        view_1586 = torch.ops.aten.view.default(permute_902, [256, 512, 512]);  permute_902 = None
        view_1587 = torch.ops.aten.view.default(view_1581, [1, 512, 512, 512]);  view_1581 = None
        split_tensor_150 = torch.ops.aten.split.Tensor(view_1587, 256, dim = -1);  view_1587 = None
        getitem_1449 = split_tensor_150[1];  split_tensor_150 = None
        unsqueeze_579 = torch.ops.aten.unsqueeze.default(getitem_1449, 4);  getitem_1449 = None
        permute_903 = torch.ops.aten.permute.default(unsqueeze_579, [0, 4, 2, 3, 1]);  unsqueeze_579 = None
        permute_904 = torch.ops.aten.permute.default(permute_903, [3, 4, 0, 2, 1]);  permute_903 = None
        view_1588 = torch.ops.aten.view.default(permute_904, [256, 512, 512]);  permute_904 = None
        bmm_139 = torch.ops.aten.bmm.default(view_1586, view_1588);  view_1586 = view_1588 = None
        view_1589 = torch.ops.aten.view.default(bmm_139, [256, 512, 1, 1, 512]);  bmm_139 = None
        permute_905 = torch.ops.aten.permute.default(view_1589, [3, 1, 4, 0, 2]);  view_1589 = None
        view_1590 = torch.ops.aten.view.default(permute_905, [1, 512, 512, 256]);  permute_905 = None
        _to_copy_904 = torch.ops.aten._to_copy.default(view_1574, dtype = torch.float32);  view_1574 = None
        native_layer_norm_default_187 = torch.ops.aten.native_layer_norm.default(_to_copy_904, [256], None, None, 1e-05);  _to_copy_904 = None
        getitem_1450 = native_layer_norm_default_187[0]
        _to_copy_905 = torch.ops.aten._to_copy.default(view_1590, dtype = torch.float32);  view_1590 = None
        native_layer_norm_default_188 = torch.ops.aten.native_layer_norm.default(_to_copy_905, [256], None, None, 1e-05);  _to_copy_905 = None
        getitem_1453 = native_layer_norm_default_188[0]
        add_152 = torch.ops.aten.add.Tensor(getitem_1450, getitem_1453);  getitem_1450 = getitem_1453 = None
        _to_copy_906 = torch.ops.aten._to_copy.default(arg462_1, dtype = torch.bfloat16);  arg462_1 = None
        _to_copy_907 = torch.ops.aten._to_copy.default(add_152, dtype = torch.bfloat16);  add_152 = None
        t_325 = torch.ops.aten.t.default(_to_copy_906);  _to_copy_906 = None
        view_1591 = torch.ops.aten.view.default(_to_copy_907, [262144, 256]);  _to_copy_907 = None
        mm_303 = torch.ops.aten.mm.default(view_1591, t_325);  view_1591 = t_325 = None
        view_1592 = torch.ops.aten.view.default(mm_303, [1, 512, 512, 256]);  mm_303 = None
        _to_copy_908 = torch.ops.aten._to_copy.default(getitem_1437, dtype = torch.bfloat16);  getitem_1437 = None
        _to_copy_909 = torch.ops.aten._to_copy.default(getitem_1430, dtype = torch.bfloat16);  getitem_1430 = None
        t_326 = torch.ops.aten.t.default(_to_copy_908);  _to_copy_908 = None
        view_1593 = torch.ops.aten.view.default(_to_copy_909, [262144, 256]);  _to_copy_909 = None
        mm_304 = torch.ops.aten.mm.default(view_1593, t_326);  view_1593 = t_326 = None
        view_1594 = torch.ops.aten.view.default(mm_304, [1, 512, 512, 256]);  mm_304 = None
        sigmoid_116 = torch.ops.aten.sigmoid.default(view_1594);  view_1594 = None
        mul_189 = torch.ops.aten.mul.Tensor(view_1592, sigmoid_116);  view_1592 = sigmoid_116 = None
        add_153 = torch.ops.aten.add.Tensor(add_147, mul_189);  mul_189 = None
        _to_copy_910 = torch.ops.aten._to_copy.default(add_147, dtype = torch.float32)
        native_layer_norm_default_189 = torch.ops.aten.native_layer_norm.default(_to_copy_910, [256], None, None, 1e-05);  _to_copy_910 = None
        getitem_1456 = native_layer_norm_default_189[0]
        _to_copy_911 = torch.ops.aten._to_copy.default(arg466_1, dtype = torch.bfloat16);  arg466_1 = None
        _to_copy_912 = torch.ops.aten._to_copy.default(getitem_1456, dtype = torch.bfloat16)
        t_327 = torch.ops.aten.t.default(_to_copy_911);  _to_copy_911 = None
        view_1595 = torch.ops.aten.view.default(_to_copy_912, [262144, 256]);  _to_copy_912 = None
        mm_305 = torch.ops.aten.mm.default(view_1595, t_327);  view_1595 = t_327 = None
        view_1596 = torch.ops.aten.view.default(mm_305, [1, 512, 512, 8]);  mm_305 = None
        view_1597 = torch.ops.aten.view.default(view_1596, [1, 512, 512, 2, 4]);  view_1596 = None
        permute_906 = torch.ops.aten.permute.default(view_1597, [0, 3, 4, 1, 2]);  view_1597 = None
        view_1598 = torch.ops.aten.view.default(permute_906, [1, 2, 4, 1, 512, 512]);  permute_906 = None
        view_1599 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_108 = torch.ops.aten.bitwise_not.default(view_1599);  view_1599 = None
        masked_fill_108 = torch.ops.aten.masked_fill.Scalar(view_1598, bitwise_not_108, -10000);  view_1598 = bitwise_not_108 = None
        view_1600 = torch.ops.aten.view.default(masked_fill_108, [1, 2, 4, 512, 512]);  masked_fill_108 = None
        permute_907 = torch.ops.aten.permute.default(view_1600, [1, 0, 2, 3, 4]);  view_1600 = None
        view_1601 = torch.ops.aten.view.default(permute_907, [2, 4, 1, 512, 512]);  permute_907 = None
        _to_copy_913 = torch.ops.aten._to_copy.default(arg467_1, dtype = torch.bfloat16);  arg467_1 = None
        _to_copy_914 = torch.ops.aten._to_copy.default(getitem_1456, dtype = torch.bfloat16)
        t_328 = torch.ops.aten.t.default(_to_copy_913);  _to_copy_913 = None
        view_1602 = torch.ops.aten.view.default(_to_copy_914, [262144, 256]);  _to_copy_914 = None
        mm_306 = torch.ops.aten.mm.default(view_1602, t_328);  view_1602 = t_328 = None
        view_1603 = torch.ops.aten.view.default(mm_306, [1, 512, 512, 1024]);  mm_306 = None
        select_37 = torch.ops.aten.select.int(view_1601, 0, 0)
        view_1604 = torch.ops.aten.view.default(view_1603, [1, 512, 512, 4, 4, 64]);  view_1603 = None
        permute_908 = torch.ops.aten.permute.default(view_1604, [4, 0, 3, 1, 2, 5]);  view_1604 = None
        view_1605 = torch.ops.aten.view.default(permute_908, [4, 4, 512, 512, 64]);  permute_908 = None
        unbind_int_76 = torch.ops.aten.unbind.int(view_1605);  view_1605 = None
        getitem_1459 = unbind_int_76[0]
        getitem_1460 = unbind_int_76[1]
        getitem_1461 = unbind_int_76[2]
        getitem_1462 = unbind_int_76[3];  unbind_int_76 = None
        expand_87 = torch.ops.aten.expand.default(select_37, [4, 512, 512, 512]);  select_37 = None
        _scaled_dot_product_efficient_attention_default_48 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1459, getitem_1460, getitem_1461, expand_87, False);  getitem_1459 = getitem_1460 = getitem_1461 = expand_87 = None
        getitem_1463 = _scaled_dot_product_efficient_attention_default_48[0]
        sigmoid_117 = torch.ops.aten.sigmoid.default(getitem_1462);  getitem_1462 = None
        mul_190 = torch.ops.aten.mul.Tensor(getitem_1463, sigmoid_117);  getitem_1463 = sigmoid_117 = None
        view_1606 = torch.ops.aten.view.default(mul_190, [1, 4, 512, 512, 64]);  mul_190 = None
        permute_909 = torch.ops.aten.permute.default(view_1606, [0, 2, 3, 1, 4]);  view_1606 = None
        clone_156 = torch.ops.aten.clone.default(permute_909, memory_format = torch.contiguous_format);  permute_909 = None
        _unsafe_view_136 = torch.ops.aten._unsafe_view.default(clone_156, [1, 512, 512, 256]);  clone_156 = None
        transpose_37 = torch.ops.aten.transpose.int(getitem_1456, 1, 2);  getitem_1456 = None
        _to_copy_915 = torch.ops.aten._to_copy.default(arg468_1, dtype = torch.bfloat16);  arg468_1 = None
        _to_copy_916 = torch.ops.aten._to_copy.default(transpose_37, dtype = torch.bfloat16);  transpose_37 = None
        t_329 = torch.ops.aten.t.default(_to_copy_915);  _to_copy_915 = None
        expand_88 = torch.ops.aten.expand.default(_to_copy_916, [1, 512, 512, 256]);  _to_copy_916 = None
        view_1607 = torch.ops.aten.view.default(expand_88, [512, 512, 256]);  expand_88 = None
        expand_89 = torch.ops.aten.expand.default(t_329, [1, 512, 256, 1024]);  t_329 = None
        view_1608 = torch.ops.aten.view.default(expand_89, [512, 256, 1024]);  expand_89 = None
        bmm_140 = torch.ops.aten.bmm.default(view_1607, view_1608);  view_1607 = view_1608 = None
        view_1609 = torch.ops.aten.view.default(bmm_140, [1, 512, 512, 1024]);  bmm_140 = None
        select_38 = torch.ops.aten.select.int(view_1601, 0, 1);  view_1601 = None
        view_1610 = torch.ops.aten.view.default(view_1609, [1, 512, 512, 4, 4, 64]);  view_1609 = None
        permute_910 = torch.ops.aten.permute.default(view_1610, [4, 0, 3, 1, 2, 5]);  view_1610 = None
        view_1611 = torch.ops.aten.view.default(permute_910, [4, 4, 512, 512, 64]);  permute_910 = None
        unbind_int_77 = torch.ops.aten.unbind.int(view_1611);  view_1611 = None
        getitem_1467 = unbind_int_77[0]
        getitem_1468 = unbind_int_77[1]
        getitem_1469 = unbind_int_77[2]
        getitem_1470 = unbind_int_77[3];  unbind_int_77 = None
        expand_90 = torch.ops.aten.expand.default(select_38, [4, 512, 512, 512]);  select_38 = None
        _scaled_dot_product_efficient_attention_default_49 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1467, getitem_1468, getitem_1469, expand_90, False);  getitem_1467 = getitem_1468 = getitem_1469 = expand_90 = None
        getitem_1471 = _scaled_dot_product_efficient_attention_default_49[0]
        sigmoid_118 = torch.ops.aten.sigmoid.default(getitem_1470);  getitem_1470 = None
        mul_191 = torch.ops.aten.mul.Tensor(getitem_1471, sigmoid_118);  getitem_1471 = sigmoid_118 = None
        view_1612 = torch.ops.aten.view.default(mul_191, [1, 4, 512, 512, 64]);  mul_191 = None
        permute_911 = torch.ops.aten.permute.default(view_1612, [0, 2, 3, 1, 4]);  view_1612 = None
        clone_157 = torch.ops.aten.clone.default(permute_911, memory_format = torch.contiguous_format);  permute_911 = None
        _unsafe_view_137 = torch.ops.aten._unsafe_view.default(clone_157, [1, 512, 512, 256]);  clone_157 = None
        cat_24 = torch.ops.aten.cat.default([_unsafe_view_136, _unsafe_view_137], dim = -1);  _unsafe_view_136 = _unsafe_view_137 = None
        slice_217 = torch.ops.aten.slice.Tensor(arg465_1, dim = 0, start = 0, end = 9223372036854775807);  arg465_1 = None
        unsqueeze_580 = torch.ops.aten.unsqueeze.default(slice_217, 1);  slice_217 = None
        mul_192 = torch.ops.aten.mul.Tensor(arg469_1, unsqueeze_580);  arg469_1 = unsqueeze_580 = None
        _to_copy_917 = torch.ops.aten._to_copy.default(mul_192, dtype = torch.bfloat16);  mul_192 = None
        t_330 = torch.ops.aten.t.default(_to_copy_917);  _to_copy_917 = None
        view_1613 = torch.ops.aten.view.default(cat_24, [262144, 512]);  cat_24 = None
        mm_307 = torch.ops.aten.mm.default(view_1613, t_330);  view_1613 = t_330 = None
        view_1614 = torch.ops.aten.view.default(mm_307, [1, 512, 512, 256]);  mm_307 = None
        add_154 = torch.ops.aten.add.Tensor(add_153, view_1614);  add_153 = view_1614 = None
        split_tensor_151 = torch.ops.aten.split.Tensor(add_147, 512, dim = -2)
        getitem_1475 = split_tensor_151[0];  split_tensor_151 = None
        _to_copy_918 = torch.ops.aten._to_copy.default(getitem_1475, dtype = torch.float32);  getitem_1475 = None
        native_layer_norm_default_190 = torch.ops.aten.native_layer_norm.default(_to_copy_918, [256], arg456_1, arg457_1, 1e-05);  _to_copy_918 = arg456_1 = arg457_1 = None
        getitem_1476 = native_layer_norm_default_190[0]
        _to_copy_919 = torch.ops.aten._to_copy.default(arg458_1, dtype = torch.bfloat16);  arg458_1 = None
        _to_copy_920 = torch.ops.aten._to_copy.default(getitem_1476, dtype = torch.bfloat16);  getitem_1476 = None
        t_331 = torch.ops.aten.t.default(_to_copy_919);  _to_copy_919 = None
        view_1615 = torch.ops.aten.view.default(_to_copy_920, [262144, 256]);  _to_copy_920 = None
        mm_308 = torch.ops.aten.mm.default(view_1615, t_331);  view_1615 = t_331 = None
        view_1616 = torch.ops.aten.view.default(mm_308, [1, 512, 512, 1024]);  mm_308 = None
        split_tensor_152 = torch.ops.aten.split.Tensor(view_1616, 512, dim = -1);  view_1616 = None
        getitem_1479 = split_tensor_152[0]
        getitem_1480 = split_tensor_152[1];  split_tensor_152 = None
        silu_42 = torch.ops.aten.silu.default(getitem_1479);  getitem_1479 = None
        mul_193 = torch.ops.aten.mul.Tensor(silu_42, getitem_1480);  silu_42 = getitem_1480 = None
        _to_copy_921 = torch.ops.aten._to_copy.default(arg459_1, dtype = torch.bfloat16);  arg459_1 = None
        t_332 = torch.ops.aten.t.default(_to_copy_921);  _to_copy_921 = None
        view_1618 = torch.ops.aten.view.default(mul_193, [262144, 512]);  mul_193 = None
        mm_309 = torch.ops.aten.mm.default(view_1618, t_332);  view_1618 = t_332 = None
        view_1619 = torch.ops.aten.view.default(mm_309, [1, 512, 512, 256]);  mm_309 = None
        add_155 = torch.ops.aten.add.Tensor(add_154, view_1619);  add_154 = view_1619 = None
        _to_copy_922 = torch.ops.aten._to_copy.default(add_151, dtype = torch.float32)
        native_layer_norm_default_191 = torch.ops.aten.native_layer_norm.default(_to_copy_922, [384], arg474_1, arg475_1, 1e-05);  _to_copy_922 = arg474_1 = arg475_1 = None
        getitem_1481 = native_layer_norm_default_191[0]
        _to_copy_923 = torch.ops.aten._to_copy.default(add_147, dtype = torch.float32);  add_147 = None
        native_layer_norm_default_192 = torch.ops.aten.native_layer_norm.default(_to_copy_923, [256], arg476_1, arg477_1, 1e-05);  _to_copy_923 = arg476_1 = arg477_1 = None
        getitem_1484 = native_layer_norm_default_192[0]
        _to_copy_924 = torch.ops.aten._to_copy.default(arg478_1, dtype = torch.bfloat16);  arg478_1 = None
        _to_copy_925 = torch.ops.aten._to_copy.default(getitem_1484, dtype = torch.bfloat16);  getitem_1484 = None
        t_333 = torch.ops.aten.t.default(_to_copy_924);  _to_copy_924 = None
        view_1620 = torch.ops.aten.view.default(_to_copy_925, [262144, 256]);  _to_copy_925 = None
        mm_310 = torch.ops.aten.mm.default(view_1620, t_333);  view_1620 = t_333 = None
        view_1621 = torch.ops.aten.view.default(mm_310, [1, 512, 512, 16]);  mm_310 = None
        permute_912 = torch.ops.aten.permute.default(view_1621, [0, 3, 1, 2]);  view_1621 = None
        view_1622 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_109 = torch.ops.aten.bitwise_not.default(view_1622);  view_1622 = None
        masked_fill_109 = torch.ops.aten.masked_fill.Scalar(permute_912, bitwise_not_109, -10000);  permute_912 = bitwise_not_109 = None
        _to_copy_926 = torch.ops.aten._to_copy.default(getitem_1481, dtype = torch.bfloat16);  getitem_1481 = None
        _to_copy_927 = torch.ops.aten._to_copy.default(arg480_1, dtype = torch.bfloat16);  arg480_1 = None
        unsqueeze_581 = torch.ops.aten.unsqueeze.default(_to_copy_926, 3);  _to_copy_926 = None
        unsqueeze_582 = torch.ops.aten.unsqueeze.default(unsqueeze_581, 4);  unsqueeze_581 = None
        unsqueeze_583 = torch.ops.aten.unsqueeze.default(unsqueeze_582, 5);  unsqueeze_582 = None
        permute_913 = torch.ops.aten.permute.default(unsqueeze_583, [3, 0, 4, 1, 5, 2]);  unsqueeze_583 = None
        unsqueeze_584 = torch.ops.aten.unsqueeze.default(_to_copy_927, 4);  _to_copy_927 = None
        unsqueeze_585 = torch.ops.aten.unsqueeze.default(unsqueeze_584, 5);  unsqueeze_584 = None
        permute_914 = torch.ops.aten.permute.default(unsqueeze_585, [1, 4, 2, 5, 3, 0]);  unsqueeze_585 = None
        permute_915 = torch.ops.aten.permute.default(permute_913, [3, 5, 0, 1, 2, 4]);  permute_913 = None
        view_1623 = torch.ops.aten.view.default(permute_915, [1, 512, 384]);  permute_915 = None
        permute_916 = torch.ops.aten.permute.default(permute_914, [5, 0, 1, 2, 4, 3]);  permute_914 = None
        view_1624 = torch.ops.aten.view.default(permute_916, [1, 384, 1536]);  permute_916 = None
        bmm_141 = torch.ops.aten.bmm.default(view_1623, view_1624);  view_1623 = view_1624 = None
        view_1625 = torch.ops.aten.view.default(bmm_141, [512, 1, 4, 1, 16, 24]);  bmm_141 = None
        permute_917 = torch.ops.aten.permute.default(view_1625, [2, 3, 4, 0, 5, 1]);  view_1625 = None
        view_1626 = torch.ops.aten.view.default(permute_917, [4, 1, 16, 512, 24]);  permute_917 = None
        unbind_int_78 = torch.ops.aten.unbind.int(view_1626);  view_1626 = None
        getitem_1487 = unbind_int_78[0]
        getitem_1488 = unbind_int_78[1]
        getitem_1489 = unbind_int_78[2]
        getitem_1490 = unbind_int_78[3];  unbind_int_78 = None
        view_1627 = torch.ops.aten.view.default(arg479_1, [1, 16, 1, 24]);  arg479_1 = None
        add_156 = torch.ops.aten.add.Tensor(getitem_1487, view_1627);  getitem_1487 = view_1627 = None
        _to_copy_928 = torch.ops.aten._to_copy.default(add_156, dtype = torch.bfloat16);  add_156 = None
        expand_91 = torch.ops.aten.expand.default(masked_fill_109, [1, 16, 512, 512]);  masked_fill_109 = None
        _scaled_dot_product_efficient_attention_default_50 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_928, getitem_1488, getitem_1489, expand_91, False);  _to_copy_928 = getitem_1488 = getitem_1489 = expand_91 = None
        getitem_1491 = _scaled_dot_product_efficient_attention_default_50[0]
        add_157 = torch.ops.aten.add.Tensor(getitem_1490, 1);  getitem_1490 = None
        sigmoid_119 = torch.ops.aten.sigmoid.default(add_157);  add_157 = None
        mul_194 = torch.ops.aten.mul.Tensor(getitem_1491, sigmoid_119);  getitem_1491 = sigmoid_119 = None
        _to_copy_929 = torch.ops.aten._to_copy.default(arg481_1, dtype = torch.bfloat16);  arg481_1 = None
        unsqueeze_586 = torch.ops.aten.unsqueeze.default(mul_194, 4);  mul_194 = None
        permute_918 = torch.ops.aten.permute.default(unsqueeze_586, [0, 2, 4, 3, 1]);  unsqueeze_586 = None
        unsqueeze_587 = torch.ops.aten.unsqueeze.default(_to_copy_929, 3);  _to_copy_929 = None
        unsqueeze_588 = torch.ops.aten.unsqueeze.default(unsqueeze_587, 4);  unsqueeze_587 = None
        permute_919 = torch.ops.aten.permute.default(unsqueeze_588, [3, 4, 2, 1, 0]);  unsqueeze_588 = None
        permute_920 = torch.ops.aten.permute.default(permute_918, [1, 3, 4, 0, 2]);  permute_918 = None
        clone_158 = torch.ops.aten.clone.default(permute_920, memory_format = torch.contiguous_format);  permute_920 = None
        _unsafe_view_138 = torch.ops.aten._unsafe_view.default(clone_158, [1, 512, 384]);  clone_158 = None
        permute_921 = torch.ops.aten.permute.default(permute_919, [3, 4, 0, 2, 1]);  permute_919 = None
        clone_159 = torch.ops.aten.clone.default(permute_921, memory_format = torch.contiguous_format);  permute_921 = None
        _unsafe_view_139 = torch.ops.aten._unsafe_view.default(clone_159, [1, 384, 384]);  clone_159 = None
        bmm_142 = torch.ops.aten.bmm.default(_unsafe_view_138, _unsafe_view_139);  _unsafe_view_138 = _unsafe_view_139 = None
        view_1628 = torch.ops.aten.view.default(bmm_142, [512, 1, 1, 1, 384]);  bmm_142 = None
        permute_922 = torch.ops.aten.permute.default(view_1628, [3, 0, 4, 1, 2]);  view_1628 = None
        view_1629 = torch.ops.aten.view.default(permute_922, [1, 512, 384]);  permute_922 = None
        unsqueeze_589 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_195 = torch.ops.aten.mul.Tensor(view_1629, unsqueeze_589);  view_1629 = unsqueeze_589 = None
        add_158 = torch.ops.aten.add.Tensor(add_151, mul_195);  mul_195 = None
        split_tensor_153 = torch.ops.aten.split.Tensor(add_151, 512, dim = -2);  add_151 = None
        getitem_1495 = split_tensor_153[0];  split_tensor_153 = None
        _to_copy_930 = torch.ops.aten._to_copy.default(getitem_1495, dtype = torch.float32);  getitem_1495 = None
        native_layer_norm_default_193 = torch.ops.aten.native_layer_norm.default(_to_copy_930, [384], arg470_1, arg471_1, 1e-05);  _to_copy_930 = arg470_1 = arg471_1 = None
        getitem_1496 = native_layer_norm_default_193[0]
        _to_copy_931 = torch.ops.aten._to_copy.default(arg472_1, dtype = torch.bfloat16);  arg472_1 = None
        _to_copy_932 = torch.ops.aten._to_copy.default(getitem_1496, dtype = torch.bfloat16);  getitem_1496 = None
        t_334 = torch.ops.aten.t.default(_to_copy_931);  _to_copy_931 = None
        view_1630 = torch.ops.aten.view.default(_to_copy_932, [512, 384]);  _to_copy_932 = None
        mm_311 = torch.ops.aten.mm.default(view_1630, t_334);  view_1630 = t_334 = None
        view_1631 = torch.ops.aten.view.default(mm_311, [1, 512, 1536]);  mm_311 = None
        split_tensor_154 = torch.ops.aten.split.Tensor(view_1631, 768, dim = -1);  view_1631 = None
        getitem_1499 = split_tensor_154[0]
        getitem_1500 = split_tensor_154[1];  split_tensor_154 = None
        silu_43 = torch.ops.aten.silu.default(getitem_1499);  getitem_1499 = None
        mul_196 = torch.ops.aten.mul.Tensor(silu_43, getitem_1500);  silu_43 = getitem_1500 = None
        _to_copy_933 = torch.ops.aten._to_copy.default(arg473_1, dtype = torch.bfloat16);  arg473_1 = None
        t_335 = torch.ops.aten.t.default(_to_copy_933);  _to_copy_933 = None
        view_1633 = torch.ops.aten.view.default(mul_196, [512, 768]);  mul_196 = None
        mm_312 = torch.ops.aten.mm.default(view_1633, t_335);  view_1633 = t_335 = None
        view_1634 = torch.ops.aten.view.default(mm_312, [1, 512, 384]);  mm_312 = None
        add_159 = torch.ops.aten.add.Tensor(add_158, view_1634);  add_158 = view_1634 = None
        _to_copy_934 = torch.ops.aten._to_copy.default(add_155, dtype = torch.float32)
        native_layer_norm_default_194 = torch.ops.aten.native_layer_norm.default(_to_copy_934, [256], arg486_1, arg487_1, 1e-05);  _to_copy_934 = arg486_1 = arg487_1 = None
        getitem_1501 = native_layer_norm_default_194[0]
        split_with_sizes_default_38 = torch.ops.aten.split_with_sizes.default(arg489_1, [512, 512]);  arg489_1 = None
        getitem_1504 = split_with_sizes_default_38[0]
        getitem_1505 = split_with_sizes_default_38[1];  split_with_sizes_default_38 = None
        split_with_sizes_default_39 = torch.ops.aten.split_with_sizes.default(arg490_1, [512, 512, 256]);  arg490_1 = None
        getitem_1506 = split_with_sizes_default_39[0]
        getitem_1507 = split_with_sizes_default_39[1]
        getitem_1508 = split_with_sizes_default_39[2];  split_with_sizes_default_39 = None
        _to_copy_935 = torch.ops.aten._to_copy.default(getitem_1504, dtype = torch.bfloat16);  getitem_1504 = None
        _to_copy_936 = torch.ops.aten._to_copy.default(getitem_1501, dtype = torch.bfloat16)
        t_336 = torch.ops.aten.t.default(_to_copy_935);  _to_copy_935 = None
        view_1635 = torch.ops.aten.view.default(_to_copy_936, [262144, 256]);  _to_copy_936 = None
        mm_313 = torch.ops.aten.mm.default(view_1635, t_336);  view_1635 = t_336 = None
        view_1636 = torch.ops.aten.view.default(mm_313, [1, 512, 512, 512]);  mm_313 = None
        _to_copy_937 = torch.ops.aten._to_copy.default(getitem_1506, dtype = torch.bfloat16);  getitem_1506 = None
        _to_copy_938 = torch.ops.aten._to_copy.default(getitem_1501, dtype = torch.bfloat16)
        t_337 = torch.ops.aten.t.default(_to_copy_937);  _to_copy_937 = None
        view_1637 = torch.ops.aten.view.default(_to_copy_938, [262144, 256]);  _to_copy_938 = None
        mm_314 = torch.ops.aten.mm.default(view_1637, t_337);  view_1637 = t_337 = None
        view_1638 = torch.ops.aten.view.default(mm_314, [1, 512, 512, 512]);  mm_314 = None
        sigmoid_120 = torch.ops.aten.sigmoid.default(view_1638);  view_1638 = None
        mul_197 = torch.ops.aten.mul.Tensor(view_1636, sigmoid_120);  view_1636 = sigmoid_120 = None
        unsqueeze_590 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_110 = torch.ops.aten.bitwise_not.default(unsqueeze_590);  unsqueeze_590 = None
        masked_fill_110 = torch.ops.aten.masked_fill.Scalar(mul_197, bitwise_not_110, 0);  mul_197 = bitwise_not_110 = None
        split_tensor_155 = torch.ops.aten.split.Tensor(masked_fill_110, 256, dim = -1)
        getitem_1511 = split_tensor_155[0]
        unsqueeze_593 = torch.ops.aten.unsqueeze.default(getitem_1511, 4);  getitem_1511 = None
        permute_927 = torch.ops.aten.permute.default(unsqueeze_593, [0, 1, 4, 3, 2]);  unsqueeze_593 = None
        permute_928 = torch.ops.aten.permute.default(permute_927, [3, 1, 4, 0, 2]);  permute_927 = None
        view_1641 = torch.ops.aten.view.default(permute_928, [256, 512, 512]);  permute_928 = None
        split_tensor_156 = torch.ops.aten.split.Tensor(masked_fill_110, 256, dim = -1);  masked_fill_110 = None
        getitem_1514 = split_tensor_156[1];  split_tensor_156 = None
        unsqueeze_594 = torch.ops.aten.unsqueeze.default(getitem_1514, 4);  getitem_1514 = None
        permute_929 = torch.ops.aten.permute.default(unsqueeze_594, [0, 4, 1, 3, 2]);  unsqueeze_594 = None
        permute_930 = torch.ops.aten.permute.default(permute_929, [3, 4, 0, 2, 1]);  permute_929 = None
        view_1642 = torch.ops.aten.view.default(permute_930, [256, 512, 512]);  permute_930 = None
        bmm_143 = torch.ops.aten.bmm.default(view_1641, view_1642);  view_1641 = view_1642 = None
        view_1643 = torch.ops.aten.view.default(bmm_143, [256, 512, 1, 1, 512]);  bmm_143 = None
        permute_931 = torch.ops.aten.permute.default(view_1643, [3, 1, 4, 0, 2]);  view_1643 = None
        view_1644 = torch.ops.aten.view.default(permute_931, [1, 512, 512, 256]);  permute_931 = None
        _to_copy_939 = torch.ops.aten._to_copy.default(getitem_1505, dtype = torch.bfloat16);  getitem_1505 = None
        _to_copy_940 = torch.ops.aten._to_copy.default(getitem_1501, dtype = torch.bfloat16)
        t_338 = torch.ops.aten.t.default(_to_copy_939);  _to_copy_939 = None
        view_1645 = torch.ops.aten.view.default(_to_copy_940, [262144, 256]);  _to_copy_940 = None
        mm_315 = torch.ops.aten.mm.default(view_1645, t_338);  view_1645 = t_338 = None
        view_1646 = torch.ops.aten.view.default(mm_315, [1, 512, 512, 512]);  mm_315 = None
        _to_copy_941 = torch.ops.aten._to_copy.default(getitem_1507, dtype = torch.bfloat16);  getitem_1507 = None
        _to_copy_942 = torch.ops.aten._to_copy.default(getitem_1501, dtype = torch.bfloat16)
        t_339 = torch.ops.aten.t.default(_to_copy_941);  _to_copy_941 = None
        view_1647 = torch.ops.aten.view.default(_to_copy_942, [262144, 256]);  _to_copy_942 = None
        mm_316 = torch.ops.aten.mm.default(view_1647, t_339);  view_1647 = t_339 = None
        view_1648 = torch.ops.aten.view.default(mm_316, [1, 512, 512, 512]);  mm_316 = None
        sigmoid_121 = torch.ops.aten.sigmoid.default(view_1648);  view_1648 = None
        mul_198 = torch.ops.aten.mul.Tensor(view_1646, sigmoid_121);  view_1646 = sigmoid_121 = None
        view_1649 = torch.ops.aten.view.default(mul_198, [262144, 512]);  mul_198 = None
        view_1650 = torch.ops.aten.view.default(view_1649, [1, 512, 512, 512]);  view_1649 = None
        transpose_38 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_595 = torch.ops.aten.unsqueeze.default(transpose_38, 3);  transpose_38 = None
        clone_160 = torch.ops.aten.clone.default(unsqueeze_595, memory_format = torch.contiguous_format);  unsqueeze_595 = None
        bitwise_not_111 = torch.ops.aten.bitwise_not.default(clone_160);  clone_160 = None
        masked_fill_111 = torch.ops.aten.masked_fill.Scalar(view_1650, bitwise_not_111, 0);  view_1650 = bitwise_not_111 = None
        view_1651 = torch.ops.aten.view.default(masked_fill_111, [262144, 512]);  masked_fill_111 = None
        view_1655 = torch.ops.aten.view.default(view_1651, [1, 512, 512, 512])
        split_tensor_157 = torch.ops.aten.split.Tensor(view_1655, 256, dim = -1);  view_1655 = None
        getitem_1517 = split_tensor_157[0]
        unsqueeze_598 = torch.ops.aten.unsqueeze.default(getitem_1517, 4);  getitem_1517 = None
        permute_936 = torch.ops.aten.permute.default(unsqueeze_598, [0, 2, 4, 3, 1]);  unsqueeze_598 = None
        permute_937 = torch.ops.aten.permute.default(permute_936, [3, 1, 4, 0, 2]);  permute_936 = None
        view_1656 = torch.ops.aten.view.default(permute_937, [256, 512, 512]);  permute_937 = None
        view_1657 = torch.ops.aten.view.default(view_1651, [1, 512, 512, 512]);  view_1651 = None
        split_tensor_158 = torch.ops.aten.split.Tensor(view_1657, 256, dim = -1);  view_1657 = None
        getitem_1520 = split_tensor_158[1];  split_tensor_158 = None
        unsqueeze_599 = torch.ops.aten.unsqueeze.default(getitem_1520, 4);  getitem_1520 = None
        permute_938 = torch.ops.aten.permute.default(unsqueeze_599, [0, 4, 2, 3, 1]);  unsqueeze_599 = None
        permute_939 = torch.ops.aten.permute.default(permute_938, [3, 4, 0, 2, 1]);  permute_938 = None
        view_1658 = torch.ops.aten.view.default(permute_939, [256, 512, 512]);  permute_939 = None
        bmm_144 = torch.ops.aten.bmm.default(view_1656, view_1658);  view_1656 = view_1658 = None
        view_1659 = torch.ops.aten.view.default(bmm_144, [256, 512, 1, 1, 512]);  bmm_144 = None
        permute_940 = torch.ops.aten.permute.default(view_1659, [3, 1, 4, 0, 2]);  view_1659 = None
        view_1660 = torch.ops.aten.view.default(permute_940, [1, 512, 512, 256]);  permute_940 = None
        _to_copy_943 = torch.ops.aten._to_copy.default(view_1644, dtype = torch.float32);  view_1644 = None
        native_layer_norm_default_195 = torch.ops.aten.native_layer_norm.default(_to_copy_943, [256], None, None, 1e-05);  _to_copy_943 = None
        getitem_1521 = native_layer_norm_default_195[0]
        _to_copy_944 = torch.ops.aten._to_copy.default(view_1660, dtype = torch.float32);  view_1660 = None
        native_layer_norm_default_196 = torch.ops.aten.native_layer_norm.default(_to_copy_944, [256], None, None, 1e-05);  _to_copy_944 = None
        getitem_1524 = native_layer_norm_default_196[0]
        add_160 = torch.ops.aten.add.Tensor(getitem_1521, getitem_1524);  getitem_1521 = getitem_1524 = None
        _to_copy_945 = torch.ops.aten._to_copy.default(arg488_1, dtype = torch.bfloat16);  arg488_1 = None
        _to_copy_946 = torch.ops.aten._to_copy.default(add_160, dtype = torch.bfloat16);  add_160 = None
        t_340 = torch.ops.aten.t.default(_to_copy_945);  _to_copy_945 = None
        view_1661 = torch.ops.aten.view.default(_to_copy_946, [262144, 256]);  _to_copy_946 = None
        mm_317 = torch.ops.aten.mm.default(view_1661, t_340);  view_1661 = t_340 = None
        view_1662 = torch.ops.aten.view.default(mm_317, [1, 512, 512, 256]);  mm_317 = None
        _to_copy_947 = torch.ops.aten._to_copy.default(getitem_1508, dtype = torch.bfloat16);  getitem_1508 = None
        _to_copy_948 = torch.ops.aten._to_copy.default(getitem_1501, dtype = torch.bfloat16);  getitem_1501 = None
        t_341 = torch.ops.aten.t.default(_to_copy_947);  _to_copy_947 = None
        view_1663 = torch.ops.aten.view.default(_to_copy_948, [262144, 256]);  _to_copy_948 = None
        mm_318 = torch.ops.aten.mm.default(view_1663, t_341);  view_1663 = t_341 = None
        view_1664 = torch.ops.aten.view.default(mm_318, [1, 512, 512, 256]);  mm_318 = None
        sigmoid_122 = torch.ops.aten.sigmoid.default(view_1664);  view_1664 = None
        mul_199 = torch.ops.aten.mul.Tensor(view_1662, sigmoid_122);  view_1662 = sigmoid_122 = None
        add_161 = torch.ops.aten.add.Tensor(add_155, mul_199);  mul_199 = None
        _to_copy_949 = torch.ops.aten._to_copy.default(add_155, dtype = torch.float32)
        native_layer_norm_default_197 = torch.ops.aten.native_layer_norm.default(_to_copy_949, [256], None, None, 1e-05);  _to_copy_949 = None
        getitem_1527 = native_layer_norm_default_197[0]
        _to_copy_950 = torch.ops.aten._to_copy.default(arg492_1, dtype = torch.bfloat16);  arg492_1 = None
        _to_copy_951 = torch.ops.aten._to_copy.default(getitem_1527, dtype = torch.bfloat16)
        t_342 = torch.ops.aten.t.default(_to_copy_950);  _to_copy_950 = None
        view_1665 = torch.ops.aten.view.default(_to_copy_951, [262144, 256]);  _to_copy_951 = None
        mm_319 = torch.ops.aten.mm.default(view_1665, t_342);  view_1665 = t_342 = None
        view_1666 = torch.ops.aten.view.default(mm_319, [1, 512, 512, 8]);  mm_319 = None
        view_1667 = torch.ops.aten.view.default(view_1666, [1, 512, 512, 2, 4]);  view_1666 = None
        permute_941 = torch.ops.aten.permute.default(view_1667, [0, 3, 4, 1, 2]);  view_1667 = None
        view_1668 = torch.ops.aten.view.default(permute_941, [1, 2, 4, 1, 512, 512]);  permute_941 = None
        view_1669 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_112 = torch.ops.aten.bitwise_not.default(view_1669);  view_1669 = None
        masked_fill_112 = torch.ops.aten.masked_fill.Scalar(view_1668, bitwise_not_112, -10000);  view_1668 = bitwise_not_112 = None
        view_1670 = torch.ops.aten.view.default(masked_fill_112, [1, 2, 4, 512, 512]);  masked_fill_112 = None
        permute_942 = torch.ops.aten.permute.default(view_1670, [1, 0, 2, 3, 4]);  view_1670 = None
        view_1671 = torch.ops.aten.view.default(permute_942, [2, 4, 1, 512, 512]);  permute_942 = None
        _to_copy_952 = torch.ops.aten._to_copy.default(arg493_1, dtype = torch.bfloat16);  arg493_1 = None
        _to_copy_953 = torch.ops.aten._to_copy.default(getitem_1527, dtype = torch.bfloat16)
        t_343 = torch.ops.aten.t.default(_to_copy_952);  _to_copy_952 = None
        view_1672 = torch.ops.aten.view.default(_to_copy_953, [262144, 256]);  _to_copy_953 = None
        mm_320 = torch.ops.aten.mm.default(view_1672, t_343);  view_1672 = t_343 = None
        view_1673 = torch.ops.aten.view.default(mm_320, [1, 512, 512, 1024]);  mm_320 = None
        select_39 = torch.ops.aten.select.int(view_1671, 0, 0)
        view_1674 = torch.ops.aten.view.default(view_1673, [1, 512, 512, 4, 4, 64]);  view_1673 = None
        permute_943 = torch.ops.aten.permute.default(view_1674, [4, 0, 3, 1, 2, 5]);  view_1674 = None
        view_1675 = torch.ops.aten.view.default(permute_943, [4, 4, 512, 512, 64]);  permute_943 = None
        unbind_int_79 = torch.ops.aten.unbind.int(view_1675);  view_1675 = None
        getitem_1530 = unbind_int_79[0]
        getitem_1531 = unbind_int_79[1]
        getitem_1532 = unbind_int_79[2]
        getitem_1533 = unbind_int_79[3];  unbind_int_79 = None
        expand_92 = torch.ops.aten.expand.default(select_39, [4, 512, 512, 512]);  select_39 = None
        _scaled_dot_product_efficient_attention_default_51 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1530, getitem_1531, getitem_1532, expand_92, False);  getitem_1530 = getitem_1531 = getitem_1532 = expand_92 = None
        getitem_1534 = _scaled_dot_product_efficient_attention_default_51[0]
        sigmoid_123 = torch.ops.aten.sigmoid.default(getitem_1533);  getitem_1533 = None
        mul_200 = torch.ops.aten.mul.Tensor(getitem_1534, sigmoid_123);  getitem_1534 = sigmoid_123 = None
        view_1676 = torch.ops.aten.view.default(mul_200, [1, 4, 512, 512, 64]);  mul_200 = None
        permute_944 = torch.ops.aten.permute.default(view_1676, [0, 2, 3, 1, 4]);  view_1676 = None
        clone_161 = torch.ops.aten.clone.default(permute_944, memory_format = torch.contiguous_format);  permute_944 = None
        _unsafe_view_140 = torch.ops.aten._unsafe_view.default(clone_161, [1, 512, 512, 256]);  clone_161 = None
        transpose_39 = torch.ops.aten.transpose.int(getitem_1527, 1, 2);  getitem_1527 = None
        _to_copy_954 = torch.ops.aten._to_copy.default(arg494_1, dtype = torch.bfloat16);  arg494_1 = None
        _to_copy_955 = torch.ops.aten._to_copy.default(transpose_39, dtype = torch.bfloat16);  transpose_39 = None
        t_344 = torch.ops.aten.t.default(_to_copy_954);  _to_copy_954 = None
        expand_93 = torch.ops.aten.expand.default(_to_copy_955, [1, 512, 512, 256]);  _to_copy_955 = None
        view_1677 = torch.ops.aten.view.default(expand_93, [512, 512, 256]);  expand_93 = None
        expand_94 = torch.ops.aten.expand.default(t_344, [1, 512, 256, 1024]);  t_344 = None
        view_1678 = torch.ops.aten.view.default(expand_94, [512, 256, 1024]);  expand_94 = None
        bmm_145 = torch.ops.aten.bmm.default(view_1677, view_1678);  view_1677 = view_1678 = None
        view_1679 = torch.ops.aten.view.default(bmm_145, [1, 512, 512, 1024]);  bmm_145 = None
        select_40 = torch.ops.aten.select.int(view_1671, 0, 1);  view_1671 = None
        view_1680 = torch.ops.aten.view.default(view_1679, [1, 512, 512, 4, 4, 64]);  view_1679 = None
        permute_945 = torch.ops.aten.permute.default(view_1680, [4, 0, 3, 1, 2, 5]);  view_1680 = None
        view_1681 = torch.ops.aten.view.default(permute_945, [4, 4, 512, 512, 64]);  permute_945 = None
        unbind_int_80 = torch.ops.aten.unbind.int(view_1681);  view_1681 = None
        getitem_1538 = unbind_int_80[0]
        getitem_1539 = unbind_int_80[1]
        getitem_1540 = unbind_int_80[2]
        getitem_1541 = unbind_int_80[3];  unbind_int_80 = None
        expand_95 = torch.ops.aten.expand.default(select_40, [4, 512, 512, 512]);  select_40 = None
        _scaled_dot_product_efficient_attention_default_52 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1538, getitem_1539, getitem_1540, expand_95, False);  getitem_1538 = getitem_1539 = getitem_1540 = expand_95 = None
        getitem_1542 = _scaled_dot_product_efficient_attention_default_52[0]
        sigmoid_124 = torch.ops.aten.sigmoid.default(getitem_1541);  getitem_1541 = None
        mul_201 = torch.ops.aten.mul.Tensor(getitem_1542, sigmoid_124);  getitem_1542 = sigmoid_124 = None
        view_1682 = torch.ops.aten.view.default(mul_201, [1, 4, 512, 512, 64]);  mul_201 = None
        permute_946 = torch.ops.aten.permute.default(view_1682, [0, 2, 3, 1, 4]);  view_1682 = None
        clone_162 = torch.ops.aten.clone.default(permute_946, memory_format = torch.contiguous_format);  permute_946 = None
        _unsafe_view_141 = torch.ops.aten._unsafe_view.default(clone_162, [1, 512, 512, 256]);  clone_162 = None
        cat_25 = torch.ops.aten.cat.default([_unsafe_view_140, _unsafe_view_141], dim = -1);  _unsafe_view_140 = _unsafe_view_141 = None
        slice_218 = torch.ops.aten.slice.Tensor(arg491_1, dim = 0, start = 0, end = 9223372036854775807);  arg491_1 = None
        unsqueeze_600 = torch.ops.aten.unsqueeze.default(slice_218, 1);  slice_218 = None
        mul_202 = torch.ops.aten.mul.Tensor(arg495_1, unsqueeze_600);  arg495_1 = unsqueeze_600 = None
        _to_copy_956 = torch.ops.aten._to_copy.default(mul_202, dtype = torch.bfloat16);  mul_202 = None
        t_345 = torch.ops.aten.t.default(_to_copy_956);  _to_copy_956 = None
        view_1683 = torch.ops.aten.view.default(cat_25, [262144, 512]);  cat_25 = None
        mm_321 = torch.ops.aten.mm.default(view_1683, t_345);  view_1683 = t_345 = None
        view_1684 = torch.ops.aten.view.default(mm_321, [1, 512, 512, 256]);  mm_321 = None
        add_162 = torch.ops.aten.add.Tensor(add_161, view_1684);  add_161 = view_1684 = None
        split_tensor_159 = torch.ops.aten.split.Tensor(add_155, 512, dim = -2)
        getitem_1546 = split_tensor_159[0];  split_tensor_159 = None
        _to_copy_957 = torch.ops.aten._to_copy.default(getitem_1546, dtype = torch.float32);  getitem_1546 = None
        native_layer_norm_default_198 = torch.ops.aten.native_layer_norm.default(_to_copy_957, [256], arg482_1, arg483_1, 1e-05);  _to_copy_957 = arg482_1 = arg483_1 = None
        getitem_1547 = native_layer_norm_default_198[0]
        _to_copy_958 = torch.ops.aten._to_copy.default(arg484_1, dtype = torch.bfloat16);  arg484_1 = None
        _to_copy_959 = torch.ops.aten._to_copy.default(getitem_1547, dtype = torch.bfloat16);  getitem_1547 = None
        t_346 = torch.ops.aten.t.default(_to_copy_958);  _to_copy_958 = None
        view_1685 = torch.ops.aten.view.default(_to_copy_959, [262144, 256]);  _to_copy_959 = None
        mm_322 = torch.ops.aten.mm.default(view_1685, t_346);  view_1685 = t_346 = None
        view_1686 = torch.ops.aten.view.default(mm_322, [1, 512, 512, 1024]);  mm_322 = None
        split_tensor_160 = torch.ops.aten.split.Tensor(view_1686, 512, dim = -1);  view_1686 = None
        getitem_1550 = split_tensor_160[0]
        getitem_1551 = split_tensor_160[1];  split_tensor_160 = None
        silu_44 = torch.ops.aten.silu.default(getitem_1550);  getitem_1550 = None
        mul_203 = torch.ops.aten.mul.Tensor(silu_44, getitem_1551);  silu_44 = getitem_1551 = None
        _to_copy_960 = torch.ops.aten._to_copy.default(arg485_1, dtype = torch.bfloat16);  arg485_1 = None
        t_347 = torch.ops.aten.t.default(_to_copy_960);  _to_copy_960 = None
        view_1688 = torch.ops.aten.view.default(mul_203, [262144, 512]);  mul_203 = None
        mm_323 = torch.ops.aten.mm.default(view_1688, t_347);  view_1688 = t_347 = None
        view_1689 = torch.ops.aten.view.default(mm_323, [1, 512, 512, 256]);  mm_323 = None
        add_163 = torch.ops.aten.add.Tensor(add_162, view_1689);  add_162 = view_1689 = None
        _to_copy_961 = torch.ops.aten._to_copy.default(add_159, dtype = torch.float32)
        native_layer_norm_default_199 = torch.ops.aten.native_layer_norm.default(_to_copy_961, [384], arg500_1, arg501_1, 1e-05);  _to_copy_961 = arg500_1 = arg501_1 = None
        getitem_1552 = native_layer_norm_default_199[0]
        _to_copy_962 = torch.ops.aten._to_copy.default(add_155, dtype = torch.float32);  add_155 = None
        native_layer_norm_default_200 = torch.ops.aten.native_layer_norm.default(_to_copy_962, [256], arg502_1, arg503_1, 1e-05);  _to_copy_962 = arg502_1 = arg503_1 = None
        getitem_1555 = native_layer_norm_default_200[0]
        _to_copy_963 = torch.ops.aten._to_copy.default(arg504_1, dtype = torch.bfloat16);  arg504_1 = None
        _to_copy_964 = torch.ops.aten._to_copy.default(getitem_1555, dtype = torch.bfloat16);  getitem_1555 = None
        t_348 = torch.ops.aten.t.default(_to_copy_963);  _to_copy_963 = None
        view_1690 = torch.ops.aten.view.default(_to_copy_964, [262144, 256]);  _to_copy_964 = None
        mm_324 = torch.ops.aten.mm.default(view_1690, t_348);  view_1690 = t_348 = None
        view_1691 = torch.ops.aten.view.default(mm_324, [1, 512, 512, 16]);  mm_324 = None
        permute_947 = torch.ops.aten.permute.default(view_1691, [0, 3, 1, 2]);  view_1691 = None
        view_1692 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_113 = torch.ops.aten.bitwise_not.default(view_1692);  view_1692 = None
        masked_fill_113 = torch.ops.aten.masked_fill.Scalar(permute_947, bitwise_not_113, -10000);  permute_947 = bitwise_not_113 = None
        _to_copy_965 = torch.ops.aten._to_copy.default(getitem_1552, dtype = torch.bfloat16);  getitem_1552 = None
        _to_copy_966 = torch.ops.aten._to_copy.default(arg506_1, dtype = torch.bfloat16);  arg506_1 = None
        unsqueeze_601 = torch.ops.aten.unsqueeze.default(_to_copy_965, 3);  _to_copy_965 = None
        unsqueeze_602 = torch.ops.aten.unsqueeze.default(unsqueeze_601, 4);  unsqueeze_601 = None
        unsqueeze_603 = torch.ops.aten.unsqueeze.default(unsqueeze_602, 5);  unsqueeze_602 = None
        permute_948 = torch.ops.aten.permute.default(unsqueeze_603, [3, 0, 4, 1, 5, 2]);  unsqueeze_603 = None
        unsqueeze_604 = torch.ops.aten.unsqueeze.default(_to_copy_966, 4);  _to_copy_966 = None
        unsqueeze_605 = torch.ops.aten.unsqueeze.default(unsqueeze_604, 5);  unsqueeze_604 = None
        permute_949 = torch.ops.aten.permute.default(unsqueeze_605, [1, 4, 2, 5, 3, 0]);  unsqueeze_605 = None
        permute_950 = torch.ops.aten.permute.default(permute_948, [3, 5, 0, 1, 2, 4]);  permute_948 = None
        view_1693 = torch.ops.aten.view.default(permute_950, [1, 512, 384]);  permute_950 = None
        permute_951 = torch.ops.aten.permute.default(permute_949, [5, 0, 1, 2, 4, 3]);  permute_949 = None
        view_1694 = torch.ops.aten.view.default(permute_951, [1, 384, 1536]);  permute_951 = None
        bmm_146 = torch.ops.aten.bmm.default(view_1693, view_1694);  view_1693 = view_1694 = None
        view_1695 = torch.ops.aten.view.default(bmm_146, [512, 1, 4, 1, 16, 24]);  bmm_146 = None
        permute_952 = torch.ops.aten.permute.default(view_1695, [2, 3, 4, 0, 5, 1]);  view_1695 = None
        view_1696 = torch.ops.aten.view.default(permute_952, [4, 1, 16, 512, 24]);  permute_952 = None
        unbind_int_81 = torch.ops.aten.unbind.int(view_1696);  view_1696 = None
        getitem_1558 = unbind_int_81[0]
        getitem_1559 = unbind_int_81[1]
        getitem_1560 = unbind_int_81[2]
        getitem_1561 = unbind_int_81[3];  unbind_int_81 = None
        view_1697 = torch.ops.aten.view.default(arg505_1, [1, 16, 1, 24]);  arg505_1 = None
        add_164 = torch.ops.aten.add.Tensor(getitem_1558, view_1697);  getitem_1558 = view_1697 = None
        _to_copy_967 = torch.ops.aten._to_copy.default(add_164, dtype = torch.bfloat16);  add_164 = None
        expand_96 = torch.ops.aten.expand.default(masked_fill_113, [1, 16, 512, 512]);  masked_fill_113 = None
        _scaled_dot_product_efficient_attention_default_53 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_967, getitem_1559, getitem_1560, expand_96, False);  _to_copy_967 = getitem_1559 = getitem_1560 = expand_96 = None
        getitem_1562 = _scaled_dot_product_efficient_attention_default_53[0]
        add_165 = torch.ops.aten.add.Tensor(getitem_1561, 1);  getitem_1561 = None
        sigmoid_125 = torch.ops.aten.sigmoid.default(add_165);  add_165 = None
        mul_204 = torch.ops.aten.mul.Tensor(getitem_1562, sigmoid_125);  getitem_1562 = sigmoid_125 = None
        _to_copy_968 = torch.ops.aten._to_copy.default(arg507_1, dtype = torch.bfloat16);  arg507_1 = None
        unsqueeze_606 = torch.ops.aten.unsqueeze.default(mul_204, 4);  mul_204 = None
        permute_953 = torch.ops.aten.permute.default(unsqueeze_606, [0, 2, 4, 3, 1]);  unsqueeze_606 = None
        unsqueeze_607 = torch.ops.aten.unsqueeze.default(_to_copy_968, 3);  _to_copy_968 = None
        unsqueeze_608 = torch.ops.aten.unsqueeze.default(unsqueeze_607, 4);  unsqueeze_607 = None
        permute_954 = torch.ops.aten.permute.default(unsqueeze_608, [3, 4, 2, 1, 0]);  unsqueeze_608 = None
        permute_955 = torch.ops.aten.permute.default(permute_953, [1, 3, 4, 0, 2]);  permute_953 = None
        clone_163 = torch.ops.aten.clone.default(permute_955, memory_format = torch.contiguous_format);  permute_955 = None
        _unsafe_view_142 = torch.ops.aten._unsafe_view.default(clone_163, [1, 512, 384]);  clone_163 = None
        permute_956 = torch.ops.aten.permute.default(permute_954, [3, 4, 0, 2, 1]);  permute_954 = None
        clone_164 = torch.ops.aten.clone.default(permute_956, memory_format = torch.contiguous_format);  permute_956 = None
        _unsafe_view_143 = torch.ops.aten._unsafe_view.default(clone_164, [1, 384, 384]);  clone_164 = None
        bmm_147 = torch.ops.aten.bmm.default(_unsafe_view_142, _unsafe_view_143);  _unsafe_view_142 = _unsafe_view_143 = None
        view_1698 = torch.ops.aten.view.default(bmm_147, [512, 1, 1, 1, 384]);  bmm_147 = None
        permute_957 = torch.ops.aten.permute.default(view_1698, [3, 0, 4, 1, 2]);  view_1698 = None
        view_1699 = torch.ops.aten.view.default(permute_957, [1, 512, 384]);  permute_957 = None
        unsqueeze_609 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_205 = torch.ops.aten.mul.Tensor(view_1699, unsqueeze_609);  view_1699 = unsqueeze_609 = None
        add_166 = torch.ops.aten.add.Tensor(add_159, mul_205);  mul_205 = None
        split_tensor_161 = torch.ops.aten.split.Tensor(add_159, 512, dim = -2);  add_159 = None
        getitem_1566 = split_tensor_161[0];  split_tensor_161 = None
        _to_copy_969 = torch.ops.aten._to_copy.default(getitem_1566, dtype = torch.float32);  getitem_1566 = None
        native_layer_norm_default_201 = torch.ops.aten.native_layer_norm.default(_to_copy_969, [384], arg496_1, arg497_1, 1e-05);  _to_copy_969 = arg496_1 = arg497_1 = None
        getitem_1567 = native_layer_norm_default_201[0]
        _to_copy_970 = torch.ops.aten._to_copy.default(arg498_1, dtype = torch.bfloat16);  arg498_1 = None
        _to_copy_971 = torch.ops.aten._to_copy.default(getitem_1567, dtype = torch.bfloat16);  getitem_1567 = None
        t_349 = torch.ops.aten.t.default(_to_copy_970);  _to_copy_970 = None
        view_1700 = torch.ops.aten.view.default(_to_copy_971, [512, 384]);  _to_copy_971 = None
        mm_325 = torch.ops.aten.mm.default(view_1700, t_349);  view_1700 = t_349 = None
        view_1701 = torch.ops.aten.view.default(mm_325, [1, 512, 1536]);  mm_325 = None
        split_tensor_162 = torch.ops.aten.split.Tensor(view_1701, 768, dim = -1);  view_1701 = None
        getitem_1570 = split_tensor_162[0]
        getitem_1571 = split_tensor_162[1];  split_tensor_162 = None
        silu_45 = torch.ops.aten.silu.default(getitem_1570);  getitem_1570 = None
        mul_206 = torch.ops.aten.mul.Tensor(silu_45, getitem_1571);  silu_45 = getitem_1571 = None
        _to_copy_972 = torch.ops.aten._to_copy.default(arg499_1, dtype = torch.bfloat16);  arg499_1 = None
        t_350 = torch.ops.aten.t.default(_to_copy_972);  _to_copy_972 = None
        view_1703 = torch.ops.aten.view.default(mul_206, [512, 768]);  mul_206 = None
        mm_326 = torch.ops.aten.mm.default(view_1703, t_350);  view_1703 = t_350 = None
        view_1704 = torch.ops.aten.view.default(mm_326, [1, 512, 384]);  mm_326 = None
        add_167 = torch.ops.aten.add.Tensor(add_166, view_1704);  add_166 = view_1704 = None
        _to_copy_973 = torch.ops.aten._to_copy.default(add_163, dtype = torch.float32)
        native_layer_norm_default_202 = torch.ops.aten.native_layer_norm.default(_to_copy_973, [256], arg512_1, arg513_1, 1e-05);  _to_copy_973 = arg512_1 = arg513_1 = None
        getitem_1572 = native_layer_norm_default_202[0]
        split_with_sizes_default_40 = torch.ops.aten.split_with_sizes.default(arg515_1, [512, 512]);  arg515_1 = None
        getitem_1575 = split_with_sizes_default_40[0]
        getitem_1576 = split_with_sizes_default_40[1];  split_with_sizes_default_40 = None
        split_with_sizes_default_41 = torch.ops.aten.split_with_sizes.default(arg516_1, [512, 512, 256]);  arg516_1 = None
        getitem_1577 = split_with_sizes_default_41[0]
        getitem_1578 = split_with_sizes_default_41[1]
        getitem_1579 = split_with_sizes_default_41[2];  split_with_sizes_default_41 = None
        _to_copy_974 = torch.ops.aten._to_copy.default(getitem_1575, dtype = torch.bfloat16);  getitem_1575 = None
        _to_copy_975 = torch.ops.aten._to_copy.default(getitem_1572, dtype = torch.bfloat16)
        t_351 = torch.ops.aten.t.default(_to_copy_974);  _to_copy_974 = None
        view_1705 = torch.ops.aten.view.default(_to_copy_975, [262144, 256]);  _to_copy_975 = None
        mm_327 = torch.ops.aten.mm.default(view_1705, t_351);  view_1705 = t_351 = None
        view_1706 = torch.ops.aten.view.default(mm_327, [1, 512, 512, 512]);  mm_327 = None
        _to_copy_976 = torch.ops.aten._to_copy.default(getitem_1577, dtype = torch.bfloat16);  getitem_1577 = None
        _to_copy_977 = torch.ops.aten._to_copy.default(getitem_1572, dtype = torch.bfloat16)
        t_352 = torch.ops.aten.t.default(_to_copy_976);  _to_copy_976 = None
        view_1707 = torch.ops.aten.view.default(_to_copy_977, [262144, 256]);  _to_copy_977 = None
        mm_328 = torch.ops.aten.mm.default(view_1707, t_352);  view_1707 = t_352 = None
        view_1708 = torch.ops.aten.view.default(mm_328, [1, 512, 512, 512]);  mm_328 = None
        sigmoid_126 = torch.ops.aten.sigmoid.default(view_1708);  view_1708 = None
        mul_207 = torch.ops.aten.mul.Tensor(view_1706, sigmoid_126);  view_1706 = sigmoid_126 = None
        unsqueeze_610 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_114 = torch.ops.aten.bitwise_not.default(unsqueeze_610);  unsqueeze_610 = None
        masked_fill_114 = torch.ops.aten.masked_fill.Scalar(mul_207, bitwise_not_114, 0);  mul_207 = bitwise_not_114 = None
        split_tensor_163 = torch.ops.aten.split.Tensor(masked_fill_114, 256, dim = -1)
        getitem_1582 = split_tensor_163[0]
        unsqueeze_613 = torch.ops.aten.unsqueeze.default(getitem_1582, 4);  getitem_1582 = None
        permute_962 = torch.ops.aten.permute.default(unsqueeze_613, [0, 1, 4, 3, 2]);  unsqueeze_613 = None
        permute_963 = torch.ops.aten.permute.default(permute_962, [3, 1, 4, 0, 2]);  permute_962 = None
        view_1711 = torch.ops.aten.view.default(permute_963, [256, 512, 512]);  permute_963 = None
        split_tensor_164 = torch.ops.aten.split.Tensor(masked_fill_114, 256, dim = -1);  masked_fill_114 = None
        getitem_1585 = split_tensor_164[1];  split_tensor_164 = None
        unsqueeze_614 = torch.ops.aten.unsqueeze.default(getitem_1585, 4);  getitem_1585 = None
        permute_964 = torch.ops.aten.permute.default(unsqueeze_614, [0, 4, 1, 3, 2]);  unsqueeze_614 = None
        permute_965 = torch.ops.aten.permute.default(permute_964, [3, 4, 0, 2, 1]);  permute_964 = None
        view_1712 = torch.ops.aten.view.default(permute_965, [256, 512, 512]);  permute_965 = None
        bmm_148 = torch.ops.aten.bmm.default(view_1711, view_1712);  view_1711 = view_1712 = None
        view_1713 = torch.ops.aten.view.default(bmm_148, [256, 512, 1, 1, 512]);  bmm_148 = None
        permute_966 = torch.ops.aten.permute.default(view_1713, [3, 1, 4, 0, 2]);  view_1713 = None
        view_1714 = torch.ops.aten.view.default(permute_966, [1, 512, 512, 256]);  permute_966 = None
        _to_copy_978 = torch.ops.aten._to_copy.default(getitem_1576, dtype = torch.bfloat16);  getitem_1576 = None
        _to_copy_979 = torch.ops.aten._to_copy.default(getitem_1572, dtype = torch.bfloat16)
        t_353 = torch.ops.aten.t.default(_to_copy_978);  _to_copy_978 = None
        view_1715 = torch.ops.aten.view.default(_to_copy_979, [262144, 256]);  _to_copy_979 = None
        mm_329 = torch.ops.aten.mm.default(view_1715, t_353);  view_1715 = t_353 = None
        view_1716 = torch.ops.aten.view.default(mm_329, [1, 512, 512, 512]);  mm_329 = None
        _to_copy_980 = torch.ops.aten._to_copy.default(getitem_1578, dtype = torch.bfloat16);  getitem_1578 = None
        _to_copy_981 = torch.ops.aten._to_copy.default(getitem_1572, dtype = torch.bfloat16)
        t_354 = torch.ops.aten.t.default(_to_copy_980);  _to_copy_980 = None
        view_1717 = torch.ops.aten.view.default(_to_copy_981, [262144, 256]);  _to_copy_981 = None
        mm_330 = torch.ops.aten.mm.default(view_1717, t_354);  view_1717 = t_354 = None
        view_1718 = torch.ops.aten.view.default(mm_330, [1, 512, 512, 512]);  mm_330 = None
        sigmoid_127 = torch.ops.aten.sigmoid.default(view_1718);  view_1718 = None
        mul_208 = torch.ops.aten.mul.Tensor(view_1716, sigmoid_127);  view_1716 = sigmoid_127 = None
        view_1719 = torch.ops.aten.view.default(mul_208, [262144, 512]);  mul_208 = None
        view_1720 = torch.ops.aten.view.default(view_1719, [1, 512, 512, 512]);  view_1719 = None
        transpose_40 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_615 = torch.ops.aten.unsqueeze.default(transpose_40, 3);  transpose_40 = None
        clone_165 = torch.ops.aten.clone.default(unsqueeze_615, memory_format = torch.contiguous_format);  unsqueeze_615 = None
        bitwise_not_115 = torch.ops.aten.bitwise_not.default(clone_165);  clone_165 = None
        masked_fill_115 = torch.ops.aten.masked_fill.Scalar(view_1720, bitwise_not_115, 0);  view_1720 = bitwise_not_115 = None
        view_1721 = torch.ops.aten.view.default(masked_fill_115, [262144, 512]);  masked_fill_115 = None
        view_1725 = torch.ops.aten.view.default(view_1721, [1, 512, 512, 512])
        split_tensor_165 = torch.ops.aten.split.Tensor(view_1725, 256, dim = -1);  view_1725 = None
        getitem_1588 = split_tensor_165[0]
        unsqueeze_618 = torch.ops.aten.unsqueeze.default(getitem_1588, 4);  getitem_1588 = None
        permute_971 = torch.ops.aten.permute.default(unsqueeze_618, [0, 2, 4, 3, 1]);  unsqueeze_618 = None
        permute_972 = torch.ops.aten.permute.default(permute_971, [3, 1, 4, 0, 2]);  permute_971 = None
        view_1726 = torch.ops.aten.view.default(permute_972, [256, 512, 512]);  permute_972 = None
        view_1727 = torch.ops.aten.view.default(view_1721, [1, 512, 512, 512]);  view_1721 = None
        split_tensor_166 = torch.ops.aten.split.Tensor(view_1727, 256, dim = -1);  view_1727 = None
        getitem_1591 = split_tensor_166[1];  split_tensor_166 = None
        unsqueeze_619 = torch.ops.aten.unsqueeze.default(getitem_1591, 4);  getitem_1591 = None
        permute_973 = torch.ops.aten.permute.default(unsqueeze_619, [0, 4, 2, 3, 1]);  unsqueeze_619 = None
        permute_974 = torch.ops.aten.permute.default(permute_973, [3, 4, 0, 2, 1]);  permute_973 = None
        view_1728 = torch.ops.aten.view.default(permute_974, [256, 512, 512]);  permute_974 = None
        bmm_149 = torch.ops.aten.bmm.default(view_1726, view_1728);  view_1726 = view_1728 = None
        view_1729 = torch.ops.aten.view.default(bmm_149, [256, 512, 1, 1, 512]);  bmm_149 = None
        permute_975 = torch.ops.aten.permute.default(view_1729, [3, 1, 4, 0, 2]);  view_1729 = None
        view_1730 = torch.ops.aten.view.default(permute_975, [1, 512, 512, 256]);  permute_975 = None
        _to_copy_982 = torch.ops.aten._to_copy.default(view_1714, dtype = torch.float32);  view_1714 = None
        native_layer_norm_default_203 = torch.ops.aten.native_layer_norm.default(_to_copy_982, [256], None, None, 1e-05);  _to_copy_982 = None
        getitem_1592 = native_layer_norm_default_203[0]
        _to_copy_983 = torch.ops.aten._to_copy.default(view_1730, dtype = torch.float32);  view_1730 = None
        native_layer_norm_default_204 = torch.ops.aten.native_layer_norm.default(_to_copy_983, [256], None, None, 1e-05);  _to_copy_983 = None
        getitem_1595 = native_layer_norm_default_204[0]
        add_168 = torch.ops.aten.add.Tensor(getitem_1592, getitem_1595);  getitem_1592 = getitem_1595 = None
        _to_copy_984 = torch.ops.aten._to_copy.default(arg514_1, dtype = torch.bfloat16);  arg514_1 = None
        _to_copy_985 = torch.ops.aten._to_copy.default(add_168, dtype = torch.bfloat16);  add_168 = None
        t_355 = torch.ops.aten.t.default(_to_copy_984);  _to_copy_984 = None
        view_1731 = torch.ops.aten.view.default(_to_copy_985, [262144, 256]);  _to_copy_985 = None
        mm_331 = torch.ops.aten.mm.default(view_1731, t_355);  view_1731 = t_355 = None
        view_1732 = torch.ops.aten.view.default(mm_331, [1, 512, 512, 256]);  mm_331 = None
        _to_copy_986 = torch.ops.aten._to_copy.default(getitem_1579, dtype = torch.bfloat16);  getitem_1579 = None
        _to_copy_987 = torch.ops.aten._to_copy.default(getitem_1572, dtype = torch.bfloat16);  getitem_1572 = None
        t_356 = torch.ops.aten.t.default(_to_copy_986);  _to_copy_986 = None
        view_1733 = torch.ops.aten.view.default(_to_copy_987, [262144, 256]);  _to_copy_987 = None
        mm_332 = torch.ops.aten.mm.default(view_1733, t_356);  view_1733 = t_356 = None
        view_1734 = torch.ops.aten.view.default(mm_332, [1, 512, 512, 256]);  mm_332 = None
        sigmoid_128 = torch.ops.aten.sigmoid.default(view_1734);  view_1734 = None
        mul_209 = torch.ops.aten.mul.Tensor(view_1732, sigmoid_128);  view_1732 = sigmoid_128 = None
        add_169 = torch.ops.aten.add.Tensor(add_163, mul_209);  mul_209 = None
        _to_copy_988 = torch.ops.aten._to_copy.default(add_163, dtype = torch.float32)
        native_layer_norm_default_205 = torch.ops.aten.native_layer_norm.default(_to_copy_988, [256], None, None, 1e-05);  _to_copy_988 = None
        getitem_1598 = native_layer_norm_default_205[0]
        _to_copy_989 = torch.ops.aten._to_copy.default(arg518_1, dtype = torch.bfloat16);  arg518_1 = None
        _to_copy_990 = torch.ops.aten._to_copy.default(getitem_1598, dtype = torch.bfloat16)
        t_357 = torch.ops.aten.t.default(_to_copy_989);  _to_copy_989 = None
        view_1735 = torch.ops.aten.view.default(_to_copy_990, [262144, 256]);  _to_copy_990 = None
        mm_333 = torch.ops.aten.mm.default(view_1735, t_357);  view_1735 = t_357 = None
        view_1736 = torch.ops.aten.view.default(mm_333, [1, 512, 512, 8]);  mm_333 = None
        view_1737 = torch.ops.aten.view.default(view_1736, [1, 512, 512, 2, 4]);  view_1736 = None
        permute_976 = torch.ops.aten.permute.default(view_1737, [0, 3, 4, 1, 2]);  view_1737 = None
        view_1738 = torch.ops.aten.view.default(permute_976, [1, 2, 4, 1, 512, 512]);  permute_976 = None
        view_1739 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_116 = torch.ops.aten.bitwise_not.default(view_1739);  view_1739 = None
        masked_fill_116 = torch.ops.aten.masked_fill.Scalar(view_1738, bitwise_not_116, -10000);  view_1738 = bitwise_not_116 = None
        view_1740 = torch.ops.aten.view.default(masked_fill_116, [1, 2, 4, 512, 512]);  masked_fill_116 = None
        permute_977 = torch.ops.aten.permute.default(view_1740, [1, 0, 2, 3, 4]);  view_1740 = None
        view_1741 = torch.ops.aten.view.default(permute_977, [2, 4, 1, 512, 512]);  permute_977 = None
        _to_copy_991 = torch.ops.aten._to_copy.default(arg519_1, dtype = torch.bfloat16);  arg519_1 = None
        _to_copy_992 = torch.ops.aten._to_copy.default(getitem_1598, dtype = torch.bfloat16)
        t_358 = torch.ops.aten.t.default(_to_copy_991);  _to_copy_991 = None
        view_1742 = torch.ops.aten.view.default(_to_copy_992, [262144, 256]);  _to_copy_992 = None
        mm_334 = torch.ops.aten.mm.default(view_1742, t_358);  view_1742 = t_358 = None
        view_1743 = torch.ops.aten.view.default(mm_334, [1, 512, 512, 1024]);  mm_334 = None
        select_41 = torch.ops.aten.select.int(view_1741, 0, 0)
        view_1744 = torch.ops.aten.view.default(view_1743, [1, 512, 512, 4, 4, 64]);  view_1743 = None
        permute_978 = torch.ops.aten.permute.default(view_1744, [4, 0, 3, 1, 2, 5]);  view_1744 = None
        view_1745 = torch.ops.aten.view.default(permute_978, [4, 4, 512, 512, 64]);  permute_978 = None
        unbind_int_82 = torch.ops.aten.unbind.int(view_1745);  view_1745 = None
        getitem_1601 = unbind_int_82[0]
        getitem_1602 = unbind_int_82[1]
        getitem_1603 = unbind_int_82[2]
        getitem_1604 = unbind_int_82[3];  unbind_int_82 = None
        expand_97 = torch.ops.aten.expand.default(select_41, [4, 512, 512, 512]);  select_41 = None
        _scaled_dot_product_efficient_attention_default_54 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1601, getitem_1602, getitem_1603, expand_97, False);  getitem_1601 = getitem_1602 = getitem_1603 = expand_97 = None
        getitem_1605 = _scaled_dot_product_efficient_attention_default_54[0]
        sigmoid_129 = torch.ops.aten.sigmoid.default(getitem_1604);  getitem_1604 = None
        mul_210 = torch.ops.aten.mul.Tensor(getitem_1605, sigmoid_129);  getitem_1605 = sigmoid_129 = None
        view_1746 = torch.ops.aten.view.default(mul_210, [1, 4, 512, 512, 64]);  mul_210 = None
        permute_979 = torch.ops.aten.permute.default(view_1746, [0, 2, 3, 1, 4]);  view_1746 = None
        clone_166 = torch.ops.aten.clone.default(permute_979, memory_format = torch.contiguous_format);  permute_979 = None
        _unsafe_view_144 = torch.ops.aten._unsafe_view.default(clone_166, [1, 512, 512, 256]);  clone_166 = None
        transpose_41 = torch.ops.aten.transpose.int(getitem_1598, 1, 2);  getitem_1598 = None
        _to_copy_993 = torch.ops.aten._to_copy.default(arg520_1, dtype = torch.bfloat16);  arg520_1 = None
        _to_copy_994 = torch.ops.aten._to_copy.default(transpose_41, dtype = torch.bfloat16);  transpose_41 = None
        t_359 = torch.ops.aten.t.default(_to_copy_993);  _to_copy_993 = None
        expand_98 = torch.ops.aten.expand.default(_to_copy_994, [1, 512, 512, 256]);  _to_copy_994 = None
        view_1747 = torch.ops.aten.view.default(expand_98, [512, 512, 256]);  expand_98 = None
        expand_99 = torch.ops.aten.expand.default(t_359, [1, 512, 256, 1024]);  t_359 = None
        view_1748 = torch.ops.aten.view.default(expand_99, [512, 256, 1024]);  expand_99 = None
        bmm_150 = torch.ops.aten.bmm.default(view_1747, view_1748);  view_1747 = view_1748 = None
        view_1749 = torch.ops.aten.view.default(bmm_150, [1, 512, 512, 1024]);  bmm_150 = None
        select_42 = torch.ops.aten.select.int(view_1741, 0, 1);  view_1741 = None
        view_1750 = torch.ops.aten.view.default(view_1749, [1, 512, 512, 4, 4, 64]);  view_1749 = None
        permute_980 = torch.ops.aten.permute.default(view_1750, [4, 0, 3, 1, 2, 5]);  view_1750 = None
        view_1751 = torch.ops.aten.view.default(permute_980, [4, 4, 512, 512, 64]);  permute_980 = None
        unbind_int_83 = torch.ops.aten.unbind.int(view_1751);  view_1751 = None
        getitem_1609 = unbind_int_83[0]
        getitem_1610 = unbind_int_83[1]
        getitem_1611 = unbind_int_83[2]
        getitem_1612 = unbind_int_83[3];  unbind_int_83 = None
        expand_100 = torch.ops.aten.expand.default(select_42, [4, 512, 512, 512]);  select_42 = None
        _scaled_dot_product_efficient_attention_default_55 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1609, getitem_1610, getitem_1611, expand_100, False);  getitem_1609 = getitem_1610 = getitem_1611 = expand_100 = None
        getitem_1613 = _scaled_dot_product_efficient_attention_default_55[0]
        sigmoid_130 = torch.ops.aten.sigmoid.default(getitem_1612);  getitem_1612 = None
        mul_211 = torch.ops.aten.mul.Tensor(getitem_1613, sigmoid_130);  getitem_1613 = sigmoid_130 = None
        view_1752 = torch.ops.aten.view.default(mul_211, [1, 4, 512, 512, 64]);  mul_211 = None
        permute_981 = torch.ops.aten.permute.default(view_1752, [0, 2, 3, 1, 4]);  view_1752 = None
        clone_167 = torch.ops.aten.clone.default(permute_981, memory_format = torch.contiguous_format);  permute_981 = None
        _unsafe_view_145 = torch.ops.aten._unsafe_view.default(clone_167, [1, 512, 512, 256]);  clone_167 = None
        cat_26 = torch.ops.aten.cat.default([_unsafe_view_144, _unsafe_view_145], dim = -1);  _unsafe_view_144 = _unsafe_view_145 = None
        slice_219 = torch.ops.aten.slice.Tensor(arg517_1, dim = 0, start = 0, end = 9223372036854775807);  arg517_1 = None
        unsqueeze_620 = torch.ops.aten.unsqueeze.default(slice_219, 1);  slice_219 = None
        mul_212 = torch.ops.aten.mul.Tensor(arg521_1, unsqueeze_620);  arg521_1 = unsqueeze_620 = None
        _to_copy_995 = torch.ops.aten._to_copy.default(mul_212, dtype = torch.bfloat16);  mul_212 = None
        t_360 = torch.ops.aten.t.default(_to_copy_995);  _to_copy_995 = None
        view_1753 = torch.ops.aten.view.default(cat_26, [262144, 512]);  cat_26 = None
        mm_335 = torch.ops.aten.mm.default(view_1753, t_360);  view_1753 = t_360 = None
        view_1754 = torch.ops.aten.view.default(mm_335, [1, 512, 512, 256]);  mm_335 = None
        add_170 = torch.ops.aten.add.Tensor(add_169, view_1754);  add_169 = view_1754 = None
        split_tensor_167 = torch.ops.aten.split.Tensor(add_163, 512, dim = -2)
        getitem_1617 = split_tensor_167[0];  split_tensor_167 = None
        _to_copy_996 = torch.ops.aten._to_copy.default(getitem_1617, dtype = torch.float32);  getitem_1617 = None
        native_layer_norm_default_206 = torch.ops.aten.native_layer_norm.default(_to_copy_996, [256], arg508_1, arg509_1, 1e-05);  _to_copy_996 = arg508_1 = arg509_1 = None
        getitem_1618 = native_layer_norm_default_206[0]
        _to_copy_997 = torch.ops.aten._to_copy.default(arg510_1, dtype = torch.bfloat16);  arg510_1 = None
        _to_copy_998 = torch.ops.aten._to_copy.default(getitem_1618, dtype = torch.bfloat16);  getitem_1618 = None
        t_361 = torch.ops.aten.t.default(_to_copy_997);  _to_copy_997 = None
        view_1755 = torch.ops.aten.view.default(_to_copy_998, [262144, 256]);  _to_copy_998 = None
        mm_336 = torch.ops.aten.mm.default(view_1755, t_361);  view_1755 = t_361 = None
        view_1756 = torch.ops.aten.view.default(mm_336, [1, 512, 512, 1024]);  mm_336 = None
        split_tensor_168 = torch.ops.aten.split.Tensor(view_1756, 512, dim = -1);  view_1756 = None
        getitem_1621 = split_tensor_168[0]
        getitem_1622 = split_tensor_168[1];  split_tensor_168 = None
        silu_46 = torch.ops.aten.silu.default(getitem_1621);  getitem_1621 = None
        mul_213 = torch.ops.aten.mul.Tensor(silu_46, getitem_1622);  silu_46 = getitem_1622 = None
        _to_copy_999 = torch.ops.aten._to_copy.default(arg511_1, dtype = torch.bfloat16);  arg511_1 = None
        t_362 = torch.ops.aten.t.default(_to_copy_999);  _to_copy_999 = None
        view_1758 = torch.ops.aten.view.default(mul_213, [262144, 512]);  mul_213 = None
        mm_337 = torch.ops.aten.mm.default(view_1758, t_362);  view_1758 = t_362 = None
        view_1759 = torch.ops.aten.view.default(mm_337, [1, 512, 512, 256]);  mm_337 = None
        add_171 = torch.ops.aten.add.Tensor(add_170, view_1759);  add_170 = view_1759 = None
        _to_copy_1000 = torch.ops.aten._to_copy.default(add_167, dtype = torch.float32)
        native_layer_norm_default_207 = torch.ops.aten.native_layer_norm.default(_to_copy_1000, [384], arg526_1, arg527_1, 1e-05);  _to_copy_1000 = arg526_1 = arg527_1 = None
        getitem_1623 = native_layer_norm_default_207[0]
        _to_copy_1001 = torch.ops.aten._to_copy.default(add_163, dtype = torch.float32);  add_163 = None
        native_layer_norm_default_208 = torch.ops.aten.native_layer_norm.default(_to_copy_1001, [256], arg528_1, arg529_1, 1e-05);  _to_copy_1001 = arg528_1 = arg529_1 = None
        getitem_1626 = native_layer_norm_default_208[0]
        _to_copy_1002 = torch.ops.aten._to_copy.default(arg530_1, dtype = torch.bfloat16);  arg530_1 = None
        _to_copy_1003 = torch.ops.aten._to_copy.default(getitem_1626, dtype = torch.bfloat16);  getitem_1626 = None
        t_363 = torch.ops.aten.t.default(_to_copy_1002);  _to_copy_1002 = None
        view_1760 = torch.ops.aten.view.default(_to_copy_1003, [262144, 256]);  _to_copy_1003 = None
        mm_338 = torch.ops.aten.mm.default(view_1760, t_363);  view_1760 = t_363 = None
        view_1761 = torch.ops.aten.view.default(mm_338, [1, 512, 512, 16]);  mm_338 = None
        permute_982 = torch.ops.aten.permute.default(view_1761, [0, 3, 1, 2]);  view_1761 = None
        view_1762 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_117 = torch.ops.aten.bitwise_not.default(view_1762);  view_1762 = None
        masked_fill_117 = torch.ops.aten.masked_fill.Scalar(permute_982, bitwise_not_117, -10000);  permute_982 = bitwise_not_117 = None
        _to_copy_1004 = torch.ops.aten._to_copy.default(getitem_1623, dtype = torch.bfloat16);  getitem_1623 = None
        _to_copy_1005 = torch.ops.aten._to_copy.default(arg532_1, dtype = torch.bfloat16);  arg532_1 = None
        unsqueeze_621 = torch.ops.aten.unsqueeze.default(_to_copy_1004, 3);  _to_copy_1004 = None
        unsqueeze_622 = torch.ops.aten.unsqueeze.default(unsqueeze_621, 4);  unsqueeze_621 = None
        unsqueeze_623 = torch.ops.aten.unsqueeze.default(unsqueeze_622, 5);  unsqueeze_622 = None
        permute_983 = torch.ops.aten.permute.default(unsqueeze_623, [3, 0, 4, 1, 5, 2]);  unsqueeze_623 = None
        unsqueeze_624 = torch.ops.aten.unsqueeze.default(_to_copy_1005, 4);  _to_copy_1005 = None
        unsqueeze_625 = torch.ops.aten.unsqueeze.default(unsqueeze_624, 5);  unsqueeze_624 = None
        permute_984 = torch.ops.aten.permute.default(unsqueeze_625, [1, 4, 2, 5, 3, 0]);  unsqueeze_625 = None
        permute_985 = torch.ops.aten.permute.default(permute_983, [3, 5, 0, 1, 2, 4]);  permute_983 = None
        view_1763 = torch.ops.aten.view.default(permute_985, [1, 512, 384]);  permute_985 = None
        permute_986 = torch.ops.aten.permute.default(permute_984, [5, 0, 1, 2, 4, 3]);  permute_984 = None
        view_1764 = torch.ops.aten.view.default(permute_986, [1, 384, 1536]);  permute_986 = None
        bmm_151 = torch.ops.aten.bmm.default(view_1763, view_1764);  view_1763 = view_1764 = None
        view_1765 = torch.ops.aten.view.default(bmm_151, [512, 1, 4, 1, 16, 24]);  bmm_151 = None
        permute_987 = torch.ops.aten.permute.default(view_1765, [2, 3, 4, 0, 5, 1]);  view_1765 = None
        view_1766 = torch.ops.aten.view.default(permute_987, [4, 1, 16, 512, 24]);  permute_987 = None
        unbind_int_84 = torch.ops.aten.unbind.int(view_1766);  view_1766 = None
        getitem_1629 = unbind_int_84[0]
        getitem_1630 = unbind_int_84[1]
        getitem_1631 = unbind_int_84[2]
        getitem_1632 = unbind_int_84[3];  unbind_int_84 = None
        view_1767 = torch.ops.aten.view.default(arg531_1, [1, 16, 1, 24]);  arg531_1 = None
        add_172 = torch.ops.aten.add.Tensor(getitem_1629, view_1767);  getitem_1629 = view_1767 = None
        _to_copy_1006 = torch.ops.aten._to_copy.default(add_172, dtype = torch.bfloat16);  add_172 = None
        expand_101 = torch.ops.aten.expand.default(masked_fill_117, [1, 16, 512, 512]);  masked_fill_117 = None
        _scaled_dot_product_efficient_attention_default_56 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1006, getitem_1630, getitem_1631, expand_101, False);  _to_copy_1006 = getitem_1630 = getitem_1631 = expand_101 = None
        getitem_1633 = _scaled_dot_product_efficient_attention_default_56[0]
        add_173 = torch.ops.aten.add.Tensor(getitem_1632, 1);  getitem_1632 = None
        sigmoid_131 = torch.ops.aten.sigmoid.default(add_173);  add_173 = None
        mul_214 = torch.ops.aten.mul.Tensor(getitem_1633, sigmoid_131);  getitem_1633 = sigmoid_131 = None
        _to_copy_1007 = torch.ops.aten._to_copy.default(arg533_1, dtype = torch.bfloat16);  arg533_1 = None
        unsqueeze_626 = torch.ops.aten.unsqueeze.default(mul_214, 4);  mul_214 = None
        permute_988 = torch.ops.aten.permute.default(unsqueeze_626, [0, 2, 4, 3, 1]);  unsqueeze_626 = None
        unsqueeze_627 = torch.ops.aten.unsqueeze.default(_to_copy_1007, 3);  _to_copy_1007 = None
        unsqueeze_628 = torch.ops.aten.unsqueeze.default(unsqueeze_627, 4);  unsqueeze_627 = None
        permute_989 = torch.ops.aten.permute.default(unsqueeze_628, [3, 4, 2, 1, 0]);  unsqueeze_628 = None
        permute_990 = torch.ops.aten.permute.default(permute_988, [1, 3, 4, 0, 2]);  permute_988 = None
        clone_168 = torch.ops.aten.clone.default(permute_990, memory_format = torch.contiguous_format);  permute_990 = None
        _unsafe_view_146 = torch.ops.aten._unsafe_view.default(clone_168, [1, 512, 384]);  clone_168 = None
        permute_991 = torch.ops.aten.permute.default(permute_989, [3, 4, 0, 2, 1]);  permute_989 = None
        clone_169 = torch.ops.aten.clone.default(permute_991, memory_format = torch.contiguous_format);  permute_991 = None
        _unsafe_view_147 = torch.ops.aten._unsafe_view.default(clone_169, [1, 384, 384]);  clone_169 = None
        bmm_152 = torch.ops.aten.bmm.default(_unsafe_view_146, _unsafe_view_147);  _unsafe_view_146 = _unsafe_view_147 = None
        view_1768 = torch.ops.aten.view.default(bmm_152, [512, 1, 1, 1, 384]);  bmm_152 = None
        permute_992 = torch.ops.aten.permute.default(view_1768, [3, 0, 4, 1, 2]);  view_1768 = None
        view_1769 = torch.ops.aten.view.default(permute_992, [1, 512, 384]);  permute_992 = None
        unsqueeze_629 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_215 = torch.ops.aten.mul.Tensor(view_1769, unsqueeze_629);  view_1769 = unsqueeze_629 = None
        add_174 = torch.ops.aten.add.Tensor(add_167, mul_215);  mul_215 = None
        split_tensor_169 = torch.ops.aten.split.Tensor(add_167, 512, dim = -2);  add_167 = None
        getitem_1637 = split_tensor_169[0];  split_tensor_169 = None
        _to_copy_1008 = torch.ops.aten._to_copy.default(getitem_1637, dtype = torch.float32);  getitem_1637 = None
        native_layer_norm_default_209 = torch.ops.aten.native_layer_norm.default(_to_copy_1008, [384], arg522_1, arg523_1, 1e-05);  _to_copy_1008 = arg522_1 = arg523_1 = None
        getitem_1638 = native_layer_norm_default_209[0]
        _to_copy_1009 = torch.ops.aten._to_copy.default(arg524_1, dtype = torch.bfloat16);  arg524_1 = None
        _to_copy_1010 = torch.ops.aten._to_copy.default(getitem_1638, dtype = torch.bfloat16);  getitem_1638 = None
        t_364 = torch.ops.aten.t.default(_to_copy_1009);  _to_copy_1009 = None
        view_1770 = torch.ops.aten.view.default(_to_copy_1010, [512, 384]);  _to_copy_1010 = None
        mm_339 = torch.ops.aten.mm.default(view_1770, t_364);  view_1770 = t_364 = None
        view_1771 = torch.ops.aten.view.default(mm_339, [1, 512, 1536]);  mm_339 = None
        split_tensor_170 = torch.ops.aten.split.Tensor(view_1771, 768, dim = -1);  view_1771 = None
        getitem_1641 = split_tensor_170[0]
        getitem_1642 = split_tensor_170[1];  split_tensor_170 = None
        silu_47 = torch.ops.aten.silu.default(getitem_1641);  getitem_1641 = None
        mul_216 = torch.ops.aten.mul.Tensor(silu_47, getitem_1642);  silu_47 = getitem_1642 = None
        _to_copy_1011 = torch.ops.aten._to_copy.default(arg525_1, dtype = torch.bfloat16);  arg525_1 = None
        t_365 = torch.ops.aten.t.default(_to_copy_1011);  _to_copy_1011 = None
        view_1773 = torch.ops.aten.view.default(mul_216, [512, 768]);  mul_216 = None
        mm_340 = torch.ops.aten.mm.default(view_1773, t_365);  view_1773 = t_365 = None
        view_1774 = torch.ops.aten.view.default(mm_340, [1, 512, 384]);  mm_340 = None
        add_175 = torch.ops.aten.add.Tensor(add_174, view_1774);  add_174 = view_1774 = None
        _to_copy_1012 = torch.ops.aten._to_copy.default(add_171, dtype = torch.float32)
        native_layer_norm_default_210 = torch.ops.aten.native_layer_norm.default(_to_copy_1012, [256], arg538_1, arg539_1, 1e-05);  _to_copy_1012 = arg538_1 = arg539_1 = None
        getitem_1643 = native_layer_norm_default_210[0]
        split_with_sizes_default_42 = torch.ops.aten.split_with_sizes.default(arg541_1, [512, 512]);  arg541_1 = None
        getitem_1646 = split_with_sizes_default_42[0]
        getitem_1647 = split_with_sizes_default_42[1];  split_with_sizes_default_42 = None
        split_with_sizes_default_43 = torch.ops.aten.split_with_sizes.default(arg542_1, [512, 512, 256]);  arg542_1 = None
        getitem_1648 = split_with_sizes_default_43[0]
        getitem_1649 = split_with_sizes_default_43[1]
        getitem_1650 = split_with_sizes_default_43[2];  split_with_sizes_default_43 = None
        _to_copy_1013 = torch.ops.aten._to_copy.default(getitem_1646, dtype = torch.bfloat16);  getitem_1646 = None
        _to_copy_1014 = torch.ops.aten._to_copy.default(getitem_1643, dtype = torch.bfloat16)
        t_366 = torch.ops.aten.t.default(_to_copy_1013);  _to_copy_1013 = None
        view_1775 = torch.ops.aten.view.default(_to_copy_1014, [262144, 256]);  _to_copy_1014 = None
        mm_341 = torch.ops.aten.mm.default(view_1775, t_366);  view_1775 = t_366 = None
        view_1776 = torch.ops.aten.view.default(mm_341, [1, 512, 512, 512]);  mm_341 = None
        _to_copy_1015 = torch.ops.aten._to_copy.default(getitem_1648, dtype = torch.bfloat16);  getitem_1648 = None
        _to_copy_1016 = torch.ops.aten._to_copy.default(getitem_1643, dtype = torch.bfloat16)
        t_367 = torch.ops.aten.t.default(_to_copy_1015);  _to_copy_1015 = None
        view_1777 = torch.ops.aten.view.default(_to_copy_1016, [262144, 256]);  _to_copy_1016 = None
        mm_342 = torch.ops.aten.mm.default(view_1777, t_367);  view_1777 = t_367 = None
        view_1778 = torch.ops.aten.view.default(mm_342, [1, 512, 512, 512]);  mm_342 = None
        sigmoid_132 = torch.ops.aten.sigmoid.default(view_1778);  view_1778 = None
        mul_217 = torch.ops.aten.mul.Tensor(view_1776, sigmoid_132);  view_1776 = sigmoid_132 = None
        unsqueeze_630 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_118 = torch.ops.aten.bitwise_not.default(unsqueeze_630);  unsqueeze_630 = None
        masked_fill_118 = torch.ops.aten.masked_fill.Scalar(mul_217, bitwise_not_118, 0);  mul_217 = bitwise_not_118 = None
        split_tensor_171 = torch.ops.aten.split.Tensor(masked_fill_118, 256, dim = -1)
        getitem_1653 = split_tensor_171[0]
        unsqueeze_633 = torch.ops.aten.unsqueeze.default(getitem_1653, 4);  getitem_1653 = None
        permute_997 = torch.ops.aten.permute.default(unsqueeze_633, [0, 1, 4, 3, 2]);  unsqueeze_633 = None
        permute_998 = torch.ops.aten.permute.default(permute_997, [3, 1, 4, 0, 2]);  permute_997 = None
        view_1781 = torch.ops.aten.view.default(permute_998, [256, 512, 512]);  permute_998 = None
        split_tensor_172 = torch.ops.aten.split.Tensor(masked_fill_118, 256, dim = -1);  masked_fill_118 = None
        getitem_1656 = split_tensor_172[1];  split_tensor_172 = None
        unsqueeze_634 = torch.ops.aten.unsqueeze.default(getitem_1656, 4);  getitem_1656 = None
        permute_999 = torch.ops.aten.permute.default(unsqueeze_634, [0, 4, 1, 3, 2]);  unsqueeze_634 = None
        permute_1000 = torch.ops.aten.permute.default(permute_999, [3, 4, 0, 2, 1]);  permute_999 = None
        view_1782 = torch.ops.aten.view.default(permute_1000, [256, 512, 512]);  permute_1000 = None
        bmm_153 = torch.ops.aten.bmm.default(view_1781, view_1782);  view_1781 = view_1782 = None
        view_1783 = torch.ops.aten.view.default(bmm_153, [256, 512, 1, 1, 512]);  bmm_153 = None
        permute_1001 = torch.ops.aten.permute.default(view_1783, [3, 1, 4, 0, 2]);  view_1783 = None
        view_1784 = torch.ops.aten.view.default(permute_1001, [1, 512, 512, 256]);  permute_1001 = None
        _to_copy_1017 = torch.ops.aten._to_copy.default(getitem_1647, dtype = torch.bfloat16);  getitem_1647 = None
        _to_copy_1018 = torch.ops.aten._to_copy.default(getitem_1643, dtype = torch.bfloat16)
        t_368 = torch.ops.aten.t.default(_to_copy_1017);  _to_copy_1017 = None
        view_1785 = torch.ops.aten.view.default(_to_copy_1018, [262144, 256]);  _to_copy_1018 = None
        mm_343 = torch.ops.aten.mm.default(view_1785, t_368);  view_1785 = t_368 = None
        view_1786 = torch.ops.aten.view.default(mm_343, [1, 512, 512, 512]);  mm_343 = None
        _to_copy_1019 = torch.ops.aten._to_copy.default(getitem_1649, dtype = torch.bfloat16);  getitem_1649 = None
        _to_copy_1020 = torch.ops.aten._to_copy.default(getitem_1643, dtype = torch.bfloat16)
        t_369 = torch.ops.aten.t.default(_to_copy_1019);  _to_copy_1019 = None
        view_1787 = torch.ops.aten.view.default(_to_copy_1020, [262144, 256]);  _to_copy_1020 = None
        mm_344 = torch.ops.aten.mm.default(view_1787, t_369);  view_1787 = t_369 = None
        view_1788 = torch.ops.aten.view.default(mm_344, [1, 512, 512, 512]);  mm_344 = None
        sigmoid_133 = torch.ops.aten.sigmoid.default(view_1788);  view_1788 = None
        mul_218 = torch.ops.aten.mul.Tensor(view_1786, sigmoid_133);  view_1786 = sigmoid_133 = None
        view_1789 = torch.ops.aten.view.default(mul_218, [262144, 512]);  mul_218 = None
        view_1790 = torch.ops.aten.view.default(view_1789, [1, 512, 512, 512]);  view_1789 = None
        transpose_42 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_635 = torch.ops.aten.unsqueeze.default(transpose_42, 3);  transpose_42 = None
        clone_170 = torch.ops.aten.clone.default(unsqueeze_635, memory_format = torch.contiguous_format);  unsqueeze_635 = None
        bitwise_not_119 = torch.ops.aten.bitwise_not.default(clone_170);  clone_170 = None
        masked_fill_119 = torch.ops.aten.masked_fill.Scalar(view_1790, bitwise_not_119, 0);  view_1790 = bitwise_not_119 = None
        view_1791 = torch.ops.aten.view.default(masked_fill_119, [262144, 512]);  masked_fill_119 = None
        view_1795 = torch.ops.aten.view.default(view_1791, [1, 512, 512, 512])
        split_tensor_173 = torch.ops.aten.split.Tensor(view_1795, 256, dim = -1);  view_1795 = None
        getitem_1659 = split_tensor_173[0]
        unsqueeze_638 = torch.ops.aten.unsqueeze.default(getitem_1659, 4);  getitem_1659 = None
        permute_1006 = torch.ops.aten.permute.default(unsqueeze_638, [0, 2, 4, 3, 1]);  unsqueeze_638 = None
        permute_1007 = torch.ops.aten.permute.default(permute_1006, [3, 1, 4, 0, 2]);  permute_1006 = None
        view_1796 = torch.ops.aten.view.default(permute_1007, [256, 512, 512]);  permute_1007 = None
        view_1797 = torch.ops.aten.view.default(view_1791, [1, 512, 512, 512]);  view_1791 = None
        split_tensor_174 = torch.ops.aten.split.Tensor(view_1797, 256, dim = -1);  view_1797 = None
        getitem_1662 = split_tensor_174[1];  split_tensor_174 = None
        unsqueeze_639 = torch.ops.aten.unsqueeze.default(getitem_1662, 4);  getitem_1662 = None
        permute_1008 = torch.ops.aten.permute.default(unsqueeze_639, [0, 4, 2, 3, 1]);  unsqueeze_639 = None
        permute_1009 = torch.ops.aten.permute.default(permute_1008, [3, 4, 0, 2, 1]);  permute_1008 = None
        view_1798 = torch.ops.aten.view.default(permute_1009, [256, 512, 512]);  permute_1009 = None
        bmm_154 = torch.ops.aten.bmm.default(view_1796, view_1798);  view_1796 = view_1798 = None
        view_1799 = torch.ops.aten.view.default(bmm_154, [256, 512, 1, 1, 512]);  bmm_154 = None
        permute_1010 = torch.ops.aten.permute.default(view_1799, [3, 1, 4, 0, 2]);  view_1799 = None
        view_1800 = torch.ops.aten.view.default(permute_1010, [1, 512, 512, 256]);  permute_1010 = None
        _to_copy_1021 = torch.ops.aten._to_copy.default(view_1784, dtype = torch.float32);  view_1784 = None
        native_layer_norm_default_211 = torch.ops.aten.native_layer_norm.default(_to_copy_1021, [256], None, None, 1e-05);  _to_copy_1021 = None
        getitem_1663 = native_layer_norm_default_211[0]
        _to_copy_1022 = torch.ops.aten._to_copy.default(view_1800, dtype = torch.float32);  view_1800 = None
        native_layer_norm_default_212 = torch.ops.aten.native_layer_norm.default(_to_copy_1022, [256], None, None, 1e-05);  _to_copy_1022 = None
        getitem_1666 = native_layer_norm_default_212[0]
        add_176 = torch.ops.aten.add.Tensor(getitem_1663, getitem_1666);  getitem_1663 = getitem_1666 = None
        _to_copy_1023 = torch.ops.aten._to_copy.default(arg540_1, dtype = torch.bfloat16);  arg540_1 = None
        _to_copy_1024 = torch.ops.aten._to_copy.default(add_176, dtype = torch.bfloat16);  add_176 = None
        t_370 = torch.ops.aten.t.default(_to_copy_1023);  _to_copy_1023 = None
        view_1801 = torch.ops.aten.view.default(_to_copy_1024, [262144, 256]);  _to_copy_1024 = None
        mm_345 = torch.ops.aten.mm.default(view_1801, t_370);  view_1801 = t_370 = None
        view_1802 = torch.ops.aten.view.default(mm_345, [1, 512, 512, 256]);  mm_345 = None
        _to_copy_1025 = torch.ops.aten._to_copy.default(getitem_1650, dtype = torch.bfloat16);  getitem_1650 = None
        _to_copy_1026 = torch.ops.aten._to_copy.default(getitem_1643, dtype = torch.bfloat16);  getitem_1643 = None
        t_371 = torch.ops.aten.t.default(_to_copy_1025);  _to_copy_1025 = None
        view_1803 = torch.ops.aten.view.default(_to_copy_1026, [262144, 256]);  _to_copy_1026 = None
        mm_346 = torch.ops.aten.mm.default(view_1803, t_371);  view_1803 = t_371 = None
        view_1804 = torch.ops.aten.view.default(mm_346, [1, 512, 512, 256]);  mm_346 = None
        sigmoid_134 = torch.ops.aten.sigmoid.default(view_1804);  view_1804 = None
        mul_219 = torch.ops.aten.mul.Tensor(view_1802, sigmoid_134);  view_1802 = sigmoid_134 = None
        add_177 = torch.ops.aten.add.Tensor(add_171, mul_219);  mul_219 = None
        _to_copy_1027 = torch.ops.aten._to_copy.default(add_171, dtype = torch.float32)
        native_layer_norm_default_213 = torch.ops.aten.native_layer_norm.default(_to_copy_1027, [256], None, None, 1e-05);  _to_copy_1027 = None
        getitem_1669 = native_layer_norm_default_213[0]
        _to_copy_1028 = torch.ops.aten._to_copy.default(arg544_1, dtype = torch.bfloat16);  arg544_1 = None
        _to_copy_1029 = torch.ops.aten._to_copy.default(getitem_1669, dtype = torch.bfloat16)
        t_372 = torch.ops.aten.t.default(_to_copy_1028);  _to_copy_1028 = None
        view_1805 = torch.ops.aten.view.default(_to_copy_1029, [262144, 256]);  _to_copy_1029 = None
        mm_347 = torch.ops.aten.mm.default(view_1805, t_372);  view_1805 = t_372 = None
        view_1806 = torch.ops.aten.view.default(mm_347, [1, 512, 512, 8]);  mm_347 = None
        view_1807 = torch.ops.aten.view.default(view_1806, [1, 512, 512, 2, 4]);  view_1806 = None
        permute_1011 = torch.ops.aten.permute.default(view_1807, [0, 3, 4, 1, 2]);  view_1807 = None
        view_1808 = torch.ops.aten.view.default(permute_1011, [1, 2, 4, 1, 512, 512]);  permute_1011 = None
        view_1809 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_120 = torch.ops.aten.bitwise_not.default(view_1809);  view_1809 = None
        masked_fill_120 = torch.ops.aten.masked_fill.Scalar(view_1808, bitwise_not_120, -10000);  view_1808 = bitwise_not_120 = None
        view_1810 = torch.ops.aten.view.default(masked_fill_120, [1, 2, 4, 512, 512]);  masked_fill_120 = None
        permute_1012 = torch.ops.aten.permute.default(view_1810, [1, 0, 2, 3, 4]);  view_1810 = None
        view_1811 = torch.ops.aten.view.default(permute_1012, [2, 4, 1, 512, 512]);  permute_1012 = None
        _to_copy_1030 = torch.ops.aten._to_copy.default(arg545_1, dtype = torch.bfloat16);  arg545_1 = None
        _to_copy_1031 = torch.ops.aten._to_copy.default(getitem_1669, dtype = torch.bfloat16)
        t_373 = torch.ops.aten.t.default(_to_copy_1030);  _to_copy_1030 = None
        view_1812 = torch.ops.aten.view.default(_to_copy_1031, [262144, 256]);  _to_copy_1031 = None
        mm_348 = torch.ops.aten.mm.default(view_1812, t_373);  view_1812 = t_373 = None
        view_1813 = torch.ops.aten.view.default(mm_348, [1, 512, 512, 1024]);  mm_348 = None
        select_43 = torch.ops.aten.select.int(view_1811, 0, 0)
        view_1814 = torch.ops.aten.view.default(view_1813, [1, 512, 512, 4, 4, 64]);  view_1813 = None
        permute_1013 = torch.ops.aten.permute.default(view_1814, [4, 0, 3, 1, 2, 5]);  view_1814 = None
        view_1815 = torch.ops.aten.view.default(permute_1013, [4, 4, 512, 512, 64]);  permute_1013 = None
        unbind_int_85 = torch.ops.aten.unbind.int(view_1815);  view_1815 = None
        getitem_1672 = unbind_int_85[0]
        getitem_1673 = unbind_int_85[1]
        getitem_1674 = unbind_int_85[2]
        getitem_1675 = unbind_int_85[3];  unbind_int_85 = None
        expand_102 = torch.ops.aten.expand.default(select_43, [4, 512, 512, 512]);  select_43 = None
        _scaled_dot_product_efficient_attention_default_57 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1672, getitem_1673, getitem_1674, expand_102, False);  getitem_1672 = getitem_1673 = getitem_1674 = expand_102 = None
        getitem_1676 = _scaled_dot_product_efficient_attention_default_57[0]
        sigmoid_135 = torch.ops.aten.sigmoid.default(getitem_1675);  getitem_1675 = None
        mul_220 = torch.ops.aten.mul.Tensor(getitem_1676, sigmoid_135);  getitem_1676 = sigmoid_135 = None
        view_1816 = torch.ops.aten.view.default(mul_220, [1, 4, 512, 512, 64]);  mul_220 = None
        permute_1014 = torch.ops.aten.permute.default(view_1816, [0, 2, 3, 1, 4]);  view_1816 = None
        clone_171 = torch.ops.aten.clone.default(permute_1014, memory_format = torch.contiguous_format);  permute_1014 = None
        _unsafe_view_148 = torch.ops.aten._unsafe_view.default(clone_171, [1, 512, 512, 256]);  clone_171 = None
        transpose_43 = torch.ops.aten.transpose.int(getitem_1669, 1, 2);  getitem_1669 = None
        _to_copy_1032 = torch.ops.aten._to_copy.default(arg546_1, dtype = torch.bfloat16);  arg546_1 = None
        _to_copy_1033 = torch.ops.aten._to_copy.default(transpose_43, dtype = torch.bfloat16);  transpose_43 = None
        t_374 = torch.ops.aten.t.default(_to_copy_1032);  _to_copy_1032 = None
        expand_103 = torch.ops.aten.expand.default(_to_copy_1033, [1, 512, 512, 256]);  _to_copy_1033 = None
        view_1817 = torch.ops.aten.view.default(expand_103, [512, 512, 256]);  expand_103 = None
        expand_104 = torch.ops.aten.expand.default(t_374, [1, 512, 256, 1024]);  t_374 = None
        view_1818 = torch.ops.aten.view.default(expand_104, [512, 256, 1024]);  expand_104 = None
        bmm_155 = torch.ops.aten.bmm.default(view_1817, view_1818);  view_1817 = view_1818 = None
        view_1819 = torch.ops.aten.view.default(bmm_155, [1, 512, 512, 1024]);  bmm_155 = None
        select_44 = torch.ops.aten.select.int(view_1811, 0, 1);  view_1811 = None
        view_1820 = torch.ops.aten.view.default(view_1819, [1, 512, 512, 4, 4, 64]);  view_1819 = None
        permute_1015 = torch.ops.aten.permute.default(view_1820, [4, 0, 3, 1, 2, 5]);  view_1820 = None
        view_1821 = torch.ops.aten.view.default(permute_1015, [4, 4, 512, 512, 64]);  permute_1015 = None
        unbind_int_86 = torch.ops.aten.unbind.int(view_1821);  view_1821 = None
        getitem_1680 = unbind_int_86[0]
        getitem_1681 = unbind_int_86[1]
        getitem_1682 = unbind_int_86[2]
        getitem_1683 = unbind_int_86[3];  unbind_int_86 = None
        expand_105 = torch.ops.aten.expand.default(select_44, [4, 512, 512, 512]);  select_44 = None
        _scaled_dot_product_efficient_attention_default_58 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1680, getitem_1681, getitem_1682, expand_105, False);  getitem_1680 = getitem_1681 = getitem_1682 = expand_105 = None
        getitem_1684 = _scaled_dot_product_efficient_attention_default_58[0]
        sigmoid_136 = torch.ops.aten.sigmoid.default(getitem_1683);  getitem_1683 = None
        mul_221 = torch.ops.aten.mul.Tensor(getitem_1684, sigmoid_136);  getitem_1684 = sigmoid_136 = None
        view_1822 = torch.ops.aten.view.default(mul_221, [1, 4, 512, 512, 64]);  mul_221 = None
        permute_1016 = torch.ops.aten.permute.default(view_1822, [0, 2, 3, 1, 4]);  view_1822 = None
        clone_172 = torch.ops.aten.clone.default(permute_1016, memory_format = torch.contiguous_format);  permute_1016 = None
        _unsafe_view_149 = torch.ops.aten._unsafe_view.default(clone_172, [1, 512, 512, 256]);  clone_172 = None
        cat_27 = torch.ops.aten.cat.default([_unsafe_view_148, _unsafe_view_149], dim = -1);  _unsafe_view_148 = _unsafe_view_149 = None
        slice_220 = torch.ops.aten.slice.Tensor(arg543_1, dim = 0, start = 0, end = 9223372036854775807);  arg543_1 = None
        unsqueeze_640 = torch.ops.aten.unsqueeze.default(slice_220, 1);  slice_220 = None
        mul_222 = torch.ops.aten.mul.Tensor(arg547_1, unsqueeze_640);  arg547_1 = unsqueeze_640 = None
        _to_copy_1034 = torch.ops.aten._to_copy.default(mul_222, dtype = torch.bfloat16);  mul_222 = None
        t_375 = torch.ops.aten.t.default(_to_copy_1034);  _to_copy_1034 = None
        view_1823 = torch.ops.aten.view.default(cat_27, [262144, 512]);  cat_27 = None
        mm_349 = torch.ops.aten.mm.default(view_1823, t_375);  view_1823 = t_375 = None
        view_1824 = torch.ops.aten.view.default(mm_349, [1, 512, 512, 256]);  mm_349 = None
        add_178 = torch.ops.aten.add.Tensor(add_177, view_1824);  add_177 = view_1824 = None
        split_tensor_175 = torch.ops.aten.split.Tensor(add_171, 512, dim = -2)
        getitem_1688 = split_tensor_175[0];  split_tensor_175 = None
        _to_copy_1035 = torch.ops.aten._to_copy.default(getitem_1688, dtype = torch.float32);  getitem_1688 = None
        native_layer_norm_default_214 = torch.ops.aten.native_layer_norm.default(_to_copy_1035, [256], arg534_1, arg535_1, 1e-05);  _to_copy_1035 = arg534_1 = arg535_1 = None
        getitem_1689 = native_layer_norm_default_214[0]
        _to_copy_1036 = torch.ops.aten._to_copy.default(arg536_1, dtype = torch.bfloat16);  arg536_1 = None
        _to_copy_1037 = torch.ops.aten._to_copy.default(getitem_1689, dtype = torch.bfloat16);  getitem_1689 = None
        t_376 = torch.ops.aten.t.default(_to_copy_1036);  _to_copy_1036 = None
        view_1825 = torch.ops.aten.view.default(_to_copy_1037, [262144, 256]);  _to_copy_1037 = None
        mm_350 = torch.ops.aten.mm.default(view_1825, t_376);  view_1825 = t_376 = None
        view_1826 = torch.ops.aten.view.default(mm_350, [1, 512, 512, 1024]);  mm_350 = None
        split_tensor_176 = torch.ops.aten.split.Tensor(view_1826, 512, dim = -1);  view_1826 = None
        getitem_1692 = split_tensor_176[0]
        getitem_1693 = split_tensor_176[1];  split_tensor_176 = None
        silu_48 = torch.ops.aten.silu.default(getitem_1692);  getitem_1692 = None
        mul_223 = torch.ops.aten.mul.Tensor(silu_48, getitem_1693);  silu_48 = getitem_1693 = None
        _to_copy_1038 = torch.ops.aten._to_copy.default(arg537_1, dtype = torch.bfloat16);  arg537_1 = None
        t_377 = torch.ops.aten.t.default(_to_copy_1038);  _to_copy_1038 = None
        view_1828 = torch.ops.aten.view.default(mul_223, [262144, 512]);  mul_223 = None
        mm_351 = torch.ops.aten.mm.default(view_1828, t_377);  view_1828 = t_377 = None
        view_1829 = torch.ops.aten.view.default(mm_351, [1, 512, 512, 256]);  mm_351 = None
        add_179 = torch.ops.aten.add.Tensor(add_178, view_1829);  add_178 = view_1829 = None
        _to_copy_1039 = torch.ops.aten._to_copy.default(add_175, dtype = torch.float32)
        native_layer_norm_default_215 = torch.ops.aten.native_layer_norm.default(_to_copy_1039, [384], arg552_1, arg553_1, 1e-05);  _to_copy_1039 = arg552_1 = arg553_1 = None
        getitem_1694 = native_layer_norm_default_215[0]
        _to_copy_1040 = torch.ops.aten._to_copy.default(add_171, dtype = torch.float32);  add_171 = None
        native_layer_norm_default_216 = torch.ops.aten.native_layer_norm.default(_to_copy_1040, [256], arg554_1, arg555_1, 1e-05);  _to_copy_1040 = arg554_1 = arg555_1 = None
        getitem_1697 = native_layer_norm_default_216[0]
        _to_copy_1041 = torch.ops.aten._to_copy.default(arg556_1, dtype = torch.bfloat16);  arg556_1 = None
        _to_copy_1042 = torch.ops.aten._to_copy.default(getitem_1697, dtype = torch.bfloat16);  getitem_1697 = None
        t_378 = torch.ops.aten.t.default(_to_copy_1041);  _to_copy_1041 = None
        view_1830 = torch.ops.aten.view.default(_to_copy_1042, [262144, 256]);  _to_copy_1042 = None
        mm_352 = torch.ops.aten.mm.default(view_1830, t_378);  view_1830 = t_378 = None
        view_1831 = torch.ops.aten.view.default(mm_352, [1, 512, 512, 16]);  mm_352 = None
        permute_1017 = torch.ops.aten.permute.default(view_1831, [0, 3, 1, 2]);  view_1831 = None
        view_1832 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_121 = torch.ops.aten.bitwise_not.default(view_1832);  view_1832 = None
        masked_fill_121 = torch.ops.aten.masked_fill.Scalar(permute_1017, bitwise_not_121, -10000);  permute_1017 = bitwise_not_121 = None
        _to_copy_1043 = torch.ops.aten._to_copy.default(getitem_1694, dtype = torch.bfloat16);  getitem_1694 = None
        _to_copy_1044 = torch.ops.aten._to_copy.default(arg558_1, dtype = torch.bfloat16);  arg558_1 = None
        unsqueeze_641 = torch.ops.aten.unsqueeze.default(_to_copy_1043, 3);  _to_copy_1043 = None
        unsqueeze_642 = torch.ops.aten.unsqueeze.default(unsqueeze_641, 4);  unsqueeze_641 = None
        unsqueeze_643 = torch.ops.aten.unsqueeze.default(unsqueeze_642, 5);  unsqueeze_642 = None
        permute_1018 = torch.ops.aten.permute.default(unsqueeze_643, [3, 0, 4, 1, 5, 2]);  unsqueeze_643 = None
        unsqueeze_644 = torch.ops.aten.unsqueeze.default(_to_copy_1044, 4);  _to_copy_1044 = None
        unsqueeze_645 = torch.ops.aten.unsqueeze.default(unsqueeze_644, 5);  unsqueeze_644 = None
        permute_1019 = torch.ops.aten.permute.default(unsqueeze_645, [1, 4, 2, 5, 3, 0]);  unsqueeze_645 = None
        permute_1020 = torch.ops.aten.permute.default(permute_1018, [3, 5, 0, 1, 2, 4]);  permute_1018 = None
        view_1833 = torch.ops.aten.view.default(permute_1020, [1, 512, 384]);  permute_1020 = None
        permute_1021 = torch.ops.aten.permute.default(permute_1019, [5, 0, 1, 2, 4, 3]);  permute_1019 = None
        view_1834 = torch.ops.aten.view.default(permute_1021, [1, 384, 1536]);  permute_1021 = None
        bmm_156 = torch.ops.aten.bmm.default(view_1833, view_1834);  view_1833 = view_1834 = None
        view_1835 = torch.ops.aten.view.default(bmm_156, [512, 1, 4, 1, 16, 24]);  bmm_156 = None
        permute_1022 = torch.ops.aten.permute.default(view_1835, [2, 3, 4, 0, 5, 1]);  view_1835 = None
        view_1836 = torch.ops.aten.view.default(permute_1022, [4, 1, 16, 512, 24]);  permute_1022 = None
        unbind_int_87 = torch.ops.aten.unbind.int(view_1836);  view_1836 = None
        getitem_1700 = unbind_int_87[0]
        getitem_1701 = unbind_int_87[1]
        getitem_1702 = unbind_int_87[2]
        getitem_1703 = unbind_int_87[3];  unbind_int_87 = None
        view_1837 = torch.ops.aten.view.default(arg557_1, [1, 16, 1, 24]);  arg557_1 = None
        add_180 = torch.ops.aten.add.Tensor(getitem_1700, view_1837);  getitem_1700 = view_1837 = None
        _to_copy_1045 = torch.ops.aten._to_copy.default(add_180, dtype = torch.bfloat16);  add_180 = None
        expand_106 = torch.ops.aten.expand.default(masked_fill_121, [1, 16, 512, 512]);  masked_fill_121 = None
        _scaled_dot_product_efficient_attention_default_59 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1045, getitem_1701, getitem_1702, expand_106, False);  _to_copy_1045 = getitem_1701 = getitem_1702 = expand_106 = None
        getitem_1704 = _scaled_dot_product_efficient_attention_default_59[0]
        add_181 = torch.ops.aten.add.Tensor(getitem_1703, 1);  getitem_1703 = None
        sigmoid_137 = torch.ops.aten.sigmoid.default(add_181);  add_181 = None
        mul_224 = torch.ops.aten.mul.Tensor(getitem_1704, sigmoid_137);  getitem_1704 = sigmoid_137 = None
        _to_copy_1046 = torch.ops.aten._to_copy.default(arg559_1, dtype = torch.bfloat16);  arg559_1 = None
        unsqueeze_646 = torch.ops.aten.unsqueeze.default(mul_224, 4);  mul_224 = None
        permute_1023 = torch.ops.aten.permute.default(unsqueeze_646, [0, 2, 4, 3, 1]);  unsqueeze_646 = None
        unsqueeze_647 = torch.ops.aten.unsqueeze.default(_to_copy_1046, 3);  _to_copy_1046 = None
        unsqueeze_648 = torch.ops.aten.unsqueeze.default(unsqueeze_647, 4);  unsqueeze_647 = None
        permute_1024 = torch.ops.aten.permute.default(unsqueeze_648, [3, 4, 2, 1, 0]);  unsqueeze_648 = None
        permute_1025 = torch.ops.aten.permute.default(permute_1023, [1, 3, 4, 0, 2]);  permute_1023 = None
        clone_173 = torch.ops.aten.clone.default(permute_1025, memory_format = torch.contiguous_format);  permute_1025 = None
        _unsafe_view_150 = torch.ops.aten._unsafe_view.default(clone_173, [1, 512, 384]);  clone_173 = None
        permute_1026 = torch.ops.aten.permute.default(permute_1024, [3, 4, 0, 2, 1]);  permute_1024 = None
        clone_174 = torch.ops.aten.clone.default(permute_1026, memory_format = torch.contiguous_format);  permute_1026 = None
        _unsafe_view_151 = torch.ops.aten._unsafe_view.default(clone_174, [1, 384, 384]);  clone_174 = None
        bmm_157 = torch.ops.aten.bmm.default(_unsafe_view_150, _unsafe_view_151);  _unsafe_view_150 = _unsafe_view_151 = None
        view_1838 = torch.ops.aten.view.default(bmm_157, [512, 1, 1, 1, 384]);  bmm_157 = None
        permute_1027 = torch.ops.aten.permute.default(view_1838, [3, 0, 4, 1, 2]);  view_1838 = None
        view_1839 = torch.ops.aten.view.default(permute_1027, [1, 512, 384]);  permute_1027 = None
        unsqueeze_649 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_225 = torch.ops.aten.mul.Tensor(view_1839, unsqueeze_649);  view_1839 = unsqueeze_649 = None
        add_182 = torch.ops.aten.add.Tensor(add_175, mul_225);  mul_225 = None
        split_tensor_177 = torch.ops.aten.split.Tensor(add_175, 512, dim = -2);  add_175 = None
        getitem_1708 = split_tensor_177[0];  split_tensor_177 = None
        _to_copy_1047 = torch.ops.aten._to_copy.default(getitem_1708, dtype = torch.float32);  getitem_1708 = None
        native_layer_norm_default_217 = torch.ops.aten.native_layer_norm.default(_to_copy_1047, [384], arg548_1, arg549_1, 1e-05);  _to_copy_1047 = arg548_1 = arg549_1 = None
        getitem_1709 = native_layer_norm_default_217[0]
        _to_copy_1048 = torch.ops.aten._to_copy.default(arg550_1, dtype = torch.bfloat16);  arg550_1 = None
        _to_copy_1049 = torch.ops.aten._to_copy.default(getitem_1709, dtype = torch.bfloat16);  getitem_1709 = None
        t_379 = torch.ops.aten.t.default(_to_copy_1048);  _to_copy_1048 = None
        view_1840 = torch.ops.aten.view.default(_to_copy_1049, [512, 384]);  _to_copy_1049 = None
        mm_353 = torch.ops.aten.mm.default(view_1840, t_379);  view_1840 = t_379 = None
        view_1841 = torch.ops.aten.view.default(mm_353, [1, 512, 1536]);  mm_353 = None
        split_tensor_178 = torch.ops.aten.split.Tensor(view_1841, 768, dim = -1);  view_1841 = None
        getitem_1712 = split_tensor_178[0]
        getitem_1713 = split_tensor_178[1];  split_tensor_178 = None
        silu_49 = torch.ops.aten.silu.default(getitem_1712);  getitem_1712 = None
        mul_226 = torch.ops.aten.mul.Tensor(silu_49, getitem_1713);  silu_49 = getitem_1713 = None
        _to_copy_1050 = torch.ops.aten._to_copy.default(arg551_1, dtype = torch.bfloat16);  arg551_1 = None
        t_380 = torch.ops.aten.t.default(_to_copy_1050);  _to_copy_1050 = None
        view_1843 = torch.ops.aten.view.default(mul_226, [512, 768]);  mul_226 = None
        mm_354 = torch.ops.aten.mm.default(view_1843, t_380);  view_1843 = t_380 = None
        view_1844 = torch.ops.aten.view.default(mm_354, [1, 512, 384]);  mm_354 = None
        add_183 = torch.ops.aten.add.Tensor(add_182, view_1844);  add_182 = view_1844 = None
        _to_copy_1051 = torch.ops.aten._to_copy.default(add_179, dtype = torch.float32)
        native_layer_norm_default_218 = torch.ops.aten.native_layer_norm.default(_to_copy_1051, [256], arg564_1, arg565_1, 1e-05);  _to_copy_1051 = arg564_1 = arg565_1 = None
        getitem_1714 = native_layer_norm_default_218[0]
        split_with_sizes_default_44 = torch.ops.aten.split_with_sizes.default(arg567_1, [512, 512]);  arg567_1 = None
        getitem_1717 = split_with_sizes_default_44[0]
        getitem_1718 = split_with_sizes_default_44[1];  split_with_sizes_default_44 = None
        split_with_sizes_default_45 = torch.ops.aten.split_with_sizes.default(arg568_1, [512, 512, 256]);  arg568_1 = None
        getitem_1719 = split_with_sizes_default_45[0]
        getitem_1720 = split_with_sizes_default_45[1]
        getitem_1721 = split_with_sizes_default_45[2];  split_with_sizes_default_45 = None
        _to_copy_1052 = torch.ops.aten._to_copy.default(getitem_1717, dtype = torch.bfloat16);  getitem_1717 = None
        _to_copy_1053 = torch.ops.aten._to_copy.default(getitem_1714, dtype = torch.bfloat16)
        t_381 = torch.ops.aten.t.default(_to_copy_1052);  _to_copy_1052 = None
        view_1845 = torch.ops.aten.view.default(_to_copy_1053, [262144, 256]);  _to_copy_1053 = None
        mm_355 = torch.ops.aten.mm.default(view_1845, t_381);  view_1845 = t_381 = None
        view_1846 = torch.ops.aten.view.default(mm_355, [1, 512, 512, 512]);  mm_355 = None
        _to_copy_1054 = torch.ops.aten._to_copy.default(getitem_1719, dtype = torch.bfloat16);  getitem_1719 = None
        _to_copy_1055 = torch.ops.aten._to_copy.default(getitem_1714, dtype = torch.bfloat16)
        t_382 = torch.ops.aten.t.default(_to_copy_1054);  _to_copy_1054 = None
        view_1847 = torch.ops.aten.view.default(_to_copy_1055, [262144, 256]);  _to_copy_1055 = None
        mm_356 = torch.ops.aten.mm.default(view_1847, t_382);  view_1847 = t_382 = None
        view_1848 = torch.ops.aten.view.default(mm_356, [1, 512, 512, 512]);  mm_356 = None
        sigmoid_138 = torch.ops.aten.sigmoid.default(view_1848);  view_1848 = None
        mul_227 = torch.ops.aten.mul.Tensor(view_1846, sigmoid_138);  view_1846 = sigmoid_138 = None
        unsqueeze_650 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_122 = torch.ops.aten.bitwise_not.default(unsqueeze_650);  unsqueeze_650 = None
        masked_fill_122 = torch.ops.aten.masked_fill.Scalar(mul_227, bitwise_not_122, 0);  mul_227 = bitwise_not_122 = None
        split_tensor_179 = torch.ops.aten.split.Tensor(masked_fill_122, 256, dim = -1)
        getitem_1724 = split_tensor_179[0]
        unsqueeze_653 = torch.ops.aten.unsqueeze.default(getitem_1724, 4);  getitem_1724 = None
        permute_1032 = torch.ops.aten.permute.default(unsqueeze_653, [0, 1, 4, 3, 2]);  unsqueeze_653 = None
        permute_1033 = torch.ops.aten.permute.default(permute_1032, [3, 1, 4, 0, 2]);  permute_1032 = None
        view_1851 = torch.ops.aten.view.default(permute_1033, [256, 512, 512]);  permute_1033 = None
        split_tensor_180 = torch.ops.aten.split.Tensor(masked_fill_122, 256, dim = -1);  masked_fill_122 = None
        getitem_1727 = split_tensor_180[1];  split_tensor_180 = None
        unsqueeze_654 = torch.ops.aten.unsqueeze.default(getitem_1727, 4);  getitem_1727 = None
        permute_1034 = torch.ops.aten.permute.default(unsqueeze_654, [0, 4, 1, 3, 2]);  unsqueeze_654 = None
        permute_1035 = torch.ops.aten.permute.default(permute_1034, [3, 4, 0, 2, 1]);  permute_1034 = None
        view_1852 = torch.ops.aten.view.default(permute_1035, [256, 512, 512]);  permute_1035 = None
        bmm_158 = torch.ops.aten.bmm.default(view_1851, view_1852);  view_1851 = view_1852 = None
        view_1853 = torch.ops.aten.view.default(bmm_158, [256, 512, 1, 1, 512]);  bmm_158 = None
        permute_1036 = torch.ops.aten.permute.default(view_1853, [3, 1, 4, 0, 2]);  view_1853 = None
        view_1854 = torch.ops.aten.view.default(permute_1036, [1, 512, 512, 256]);  permute_1036 = None
        _to_copy_1056 = torch.ops.aten._to_copy.default(getitem_1718, dtype = torch.bfloat16);  getitem_1718 = None
        _to_copy_1057 = torch.ops.aten._to_copy.default(getitem_1714, dtype = torch.bfloat16)
        t_383 = torch.ops.aten.t.default(_to_copy_1056);  _to_copy_1056 = None
        view_1855 = torch.ops.aten.view.default(_to_copy_1057, [262144, 256]);  _to_copy_1057 = None
        mm_357 = torch.ops.aten.mm.default(view_1855, t_383);  view_1855 = t_383 = None
        view_1856 = torch.ops.aten.view.default(mm_357, [1, 512, 512, 512]);  mm_357 = None
        _to_copy_1058 = torch.ops.aten._to_copy.default(getitem_1720, dtype = torch.bfloat16);  getitem_1720 = None
        _to_copy_1059 = torch.ops.aten._to_copy.default(getitem_1714, dtype = torch.bfloat16)
        t_384 = torch.ops.aten.t.default(_to_copy_1058);  _to_copy_1058 = None
        view_1857 = torch.ops.aten.view.default(_to_copy_1059, [262144, 256]);  _to_copy_1059 = None
        mm_358 = torch.ops.aten.mm.default(view_1857, t_384);  view_1857 = t_384 = None
        view_1858 = torch.ops.aten.view.default(mm_358, [1, 512, 512, 512]);  mm_358 = None
        sigmoid_139 = torch.ops.aten.sigmoid.default(view_1858);  view_1858 = None
        mul_228 = torch.ops.aten.mul.Tensor(view_1856, sigmoid_139);  view_1856 = sigmoid_139 = None
        view_1859 = torch.ops.aten.view.default(mul_228, [262144, 512]);  mul_228 = None
        view_1860 = torch.ops.aten.view.default(view_1859, [1, 512, 512, 512]);  view_1859 = None
        transpose_44 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_655 = torch.ops.aten.unsqueeze.default(transpose_44, 3);  transpose_44 = None
        clone_175 = torch.ops.aten.clone.default(unsqueeze_655, memory_format = torch.contiguous_format);  unsqueeze_655 = None
        bitwise_not_123 = torch.ops.aten.bitwise_not.default(clone_175);  clone_175 = None
        masked_fill_123 = torch.ops.aten.masked_fill.Scalar(view_1860, bitwise_not_123, 0);  view_1860 = bitwise_not_123 = None
        view_1861 = torch.ops.aten.view.default(masked_fill_123, [262144, 512]);  masked_fill_123 = None
        view_1865 = torch.ops.aten.view.default(view_1861, [1, 512, 512, 512])
        split_tensor_181 = torch.ops.aten.split.Tensor(view_1865, 256, dim = -1);  view_1865 = None
        getitem_1730 = split_tensor_181[0]
        unsqueeze_658 = torch.ops.aten.unsqueeze.default(getitem_1730, 4);  getitem_1730 = None
        permute_1041 = torch.ops.aten.permute.default(unsqueeze_658, [0, 2, 4, 3, 1]);  unsqueeze_658 = None
        permute_1042 = torch.ops.aten.permute.default(permute_1041, [3, 1, 4, 0, 2]);  permute_1041 = None
        view_1866 = torch.ops.aten.view.default(permute_1042, [256, 512, 512]);  permute_1042 = None
        view_1867 = torch.ops.aten.view.default(view_1861, [1, 512, 512, 512]);  view_1861 = None
        split_tensor_182 = torch.ops.aten.split.Tensor(view_1867, 256, dim = -1);  view_1867 = None
        getitem_1733 = split_tensor_182[1];  split_tensor_182 = None
        unsqueeze_659 = torch.ops.aten.unsqueeze.default(getitem_1733, 4);  getitem_1733 = None
        permute_1043 = torch.ops.aten.permute.default(unsqueeze_659, [0, 4, 2, 3, 1]);  unsqueeze_659 = None
        permute_1044 = torch.ops.aten.permute.default(permute_1043, [3, 4, 0, 2, 1]);  permute_1043 = None
        view_1868 = torch.ops.aten.view.default(permute_1044, [256, 512, 512]);  permute_1044 = None
        bmm_159 = torch.ops.aten.bmm.default(view_1866, view_1868);  view_1866 = view_1868 = None
        view_1869 = torch.ops.aten.view.default(bmm_159, [256, 512, 1, 1, 512]);  bmm_159 = None
        permute_1045 = torch.ops.aten.permute.default(view_1869, [3, 1, 4, 0, 2]);  view_1869 = None
        view_1870 = torch.ops.aten.view.default(permute_1045, [1, 512, 512, 256]);  permute_1045 = None
        _to_copy_1060 = torch.ops.aten._to_copy.default(view_1854, dtype = torch.float32);  view_1854 = None
        native_layer_norm_default_219 = torch.ops.aten.native_layer_norm.default(_to_copy_1060, [256], None, None, 1e-05);  _to_copy_1060 = None
        getitem_1734 = native_layer_norm_default_219[0]
        _to_copy_1061 = torch.ops.aten._to_copy.default(view_1870, dtype = torch.float32);  view_1870 = None
        native_layer_norm_default_220 = torch.ops.aten.native_layer_norm.default(_to_copy_1061, [256], None, None, 1e-05);  _to_copy_1061 = None
        getitem_1737 = native_layer_norm_default_220[0]
        add_184 = torch.ops.aten.add.Tensor(getitem_1734, getitem_1737);  getitem_1734 = getitem_1737 = None
        _to_copy_1062 = torch.ops.aten._to_copy.default(arg566_1, dtype = torch.bfloat16);  arg566_1 = None
        _to_copy_1063 = torch.ops.aten._to_copy.default(add_184, dtype = torch.bfloat16);  add_184 = None
        t_385 = torch.ops.aten.t.default(_to_copy_1062);  _to_copy_1062 = None
        view_1871 = torch.ops.aten.view.default(_to_copy_1063, [262144, 256]);  _to_copy_1063 = None
        mm_359 = torch.ops.aten.mm.default(view_1871, t_385);  view_1871 = t_385 = None
        view_1872 = torch.ops.aten.view.default(mm_359, [1, 512, 512, 256]);  mm_359 = None
        _to_copy_1064 = torch.ops.aten._to_copy.default(getitem_1721, dtype = torch.bfloat16);  getitem_1721 = None
        _to_copy_1065 = torch.ops.aten._to_copy.default(getitem_1714, dtype = torch.bfloat16);  getitem_1714 = None
        t_386 = torch.ops.aten.t.default(_to_copy_1064);  _to_copy_1064 = None
        view_1873 = torch.ops.aten.view.default(_to_copy_1065, [262144, 256]);  _to_copy_1065 = None
        mm_360 = torch.ops.aten.mm.default(view_1873, t_386);  view_1873 = t_386 = None
        view_1874 = torch.ops.aten.view.default(mm_360, [1, 512, 512, 256]);  mm_360 = None
        sigmoid_140 = torch.ops.aten.sigmoid.default(view_1874);  view_1874 = None
        mul_229 = torch.ops.aten.mul.Tensor(view_1872, sigmoid_140);  view_1872 = sigmoid_140 = None
        add_185 = torch.ops.aten.add.Tensor(add_179, mul_229);  mul_229 = None
        _to_copy_1066 = torch.ops.aten._to_copy.default(add_179, dtype = torch.float32)
        native_layer_norm_default_221 = torch.ops.aten.native_layer_norm.default(_to_copy_1066, [256], None, None, 1e-05);  _to_copy_1066 = None
        getitem_1740 = native_layer_norm_default_221[0]
        _to_copy_1067 = torch.ops.aten._to_copy.default(arg570_1, dtype = torch.bfloat16);  arg570_1 = None
        _to_copy_1068 = torch.ops.aten._to_copy.default(getitem_1740, dtype = torch.bfloat16)
        t_387 = torch.ops.aten.t.default(_to_copy_1067);  _to_copy_1067 = None
        view_1875 = torch.ops.aten.view.default(_to_copy_1068, [262144, 256]);  _to_copy_1068 = None
        mm_361 = torch.ops.aten.mm.default(view_1875, t_387);  view_1875 = t_387 = None
        view_1876 = torch.ops.aten.view.default(mm_361, [1, 512, 512, 8]);  mm_361 = None
        view_1877 = torch.ops.aten.view.default(view_1876, [1, 512, 512, 2, 4]);  view_1876 = None
        permute_1046 = torch.ops.aten.permute.default(view_1877, [0, 3, 4, 1, 2]);  view_1877 = None
        view_1878 = torch.ops.aten.view.default(permute_1046, [1, 2, 4, 1, 512, 512]);  permute_1046 = None
        view_1879 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_124 = torch.ops.aten.bitwise_not.default(view_1879);  view_1879 = None
        masked_fill_124 = torch.ops.aten.masked_fill.Scalar(view_1878, bitwise_not_124, -10000);  view_1878 = bitwise_not_124 = None
        view_1880 = torch.ops.aten.view.default(masked_fill_124, [1, 2, 4, 512, 512]);  masked_fill_124 = None
        permute_1047 = torch.ops.aten.permute.default(view_1880, [1, 0, 2, 3, 4]);  view_1880 = None
        view_1881 = torch.ops.aten.view.default(permute_1047, [2, 4, 1, 512, 512]);  permute_1047 = None
        _to_copy_1069 = torch.ops.aten._to_copy.default(arg571_1, dtype = torch.bfloat16);  arg571_1 = None
        _to_copy_1070 = torch.ops.aten._to_copy.default(getitem_1740, dtype = torch.bfloat16)
        t_388 = torch.ops.aten.t.default(_to_copy_1069);  _to_copy_1069 = None
        view_1882 = torch.ops.aten.view.default(_to_copy_1070, [262144, 256]);  _to_copy_1070 = None
        mm_362 = torch.ops.aten.mm.default(view_1882, t_388);  view_1882 = t_388 = None
        view_1883 = torch.ops.aten.view.default(mm_362, [1, 512, 512, 1024]);  mm_362 = None
        select_45 = torch.ops.aten.select.int(view_1881, 0, 0)
        view_1884 = torch.ops.aten.view.default(view_1883, [1, 512, 512, 4, 4, 64]);  view_1883 = None
        permute_1048 = torch.ops.aten.permute.default(view_1884, [4, 0, 3, 1, 2, 5]);  view_1884 = None
        view_1885 = torch.ops.aten.view.default(permute_1048, [4, 4, 512, 512, 64]);  permute_1048 = None
        unbind_int_88 = torch.ops.aten.unbind.int(view_1885);  view_1885 = None
        getitem_1743 = unbind_int_88[0]
        getitem_1744 = unbind_int_88[1]
        getitem_1745 = unbind_int_88[2]
        getitem_1746 = unbind_int_88[3];  unbind_int_88 = None
        expand_107 = torch.ops.aten.expand.default(select_45, [4, 512, 512, 512]);  select_45 = None
        _scaled_dot_product_efficient_attention_default_60 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1743, getitem_1744, getitem_1745, expand_107, False);  getitem_1743 = getitem_1744 = getitem_1745 = expand_107 = None
        getitem_1747 = _scaled_dot_product_efficient_attention_default_60[0]
        sigmoid_141 = torch.ops.aten.sigmoid.default(getitem_1746);  getitem_1746 = None
        mul_230 = torch.ops.aten.mul.Tensor(getitem_1747, sigmoid_141);  getitem_1747 = sigmoid_141 = None
        view_1886 = torch.ops.aten.view.default(mul_230, [1, 4, 512, 512, 64]);  mul_230 = None
        permute_1049 = torch.ops.aten.permute.default(view_1886, [0, 2, 3, 1, 4]);  view_1886 = None
        clone_176 = torch.ops.aten.clone.default(permute_1049, memory_format = torch.contiguous_format);  permute_1049 = None
        _unsafe_view_152 = torch.ops.aten._unsafe_view.default(clone_176, [1, 512, 512, 256]);  clone_176 = None
        transpose_45 = torch.ops.aten.transpose.int(getitem_1740, 1, 2);  getitem_1740 = None
        _to_copy_1071 = torch.ops.aten._to_copy.default(arg572_1, dtype = torch.bfloat16);  arg572_1 = None
        _to_copy_1072 = torch.ops.aten._to_copy.default(transpose_45, dtype = torch.bfloat16);  transpose_45 = None
        t_389 = torch.ops.aten.t.default(_to_copy_1071);  _to_copy_1071 = None
        expand_108 = torch.ops.aten.expand.default(_to_copy_1072, [1, 512, 512, 256]);  _to_copy_1072 = None
        view_1887 = torch.ops.aten.view.default(expand_108, [512, 512, 256]);  expand_108 = None
        expand_109 = torch.ops.aten.expand.default(t_389, [1, 512, 256, 1024]);  t_389 = None
        view_1888 = torch.ops.aten.view.default(expand_109, [512, 256, 1024]);  expand_109 = None
        bmm_160 = torch.ops.aten.bmm.default(view_1887, view_1888);  view_1887 = view_1888 = None
        view_1889 = torch.ops.aten.view.default(bmm_160, [1, 512, 512, 1024]);  bmm_160 = None
        select_46 = torch.ops.aten.select.int(view_1881, 0, 1);  view_1881 = None
        view_1890 = torch.ops.aten.view.default(view_1889, [1, 512, 512, 4, 4, 64]);  view_1889 = None
        permute_1050 = torch.ops.aten.permute.default(view_1890, [4, 0, 3, 1, 2, 5]);  view_1890 = None
        view_1891 = torch.ops.aten.view.default(permute_1050, [4, 4, 512, 512, 64]);  permute_1050 = None
        unbind_int_89 = torch.ops.aten.unbind.int(view_1891);  view_1891 = None
        getitem_1751 = unbind_int_89[0]
        getitem_1752 = unbind_int_89[1]
        getitem_1753 = unbind_int_89[2]
        getitem_1754 = unbind_int_89[3];  unbind_int_89 = None
        expand_110 = torch.ops.aten.expand.default(select_46, [4, 512, 512, 512]);  select_46 = None
        _scaled_dot_product_efficient_attention_default_61 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1751, getitem_1752, getitem_1753, expand_110, False);  getitem_1751 = getitem_1752 = getitem_1753 = expand_110 = None
        getitem_1755 = _scaled_dot_product_efficient_attention_default_61[0]
        sigmoid_142 = torch.ops.aten.sigmoid.default(getitem_1754);  getitem_1754 = None
        mul_231 = torch.ops.aten.mul.Tensor(getitem_1755, sigmoid_142);  getitem_1755 = sigmoid_142 = None
        view_1892 = torch.ops.aten.view.default(mul_231, [1, 4, 512, 512, 64]);  mul_231 = None
        permute_1051 = torch.ops.aten.permute.default(view_1892, [0, 2, 3, 1, 4]);  view_1892 = None
        clone_177 = torch.ops.aten.clone.default(permute_1051, memory_format = torch.contiguous_format);  permute_1051 = None
        _unsafe_view_153 = torch.ops.aten._unsafe_view.default(clone_177, [1, 512, 512, 256]);  clone_177 = None
        cat_28 = torch.ops.aten.cat.default([_unsafe_view_152, _unsafe_view_153], dim = -1);  _unsafe_view_152 = _unsafe_view_153 = None
        slice_221 = torch.ops.aten.slice.Tensor(arg569_1, dim = 0, start = 0, end = 9223372036854775807);  arg569_1 = None
        unsqueeze_660 = torch.ops.aten.unsqueeze.default(slice_221, 1);  slice_221 = None
        mul_232 = torch.ops.aten.mul.Tensor(arg573_1, unsqueeze_660);  arg573_1 = unsqueeze_660 = None
        _to_copy_1073 = torch.ops.aten._to_copy.default(mul_232, dtype = torch.bfloat16);  mul_232 = None
        t_390 = torch.ops.aten.t.default(_to_copy_1073);  _to_copy_1073 = None
        view_1893 = torch.ops.aten.view.default(cat_28, [262144, 512]);  cat_28 = None
        mm_363 = torch.ops.aten.mm.default(view_1893, t_390);  view_1893 = t_390 = None
        view_1894 = torch.ops.aten.view.default(mm_363, [1, 512, 512, 256]);  mm_363 = None
        add_186 = torch.ops.aten.add.Tensor(add_185, view_1894);  add_185 = view_1894 = None
        split_tensor_183 = torch.ops.aten.split.Tensor(add_179, 512, dim = -2)
        getitem_1759 = split_tensor_183[0];  split_tensor_183 = None
        _to_copy_1074 = torch.ops.aten._to_copy.default(getitem_1759, dtype = torch.float32);  getitem_1759 = None
        native_layer_norm_default_222 = torch.ops.aten.native_layer_norm.default(_to_copy_1074, [256], arg560_1, arg561_1, 1e-05);  _to_copy_1074 = arg560_1 = arg561_1 = None
        getitem_1760 = native_layer_norm_default_222[0]
        _to_copy_1075 = torch.ops.aten._to_copy.default(arg562_1, dtype = torch.bfloat16);  arg562_1 = None
        _to_copy_1076 = torch.ops.aten._to_copy.default(getitem_1760, dtype = torch.bfloat16);  getitem_1760 = None
        t_391 = torch.ops.aten.t.default(_to_copy_1075);  _to_copy_1075 = None
        view_1895 = torch.ops.aten.view.default(_to_copy_1076, [262144, 256]);  _to_copy_1076 = None
        mm_364 = torch.ops.aten.mm.default(view_1895, t_391);  view_1895 = t_391 = None
        view_1896 = torch.ops.aten.view.default(mm_364, [1, 512, 512, 1024]);  mm_364 = None
        split_tensor_184 = torch.ops.aten.split.Tensor(view_1896, 512, dim = -1);  view_1896 = None
        getitem_1763 = split_tensor_184[0]
        getitem_1764 = split_tensor_184[1];  split_tensor_184 = None
        silu_50 = torch.ops.aten.silu.default(getitem_1763);  getitem_1763 = None
        mul_233 = torch.ops.aten.mul.Tensor(silu_50, getitem_1764);  silu_50 = getitem_1764 = None
        _to_copy_1077 = torch.ops.aten._to_copy.default(arg563_1, dtype = torch.bfloat16);  arg563_1 = None
        t_392 = torch.ops.aten.t.default(_to_copy_1077);  _to_copy_1077 = None
        view_1898 = torch.ops.aten.view.default(mul_233, [262144, 512]);  mul_233 = None
        mm_365 = torch.ops.aten.mm.default(view_1898, t_392);  view_1898 = t_392 = None
        view_1899 = torch.ops.aten.view.default(mm_365, [1, 512, 512, 256]);  mm_365 = None
        add_187 = torch.ops.aten.add.Tensor(add_186, view_1899);  add_186 = view_1899 = None
        _to_copy_1078 = torch.ops.aten._to_copy.default(add_183, dtype = torch.float32)
        native_layer_norm_default_223 = torch.ops.aten.native_layer_norm.default(_to_copy_1078, [384], arg578_1, arg579_1, 1e-05);  _to_copy_1078 = arg578_1 = arg579_1 = None
        getitem_1765 = native_layer_norm_default_223[0]
        _to_copy_1079 = torch.ops.aten._to_copy.default(add_179, dtype = torch.float32);  add_179 = None
        native_layer_norm_default_224 = torch.ops.aten.native_layer_norm.default(_to_copy_1079, [256], arg580_1, arg581_1, 1e-05);  _to_copy_1079 = arg580_1 = arg581_1 = None
        getitem_1768 = native_layer_norm_default_224[0]
        _to_copy_1080 = torch.ops.aten._to_copy.default(arg582_1, dtype = torch.bfloat16);  arg582_1 = None
        _to_copy_1081 = torch.ops.aten._to_copy.default(getitem_1768, dtype = torch.bfloat16);  getitem_1768 = None
        t_393 = torch.ops.aten.t.default(_to_copy_1080);  _to_copy_1080 = None
        view_1900 = torch.ops.aten.view.default(_to_copy_1081, [262144, 256]);  _to_copy_1081 = None
        mm_366 = torch.ops.aten.mm.default(view_1900, t_393);  view_1900 = t_393 = None
        view_1901 = torch.ops.aten.view.default(mm_366, [1, 512, 512, 16]);  mm_366 = None
        permute_1052 = torch.ops.aten.permute.default(view_1901, [0, 3, 1, 2]);  view_1901 = None
        view_1902 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_125 = torch.ops.aten.bitwise_not.default(view_1902);  view_1902 = None
        masked_fill_125 = torch.ops.aten.masked_fill.Scalar(permute_1052, bitwise_not_125, -10000);  permute_1052 = bitwise_not_125 = None
        _to_copy_1082 = torch.ops.aten._to_copy.default(getitem_1765, dtype = torch.bfloat16);  getitem_1765 = None
        _to_copy_1083 = torch.ops.aten._to_copy.default(arg584_1, dtype = torch.bfloat16);  arg584_1 = None
        unsqueeze_661 = torch.ops.aten.unsqueeze.default(_to_copy_1082, 3);  _to_copy_1082 = None
        unsqueeze_662 = torch.ops.aten.unsqueeze.default(unsqueeze_661, 4);  unsqueeze_661 = None
        unsqueeze_663 = torch.ops.aten.unsqueeze.default(unsqueeze_662, 5);  unsqueeze_662 = None
        permute_1053 = torch.ops.aten.permute.default(unsqueeze_663, [3, 0, 4, 1, 5, 2]);  unsqueeze_663 = None
        unsqueeze_664 = torch.ops.aten.unsqueeze.default(_to_copy_1083, 4);  _to_copy_1083 = None
        unsqueeze_665 = torch.ops.aten.unsqueeze.default(unsqueeze_664, 5);  unsqueeze_664 = None
        permute_1054 = torch.ops.aten.permute.default(unsqueeze_665, [1, 4, 2, 5, 3, 0]);  unsqueeze_665 = None
        permute_1055 = torch.ops.aten.permute.default(permute_1053, [3, 5, 0, 1, 2, 4]);  permute_1053 = None
        view_1903 = torch.ops.aten.view.default(permute_1055, [1, 512, 384]);  permute_1055 = None
        permute_1056 = torch.ops.aten.permute.default(permute_1054, [5, 0, 1, 2, 4, 3]);  permute_1054 = None
        view_1904 = torch.ops.aten.view.default(permute_1056, [1, 384, 1536]);  permute_1056 = None
        bmm_161 = torch.ops.aten.bmm.default(view_1903, view_1904);  view_1903 = view_1904 = None
        view_1905 = torch.ops.aten.view.default(bmm_161, [512, 1, 4, 1, 16, 24]);  bmm_161 = None
        permute_1057 = torch.ops.aten.permute.default(view_1905, [2, 3, 4, 0, 5, 1]);  view_1905 = None
        view_1906 = torch.ops.aten.view.default(permute_1057, [4, 1, 16, 512, 24]);  permute_1057 = None
        unbind_int_90 = torch.ops.aten.unbind.int(view_1906);  view_1906 = None
        getitem_1771 = unbind_int_90[0]
        getitem_1772 = unbind_int_90[1]
        getitem_1773 = unbind_int_90[2]
        getitem_1774 = unbind_int_90[3];  unbind_int_90 = None
        view_1907 = torch.ops.aten.view.default(arg583_1, [1, 16, 1, 24]);  arg583_1 = None
        add_188 = torch.ops.aten.add.Tensor(getitem_1771, view_1907);  getitem_1771 = view_1907 = None
        _to_copy_1084 = torch.ops.aten._to_copy.default(add_188, dtype = torch.bfloat16);  add_188 = None
        expand_111 = torch.ops.aten.expand.default(masked_fill_125, [1, 16, 512, 512]);  masked_fill_125 = None
        _scaled_dot_product_efficient_attention_default_62 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1084, getitem_1772, getitem_1773, expand_111, False);  _to_copy_1084 = getitem_1772 = getitem_1773 = expand_111 = None
        getitem_1775 = _scaled_dot_product_efficient_attention_default_62[0]
        add_189 = torch.ops.aten.add.Tensor(getitem_1774, 1);  getitem_1774 = None
        sigmoid_143 = torch.ops.aten.sigmoid.default(add_189);  add_189 = None
        mul_234 = torch.ops.aten.mul.Tensor(getitem_1775, sigmoid_143);  getitem_1775 = sigmoid_143 = None
        _to_copy_1085 = torch.ops.aten._to_copy.default(arg585_1, dtype = torch.bfloat16);  arg585_1 = None
        unsqueeze_666 = torch.ops.aten.unsqueeze.default(mul_234, 4);  mul_234 = None
        permute_1058 = torch.ops.aten.permute.default(unsqueeze_666, [0, 2, 4, 3, 1]);  unsqueeze_666 = None
        unsqueeze_667 = torch.ops.aten.unsqueeze.default(_to_copy_1085, 3);  _to_copy_1085 = None
        unsqueeze_668 = torch.ops.aten.unsqueeze.default(unsqueeze_667, 4);  unsqueeze_667 = None
        permute_1059 = torch.ops.aten.permute.default(unsqueeze_668, [3, 4, 2, 1, 0]);  unsqueeze_668 = None
        permute_1060 = torch.ops.aten.permute.default(permute_1058, [1, 3, 4, 0, 2]);  permute_1058 = None
        clone_178 = torch.ops.aten.clone.default(permute_1060, memory_format = torch.contiguous_format);  permute_1060 = None
        _unsafe_view_154 = torch.ops.aten._unsafe_view.default(clone_178, [1, 512, 384]);  clone_178 = None
        permute_1061 = torch.ops.aten.permute.default(permute_1059, [3, 4, 0, 2, 1]);  permute_1059 = None
        clone_179 = torch.ops.aten.clone.default(permute_1061, memory_format = torch.contiguous_format);  permute_1061 = None
        _unsafe_view_155 = torch.ops.aten._unsafe_view.default(clone_179, [1, 384, 384]);  clone_179 = None
        bmm_162 = torch.ops.aten.bmm.default(_unsafe_view_154, _unsafe_view_155);  _unsafe_view_154 = _unsafe_view_155 = None
        view_1908 = torch.ops.aten.view.default(bmm_162, [512, 1, 1, 1, 384]);  bmm_162 = None
        permute_1062 = torch.ops.aten.permute.default(view_1908, [3, 0, 4, 1, 2]);  view_1908 = None
        view_1909 = torch.ops.aten.view.default(permute_1062, [1, 512, 384]);  permute_1062 = None
        unsqueeze_669 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_235 = torch.ops.aten.mul.Tensor(view_1909, unsqueeze_669);  view_1909 = unsqueeze_669 = None
        add_190 = torch.ops.aten.add.Tensor(add_183, mul_235);  mul_235 = None
        split_tensor_185 = torch.ops.aten.split.Tensor(add_183, 512, dim = -2);  add_183 = None
        getitem_1779 = split_tensor_185[0];  split_tensor_185 = None
        _to_copy_1086 = torch.ops.aten._to_copy.default(getitem_1779, dtype = torch.float32);  getitem_1779 = None
        native_layer_norm_default_225 = torch.ops.aten.native_layer_norm.default(_to_copy_1086, [384], arg574_1, arg575_1, 1e-05);  _to_copy_1086 = arg574_1 = arg575_1 = None
        getitem_1780 = native_layer_norm_default_225[0]
        _to_copy_1087 = torch.ops.aten._to_copy.default(arg576_1, dtype = torch.bfloat16);  arg576_1 = None
        _to_copy_1088 = torch.ops.aten._to_copy.default(getitem_1780, dtype = torch.bfloat16);  getitem_1780 = None
        t_394 = torch.ops.aten.t.default(_to_copy_1087);  _to_copy_1087 = None
        view_1910 = torch.ops.aten.view.default(_to_copy_1088, [512, 384]);  _to_copy_1088 = None
        mm_367 = torch.ops.aten.mm.default(view_1910, t_394);  view_1910 = t_394 = None
        view_1911 = torch.ops.aten.view.default(mm_367, [1, 512, 1536]);  mm_367 = None
        split_tensor_186 = torch.ops.aten.split.Tensor(view_1911, 768, dim = -1);  view_1911 = None
        getitem_1783 = split_tensor_186[0]
        getitem_1784 = split_tensor_186[1];  split_tensor_186 = None
        silu_51 = torch.ops.aten.silu.default(getitem_1783);  getitem_1783 = None
        mul_236 = torch.ops.aten.mul.Tensor(silu_51, getitem_1784);  silu_51 = getitem_1784 = None
        _to_copy_1089 = torch.ops.aten._to_copy.default(arg577_1, dtype = torch.bfloat16);  arg577_1 = None
        t_395 = torch.ops.aten.t.default(_to_copy_1089);  _to_copy_1089 = None
        view_1913 = torch.ops.aten.view.default(mul_236, [512, 768]);  mul_236 = None
        mm_368 = torch.ops.aten.mm.default(view_1913, t_395);  view_1913 = t_395 = None
        view_1914 = torch.ops.aten.view.default(mm_368, [1, 512, 384]);  mm_368 = None
        add_191 = torch.ops.aten.add.Tensor(add_190, view_1914);  add_190 = view_1914 = None
        _to_copy_1090 = torch.ops.aten._to_copy.default(add_187, dtype = torch.float32)
        native_layer_norm_default_226 = torch.ops.aten.native_layer_norm.default(_to_copy_1090, [256], arg590_1, arg591_1, 1e-05);  _to_copy_1090 = arg590_1 = arg591_1 = None
        getitem_1785 = native_layer_norm_default_226[0]
        split_with_sizes_default_46 = torch.ops.aten.split_with_sizes.default(arg593_1, [512, 512]);  arg593_1 = None
        getitem_1788 = split_with_sizes_default_46[0]
        getitem_1789 = split_with_sizes_default_46[1];  split_with_sizes_default_46 = None
        split_with_sizes_default_47 = torch.ops.aten.split_with_sizes.default(arg594_1, [512, 512, 256]);  arg594_1 = None
        getitem_1790 = split_with_sizes_default_47[0]
        getitem_1791 = split_with_sizes_default_47[1]
        getitem_1792 = split_with_sizes_default_47[2];  split_with_sizes_default_47 = None
        _to_copy_1091 = torch.ops.aten._to_copy.default(getitem_1788, dtype = torch.bfloat16);  getitem_1788 = None
        _to_copy_1092 = torch.ops.aten._to_copy.default(getitem_1785, dtype = torch.bfloat16)
        t_396 = torch.ops.aten.t.default(_to_copy_1091);  _to_copy_1091 = None
        view_1915 = torch.ops.aten.view.default(_to_copy_1092, [262144, 256]);  _to_copy_1092 = None
        mm_369 = torch.ops.aten.mm.default(view_1915, t_396);  view_1915 = t_396 = None
        view_1916 = torch.ops.aten.view.default(mm_369, [1, 512, 512, 512]);  mm_369 = None
        _to_copy_1093 = torch.ops.aten._to_copy.default(getitem_1790, dtype = torch.bfloat16);  getitem_1790 = None
        _to_copy_1094 = torch.ops.aten._to_copy.default(getitem_1785, dtype = torch.bfloat16)
        t_397 = torch.ops.aten.t.default(_to_copy_1093);  _to_copy_1093 = None
        view_1917 = torch.ops.aten.view.default(_to_copy_1094, [262144, 256]);  _to_copy_1094 = None
        mm_370 = torch.ops.aten.mm.default(view_1917, t_397);  view_1917 = t_397 = None
        view_1918 = torch.ops.aten.view.default(mm_370, [1, 512, 512, 512]);  mm_370 = None
        sigmoid_144 = torch.ops.aten.sigmoid.default(view_1918);  view_1918 = None
        mul_237 = torch.ops.aten.mul.Tensor(view_1916, sigmoid_144);  view_1916 = sigmoid_144 = None
        unsqueeze_670 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_126 = torch.ops.aten.bitwise_not.default(unsqueeze_670);  unsqueeze_670 = None
        masked_fill_126 = torch.ops.aten.masked_fill.Scalar(mul_237, bitwise_not_126, 0);  mul_237 = bitwise_not_126 = None
        split_tensor_187 = torch.ops.aten.split.Tensor(masked_fill_126, 256, dim = -1)
        getitem_1795 = split_tensor_187[0]
        unsqueeze_673 = torch.ops.aten.unsqueeze.default(getitem_1795, 4);  getitem_1795 = None
        permute_1067 = torch.ops.aten.permute.default(unsqueeze_673, [0, 1, 4, 3, 2]);  unsqueeze_673 = None
        permute_1068 = torch.ops.aten.permute.default(permute_1067, [3, 1, 4, 0, 2]);  permute_1067 = None
        view_1921 = torch.ops.aten.view.default(permute_1068, [256, 512, 512]);  permute_1068 = None
        split_tensor_188 = torch.ops.aten.split.Tensor(masked_fill_126, 256, dim = -1);  masked_fill_126 = None
        getitem_1798 = split_tensor_188[1];  split_tensor_188 = None
        unsqueeze_674 = torch.ops.aten.unsqueeze.default(getitem_1798, 4);  getitem_1798 = None
        permute_1069 = torch.ops.aten.permute.default(unsqueeze_674, [0, 4, 1, 3, 2]);  unsqueeze_674 = None
        permute_1070 = torch.ops.aten.permute.default(permute_1069, [3, 4, 0, 2, 1]);  permute_1069 = None
        view_1922 = torch.ops.aten.view.default(permute_1070, [256, 512, 512]);  permute_1070 = None
        bmm_163 = torch.ops.aten.bmm.default(view_1921, view_1922);  view_1921 = view_1922 = None
        view_1923 = torch.ops.aten.view.default(bmm_163, [256, 512, 1, 1, 512]);  bmm_163 = None
        permute_1071 = torch.ops.aten.permute.default(view_1923, [3, 1, 4, 0, 2]);  view_1923 = None
        view_1924 = torch.ops.aten.view.default(permute_1071, [1, 512, 512, 256]);  permute_1071 = None
        _to_copy_1095 = torch.ops.aten._to_copy.default(getitem_1789, dtype = torch.bfloat16);  getitem_1789 = None
        _to_copy_1096 = torch.ops.aten._to_copy.default(getitem_1785, dtype = torch.bfloat16)
        t_398 = torch.ops.aten.t.default(_to_copy_1095);  _to_copy_1095 = None
        view_1925 = torch.ops.aten.view.default(_to_copy_1096, [262144, 256]);  _to_copy_1096 = None
        mm_371 = torch.ops.aten.mm.default(view_1925, t_398);  view_1925 = t_398 = None
        view_1926 = torch.ops.aten.view.default(mm_371, [1, 512, 512, 512]);  mm_371 = None
        _to_copy_1097 = torch.ops.aten._to_copy.default(getitem_1791, dtype = torch.bfloat16);  getitem_1791 = None
        _to_copy_1098 = torch.ops.aten._to_copy.default(getitem_1785, dtype = torch.bfloat16)
        t_399 = torch.ops.aten.t.default(_to_copy_1097);  _to_copy_1097 = None
        view_1927 = torch.ops.aten.view.default(_to_copy_1098, [262144, 256]);  _to_copy_1098 = None
        mm_372 = torch.ops.aten.mm.default(view_1927, t_399);  view_1927 = t_399 = None
        view_1928 = torch.ops.aten.view.default(mm_372, [1, 512, 512, 512]);  mm_372 = None
        sigmoid_145 = torch.ops.aten.sigmoid.default(view_1928);  view_1928 = None
        mul_238 = torch.ops.aten.mul.Tensor(view_1926, sigmoid_145);  view_1926 = sigmoid_145 = None
        view_1929 = torch.ops.aten.view.default(mul_238, [262144, 512]);  mul_238 = None
        view_1930 = torch.ops.aten.view.default(view_1929, [1, 512, 512, 512]);  view_1929 = None
        transpose_46 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_675 = torch.ops.aten.unsqueeze.default(transpose_46, 3);  transpose_46 = None
        clone_180 = torch.ops.aten.clone.default(unsqueeze_675, memory_format = torch.contiguous_format);  unsqueeze_675 = None
        bitwise_not_127 = torch.ops.aten.bitwise_not.default(clone_180);  clone_180 = None
        masked_fill_127 = torch.ops.aten.masked_fill.Scalar(view_1930, bitwise_not_127, 0);  view_1930 = bitwise_not_127 = None
        view_1931 = torch.ops.aten.view.default(masked_fill_127, [262144, 512]);  masked_fill_127 = None
        view_1935 = torch.ops.aten.view.default(view_1931, [1, 512, 512, 512])
        split_tensor_189 = torch.ops.aten.split.Tensor(view_1935, 256, dim = -1);  view_1935 = None
        getitem_1801 = split_tensor_189[0]
        unsqueeze_678 = torch.ops.aten.unsqueeze.default(getitem_1801, 4);  getitem_1801 = None
        permute_1076 = torch.ops.aten.permute.default(unsqueeze_678, [0, 2, 4, 3, 1]);  unsqueeze_678 = None
        permute_1077 = torch.ops.aten.permute.default(permute_1076, [3, 1, 4, 0, 2]);  permute_1076 = None
        view_1936 = torch.ops.aten.view.default(permute_1077, [256, 512, 512]);  permute_1077 = None
        view_1937 = torch.ops.aten.view.default(view_1931, [1, 512, 512, 512]);  view_1931 = None
        split_tensor_190 = torch.ops.aten.split.Tensor(view_1937, 256, dim = -1);  view_1937 = None
        getitem_1804 = split_tensor_190[1];  split_tensor_190 = None
        unsqueeze_679 = torch.ops.aten.unsqueeze.default(getitem_1804, 4);  getitem_1804 = None
        permute_1078 = torch.ops.aten.permute.default(unsqueeze_679, [0, 4, 2, 3, 1]);  unsqueeze_679 = None
        permute_1079 = torch.ops.aten.permute.default(permute_1078, [3, 4, 0, 2, 1]);  permute_1078 = None
        view_1938 = torch.ops.aten.view.default(permute_1079, [256, 512, 512]);  permute_1079 = None
        bmm_164 = torch.ops.aten.bmm.default(view_1936, view_1938);  view_1936 = view_1938 = None
        view_1939 = torch.ops.aten.view.default(bmm_164, [256, 512, 1, 1, 512]);  bmm_164 = None
        permute_1080 = torch.ops.aten.permute.default(view_1939, [3, 1, 4, 0, 2]);  view_1939 = None
        view_1940 = torch.ops.aten.view.default(permute_1080, [1, 512, 512, 256]);  permute_1080 = None
        _to_copy_1099 = torch.ops.aten._to_copy.default(view_1924, dtype = torch.float32);  view_1924 = None
        native_layer_norm_default_227 = torch.ops.aten.native_layer_norm.default(_to_copy_1099, [256], None, None, 1e-05);  _to_copy_1099 = None
        getitem_1805 = native_layer_norm_default_227[0]
        _to_copy_1100 = torch.ops.aten._to_copy.default(view_1940, dtype = torch.float32);  view_1940 = None
        native_layer_norm_default_228 = torch.ops.aten.native_layer_norm.default(_to_copy_1100, [256], None, None, 1e-05);  _to_copy_1100 = None
        getitem_1808 = native_layer_norm_default_228[0]
        add_192 = torch.ops.aten.add.Tensor(getitem_1805, getitem_1808);  getitem_1805 = getitem_1808 = None
        _to_copy_1101 = torch.ops.aten._to_copy.default(arg592_1, dtype = torch.bfloat16);  arg592_1 = None
        _to_copy_1102 = torch.ops.aten._to_copy.default(add_192, dtype = torch.bfloat16);  add_192 = None
        t_400 = torch.ops.aten.t.default(_to_copy_1101);  _to_copy_1101 = None
        view_1941 = torch.ops.aten.view.default(_to_copy_1102, [262144, 256]);  _to_copy_1102 = None
        mm_373 = torch.ops.aten.mm.default(view_1941, t_400);  view_1941 = t_400 = None
        view_1942 = torch.ops.aten.view.default(mm_373, [1, 512, 512, 256]);  mm_373 = None
        _to_copy_1103 = torch.ops.aten._to_copy.default(getitem_1792, dtype = torch.bfloat16);  getitem_1792 = None
        _to_copy_1104 = torch.ops.aten._to_copy.default(getitem_1785, dtype = torch.bfloat16);  getitem_1785 = None
        t_401 = torch.ops.aten.t.default(_to_copy_1103);  _to_copy_1103 = None
        view_1943 = torch.ops.aten.view.default(_to_copy_1104, [262144, 256]);  _to_copy_1104 = None
        mm_374 = torch.ops.aten.mm.default(view_1943, t_401);  view_1943 = t_401 = None
        view_1944 = torch.ops.aten.view.default(mm_374, [1, 512, 512, 256]);  mm_374 = None
        sigmoid_146 = torch.ops.aten.sigmoid.default(view_1944);  view_1944 = None
        mul_239 = torch.ops.aten.mul.Tensor(view_1942, sigmoid_146);  view_1942 = sigmoid_146 = None
        add_193 = torch.ops.aten.add.Tensor(add_187, mul_239);  mul_239 = None
        _to_copy_1105 = torch.ops.aten._to_copy.default(add_187, dtype = torch.float32)
        native_layer_norm_default_229 = torch.ops.aten.native_layer_norm.default(_to_copy_1105, [256], None, None, 1e-05);  _to_copy_1105 = None
        getitem_1811 = native_layer_norm_default_229[0]
        _to_copy_1106 = torch.ops.aten._to_copy.default(arg596_1, dtype = torch.bfloat16);  arg596_1 = None
        _to_copy_1107 = torch.ops.aten._to_copy.default(getitem_1811, dtype = torch.bfloat16)
        t_402 = torch.ops.aten.t.default(_to_copy_1106);  _to_copy_1106 = None
        view_1945 = torch.ops.aten.view.default(_to_copy_1107, [262144, 256]);  _to_copy_1107 = None
        mm_375 = torch.ops.aten.mm.default(view_1945, t_402);  view_1945 = t_402 = None
        view_1946 = torch.ops.aten.view.default(mm_375, [1, 512, 512, 8]);  mm_375 = None
        view_1947 = torch.ops.aten.view.default(view_1946, [1, 512, 512, 2, 4]);  view_1946 = None
        permute_1081 = torch.ops.aten.permute.default(view_1947, [0, 3, 4, 1, 2]);  view_1947 = None
        view_1948 = torch.ops.aten.view.default(permute_1081, [1, 2, 4, 1, 512, 512]);  permute_1081 = None
        view_1949 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_128 = torch.ops.aten.bitwise_not.default(view_1949);  view_1949 = None
        masked_fill_128 = torch.ops.aten.masked_fill.Scalar(view_1948, bitwise_not_128, -10000);  view_1948 = bitwise_not_128 = None
        view_1950 = torch.ops.aten.view.default(masked_fill_128, [1, 2, 4, 512, 512]);  masked_fill_128 = None
        permute_1082 = torch.ops.aten.permute.default(view_1950, [1, 0, 2, 3, 4]);  view_1950 = None
        view_1951 = torch.ops.aten.view.default(permute_1082, [2, 4, 1, 512, 512]);  permute_1082 = None
        _to_copy_1108 = torch.ops.aten._to_copy.default(arg597_1, dtype = torch.bfloat16);  arg597_1 = None
        _to_copy_1109 = torch.ops.aten._to_copy.default(getitem_1811, dtype = torch.bfloat16)
        t_403 = torch.ops.aten.t.default(_to_copy_1108);  _to_copy_1108 = None
        view_1952 = torch.ops.aten.view.default(_to_copy_1109, [262144, 256]);  _to_copy_1109 = None
        mm_376 = torch.ops.aten.mm.default(view_1952, t_403);  view_1952 = t_403 = None
        view_1953 = torch.ops.aten.view.default(mm_376, [1, 512, 512, 1024]);  mm_376 = None
        select_47 = torch.ops.aten.select.int(view_1951, 0, 0)
        view_1954 = torch.ops.aten.view.default(view_1953, [1, 512, 512, 4, 4, 64]);  view_1953 = None
        permute_1083 = torch.ops.aten.permute.default(view_1954, [4, 0, 3, 1, 2, 5]);  view_1954 = None
        view_1955 = torch.ops.aten.view.default(permute_1083, [4, 4, 512, 512, 64]);  permute_1083 = None
        unbind_int_91 = torch.ops.aten.unbind.int(view_1955);  view_1955 = None
        getitem_1814 = unbind_int_91[0]
        getitem_1815 = unbind_int_91[1]
        getitem_1816 = unbind_int_91[2]
        getitem_1817 = unbind_int_91[3];  unbind_int_91 = None
        expand_112 = torch.ops.aten.expand.default(select_47, [4, 512, 512, 512]);  select_47 = None
        _scaled_dot_product_efficient_attention_default_63 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1814, getitem_1815, getitem_1816, expand_112, False);  getitem_1814 = getitem_1815 = getitem_1816 = expand_112 = None
        getitem_1818 = _scaled_dot_product_efficient_attention_default_63[0]
        sigmoid_147 = torch.ops.aten.sigmoid.default(getitem_1817);  getitem_1817 = None
        mul_240 = torch.ops.aten.mul.Tensor(getitem_1818, sigmoid_147);  getitem_1818 = sigmoid_147 = None
        view_1956 = torch.ops.aten.view.default(mul_240, [1, 4, 512, 512, 64]);  mul_240 = None
        permute_1084 = torch.ops.aten.permute.default(view_1956, [0, 2, 3, 1, 4]);  view_1956 = None
        clone_181 = torch.ops.aten.clone.default(permute_1084, memory_format = torch.contiguous_format);  permute_1084 = None
        _unsafe_view_156 = torch.ops.aten._unsafe_view.default(clone_181, [1, 512, 512, 256]);  clone_181 = None
        transpose_47 = torch.ops.aten.transpose.int(getitem_1811, 1, 2);  getitem_1811 = None
        _to_copy_1110 = torch.ops.aten._to_copy.default(arg598_1, dtype = torch.bfloat16);  arg598_1 = None
        _to_copy_1111 = torch.ops.aten._to_copy.default(transpose_47, dtype = torch.bfloat16);  transpose_47 = None
        t_404 = torch.ops.aten.t.default(_to_copy_1110);  _to_copy_1110 = None
        expand_113 = torch.ops.aten.expand.default(_to_copy_1111, [1, 512, 512, 256]);  _to_copy_1111 = None
        view_1957 = torch.ops.aten.view.default(expand_113, [512, 512, 256]);  expand_113 = None
        expand_114 = torch.ops.aten.expand.default(t_404, [1, 512, 256, 1024]);  t_404 = None
        view_1958 = torch.ops.aten.view.default(expand_114, [512, 256, 1024]);  expand_114 = None
        bmm_165 = torch.ops.aten.bmm.default(view_1957, view_1958);  view_1957 = view_1958 = None
        view_1959 = torch.ops.aten.view.default(bmm_165, [1, 512, 512, 1024]);  bmm_165 = None
        select_48 = torch.ops.aten.select.int(view_1951, 0, 1);  view_1951 = None
        view_1960 = torch.ops.aten.view.default(view_1959, [1, 512, 512, 4, 4, 64]);  view_1959 = None
        permute_1085 = torch.ops.aten.permute.default(view_1960, [4, 0, 3, 1, 2, 5]);  view_1960 = None
        view_1961 = torch.ops.aten.view.default(permute_1085, [4, 4, 512, 512, 64]);  permute_1085 = None
        unbind_int_92 = torch.ops.aten.unbind.int(view_1961);  view_1961 = None
        getitem_1822 = unbind_int_92[0]
        getitem_1823 = unbind_int_92[1]
        getitem_1824 = unbind_int_92[2]
        getitem_1825 = unbind_int_92[3];  unbind_int_92 = None
        expand_115 = torch.ops.aten.expand.default(select_48, [4, 512, 512, 512]);  select_48 = None
        _scaled_dot_product_efficient_attention_default_64 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1822, getitem_1823, getitem_1824, expand_115, False);  getitem_1822 = getitem_1823 = getitem_1824 = expand_115 = None
        getitem_1826 = _scaled_dot_product_efficient_attention_default_64[0]
        sigmoid_148 = torch.ops.aten.sigmoid.default(getitem_1825);  getitem_1825 = None
        mul_241 = torch.ops.aten.mul.Tensor(getitem_1826, sigmoid_148);  getitem_1826 = sigmoid_148 = None
        view_1962 = torch.ops.aten.view.default(mul_241, [1, 4, 512, 512, 64]);  mul_241 = None
        permute_1086 = torch.ops.aten.permute.default(view_1962, [0, 2, 3, 1, 4]);  view_1962 = None
        clone_182 = torch.ops.aten.clone.default(permute_1086, memory_format = torch.contiguous_format);  permute_1086 = None
        _unsafe_view_157 = torch.ops.aten._unsafe_view.default(clone_182, [1, 512, 512, 256]);  clone_182 = None
        cat_29 = torch.ops.aten.cat.default([_unsafe_view_156, _unsafe_view_157], dim = -1);  _unsafe_view_156 = _unsafe_view_157 = None
        slice_222 = torch.ops.aten.slice.Tensor(arg595_1, dim = 0, start = 0, end = 9223372036854775807);  arg595_1 = None
        unsqueeze_680 = torch.ops.aten.unsqueeze.default(slice_222, 1);  slice_222 = None
        mul_242 = torch.ops.aten.mul.Tensor(arg599_1, unsqueeze_680);  arg599_1 = unsqueeze_680 = None
        _to_copy_1112 = torch.ops.aten._to_copy.default(mul_242, dtype = torch.bfloat16);  mul_242 = None
        t_405 = torch.ops.aten.t.default(_to_copy_1112);  _to_copy_1112 = None
        view_1963 = torch.ops.aten.view.default(cat_29, [262144, 512]);  cat_29 = None
        mm_377 = torch.ops.aten.mm.default(view_1963, t_405);  view_1963 = t_405 = None
        view_1964 = torch.ops.aten.view.default(mm_377, [1, 512, 512, 256]);  mm_377 = None
        add_194 = torch.ops.aten.add.Tensor(add_193, view_1964);  add_193 = view_1964 = None
        split_tensor_191 = torch.ops.aten.split.Tensor(add_187, 512, dim = -2)
        getitem_1830 = split_tensor_191[0];  split_tensor_191 = None
        _to_copy_1113 = torch.ops.aten._to_copy.default(getitem_1830, dtype = torch.float32);  getitem_1830 = None
        native_layer_norm_default_230 = torch.ops.aten.native_layer_norm.default(_to_copy_1113, [256], arg586_1, arg587_1, 1e-05);  _to_copy_1113 = arg586_1 = arg587_1 = None
        getitem_1831 = native_layer_norm_default_230[0]
        _to_copy_1114 = torch.ops.aten._to_copy.default(arg588_1, dtype = torch.bfloat16);  arg588_1 = None
        _to_copy_1115 = torch.ops.aten._to_copy.default(getitem_1831, dtype = torch.bfloat16);  getitem_1831 = None
        t_406 = torch.ops.aten.t.default(_to_copy_1114);  _to_copy_1114 = None
        view_1965 = torch.ops.aten.view.default(_to_copy_1115, [262144, 256]);  _to_copy_1115 = None
        mm_378 = torch.ops.aten.mm.default(view_1965, t_406);  view_1965 = t_406 = None
        view_1966 = torch.ops.aten.view.default(mm_378, [1, 512, 512, 1024]);  mm_378 = None
        split_tensor_192 = torch.ops.aten.split.Tensor(view_1966, 512, dim = -1);  view_1966 = None
        getitem_1834 = split_tensor_192[0]
        getitem_1835 = split_tensor_192[1];  split_tensor_192 = None
        silu_52 = torch.ops.aten.silu.default(getitem_1834);  getitem_1834 = None
        mul_243 = torch.ops.aten.mul.Tensor(silu_52, getitem_1835);  silu_52 = getitem_1835 = None
        _to_copy_1116 = torch.ops.aten._to_copy.default(arg589_1, dtype = torch.bfloat16);  arg589_1 = None
        t_407 = torch.ops.aten.t.default(_to_copy_1116);  _to_copy_1116 = None
        view_1968 = torch.ops.aten.view.default(mul_243, [262144, 512]);  mul_243 = None
        mm_379 = torch.ops.aten.mm.default(view_1968, t_407);  view_1968 = t_407 = None
        view_1969 = torch.ops.aten.view.default(mm_379, [1, 512, 512, 256]);  mm_379 = None
        add_195 = torch.ops.aten.add.Tensor(add_194, view_1969);  add_194 = view_1969 = None
        _to_copy_1117 = torch.ops.aten._to_copy.default(add_191, dtype = torch.float32)
        native_layer_norm_default_231 = torch.ops.aten.native_layer_norm.default(_to_copy_1117, [384], arg604_1, arg605_1, 1e-05);  _to_copy_1117 = arg604_1 = arg605_1 = None
        getitem_1836 = native_layer_norm_default_231[0]
        _to_copy_1118 = torch.ops.aten._to_copy.default(add_187, dtype = torch.float32);  add_187 = None
        native_layer_norm_default_232 = torch.ops.aten.native_layer_norm.default(_to_copy_1118, [256], arg606_1, arg607_1, 1e-05);  _to_copy_1118 = arg606_1 = arg607_1 = None
        getitem_1839 = native_layer_norm_default_232[0]
        _to_copy_1119 = torch.ops.aten._to_copy.default(arg608_1, dtype = torch.bfloat16);  arg608_1 = None
        _to_copy_1120 = torch.ops.aten._to_copy.default(getitem_1839, dtype = torch.bfloat16);  getitem_1839 = None
        t_408 = torch.ops.aten.t.default(_to_copy_1119);  _to_copy_1119 = None
        view_1970 = torch.ops.aten.view.default(_to_copy_1120, [262144, 256]);  _to_copy_1120 = None
        mm_380 = torch.ops.aten.mm.default(view_1970, t_408);  view_1970 = t_408 = None
        view_1971 = torch.ops.aten.view.default(mm_380, [1, 512, 512, 16]);  mm_380 = None
        permute_1087 = torch.ops.aten.permute.default(view_1971, [0, 3, 1, 2]);  view_1971 = None
        view_1972 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_129 = torch.ops.aten.bitwise_not.default(view_1972);  view_1972 = None
        masked_fill_129 = torch.ops.aten.masked_fill.Scalar(permute_1087, bitwise_not_129, -10000);  permute_1087 = bitwise_not_129 = None
        _to_copy_1121 = torch.ops.aten._to_copy.default(getitem_1836, dtype = torch.bfloat16);  getitem_1836 = None
        _to_copy_1122 = torch.ops.aten._to_copy.default(arg610_1, dtype = torch.bfloat16);  arg610_1 = None
        unsqueeze_681 = torch.ops.aten.unsqueeze.default(_to_copy_1121, 3);  _to_copy_1121 = None
        unsqueeze_682 = torch.ops.aten.unsqueeze.default(unsqueeze_681, 4);  unsqueeze_681 = None
        unsqueeze_683 = torch.ops.aten.unsqueeze.default(unsqueeze_682, 5);  unsqueeze_682 = None
        permute_1088 = torch.ops.aten.permute.default(unsqueeze_683, [3, 0, 4, 1, 5, 2]);  unsqueeze_683 = None
        unsqueeze_684 = torch.ops.aten.unsqueeze.default(_to_copy_1122, 4);  _to_copy_1122 = None
        unsqueeze_685 = torch.ops.aten.unsqueeze.default(unsqueeze_684, 5);  unsqueeze_684 = None
        permute_1089 = torch.ops.aten.permute.default(unsqueeze_685, [1, 4, 2, 5, 3, 0]);  unsqueeze_685 = None
        permute_1090 = torch.ops.aten.permute.default(permute_1088, [3, 5, 0, 1, 2, 4]);  permute_1088 = None
        view_1973 = torch.ops.aten.view.default(permute_1090, [1, 512, 384]);  permute_1090 = None
        permute_1091 = torch.ops.aten.permute.default(permute_1089, [5, 0, 1, 2, 4, 3]);  permute_1089 = None
        view_1974 = torch.ops.aten.view.default(permute_1091, [1, 384, 1536]);  permute_1091 = None
        bmm_166 = torch.ops.aten.bmm.default(view_1973, view_1974);  view_1973 = view_1974 = None
        view_1975 = torch.ops.aten.view.default(bmm_166, [512, 1, 4, 1, 16, 24]);  bmm_166 = None
        permute_1092 = torch.ops.aten.permute.default(view_1975, [2, 3, 4, 0, 5, 1]);  view_1975 = None
        view_1976 = torch.ops.aten.view.default(permute_1092, [4, 1, 16, 512, 24]);  permute_1092 = None
        unbind_int_93 = torch.ops.aten.unbind.int(view_1976);  view_1976 = None
        getitem_1842 = unbind_int_93[0]
        getitem_1843 = unbind_int_93[1]
        getitem_1844 = unbind_int_93[2]
        getitem_1845 = unbind_int_93[3];  unbind_int_93 = None
        view_1977 = torch.ops.aten.view.default(arg609_1, [1, 16, 1, 24]);  arg609_1 = None
        add_196 = torch.ops.aten.add.Tensor(getitem_1842, view_1977);  getitem_1842 = view_1977 = None
        _to_copy_1123 = torch.ops.aten._to_copy.default(add_196, dtype = torch.bfloat16);  add_196 = None
        expand_116 = torch.ops.aten.expand.default(masked_fill_129, [1, 16, 512, 512]);  masked_fill_129 = None
        _scaled_dot_product_efficient_attention_default_65 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1123, getitem_1843, getitem_1844, expand_116, False);  _to_copy_1123 = getitem_1843 = getitem_1844 = expand_116 = None
        getitem_1846 = _scaled_dot_product_efficient_attention_default_65[0]
        add_197 = torch.ops.aten.add.Tensor(getitem_1845, 1);  getitem_1845 = None
        sigmoid_149 = torch.ops.aten.sigmoid.default(add_197);  add_197 = None
        mul_244 = torch.ops.aten.mul.Tensor(getitem_1846, sigmoid_149);  getitem_1846 = sigmoid_149 = None
        _to_copy_1124 = torch.ops.aten._to_copy.default(arg611_1, dtype = torch.bfloat16);  arg611_1 = None
        unsqueeze_686 = torch.ops.aten.unsqueeze.default(mul_244, 4);  mul_244 = None
        permute_1093 = torch.ops.aten.permute.default(unsqueeze_686, [0, 2, 4, 3, 1]);  unsqueeze_686 = None
        unsqueeze_687 = torch.ops.aten.unsqueeze.default(_to_copy_1124, 3);  _to_copy_1124 = None
        unsqueeze_688 = torch.ops.aten.unsqueeze.default(unsqueeze_687, 4);  unsqueeze_687 = None
        permute_1094 = torch.ops.aten.permute.default(unsqueeze_688, [3, 4, 2, 1, 0]);  unsqueeze_688 = None
        permute_1095 = torch.ops.aten.permute.default(permute_1093, [1, 3, 4, 0, 2]);  permute_1093 = None
        clone_183 = torch.ops.aten.clone.default(permute_1095, memory_format = torch.contiguous_format);  permute_1095 = None
        _unsafe_view_158 = torch.ops.aten._unsafe_view.default(clone_183, [1, 512, 384]);  clone_183 = None
        permute_1096 = torch.ops.aten.permute.default(permute_1094, [3, 4, 0, 2, 1]);  permute_1094 = None
        clone_184 = torch.ops.aten.clone.default(permute_1096, memory_format = torch.contiguous_format);  permute_1096 = None
        _unsafe_view_159 = torch.ops.aten._unsafe_view.default(clone_184, [1, 384, 384]);  clone_184 = None
        bmm_167 = torch.ops.aten.bmm.default(_unsafe_view_158, _unsafe_view_159);  _unsafe_view_158 = _unsafe_view_159 = None
        view_1978 = torch.ops.aten.view.default(bmm_167, [512, 1, 1, 1, 384]);  bmm_167 = None
        permute_1097 = torch.ops.aten.permute.default(view_1978, [3, 0, 4, 1, 2]);  view_1978 = None
        view_1979 = torch.ops.aten.view.default(permute_1097, [1, 512, 384]);  permute_1097 = None
        unsqueeze_689 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_245 = torch.ops.aten.mul.Tensor(view_1979, unsqueeze_689);  view_1979 = unsqueeze_689 = None
        add_198 = torch.ops.aten.add.Tensor(add_191, mul_245);  mul_245 = None
        split_tensor_193 = torch.ops.aten.split.Tensor(add_191, 512, dim = -2);  add_191 = None
        getitem_1850 = split_tensor_193[0];  split_tensor_193 = None
        _to_copy_1125 = torch.ops.aten._to_copy.default(getitem_1850, dtype = torch.float32);  getitem_1850 = None
        native_layer_norm_default_233 = torch.ops.aten.native_layer_norm.default(_to_copy_1125, [384], arg600_1, arg601_1, 1e-05);  _to_copy_1125 = arg600_1 = arg601_1 = None
        getitem_1851 = native_layer_norm_default_233[0]
        _to_copy_1126 = torch.ops.aten._to_copy.default(arg602_1, dtype = torch.bfloat16);  arg602_1 = None
        _to_copy_1127 = torch.ops.aten._to_copy.default(getitem_1851, dtype = torch.bfloat16);  getitem_1851 = None
        t_409 = torch.ops.aten.t.default(_to_copy_1126);  _to_copy_1126 = None
        view_1980 = torch.ops.aten.view.default(_to_copy_1127, [512, 384]);  _to_copy_1127 = None
        mm_381 = torch.ops.aten.mm.default(view_1980, t_409);  view_1980 = t_409 = None
        view_1981 = torch.ops.aten.view.default(mm_381, [1, 512, 1536]);  mm_381 = None
        split_tensor_194 = torch.ops.aten.split.Tensor(view_1981, 768, dim = -1);  view_1981 = None
        getitem_1854 = split_tensor_194[0]
        getitem_1855 = split_tensor_194[1];  split_tensor_194 = None
        silu_53 = torch.ops.aten.silu.default(getitem_1854);  getitem_1854 = None
        mul_246 = torch.ops.aten.mul.Tensor(silu_53, getitem_1855);  silu_53 = getitem_1855 = None
        _to_copy_1128 = torch.ops.aten._to_copy.default(arg603_1, dtype = torch.bfloat16);  arg603_1 = None
        t_410 = torch.ops.aten.t.default(_to_copy_1128);  _to_copy_1128 = None
        view_1983 = torch.ops.aten.view.default(mul_246, [512, 768]);  mul_246 = None
        mm_382 = torch.ops.aten.mm.default(view_1983, t_410);  view_1983 = t_410 = None
        view_1984 = torch.ops.aten.view.default(mm_382, [1, 512, 384]);  mm_382 = None
        add_199 = torch.ops.aten.add.Tensor(add_198, view_1984);  add_198 = view_1984 = None
        _to_copy_1129 = torch.ops.aten._to_copy.default(add_195, dtype = torch.float32)
        native_layer_norm_default_234 = torch.ops.aten.native_layer_norm.default(_to_copy_1129, [256], arg616_1, arg617_1, 1e-05);  _to_copy_1129 = arg616_1 = arg617_1 = None
        getitem_1856 = native_layer_norm_default_234[0]
        split_with_sizes_default_48 = torch.ops.aten.split_with_sizes.default(arg619_1, [512, 512]);  arg619_1 = None
        getitem_1859 = split_with_sizes_default_48[0]
        getitem_1860 = split_with_sizes_default_48[1];  split_with_sizes_default_48 = None
        split_with_sizes_default_49 = torch.ops.aten.split_with_sizes.default(arg620_1, [512, 512, 256]);  arg620_1 = None
        getitem_1861 = split_with_sizes_default_49[0]
        getitem_1862 = split_with_sizes_default_49[1]
        getitem_1863 = split_with_sizes_default_49[2];  split_with_sizes_default_49 = None
        _to_copy_1130 = torch.ops.aten._to_copy.default(getitem_1859, dtype = torch.bfloat16);  getitem_1859 = None
        _to_copy_1131 = torch.ops.aten._to_copy.default(getitem_1856, dtype = torch.bfloat16)
        t_411 = torch.ops.aten.t.default(_to_copy_1130);  _to_copy_1130 = None
        view_1985 = torch.ops.aten.view.default(_to_copy_1131, [262144, 256]);  _to_copy_1131 = None
        mm_383 = torch.ops.aten.mm.default(view_1985, t_411);  view_1985 = t_411 = None
        view_1986 = torch.ops.aten.view.default(mm_383, [1, 512, 512, 512]);  mm_383 = None
        _to_copy_1132 = torch.ops.aten._to_copy.default(getitem_1861, dtype = torch.bfloat16);  getitem_1861 = None
        _to_copy_1133 = torch.ops.aten._to_copy.default(getitem_1856, dtype = torch.bfloat16)
        t_412 = torch.ops.aten.t.default(_to_copy_1132);  _to_copy_1132 = None
        view_1987 = torch.ops.aten.view.default(_to_copy_1133, [262144, 256]);  _to_copy_1133 = None
        mm_384 = torch.ops.aten.mm.default(view_1987, t_412);  view_1987 = t_412 = None
        view_1988 = torch.ops.aten.view.default(mm_384, [1, 512, 512, 512]);  mm_384 = None
        sigmoid_150 = torch.ops.aten.sigmoid.default(view_1988);  view_1988 = None
        mul_247 = torch.ops.aten.mul.Tensor(view_1986, sigmoid_150);  view_1986 = sigmoid_150 = None
        unsqueeze_690 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_130 = torch.ops.aten.bitwise_not.default(unsqueeze_690);  unsqueeze_690 = None
        masked_fill_130 = torch.ops.aten.masked_fill.Scalar(mul_247, bitwise_not_130, 0);  mul_247 = bitwise_not_130 = None
        split_tensor_195 = torch.ops.aten.split.Tensor(masked_fill_130, 256, dim = -1)
        getitem_1866 = split_tensor_195[0]
        unsqueeze_693 = torch.ops.aten.unsqueeze.default(getitem_1866, 4);  getitem_1866 = None
        permute_1102 = torch.ops.aten.permute.default(unsqueeze_693, [0, 1, 4, 3, 2]);  unsqueeze_693 = None
        permute_1103 = torch.ops.aten.permute.default(permute_1102, [3, 1, 4, 0, 2]);  permute_1102 = None
        view_1991 = torch.ops.aten.view.default(permute_1103, [256, 512, 512]);  permute_1103 = None
        split_tensor_196 = torch.ops.aten.split.Tensor(masked_fill_130, 256, dim = -1);  masked_fill_130 = None
        getitem_1869 = split_tensor_196[1];  split_tensor_196 = None
        unsqueeze_694 = torch.ops.aten.unsqueeze.default(getitem_1869, 4);  getitem_1869 = None
        permute_1104 = torch.ops.aten.permute.default(unsqueeze_694, [0, 4, 1, 3, 2]);  unsqueeze_694 = None
        permute_1105 = torch.ops.aten.permute.default(permute_1104, [3, 4, 0, 2, 1]);  permute_1104 = None
        view_1992 = torch.ops.aten.view.default(permute_1105, [256, 512, 512]);  permute_1105 = None
        bmm_168 = torch.ops.aten.bmm.default(view_1991, view_1992);  view_1991 = view_1992 = None
        view_1993 = torch.ops.aten.view.default(bmm_168, [256, 512, 1, 1, 512]);  bmm_168 = None
        permute_1106 = torch.ops.aten.permute.default(view_1993, [3, 1, 4, 0, 2]);  view_1993 = None
        view_1994 = torch.ops.aten.view.default(permute_1106, [1, 512, 512, 256]);  permute_1106 = None
        _to_copy_1134 = torch.ops.aten._to_copy.default(getitem_1860, dtype = torch.bfloat16);  getitem_1860 = None
        _to_copy_1135 = torch.ops.aten._to_copy.default(getitem_1856, dtype = torch.bfloat16)
        t_413 = torch.ops.aten.t.default(_to_copy_1134);  _to_copy_1134 = None
        view_1995 = torch.ops.aten.view.default(_to_copy_1135, [262144, 256]);  _to_copy_1135 = None
        mm_385 = torch.ops.aten.mm.default(view_1995, t_413);  view_1995 = t_413 = None
        view_1996 = torch.ops.aten.view.default(mm_385, [1, 512, 512, 512]);  mm_385 = None
        _to_copy_1136 = torch.ops.aten._to_copy.default(getitem_1862, dtype = torch.bfloat16);  getitem_1862 = None
        _to_copy_1137 = torch.ops.aten._to_copy.default(getitem_1856, dtype = torch.bfloat16)
        t_414 = torch.ops.aten.t.default(_to_copy_1136);  _to_copy_1136 = None
        view_1997 = torch.ops.aten.view.default(_to_copy_1137, [262144, 256]);  _to_copy_1137 = None
        mm_386 = torch.ops.aten.mm.default(view_1997, t_414);  view_1997 = t_414 = None
        view_1998 = torch.ops.aten.view.default(mm_386, [1, 512, 512, 512]);  mm_386 = None
        sigmoid_151 = torch.ops.aten.sigmoid.default(view_1998);  view_1998 = None
        mul_248 = torch.ops.aten.mul.Tensor(view_1996, sigmoid_151);  view_1996 = sigmoid_151 = None
        view_1999 = torch.ops.aten.view.default(mul_248, [262144, 512]);  mul_248 = None
        view_2000 = torch.ops.aten.view.default(view_1999, [1, 512, 512, 512]);  view_1999 = None
        transpose_48 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_695 = torch.ops.aten.unsqueeze.default(transpose_48, 3);  transpose_48 = None
        clone_185 = torch.ops.aten.clone.default(unsqueeze_695, memory_format = torch.contiguous_format);  unsqueeze_695 = None
        bitwise_not_131 = torch.ops.aten.bitwise_not.default(clone_185);  clone_185 = None
        masked_fill_131 = torch.ops.aten.masked_fill.Scalar(view_2000, bitwise_not_131, 0);  view_2000 = bitwise_not_131 = None
        view_2001 = torch.ops.aten.view.default(masked_fill_131, [262144, 512]);  masked_fill_131 = None
        view_2005 = torch.ops.aten.view.default(view_2001, [1, 512, 512, 512])
        split_tensor_197 = torch.ops.aten.split.Tensor(view_2005, 256, dim = -1);  view_2005 = None
        getitem_1872 = split_tensor_197[0]
        unsqueeze_698 = torch.ops.aten.unsqueeze.default(getitem_1872, 4);  getitem_1872 = None
        permute_1111 = torch.ops.aten.permute.default(unsqueeze_698, [0, 2, 4, 3, 1]);  unsqueeze_698 = None
        permute_1112 = torch.ops.aten.permute.default(permute_1111, [3, 1, 4, 0, 2]);  permute_1111 = None
        view_2006 = torch.ops.aten.view.default(permute_1112, [256, 512, 512]);  permute_1112 = None
        view_2007 = torch.ops.aten.view.default(view_2001, [1, 512, 512, 512]);  view_2001 = None
        split_tensor_198 = torch.ops.aten.split.Tensor(view_2007, 256, dim = -1);  view_2007 = None
        getitem_1875 = split_tensor_198[1];  split_tensor_198 = None
        unsqueeze_699 = torch.ops.aten.unsqueeze.default(getitem_1875, 4);  getitem_1875 = None
        permute_1113 = torch.ops.aten.permute.default(unsqueeze_699, [0, 4, 2, 3, 1]);  unsqueeze_699 = None
        permute_1114 = torch.ops.aten.permute.default(permute_1113, [3, 4, 0, 2, 1]);  permute_1113 = None
        view_2008 = torch.ops.aten.view.default(permute_1114, [256, 512, 512]);  permute_1114 = None
        bmm_169 = torch.ops.aten.bmm.default(view_2006, view_2008);  view_2006 = view_2008 = None
        view_2009 = torch.ops.aten.view.default(bmm_169, [256, 512, 1, 1, 512]);  bmm_169 = None
        permute_1115 = torch.ops.aten.permute.default(view_2009, [3, 1, 4, 0, 2]);  view_2009 = None
        view_2010 = torch.ops.aten.view.default(permute_1115, [1, 512, 512, 256]);  permute_1115 = None
        _to_copy_1138 = torch.ops.aten._to_copy.default(view_1994, dtype = torch.float32);  view_1994 = None
        native_layer_norm_default_235 = torch.ops.aten.native_layer_norm.default(_to_copy_1138, [256], None, None, 1e-05);  _to_copy_1138 = None
        getitem_1876 = native_layer_norm_default_235[0]
        _to_copy_1139 = torch.ops.aten._to_copy.default(view_2010, dtype = torch.float32);  view_2010 = None
        native_layer_norm_default_236 = torch.ops.aten.native_layer_norm.default(_to_copy_1139, [256], None, None, 1e-05);  _to_copy_1139 = None
        getitem_1879 = native_layer_norm_default_236[0]
        add_200 = torch.ops.aten.add.Tensor(getitem_1876, getitem_1879);  getitem_1876 = getitem_1879 = None
        _to_copy_1140 = torch.ops.aten._to_copy.default(arg618_1, dtype = torch.bfloat16);  arg618_1 = None
        _to_copy_1141 = torch.ops.aten._to_copy.default(add_200, dtype = torch.bfloat16);  add_200 = None
        t_415 = torch.ops.aten.t.default(_to_copy_1140);  _to_copy_1140 = None
        view_2011 = torch.ops.aten.view.default(_to_copy_1141, [262144, 256]);  _to_copy_1141 = None
        mm_387 = torch.ops.aten.mm.default(view_2011, t_415);  view_2011 = t_415 = None
        view_2012 = torch.ops.aten.view.default(mm_387, [1, 512, 512, 256]);  mm_387 = None
        _to_copy_1142 = torch.ops.aten._to_copy.default(getitem_1863, dtype = torch.bfloat16);  getitem_1863 = None
        _to_copy_1143 = torch.ops.aten._to_copy.default(getitem_1856, dtype = torch.bfloat16);  getitem_1856 = None
        t_416 = torch.ops.aten.t.default(_to_copy_1142);  _to_copy_1142 = None
        view_2013 = torch.ops.aten.view.default(_to_copy_1143, [262144, 256]);  _to_copy_1143 = None
        mm_388 = torch.ops.aten.mm.default(view_2013, t_416);  view_2013 = t_416 = None
        view_2014 = torch.ops.aten.view.default(mm_388, [1, 512, 512, 256]);  mm_388 = None
        sigmoid_152 = torch.ops.aten.sigmoid.default(view_2014);  view_2014 = None
        mul_249 = torch.ops.aten.mul.Tensor(view_2012, sigmoid_152);  view_2012 = sigmoid_152 = None
        add_201 = torch.ops.aten.add.Tensor(add_195, mul_249);  mul_249 = None
        _to_copy_1144 = torch.ops.aten._to_copy.default(add_195, dtype = torch.float32)
        native_layer_norm_default_237 = torch.ops.aten.native_layer_norm.default(_to_copy_1144, [256], None, None, 1e-05);  _to_copy_1144 = None
        getitem_1882 = native_layer_norm_default_237[0]
        _to_copy_1145 = torch.ops.aten._to_copy.default(arg622_1, dtype = torch.bfloat16);  arg622_1 = None
        _to_copy_1146 = torch.ops.aten._to_copy.default(getitem_1882, dtype = torch.bfloat16)
        t_417 = torch.ops.aten.t.default(_to_copy_1145);  _to_copy_1145 = None
        view_2015 = torch.ops.aten.view.default(_to_copy_1146, [262144, 256]);  _to_copy_1146 = None
        mm_389 = torch.ops.aten.mm.default(view_2015, t_417);  view_2015 = t_417 = None
        view_2016 = torch.ops.aten.view.default(mm_389, [1, 512, 512, 8]);  mm_389 = None
        view_2017 = torch.ops.aten.view.default(view_2016, [1, 512, 512, 2, 4]);  view_2016 = None
        permute_1116 = torch.ops.aten.permute.default(view_2017, [0, 3, 4, 1, 2]);  view_2017 = None
        view_2018 = torch.ops.aten.view.default(permute_1116, [1, 2, 4, 1, 512, 512]);  permute_1116 = None
        view_2019 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_132 = torch.ops.aten.bitwise_not.default(view_2019);  view_2019 = None
        masked_fill_132 = torch.ops.aten.masked_fill.Scalar(view_2018, bitwise_not_132, -10000);  view_2018 = bitwise_not_132 = None
        view_2020 = torch.ops.aten.view.default(masked_fill_132, [1, 2, 4, 512, 512]);  masked_fill_132 = None
        permute_1117 = torch.ops.aten.permute.default(view_2020, [1, 0, 2, 3, 4]);  view_2020 = None
        view_2021 = torch.ops.aten.view.default(permute_1117, [2, 4, 1, 512, 512]);  permute_1117 = None
        _to_copy_1147 = torch.ops.aten._to_copy.default(arg623_1, dtype = torch.bfloat16);  arg623_1 = None
        _to_copy_1148 = torch.ops.aten._to_copy.default(getitem_1882, dtype = torch.bfloat16)
        t_418 = torch.ops.aten.t.default(_to_copy_1147);  _to_copy_1147 = None
        view_2022 = torch.ops.aten.view.default(_to_copy_1148, [262144, 256]);  _to_copy_1148 = None
        mm_390 = torch.ops.aten.mm.default(view_2022, t_418);  view_2022 = t_418 = None
        view_2023 = torch.ops.aten.view.default(mm_390, [1, 512, 512, 1024]);  mm_390 = None
        select_49 = torch.ops.aten.select.int(view_2021, 0, 0)
        view_2024 = torch.ops.aten.view.default(view_2023, [1, 512, 512, 4, 4, 64]);  view_2023 = None
        permute_1118 = torch.ops.aten.permute.default(view_2024, [4, 0, 3, 1, 2, 5]);  view_2024 = None
        view_2025 = torch.ops.aten.view.default(permute_1118, [4, 4, 512, 512, 64]);  permute_1118 = None
        unbind_int_94 = torch.ops.aten.unbind.int(view_2025);  view_2025 = None
        getitem_1885 = unbind_int_94[0]
        getitem_1886 = unbind_int_94[1]
        getitem_1887 = unbind_int_94[2]
        getitem_1888 = unbind_int_94[3];  unbind_int_94 = None
        expand_117 = torch.ops.aten.expand.default(select_49, [4, 512, 512, 512]);  select_49 = None
        _scaled_dot_product_efficient_attention_default_66 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1885, getitem_1886, getitem_1887, expand_117, False);  getitem_1885 = getitem_1886 = getitem_1887 = expand_117 = None
        getitem_1889 = _scaled_dot_product_efficient_attention_default_66[0]
        sigmoid_153 = torch.ops.aten.sigmoid.default(getitem_1888);  getitem_1888 = None
        mul_250 = torch.ops.aten.mul.Tensor(getitem_1889, sigmoid_153);  getitem_1889 = sigmoid_153 = None
        view_2026 = torch.ops.aten.view.default(mul_250, [1, 4, 512, 512, 64]);  mul_250 = None
        permute_1119 = torch.ops.aten.permute.default(view_2026, [0, 2, 3, 1, 4]);  view_2026 = None
        clone_186 = torch.ops.aten.clone.default(permute_1119, memory_format = torch.contiguous_format);  permute_1119 = None
        _unsafe_view_160 = torch.ops.aten._unsafe_view.default(clone_186, [1, 512, 512, 256]);  clone_186 = None
        transpose_49 = torch.ops.aten.transpose.int(getitem_1882, 1, 2);  getitem_1882 = None
        _to_copy_1149 = torch.ops.aten._to_copy.default(arg624_1, dtype = torch.bfloat16);  arg624_1 = None
        _to_copy_1150 = torch.ops.aten._to_copy.default(transpose_49, dtype = torch.bfloat16);  transpose_49 = None
        t_419 = torch.ops.aten.t.default(_to_copy_1149);  _to_copy_1149 = None
        expand_118 = torch.ops.aten.expand.default(_to_copy_1150, [1, 512, 512, 256]);  _to_copy_1150 = None
        view_2027 = torch.ops.aten.view.default(expand_118, [512, 512, 256]);  expand_118 = None
        expand_119 = torch.ops.aten.expand.default(t_419, [1, 512, 256, 1024]);  t_419 = None
        view_2028 = torch.ops.aten.view.default(expand_119, [512, 256, 1024]);  expand_119 = None
        bmm_170 = torch.ops.aten.bmm.default(view_2027, view_2028);  view_2027 = view_2028 = None
        view_2029 = torch.ops.aten.view.default(bmm_170, [1, 512, 512, 1024]);  bmm_170 = None
        select_50 = torch.ops.aten.select.int(view_2021, 0, 1);  view_2021 = None
        view_2030 = torch.ops.aten.view.default(view_2029, [1, 512, 512, 4, 4, 64]);  view_2029 = None
        permute_1120 = torch.ops.aten.permute.default(view_2030, [4, 0, 3, 1, 2, 5]);  view_2030 = None
        view_2031 = torch.ops.aten.view.default(permute_1120, [4, 4, 512, 512, 64]);  permute_1120 = None
        unbind_int_95 = torch.ops.aten.unbind.int(view_2031);  view_2031 = None
        getitem_1893 = unbind_int_95[0]
        getitem_1894 = unbind_int_95[1]
        getitem_1895 = unbind_int_95[2]
        getitem_1896 = unbind_int_95[3];  unbind_int_95 = None
        expand_120 = torch.ops.aten.expand.default(select_50, [4, 512, 512, 512]);  select_50 = None
        _scaled_dot_product_efficient_attention_default_67 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1893, getitem_1894, getitem_1895, expand_120, False);  getitem_1893 = getitem_1894 = getitem_1895 = expand_120 = None
        getitem_1897 = _scaled_dot_product_efficient_attention_default_67[0]
        sigmoid_154 = torch.ops.aten.sigmoid.default(getitem_1896);  getitem_1896 = None
        mul_251 = torch.ops.aten.mul.Tensor(getitem_1897, sigmoid_154);  getitem_1897 = sigmoid_154 = None
        view_2032 = torch.ops.aten.view.default(mul_251, [1, 4, 512, 512, 64]);  mul_251 = None
        permute_1121 = torch.ops.aten.permute.default(view_2032, [0, 2, 3, 1, 4]);  view_2032 = None
        clone_187 = torch.ops.aten.clone.default(permute_1121, memory_format = torch.contiguous_format);  permute_1121 = None
        _unsafe_view_161 = torch.ops.aten._unsafe_view.default(clone_187, [1, 512, 512, 256]);  clone_187 = None
        cat_30 = torch.ops.aten.cat.default([_unsafe_view_160, _unsafe_view_161], dim = -1);  _unsafe_view_160 = _unsafe_view_161 = None
        slice_223 = torch.ops.aten.slice.Tensor(arg621_1, dim = 0, start = 0, end = 9223372036854775807);  arg621_1 = None
        unsqueeze_700 = torch.ops.aten.unsqueeze.default(slice_223, 1);  slice_223 = None
        mul_252 = torch.ops.aten.mul.Tensor(arg625_1, unsqueeze_700);  arg625_1 = unsqueeze_700 = None
        _to_copy_1151 = torch.ops.aten._to_copy.default(mul_252, dtype = torch.bfloat16);  mul_252 = None
        t_420 = torch.ops.aten.t.default(_to_copy_1151);  _to_copy_1151 = None
        view_2033 = torch.ops.aten.view.default(cat_30, [262144, 512]);  cat_30 = None
        mm_391 = torch.ops.aten.mm.default(view_2033, t_420);  view_2033 = t_420 = None
        view_2034 = torch.ops.aten.view.default(mm_391, [1, 512, 512, 256]);  mm_391 = None
        add_202 = torch.ops.aten.add.Tensor(add_201, view_2034);  add_201 = view_2034 = None
        split_tensor_199 = torch.ops.aten.split.Tensor(add_195, 512, dim = -2)
        getitem_1901 = split_tensor_199[0];  split_tensor_199 = None
        _to_copy_1152 = torch.ops.aten._to_copy.default(getitem_1901, dtype = torch.float32);  getitem_1901 = None
        native_layer_norm_default_238 = torch.ops.aten.native_layer_norm.default(_to_copy_1152, [256], arg612_1, arg613_1, 1e-05);  _to_copy_1152 = arg612_1 = arg613_1 = None
        getitem_1902 = native_layer_norm_default_238[0]
        _to_copy_1153 = torch.ops.aten._to_copy.default(arg614_1, dtype = torch.bfloat16);  arg614_1 = None
        _to_copy_1154 = torch.ops.aten._to_copy.default(getitem_1902, dtype = torch.bfloat16);  getitem_1902 = None
        t_421 = torch.ops.aten.t.default(_to_copy_1153);  _to_copy_1153 = None
        view_2035 = torch.ops.aten.view.default(_to_copy_1154, [262144, 256]);  _to_copy_1154 = None
        mm_392 = torch.ops.aten.mm.default(view_2035, t_421);  view_2035 = t_421 = None
        view_2036 = torch.ops.aten.view.default(mm_392, [1, 512, 512, 1024]);  mm_392 = None
        split_tensor_200 = torch.ops.aten.split.Tensor(view_2036, 512, dim = -1);  view_2036 = None
        getitem_1905 = split_tensor_200[0]
        getitem_1906 = split_tensor_200[1];  split_tensor_200 = None
        silu_54 = torch.ops.aten.silu.default(getitem_1905);  getitem_1905 = None
        mul_253 = torch.ops.aten.mul.Tensor(silu_54, getitem_1906);  silu_54 = getitem_1906 = None
        _to_copy_1155 = torch.ops.aten._to_copy.default(arg615_1, dtype = torch.bfloat16);  arg615_1 = None
        t_422 = torch.ops.aten.t.default(_to_copy_1155);  _to_copy_1155 = None
        view_2038 = torch.ops.aten.view.default(mul_253, [262144, 512]);  mul_253 = None
        mm_393 = torch.ops.aten.mm.default(view_2038, t_422);  view_2038 = t_422 = None
        view_2039 = torch.ops.aten.view.default(mm_393, [1, 512, 512, 256]);  mm_393 = None
        add_203 = torch.ops.aten.add.Tensor(add_202, view_2039);  add_202 = view_2039 = None
        _to_copy_1156 = torch.ops.aten._to_copy.default(add_199, dtype = torch.float32)
        native_layer_norm_default_239 = torch.ops.aten.native_layer_norm.default(_to_copy_1156, [384], arg630_1, arg631_1, 1e-05);  _to_copy_1156 = arg630_1 = arg631_1 = None
        getitem_1907 = native_layer_norm_default_239[0]
        _to_copy_1157 = torch.ops.aten._to_copy.default(add_195, dtype = torch.float32);  add_195 = None
        native_layer_norm_default_240 = torch.ops.aten.native_layer_norm.default(_to_copy_1157, [256], arg632_1, arg633_1, 1e-05);  _to_copy_1157 = arg632_1 = arg633_1 = None
        getitem_1910 = native_layer_norm_default_240[0]
        _to_copy_1158 = torch.ops.aten._to_copy.default(arg634_1, dtype = torch.bfloat16);  arg634_1 = None
        _to_copy_1159 = torch.ops.aten._to_copy.default(getitem_1910, dtype = torch.bfloat16);  getitem_1910 = None
        t_423 = torch.ops.aten.t.default(_to_copy_1158);  _to_copy_1158 = None
        view_2040 = torch.ops.aten.view.default(_to_copy_1159, [262144, 256]);  _to_copy_1159 = None
        mm_394 = torch.ops.aten.mm.default(view_2040, t_423);  view_2040 = t_423 = None
        view_2041 = torch.ops.aten.view.default(mm_394, [1, 512, 512, 16]);  mm_394 = None
        permute_1122 = torch.ops.aten.permute.default(view_2041, [0, 3, 1, 2]);  view_2041 = None
        view_2042 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_133 = torch.ops.aten.bitwise_not.default(view_2042);  view_2042 = None
        masked_fill_133 = torch.ops.aten.masked_fill.Scalar(permute_1122, bitwise_not_133, -10000);  permute_1122 = bitwise_not_133 = None
        _to_copy_1160 = torch.ops.aten._to_copy.default(getitem_1907, dtype = torch.bfloat16);  getitem_1907 = None
        _to_copy_1161 = torch.ops.aten._to_copy.default(arg636_1, dtype = torch.bfloat16);  arg636_1 = None
        unsqueeze_701 = torch.ops.aten.unsqueeze.default(_to_copy_1160, 3);  _to_copy_1160 = None
        unsqueeze_702 = torch.ops.aten.unsqueeze.default(unsqueeze_701, 4);  unsqueeze_701 = None
        unsqueeze_703 = torch.ops.aten.unsqueeze.default(unsqueeze_702, 5);  unsqueeze_702 = None
        permute_1123 = torch.ops.aten.permute.default(unsqueeze_703, [3, 0, 4, 1, 5, 2]);  unsqueeze_703 = None
        unsqueeze_704 = torch.ops.aten.unsqueeze.default(_to_copy_1161, 4);  _to_copy_1161 = None
        unsqueeze_705 = torch.ops.aten.unsqueeze.default(unsqueeze_704, 5);  unsqueeze_704 = None
        permute_1124 = torch.ops.aten.permute.default(unsqueeze_705, [1, 4, 2, 5, 3, 0]);  unsqueeze_705 = None
        permute_1125 = torch.ops.aten.permute.default(permute_1123, [3, 5, 0, 1, 2, 4]);  permute_1123 = None
        view_2043 = torch.ops.aten.view.default(permute_1125, [1, 512, 384]);  permute_1125 = None
        permute_1126 = torch.ops.aten.permute.default(permute_1124, [5, 0, 1, 2, 4, 3]);  permute_1124 = None
        view_2044 = torch.ops.aten.view.default(permute_1126, [1, 384, 1536]);  permute_1126 = None
        bmm_171 = torch.ops.aten.bmm.default(view_2043, view_2044);  view_2043 = view_2044 = None
        view_2045 = torch.ops.aten.view.default(bmm_171, [512, 1, 4, 1, 16, 24]);  bmm_171 = None
        permute_1127 = torch.ops.aten.permute.default(view_2045, [2, 3, 4, 0, 5, 1]);  view_2045 = None
        view_2046 = torch.ops.aten.view.default(permute_1127, [4, 1, 16, 512, 24]);  permute_1127 = None
        unbind_int_96 = torch.ops.aten.unbind.int(view_2046);  view_2046 = None
        getitem_1913 = unbind_int_96[0]
        getitem_1914 = unbind_int_96[1]
        getitem_1915 = unbind_int_96[2]
        getitem_1916 = unbind_int_96[3];  unbind_int_96 = None
        view_2047 = torch.ops.aten.view.default(arg635_1, [1, 16, 1, 24]);  arg635_1 = None
        add_204 = torch.ops.aten.add.Tensor(getitem_1913, view_2047);  getitem_1913 = view_2047 = None
        _to_copy_1162 = torch.ops.aten._to_copy.default(add_204, dtype = torch.bfloat16);  add_204 = None
        expand_121 = torch.ops.aten.expand.default(masked_fill_133, [1, 16, 512, 512]);  masked_fill_133 = None
        _scaled_dot_product_efficient_attention_default_68 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1162, getitem_1914, getitem_1915, expand_121, False);  _to_copy_1162 = getitem_1914 = getitem_1915 = expand_121 = None
        getitem_1917 = _scaled_dot_product_efficient_attention_default_68[0]
        add_205 = torch.ops.aten.add.Tensor(getitem_1916, 1);  getitem_1916 = None
        sigmoid_155 = torch.ops.aten.sigmoid.default(add_205);  add_205 = None
        mul_254 = torch.ops.aten.mul.Tensor(getitem_1917, sigmoid_155);  getitem_1917 = sigmoid_155 = None
        _to_copy_1163 = torch.ops.aten._to_copy.default(arg637_1, dtype = torch.bfloat16);  arg637_1 = None
        unsqueeze_706 = torch.ops.aten.unsqueeze.default(mul_254, 4);  mul_254 = None
        permute_1128 = torch.ops.aten.permute.default(unsqueeze_706, [0, 2, 4, 3, 1]);  unsqueeze_706 = None
        unsqueeze_707 = torch.ops.aten.unsqueeze.default(_to_copy_1163, 3);  _to_copy_1163 = None
        unsqueeze_708 = torch.ops.aten.unsqueeze.default(unsqueeze_707, 4);  unsqueeze_707 = None
        permute_1129 = torch.ops.aten.permute.default(unsqueeze_708, [3, 4, 2, 1, 0]);  unsqueeze_708 = None
        permute_1130 = torch.ops.aten.permute.default(permute_1128, [1, 3, 4, 0, 2]);  permute_1128 = None
        clone_188 = torch.ops.aten.clone.default(permute_1130, memory_format = torch.contiguous_format);  permute_1130 = None
        _unsafe_view_162 = torch.ops.aten._unsafe_view.default(clone_188, [1, 512, 384]);  clone_188 = None
        permute_1131 = torch.ops.aten.permute.default(permute_1129, [3, 4, 0, 2, 1]);  permute_1129 = None
        clone_189 = torch.ops.aten.clone.default(permute_1131, memory_format = torch.contiguous_format);  permute_1131 = None
        _unsafe_view_163 = torch.ops.aten._unsafe_view.default(clone_189, [1, 384, 384]);  clone_189 = None
        bmm_172 = torch.ops.aten.bmm.default(_unsafe_view_162, _unsafe_view_163);  _unsafe_view_162 = _unsafe_view_163 = None
        view_2048 = torch.ops.aten.view.default(bmm_172, [512, 1, 1, 1, 384]);  bmm_172 = None
        permute_1132 = torch.ops.aten.permute.default(view_2048, [3, 0, 4, 1, 2]);  view_2048 = None
        view_2049 = torch.ops.aten.view.default(permute_1132, [1, 512, 384]);  permute_1132 = None
        unsqueeze_709 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_255 = torch.ops.aten.mul.Tensor(view_2049, unsqueeze_709);  view_2049 = unsqueeze_709 = None
        add_206 = torch.ops.aten.add.Tensor(add_199, mul_255);  mul_255 = None
        split_tensor_201 = torch.ops.aten.split.Tensor(add_199, 512, dim = -2);  add_199 = None
        getitem_1921 = split_tensor_201[0];  split_tensor_201 = None
        _to_copy_1164 = torch.ops.aten._to_copy.default(getitem_1921, dtype = torch.float32);  getitem_1921 = None
        native_layer_norm_default_241 = torch.ops.aten.native_layer_norm.default(_to_copy_1164, [384], arg626_1, arg627_1, 1e-05);  _to_copy_1164 = arg626_1 = arg627_1 = None
        getitem_1922 = native_layer_norm_default_241[0]
        _to_copy_1165 = torch.ops.aten._to_copy.default(arg628_1, dtype = torch.bfloat16);  arg628_1 = None
        _to_copy_1166 = torch.ops.aten._to_copy.default(getitem_1922, dtype = torch.bfloat16);  getitem_1922 = None
        t_424 = torch.ops.aten.t.default(_to_copy_1165);  _to_copy_1165 = None
        view_2050 = torch.ops.aten.view.default(_to_copy_1166, [512, 384]);  _to_copy_1166 = None
        mm_395 = torch.ops.aten.mm.default(view_2050, t_424);  view_2050 = t_424 = None
        view_2051 = torch.ops.aten.view.default(mm_395, [1, 512, 1536]);  mm_395 = None
        split_tensor_202 = torch.ops.aten.split.Tensor(view_2051, 768, dim = -1);  view_2051 = None
        getitem_1925 = split_tensor_202[0]
        getitem_1926 = split_tensor_202[1];  split_tensor_202 = None
        silu_55 = torch.ops.aten.silu.default(getitem_1925);  getitem_1925 = None
        mul_256 = torch.ops.aten.mul.Tensor(silu_55, getitem_1926);  silu_55 = getitem_1926 = None
        _to_copy_1167 = torch.ops.aten._to_copy.default(arg629_1, dtype = torch.bfloat16);  arg629_1 = None
        t_425 = torch.ops.aten.t.default(_to_copy_1167);  _to_copy_1167 = None
        view_2053 = torch.ops.aten.view.default(mul_256, [512, 768]);  mul_256 = None
        mm_396 = torch.ops.aten.mm.default(view_2053, t_425);  view_2053 = t_425 = None
        view_2054 = torch.ops.aten.view.default(mm_396, [1, 512, 384]);  mm_396 = None
        add_207 = torch.ops.aten.add.Tensor(add_206, view_2054);  add_206 = view_2054 = None
        _to_copy_1168 = torch.ops.aten._to_copy.default(add_203, dtype = torch.float32)
        native_layer_norm_default_242 = torch.ops.aten.native_layer_norm.default(_to_copy_1168, [256], arg642_1, arg643_1, 1e-05);  _to_copy_1168 = arg642_1 = arg643_1 = None
        getitem_1927 = native_layer_norm_default_242[0]
        split_with_sizes_default_50 = torch.ops.aten.split_with_sizes.default(arg645_1, [512, 512]);  arg645_1 = None
        getitem_1930 = split_with_sizes_default_50[0]
        getitem_1931 = split_with_sizes_default_50[1];  split_with_sizes_default_50 = None
        split_with_sizes_default_51 = torch.ops.aten.split_with_sizes.default(arg646_1, [512, 512, 256]);  arg646_1 = None
        getitem_1932 = split_with_sizes_default_51[0]
        getitem_1933 = split_with_sizes_default_51[1]
        getitem_1934 = split_with_sizes_default_51[2];  split_with_sizes_default_51 = None
        _to_copy_1169 = torch.ops.aten._to_copy.default(getitem_1930, dtype = torch.bfloat16);  getitem_1930 = None
        _to_copy_1170 = torch.ops.aten._to_copy.default(getitem_1927, dtype = torch.bfloat16)
        t_426 = torch.ops.aten.t.default(_to_copy_1169);  _to_copy_1169 = None
        view_2055 = torch.ops.aten.view.default(_to_copy_1170, [262144, 256]);  _to_copy_1170 = None
        mm_397 = torch.ops.aten.mm.default(view_2055, t_426);  view_2055 = t_426 = None
        view_2056 = torch.ops.aten.view.default(mm_397, [1, 512, 512, 512]);  mm_397 = None
        _to_copy_1171 = torch.ops.aten._to_copy.default(getitem_1932, dtype = torch.bfloat16);  getitem_1932 = None
        _to_copy_1172 = torch.ops.aten._to_copy.default(getitem_1927, dtype = torch.bfloat16)
        t_427 = torch.ops.aten.t.default(_to_copy_1171);  _to_copy_1171 = None
        view_2057 = torch.ops.aten.view.default(_to_copy_1172, [262144, 256]);  _to_copy_1172 = None
        mm_398 = torch.ops.aten.mm.default(view_2057, t_427);  view_2057 = t_427 = None
        view_2058 = torch.ops.aten.view.default(mm_398, [1, 512, 512, 512]);  mm_398 = None
        sigmoid_156 = torch.ops.aten.sigmoid.default(view_2058);  view_2058 = None
        mul_257 = torch.ops.aten.mul.Tensor(view_2056, sigmoid_156);  view_2056 = sigmoid_156 = None
        unsqueeze_710 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_134 = torch.ops.aten.bitwise_not.default(unsqueeze_710);  unsqueeze_710 = None
        masked_fill_134 = torch.ops.aten.masked_fill.Scalar(mul_257, bitwise_not_134, 0);  mul_257 = bitwise_not_134 = None
        split_tensor_203 = torch.ops.aten.split.Tensor(masked_fill_134, 256, dim = -1)
        getitem_1937 = split_tensor_203[0]
        unsqueeze_713 = torch.ops.aten.unsqueeze.default(getitem_1937, 4);  getitem_1937 = None
        permute_1137 = torch.ops.aten.permute.default(unsqueeze_713, [0, 1, 4, 3, 2]);  unsqueeze_713 = None
        permute_1138 = torch.ops.aten.permute.default(permute_1137, [3, 1, 4, 0, 2]);  permute_1137 = None
        view_2061 = torch.ops.aten.view.default(permute_1138, [256, 512, 512]);  permute_1138 = None
        split_tensor_204 = torch.ops.aten.split.Tensor(masked_fill_134, 256, dim = -1);  masked_fill_134 = None
        getitem_1940 = split_tensor_204[1];  split_tensor_204 = None
        unsqueeze_714 = torch.ops.aten.unsqueeze.default(getitem_1940, 4);  getitem_1940 = None
        permute_1139 = torch.ops.aten.permute.default(unsqueeze_714, [0, 4, 1, 3, 2]);  unsqueeze_714 = None
        permute_1140 = torch.ops.aten.permute.default(permute_1139, [3, 4, 0, 2, 1]);  permute_1139 = None
        view_2062 = torch.ops.aten.view.default(permute_1140, [256, 512, 512]);  permute_1140 = None
        bmm_173 = torch.ops.aten.bmm.default(view_2061, view_2062);  view_2061 = view_2062 = None
        view_2063 = torch.ops.aten.view.default(bmm_173, [256, 512, 1, 1, 512]);  bmm_173 = None
        permute_1141 = torch.ops.aten.permute.default(view_2063, [3, 1, 4, 0, 2]);  view_2063 = None
        view_2064 = torch.ops.aten.view.default(permute_1141, [1, 512, 512, 256]);  permute_1141 = None
        _to_copy_1173 = torch.ops.aten._to_copy.default(getitem_1931, dtype = torch.bfloat16);  getitem_1931 = None
        _to_copy_1174 = torch.ops.aten._to_copy.default(getitem_1927, dtype = torch.bfloat16)
        t_428 = torch.ops.aten.t.default(_to_copy_1173);  _to_copy_1173 = None
        view_2065 = torch.ops.aten.view.default(_to_copy_1174, [262144, 256]);  _to_copy_1174 = None
        mm_399 = torch.ops.aten.mm.default(view_2065, t_428);  view_2065 = t_428 = None
        view_2066 = torch.ops.aten.view.default(mm_399, [1, 512, 512, 512]);  mm_399 = None
        _to_copy_1175 = torch.ops.aten._to_copy.default(getitem_1933, dtype = torch.bfloat16);  getitem_1933 = None
        _to_copy_1176 = torch.ops.aten._to_copy.default(getitem_1927, dtype = torch.bfloat16)
        t_429 = torch.ops.aten.t.default(_to_copy_1175);  _to_copy_1175 = None
        view_2067 = torch.ops.aten.view.default(_to_copy_1176, [262144, 256]);  _to_copy_1176 = None
        mm_400 = torch.ops.aten.mm.default(view_2067, t_429);  view_2067 = t_429 = None
        view_2068 = torch.ops.aten.view.default(mm_400, [1, 512, 512, 512]);  mm_400 = None
        sigmoid_157 = torch.ops.aten.sigmoid.default(view_2068);  view_2068 = None
        mul_258 = torch.ops.aten.mul.Tensor(view_2066, sigmoid_157);  view_2066 = sigmoid_157 = None
        view_2069 = torch.ops.aten.view.default(mul_258, [262144, 512]);  mul_258 = None
        view_2070 = torch.ops.aten.view.default(view_2069, [1, 512, 512, 512]);  view_2069 = None
        transpose_50 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_715 = torch.ops.aten.unsqueeze.default(transpose_50, 3);  transpose_50 = None
        clone_190 = torch.ops.aten.clone.default(unsqueeze_715, memory_format = torch.contiguous_format);  unsqueeze_715 = None
        bitwise_not_135 = torch.ops.aten.bitwise_not.default(clone_190);  clone_190 = None
        masked_fill_135 = torch.ops.aten.masked_fill.Scalar(view_2070, bitwise_not_135, 0);  view_2070 = bitwise_not_135 = None
        view_2071 = torch.ops.aten.view.default(masked_fill_135, [262144, 512]);  masked_fill_135 = None
        view_2075 = torch.ops.aten.view.default(view_2071, [1, 512, 512, 512])
        split_tensor_205 = torch.ops.aten.split.Tensor(view_2075, 256, dim = -1);  view_2075 = None
        getitem_1943 = split_tensor_205[0]
        unsqueeze_718 = torch.ops.aten.unsqueeze.default(getitem_1943, 4);  getitem_1943 = None
        permute_1146 = torch.ops.aten.permute.default(unsqueeze_718, [0, 2, 4, 3, 1]);  unsqueeze_718 = None
        permute_1147 = torch.ops.aten.permute.default(permute_1146, [3, 1, 4, 0, 2]);  permute_1146 = None
        view_2076 = torch.ops.aten.view.default(permute_1147, [256, 512, 512]);  permute_1147 = None
        view_2077 = torch.ops.aten.view.default(view_2071, [1, 512, 512, 512]);  view_2071 = None
        split_tensor_206 = torch.ops.aten.split.Tensor(view_2077, 256, dim = -1);  view_2077 = None
        getitem_1946 = split_tensor_206[1];  split_tensor_206 = None
        unsqueeze_719 = torch.ops.aten.unsqueeze.default(getitem_1946, 4);  getitem_1946 = None
        permute_1148 = torch.ops.aten.permute.default(unsqueeze_719, [0, 4, 2, 3, 1]);  unsqueeze_719 = None
        permute_1149 = torch.ops.aten.permute.default(permute_1148, [3, 4, 0, 2, 1]);  permute_1148 = None
        view_2078 = torch.ops.aten.view.default(permute_1149, [256, 512, 512]);  permute_1149 = None
        bmm_174 = torch.ops.aten.bmm.default(view_2076, view_2078);  view_2076 = view_2078 = None
        view_2079 = torch.ops.aten.view.default(bmm_174, [256, 512, 1, 1, 512]);  bmm_174 = None
        permute_1150 = torch.ops.aten.permute.default(view_2079, [3, 1, 4, 0, 2]);  view_2079 = None
        view_2080 = torch.ops.aten.view.default(permute_1150, [1, 512, 512, 256]);  permute_1150 = None
        _to_copy_1177 = torch.ops.aten._to_copy.default(view_2064, dtype = torch.float32);  view_2064 = None
        native_layer_norm_default_243 = torch.ops.aten.native_layer_norm.default(_to_copy_1177, [256], None, None, 1e-05);  _to_copy_1177 = None
        getitem_1947 = native_layer_norm_default_243[0]
        _to_copy_1178 = torch.ops.aten._to_copy.default(view_2080, dtype = torch.float32);  view_2080 = None
        native_layer_norm_default_244 = torch.ops.aten.native_layer_norm.default(_to_copy_1178, [256], None, None, 1e-05);  _to_copy_1178 = None
        getitem_1950 = native_layer_norm_default_244[0]
        add_208 = torch.ops.aten.add.Tensor(getitem_1947, getitem_1950);  getitem_1947 = getitem_1950 = None
        _to_copy_1179 = torch.ops.aten._to_copy.default(arg644_1, dtype = torch.bfloat16);  arg644_1 = None
        _to_copy_1180 = torch.ops.aten._to_copy.default(add_208, dtype = torch.bfloat16);  add_208 = None
        t_430 = torch.ops.aten.t.default(_to_copy_1179);  _to_copy_1179 = None
        view_2081 = torch.ops.aten.view.default(_to_copy_1180, [262144, 256]);  _to_copy_1180 = None
        mm_401 = torch.ops.aten.mm.default(view_2081, t_430);  view_2081 = t_430 = None
        view_2082 = torch.ops.aten.view.default(mm_401, [1, 512, 512, 256]);  mm_401 = None
        _to_copy_1181 = torch.ops.aten._to_copy.default(getitem_1934, dtype = torch.bfloat16);  getitem_1934 = None
        _to_copy_1182 = torch.ops.aten._to_copy.default(getitem_1927, dtype = torch.bfloat16);  getitem_1927 = None
        t_431 = torch.ops.aten.t.default(_to_copy_1181);  _to_copy_1181 = None
        view_2083 = torch.ops.aten.view.default(_to_copy_1182, [262144, 256]);  _to_copy_1182 = None
        mm_402 = torch.ops.aten.mm.default(view_2083, t_431);  view_2083 = t_431 = None
        view_2084 = torch.ops.aten.view.default(mm_402, [1, 512, 512, 256]);  mm_402 = None
        sigmoid_158 = torch.ops.aten.sigmoid.default(view_2084);  view_2084 = None
        mul_259 = torch.ops.aten.mul.Tensor(view_2082, sigmoid_158);  view_2082 = sigmoid_158 = None
        add_209 = torch.ops.aten.add.Tensor(add_203, mul_259);  mul_259 = None
        _to_copy_1183 = torch.ops.aten._to_copy.default(add_203, dtype = torch.float32)
        native_layer_norm_default_245 = torch.ops.aten.native_layer_norm.default(_to_copy_1183, [256], None, None, 1e-05);  _to_copy_1183 = None
        getitem_1953 = native_layer_norm_default_245[0]
        _to_copy_1184 = torch.ops.aten._to_copy.default(arg648_1, dtype = torch.bfloat16);  arg648_1 = None
        _to_copy_1185 = torch.ops.aten._to_copy.default(getitem_1953, dtype = torch.bfloat16)
        t_432 = torch.ops.aten.t.default(_to_copy_1184);  _to_copy_1184 = None
        view_2085 = torch.ops.aten.view.default(_to_copy_1185, [262144, 256]);  _to_copy_1185 = None
        mm_403 = torch.ops.aten.mm.default(view_2085, t_432);  view_2085 = t_432 = None
        view_2086 = torch.ops.aten.view.default(mm_403, [1, 512, 512, 8]);  mm_403 = None
        view_2087 = torch.ops.aten.view.default(view_2086, [1, 512, 512, 2, 4]);  view_2086 = None
        permute_1151 = torch.ops.aten.permute.default(view_2087, [0, 3, 4, 1, 2]);  view_2087 = None
        view_2088 = torch.ops.aten.view.default(permute_1151, [1, 2, 4, 1, 512, 512]);  permute_1151 = None
        view_2089 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_136 = torch.ops.aten.bitwise_not.default(view_2089);  view_2089 = None
        masked_fill_136 = torch.ops.aten.masked_fill.Scalar(view_2088, bitwise_not_136, -10000);  view_2088 = bitwise_not_136 = None
        view_2090 = torch.ops.aten.view.default(masked_fill_136, [1, 2, 4, 512, 512]);  masked_fill_136 = None
        permute_1152 = torch.ops.aten.permute.default(view_2090, [1, 0, 2, 3, 4]);  view_2090 = None
        view_2091 = torch.ops.aten.view.default(permute_1152, [2, 4, 1, 512, 512]);  permute_1152 = None
        _to_copy_1186 = torch.ops.aten._to_copy.default(arg649_1, dtype = torch.bfloat16);  arg649_1 = None
        _to_copy_1187 = torch.ops.aten._to_copy.default(getitem_1953, dtype = torch.bfloat16)
        t_433 = torch.ops.aten.t.default(_to_copy_1186);  _to_copy_1186 = None
        view_2092 = torch.ops.aten.view.default(_to_copy_1187, [262144, 256]);  _to_copy_1187 = None
        mm_404 = torch.ops.aten.mm.default(view_2092, t_433);  view_2092 = t_433 = None
        view_2093 = torch.ops.aten.view.default(mm_404, [1, 512, 512, 1024]);  mm_404 = None
        select_51 = torch.ops.aten.select.int(view_2091, 0, 0)
        view_2094 = torch.ops.aten.view.default(view_2093, [1, 512, 512, 4, 4, 64]);  view_2093 = None
        permute_1153 = torch.ops.aten.permute.default(view_2094, [4, 0, 3, 1, 2, 5]);  view_2094 = None
        view_2095 = torch.ops.aten.view.default(permute_1153, [4, 4, 512, 512, 64]);  permute_1153 = None
        unbind_int_97 = torch.ops.aten.unbind.int(view_2095);  view_2095 = None
        getitem_1956 = unbind_int_97[0]
        getitem_1957 = unbind_int_97[1]
        getitem_1958 = unbind_int_97[2]
        getitem_1959 = unbind_int_97[3];  unbind_int_97 = None
        expand_122 = torch.ops.aten.expand.default(select_51, [4, 512, 512, 512]);  select_51 = None
        _scaled_dot_product_efficient_attention_default_69 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1956, getitem_1957, getitem_1958, expand_122, False);  getitem_1956 = getitem_1957 = getitem_1958 = expand_122 = None
        getitem_1960 = _scaled_dot_product_efficient_attention_default_69[0]
        sigmoid_159 = torch.ops.aten.sigmoid.default(getitem_1959);  getitem_1959 = None
        mul_260 = torch.ops.aten.mul.Tensor(getitem_1960, sigmoid_159);  getitem_1960 = sigmoid_159 = None
        view_2096 = torch.ops.aten.view.default(mul_260, [1, 4, 512, 512, 64]);  mul_260 = None
        permute_1154 = torch.ops.aten.permute.default(view_2096, [0, 2, 3, 1, 4]);  view_2096 = None
        clone_191 = torch.ops.aten.clone.default(permute_1154, memory_format = torch.contiguous_format);  permute_1154 = None
        _unsafe_view_164 = torch.ops.aten._unsafe_view.default(clone_191, [1, 512, 512, 256]);  clone_191 = None
        transpose_51 = torch.ops.aten.transpose.int(getitem_1953, 1, 2);  getitem_1953 = None
        _to_copy_1188 = torch.ops.aten._to_copy.default(arg650_1, dtype = torch.bfloat16);  arg650_1 = None
        _to_copy_1189 = torch.ops.aten._to_copy.default(transpose_51, dtype = torch.bfloat16);  transpose_51 = None
        t_434 = torch.ops.aten.t.default(_to_copy_1188);  _to_copy_1188 = None
        expand_123 = torch.ops.aten.expand.default(_to_copy_1189, [1, 512, 512, 256]);  _to_copy_1189 = None
        view_2097 = torch.ops.aten.view.default(expand_123, [512, 512, 256]);  expand_123 = None
        expand_124 = torch.ops.aten.expand.default(t_434, [1, 512, 256, 1024]);  t_434 = None
        view_2098 = torch.ops.aten.view.default(expand_124, [512, 256, 1024]);  expand_124 = None
        bmm_175 = torch.ops.aten.bmm.default(view_2097, view_2098);  view_2097 = view_2098 = None
        view_2099 = torch.ops.aten.view.default(bmm_175, [1, 512, 512, 1024]);  bmm_175 = None
        select_52 = torch.ops.aten.select.int(view_2091, 0, 1);  view_2091 = None
        view_2100 = torch.ops.aten.view.default(view_2099, [1, 512, 512, 4, 4, 64]);  view_2099 = None
        permute_1155 = torch.ops.aten.permute.default(view_2100, [4, 0, 3, 1, 2, 5]);  view_2100 = None
        view_2101 = torch.ops.aten.view.default(permute_1155, [4, 4, 512, 512, 64]);  permute_1155 = None
        unbind_int_98 = torch.ops.aten.unbind.int(view_2101);  view_2101 = None
        getitem_1964 = unbind_int_98[0]
        getitem_1965 = unbind_int_98[1]
        getitem_1966 = unbind_int_98[2]
        getitem_1967 = unbind_int_98[3];  unbind_int_98 = None
        expand_125 = torch.ops.aten.expand.default(select_52, [4, 512, 512, 512]);  select_52 = None
        _scaled_dot_product_efficient_attention_default_70 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_1964, getitem_1965, getitem_1966, expand_125, False);  getitem_1964 = getitem_1965 = getitem_1966 = expand_125 = None
        getitem_1968 = _scaled_dot_product_efficient_attention_default_70[0]
        sigmoid_160 = torch.ops.aten.sigmoid.default(getitem_1967);  getitem_1967 = None
        mul_261 = torch.ops.aten.mul.Tensor(getitem_1968, sigmoid_160);  getitem_1968 = sigmoid_160 = None
        view_2102 = torch.ops.aten.view.default(mul_261, [1, 4, 512, 512, 64]);  mul_261 = None
        permute_1156 = torch.ops.aten.permute.default(view_2102, [0, 2, 3, 1, 4]);  view_2102 = None
        clone_192 = torch.ops.aten.clone.default(permute_1156, memory_format = torch.contiguous_format);  permute_1156 = None
        _unsafe_view_165 = torch.ops.aten._unsafe_view.default(clone_192, [1, 512, 512, 256]);  clone_192 = None
        cat_31 = torch.ops.aten.cat.default([_unsafe_view_164, _unsafe_view_165], dim = -1);  _unsafe_view_164 = _unsafe_view_165 = None
        slice_224 = torch.ops.aten.slice.Tensor(arg647_1, dim = 0, start = 0, end = 9223372036854775807);  arg647_1 = None
        unsqueeze_720 = torch.ops.aten.unsqueeze.default(slice_224, 1);  slice_224 = None
        mul_262 = torch.ops.aten.mul.Tensor(arg651_1, unsqueeze_720);  arg651_1 = unsqueeze_720 = None
        _to_copy_1190 = torch.ops.aten._to_copy.default(mul_262, dtype = torch.bfloat16);  mul_262 = None
        t_435 = torch.ops.aten.t.default(_to_copy_1190);  _to_copy_1190 = None
        view_2103 = torch.ops.aten.view.default(cat_31, [262144, 512]);  cat_31 = None
        mm_405 = torch.ops.aten.mm.default(view_2103, t_435);  view_2103 = t_435 = None
        view_2104 = torch.ops.aten.view.default(mm_405, [1, 512, 512, 256]);  mm_405 = None
        add_210 = torch.ops.aten.add.Tensor(add_209, view_2104);  add_209 = view_2104 = None
        split_tensor_207 = torch.ops.aten.split.Tensor(add_203, 512, dim = -2)
        getitem_1972 = split_tensor_207[0];  split_tensor_207 = None
        _to_copy_1191 = torch.ops.aten._to_copy.default(getitem_1972, dtype = torch.float32);  getitem_1972 = None
        native_layer_norm_default_246 = torch.ops.aten.native_layer_norm.default(_to_copy_1191, [256], arg638_1, arg639_1, 1e-05);  _to_copy_1191 = arg638_1 = arg639_1 = None
        getitem_1973 = native_layer_norm_default_246[0]
        _to_copy_1192 = torch.ops.aten._to_copy.default(arg640_1, dtype = torch.bfloat16);  arg640_1 = None
        _to_copy_1193 = torch.ops.aten._to_copy.default(getitem_1973, dtype = torch.bfloat16);  getitem_1973 = None
        t_436 = torch.ops.aten.t.default(_to_copy_1192);  _to_copy_1192 = None
        view_2105 = torch.ops.aten.view.default(_to_copy_1193, [262144, 256]);  _to_copy_1193 = None
        mm_406 = torch.ops.aten.mm.default(view_2105, t_436);  view_2105 = t_436 = None
        view_2106 = torch.ops.aten.view.default(mm_406, [1, 512, 512, 1024]);  mm_406 = None
        split_tensor_208 = torch.ops.aten.split.Tensor(view_2106, 512, dim = -1);  view_2106 = None
        getitem_1976 = split_tensor_208[0]
        getitem_1977 = split_tensor_208[1];  split_tensor_208 = None
        silu_56 = torch.ops.aten.silu.default(getitem_1976);  getitem_1976 = None
        mul_263 = torch.ops.aten.mul.Tensor(silu_56, getitem_1977);  silu_56 = getitem_1977 = None
        _to_copy_1194 = torch.ops.aten._to_copy.default(arg641_1, dtype = torch.bfloat16);  arg641_1 = None
        t_437 = torch.ops.aten.t.default(_to_copy_1194);  _to_copy_1194 = None
        view_2108 = torch.ops.aten.view.default(mul_263, [262144, 512]);  mul_263 = None
        mm_407 = torch.ops.aten.mm.default(view_2108, t_437);  view_2108 = t_437 = None
        view_2109 = torch.ops.aten.view.default(mm_407, [1, 512, 512, 256]);  mm_407 = None
        add_211 = torch.ops.aten.add.Tensor(add_210, view_2109);  add_210 = view_2109 = None
        _to_copy_1195 = torch.ops.aten._to_copy.default(add_207, dtype = torch.float32)
        native_layer_norm_default_247 = torch.ops.aten.native_layer_norm.default(_to_copy_1195, [384], arg656_1, arg657_1, 1e-05);  _to_copy_1195 = arg656_1 = arg657_1 = None
        getitem_1978 = native_layer_norm_default_247[0]
        _to_copy_1196 = torch.ops.aten._to_copy.default(add_203, dtype = torch.float32);  add_203 = None
        native_layer_norm_default_248 = torch.ops.aten.native_layer_norm.default(_to_copy_1196, [256], arg658_1, arg659_1, 1e-05);  _to_copy_1196 = arg658_1 = arg659_1 = None
        getitem_1981 = native_layer_norm_default_248[0]
        _to_copy_1197 = torch.ops.aten._to_copy.default(arg660_1, dtype = torch.bfloat16);  arg660_1 = None
        _to_copy_1198 = torch.ops.aten._to_copy.default(getitem_1981, dtype = torch.bfloat16);  getitem_1981 = None
        t_438 = torch.ops.aten.t.default(_to_copy_1197);  _to_copy_1197 = None
        view_2110 = torch.ops.aten.view.default(_to_copy_1198, [262144, 256]);  _to_copy_1198 = None
        mm_408 = torch.ops.aten.mm.default(view_2110, t_438);  view_2110 = t_438 = None
        view_2111 = torch.ops.aten.view.default(mm_408, [1, 512, 512, 16]);  mm_408 = None
        permute_1157 = torch.ops.aten.permute.default(view_2111, [0, 3, 1, 2]);  view_2111 = None
        view_2112 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_137 = torch.ops.aten.bitwise_not.default(view_2112);  view_2112 = None
        masked_fill_137 = torch.ops.aten.masked_fill.Scalar(permute_1157, bitwise_not_137, -10000);  permute_1157 = bitwise_not_137 = None
        _to_copy_1199 = torch.ops.aten._to_copy.default(getitem_1978, dtype = torch.bfloat16);  getitem_1978 = None
        _to_copy_1200 = torch.ops.aten._to_copy.default(arg662_1, dtype = torch.bfloat16);  arg662_1 = None
        unsqueeze_721 = torch.ops.aten.unsqueeze.default(_to_copy_1199, 3);  _to_copy_1199 = None
        unsqueeze_722 = torch.ops.aten.unsqueeze.default(unsqueeze_721, 4);  unsqueeze_721 = None
        unsqueeze_723 = torch.ops.aten.unsqueeze.default(unsqueeze_722, 5);  unsqueeze_722 = None
        permute_1158 = torch.ops.aten.permute.default(unsqueeze_723, [3, 0, 4, 1, 5, 2]);  unsqueeze_723 = None
        unsqueeze_724 = torch.ops.aten.unsqueeze.default(_to_copy_1200, 4);  _to_copy_1200 = None
        unsqueeze_725 = torch.ops.aten.unsqueeze.default(unsqueeze_724, 5);  unsqueeze_724 = None
        permute_1159 = torch.ops.aten.permute.default(unsqueeze_725, [1, 4, 2, 5, 3, 0]);  unsqueeze_725 = None
        permute_1160 = torch.ops.aten.permute.default(permute_1158, [3, 5, 0, 1, 2, 4]);  permute_1158 = None
        view_2113 = torch.ops.aten.view.default(permute_1160, [1, 512, 384]);  permute_1160 = None
        permute_1161 = torch.ops.aten.permute.default(permute_1159, [5, 0, 1, 2, 4, 3]);  permute_1159 = None
        view_2114 = torch.ops.aten.view.default(permute_1161, [1, 384, 1536]);  permute_1161 = None
        bmm_176 = torch.ops.aten.bmm.default(view_2113, view_2114);  view_2113 = view_2114 = None
        view_2115 = torch.ops.aten.view.default(bmm_176, [512, 1, 4, 1, 16, 24]);  bmm_176 = None
        permute_1162 = torch.ops.aten.permute.default(view_2115, [2, 3, 4, 0, 5, 1]);  view_2115 = None
        view_2116 = torch.ops.aten.view.default(permute_1162, [4, 1, 16, 512, 24]);  permute_1162 = None
        unbind_int_99 = torch.ops.aten.unbind.int(view_2116);  view_2116 = None
        getitem_1984 = unbind_int_99[0]
        getitem_1985 = unbind_int_99[1]
        getitem_1986 = unbind_int_99[2]
        getitem_1987 = unbind_int_99[3];  unbind_int_99 = None
        view_2117 = torch.ops.aten.view.default(arg661_1, [1, 16, 1, 24]);  arg661_1 = None
        add_212 = torch.ops.aten.add.Tensor(getitem_1984, view_2117);  getitem_1984 = view_2117 = None
        _to_copy_1201 = torch.ops.aten._to_copy.default(add_212, dtype = torch.bfloat16);  add_212 = None
        expand_126 = torch.ops.aten.expand.default(masked_fill_137, [1, 16, 512, 512]);  masked_fill_137 = None
        _scaled_dot_product_efficient_attention_default_71 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1201, getitem_1985, getitem_1986, expand_126, False);  _to_copy_1201 = getitem_1985 = getitem_1986 = expand_126 = None
        getitem_1988 = _scaled_dot_product_efficient_attention_default_71[0]
        add_213 = torch.ops.aten.add.Tensor(getitem_1987, 1);  getitem_1987 = None
        sigmoid_161 = torch.ops.aten.sigmoid.default(add_213);  add_213 = None
        mul_264 = torch.ops.aten.mul.Tensor(getitem_1988, sigmoid_161);  getitem_1988 = sigmoid_161 = None
        _to_copy_1202 = torch.ops.aten._to_copy.default(arg663_1, dtype = torch.bfloat16);  arg663_1 = None
        unsqueeze_726 = torch.ops.aten.unsqueeze.default(mul_264, 4);  mul_264 = None
        permute_1163 = torch.ops.aten.permute.default(unsqueeze_726, [0, 2, 4, 3, 1]);  unsqueeze_726 = None
        unsqueeze_727 = torch.ops.aten.unsqueeze.default(_to_copy_1202, 3);  _to_copy_1202 = None
        unsqueeze_728 = torch.ops.aten.unsqueeze.default(unsqueeze_727, 4);  unsqueeze_727 = None
        permute_1164 = torch.ops.aten.permute.default(unsqueeze_728, [3, 4, 2, 1, 0]);  unsqueeze_728 = None
        permute_1165 = torch.ops.aten.permute.default(permute_1163, [1, 3, 4, 0, 2]);  permute_1163 = None
        clone_193 = torch.ops.aten.clone.default(permute_1165, memory_format = torch.contiguous_format);  permute_1165 = None
        _unsafe_view_166 = torch.ops.aten._unsafe_view.default(clone_193, [1, 512, 384]);  clone_193 = None
        permute_1166 = torch.ops.aten.permute.default(permute_1164, [3, 4, 0, 2, 1]);  permute_1164 = None
        clone_194 = torch.ops.aten.clone.default(permute_1166, memory_format = torch.contiguous_format);  permute_1166 = None
        _unsafe_view_167 = torch.ops.aten._unsafe_view.default(clone_194, [1, 384, 384]);  clone_194 = None
        bmm_177 = torch.ops.aten.bmm.default(_unsafe_view_166, _unsafe_view_167);  _unsafe_view_166 = _unsafe_view_167 = None
        view_2118 = torch.ops.aten.view.default(bmm_177, [512, 1, 1, 1, 384]);  bmm_177 = None
        permute_1167 = torch.ops.aten.permute.default(view_2118, [3, 0, 4, 1, 2]);  view_2118 = None
        view_2119 = torch.ops.aten.view.default(permute_1167, [1, 512, 384]);  permute_1167 = None
        unsqueeze_729 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_265 = torch.ops.aten.mul.Tensor(view_2119, unsqueeze_729);  view_2119 = unsqueeze_729 = None
        add_214 = torch.ops.aten.add.Tensor(add_207, mul_265);  mul_265 = None
        split_tensor_209 = torch.ops.aten.split.Tensor(add_207, 512, dim = -2);  add_207 = None
        getitem_1992 = split_tensor_209[0];  split_tensor_209 = None
        _to_copy_1203 = torch.ops.aten._to_copy.default(getitem_1992, dtype = torch.float32);  getitem_1992 = None
        native_layer_norm_default_249 = torch.ops.aten.native_layer_norm.default(_to_copy_1203, [384], arg652_1, arg653_1, 1e-05);  _to_copy_1203 = arg652_1 = arg653_1 = None
        getitem_1993 = native_layer_norm_default_249[0]
        _to_copy_1204 = torch.ops.aten._to_copy.default(arg654_1, dtype = torch.bfloat16);  arg654_1 = None
        _to_copy_1205 = torch.ops.aten._to_copy.default(getitem_1993, dtype = torch.bfloat16);  getitem_1993 = None
        t_439 = torch.ops.aten.t.default(_to_copy_1204);  _to_copy_1204 = None
        view_2120 = torch.ops.aten.view.default(_to_copy_1205, [512, 384]);  _to_copy_1205 = None
        mm_409 = torch.ops.aten.mm.default(view_2120, t_439);  view_2120 = t_439 = None
        view_2121 = torch.ops.aten.view.default(mm_409, [1, 512, 1536]);  mm_409 = None
        split_tensor_210 = torch.ops.aten.split.Tensor(view_2121, 768, dim = -1);  view_2121 = None
        getitem_1996 = split_tensor_210[0]
        getitem_1997 = split_tensor_210[1];  split_tensor_210 = None
        silu_57 = torch.ops.aten.silu.default(getitem_1996);  getitem_1996 = None
        mul_266 = torch.ops.aten.mul.Tensor(silu_57, getitem_1997);  silu_57 = getitem_1997 = None
        _to_copy_1206 = torch.ops.aten._to_copy.default(arg655_1, dtype = torch.bfloat16);  arg655_1 = None
        t_440 = torch.ops.aten.t.default(_to_copy_1206);  _to_copy_1206 = None
        view_2123 = torch.ops.aten.view.default(mul_266, [512, 768]);  mul_266 = None
        mm_410 = torch.ops.aten.mm.default(view_2123, t_440);  view_2123 = t_440 = None
        view_2124 = torch.ops.aten.view.default(mm_410, [1, 512, 384]);  mm_410 = None
        add_215 = torch.ops.aten.add.Tensor(add_214, view_2124);  add_214 = view_2124 = None
        _to_copy_1207 = torch.ops.aten._to_copy.default(add_211, dtype = torch.float32)
        native_layer_norm_default_250 = torch.ops.aten.native_layer_norm.default(_to_copy_1207, [256], arg668_1, arg669_1, 1e-05);  _to_copy_1207 = arg668_1 = arg669_1 = None
        getitem_1998 = native_layer_norm_default_250[0]
        split_with_sizes_default_52 = torch.ops.aten.split_with_sizes.default(arg671_1, [512, 512]);  arg671_1 = None
        getitem_2001 = split_with_sizes_default_52[0]
        getitem_2002 = split_with_sizes_default_52[1];  split_with_sizes_default_52 = None
        split_with_sizes_default_53 = torch.ops.aten.split_with_sizes.default(arg672_1, [512, 512, 256]);  arg672_1 = None
        getitem_2003 = split_with_sizes_default_53[0]
        getitem_2004 = split_with_sizes_default_53[1]
        getitem_2005 = split_with_sizes_default_53[2];  split_with_sizes_default_53 = None
        _to_copy_1208 = torch.ops.aten._to_copy.default(getitem_2001, dtype = torch.bfloat16);  getitem_2001 = None
        _to_copy_1209 = torch.ops.aten._to_copy.default(getitem_1998, dtype = torch.bfloat16)
        t_441 = torch.ops.aten.t.default(_to_copy_1208);  _to_copy_1208 = None
        view_2125 = torch.ops.aten.view.default(_to_copy_1209, [262144, 256]);  _to_copy_1209 = None
        mm_411 = torch.ops.aten.mm.default(view_2125, t_441);  view_2125 = t_441 = None
        view_2126 = torch.ops.aten.view.default(mm_411, [1, 512, 512, 512]);  mm_411 = None
        _to_copy_1210 = torch.ops.aten._to_copy.default(getitem_2003, dtype = torch.bfloat16);  getitem_2003 = None
        _to_copy_1211 = torch.ops.aten._to_copy.default(getitem_1998, dtype = torch.bfloat16)
        t_442 = torch.ops.aten.t.default(_to_copy_1210);  _to_copy_1210 = None
        view_2127 = torch.ops.aten.view.default(_to_copy_1211, [262144, 256]);  _to_copy_1211 = None
        mm_412 = torch.ops.aten.mm.default(view_2127, t_442);  view_2127 = t_442 = None
        view_2128 = torch.ops.aten.view.default(mm_412, [1, 512, 512, 512]);  mm_412 = None
        sigmoid_162 = torch.ops.aten.sigmoid.default(view_2128);  view_2128 = None
        mul_267 = torch.ops.aten.mul.Tensor(view_2126, sigmoid_162);  view_2126 = sigmoid_162 = None
        unsqueeze_730 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_138 = torch.ops.aten.bitwise_not.default(unsqueeze_730);  unsqueeze_730 = None
        masked_fill_138 = torch.ops.aten.masked_fill.Scalar(mul_267, bitwise_not_138, 0);  mul_267 = bitwise_not_138 = None
        split_tensor_211 = torch.ops.aten.split.Tensor(masked_fill_138, 256, dim = -1)
        getitem_2008 = split_tensor_211[0]
        unsqueeze_733 = torch.ops.aten.unsqueeze.default(getitem_2008, 4);  getitem_2008 = None
        permute_1172 = torch.ops.aten.permute.default(unsqueeze_733, [0, 1, 4, 3, 2]);  unsqueeze_733 = None
        permute_1173 = torch.ops.aten.permute.default(permute_1172, [3, 1, 4, 0, 2]);  permute_1172 = None
        view_2131 = torch.ops.aten.view.default(permute_1173, [256, 512, 512]);  permute_1173 = None
        split_tensor_212 = torch.ops.aten.split.Tensor(masked_fill_138, 256, dim = -1);  masked_fill_138 = None
        getitem_2011 = split_tensor_212[1];  split_tensor_212 = None
        unsqueeze_734 = torch.ops.aten.unsqueeze.default(getitem_2011, 4);  getitem_2011 = None
        permute_1174 = torch.ops.aten.permute.default(unsqueeze_734, [0, 4, 1, 3, 2]);  unsqueeze_734 = None
        permute_1175 = torch.ops.aten.permute.default(permute_1174, [3, 4, 0, 2, 1]);  permute_1174 = None
        view_2132 = torch.ops.aten.view.default(permute_1175, [256, 512, 512]);  permute_1175 = None
        bmm_178 = torch.ops.aten.bmm.default(view_2131, view_2132);  view_2131 = view_2132 = None
        view_2133 = torch.ops.aten.view.default(bmm_178, [256, 512, 1, 1, 512]);  bmm_178 = None
        permute_1176 = torch.ops.aten.permute.default(view_2133, [3, 1, 4, 0, 2]);  view_2133 = None
        view_2134 = torch.ops.aten.view.default(permute_1176, [1, 512, 512, 256]);  permute_1176 = None
        _to_copy_1212 = torch.ops.aten._to_copy.default(getitem_2002, dtype = torch.bfloat16);  getitem_2002 = None
        _to_copy_1213 = torch.ops.aten._to_copy.default(getitem_1998, dtype = torch.bfloat16)
        t_443 = torch.ops.aten.t.default(_to_copy_1212);  _to_copy_1212 = None
        view_2135 = torch.ops.aten.view.default(_to_copy_1213, [262144, 256]);  _to_copy_1213 = None
        mm_413 = torch.ops.aten.mm.default(view_2135, t_443);  view_2135 = t_443 = None
        view_2136 = torch.ops.aten.view.default(mm_413, [1, 512, 512, 512]);  mm_413 = None
        _to_copy_1214 = torch.ops.aten._to_copy.default(getitem_2004, dtype = torch.bfloat16);  getitem_2004 = None
        _to_copy_1215 = torch.ops.aten._to_copy.default(getitem_1998, dtype = torch.bfloat16)
        t_444 = torch.ops.aten.t.default(_to_copy_1214);  _to_copy_1214 = None
        view_2137 = torch.ops.aten.view.default(_to_copy_1215, [262144, 256]);  _to_copy_1215 = None
        mm_414 = torch.ops.aten.mm.default(view_2137, t_444);  view_2137 = t_444 = None
        view_2138 = torch.ops.aten.view.default(mm_414, [1, 512, 512, 512]);  mm_414 = None
        sigmoid_163 = torch.ops.aten.sigmoid.default(view_2138);  view_2138 = None
        mul_268 = torch.ops.aten.mul.Tensor(view_2136, sigmoid_163);  view_2136 = sigmoid_163 = None
        view_2139 = torch.ops.aten.view.default(mul_268, [262144, 512]);  mul_268 = None
        view_2140 = torch.ops.aten.view.default(view_2139, [1, 512, 512, 512]);  view_2139 = None
        transpose_52 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_735 = torch.ops.aten.unsqueeze.default(transpose_52, 3);  transpose_52 = None
        clone_195 = torch.ops.aten.clone.default(unsqueeze_735, memory_format = torch.contiguous_format);  unsqueeze_735 = None
        bitwise_not_139 = torch.ops.aten.bitwise_not.default(clone_195);  clone_195 = None
        masked_fill_139 = torch.ops.aten.masked_fill.Scalar(view_2140, bitwise_not_139, 0);  view_2140 = bitwise_not_139 = None
        view_2141 = torch.ops.aten.view.default(masked_fill_139, [262144, 512]);  masked_fill_139 = None
        view_2145 = torch.ops.aten.view.default(view_2141, [1, 512, 512, 512])
        split_tensor_213 = torch.ops.aten.split.Tensor(view_2145, 256, dim = -1);  view_2145 = None
        getitem_2014 = split_tensor_213[0]
        unsqueeze_738 = torch.ops.aten.unsqueeze.default(getitem_2014, 4);  getitem_2014 = None
        permute_1181 = torch.ops.aten.permute.default(unsqueeze_738, [0, 2, 4, 3, 1]);  unsqueeze_738 = None
        permute_1182 = torch.ops.aten.permute.default(permute_1181, [3, 1, 4, 0, 2]);  permute_1181 = None
        view_2146 = torch.ops.aten.view.default(permute_1182, [256, 512, 512]);  permute_1182 = None
        view_2147 = torch.ops.aten.view.default(view_2141, [1, 512, 512, 512]);  view_2141 = None
        split_tensor_214 = torch.ops.aten.split.Tensor(view_2147, 256, dim = -1);  view_2147 = None
        getitem_2017 = split_tensor_214[1];  split_tensor_214 = None
        unsqueeze_739 = torch.ops.aten.unsqueeze.default(getitem_2017, 4);  getitem_2017 = None
        permute_1183 = torch.ops.aten.permute.default(unsqueeze_739, [0, 4, 2, 3, 1]);  unsqueeze_739 = None
        permute_1184 = torch.ops.aten.permute.default(permute_1183, [3, 4, 0, 2, 1]);  permute_1183 = None
        view_2148 = torch.ops.aten.view.default(permute_1184, [256, 512, 512]);  permute_1184 = None
        bmm_179 = torch.ops.aten.bmm.default(view_2146, view_2148);  view_2146 = view_2148 = None
        view_2149 = torch.ops.aten.view.default(bmm_179, [256, 512, 1, 1, 512]);  bmm_179 = None
        permute_1185 = torch.ops.aten.permute.default(view_2149, [3, 1, 4, 0, 2]);  view_2149 = None
        view_2150 = torch.ops.aten.view.default(permute_1185, [1, 512, 512, 256]);  permute_1185 = None
        _to_copy_1216 = torch.ops.aten._to_copy.default(view_2134, dtype = torch.float32);  view_2134 = None
        native_layer_norm_default_251 = torch.ops.aten.native_layer_norm.default(_to_copy_1216, [256], None, None, 1e-05);  _to_copy_1216 = None
        getitem_2018 = native_layer_norm_default_251[0]
        _to_copy_1217 = torch.ops.aten._to_copy.default(view_2150, dtype = torch.float32);  view_2150 = None
        native_layer_norm_default_252 = torch.ops.aten.native_layer_norm.default(_to_copy_1217, [256], None, None, 1e-05);  _to_copy_1217 = None
        getitem_2021 = native_layer_norm_default_252[0]
        add_216 = torch.ops.aten.add.Tensor(getitem_2018, getitem_2021);  getitem_2018 = getitem_2021 = None
        _to_copy_1218 = torch.ops.aten._to_copy.default(arg670_1, dtype = torch.bfloat16);  arg670_1 = None
        _to_copy_1219 = torch.ops.aten._to_copy.default(add_216, dtype = torch.bfloat16);  add_216 = None
        t_445 = torch.ops.aten.t.default(_to_copy_1218);  _to_copy_1218 = None
        view_2151 = torch.ops.aten.view.default(_to_copy_1219, [262144, 256]);  _to_copy_1219 = None
        mm_415 = torch.ops.aten.mm.default(view_2151, t_445);  view_2151 = t_445 = None
        view_2152 = torch.ops.aten.view.default(mm_415, [1, 512, 512, 256]);  mm_415 = None
        _to_copy_1220 = torch.ops.aten._to_copy.default(getitem_2005, dtype = torch.bfloat16);  getitem_2005 = None
        _to_copy_1221 = torch.ops.aten._to_copy.default(getitem_1998, dtype = torch.bfloat16);  getitem_1998 = None
        t_446 = torch.ops.aten.t.default(_to_copy_1220);  _to_copy_1220 = None
        view_2153 = torch.ops.aten.view.default(_to_copy_1221, [262144, 256]);  _to_copy_1221 = None
        mm_416 = torch.ops.aten.mm.default(view_2153, t_446);  view_2153 = t_446 = None
        view_2154 = torch.ops.aten.view.default(mm_416, [1, 512, 512, 256]);  mm_416 = None
        sigmoid_164 = torch.ops.aten.sigmoid.default(view_2154);  view_2154 = None
        mul_269 = torch.ops.aten.mul.Tensor(view_2152, sigmoid_164);  view_2152 = sigmoid_164 = None
        add_217 = torch.ops.aten.add.Tensor(add_211, mul_269);  mul_269 = None
        _to_copy_1222 = torch.ops.aten._to_copy.default(add_211, dtype = torch.float32)
        native_layer_norm_default_253 = torch.ops.aten.native_layer_norm.default(_to_copy_1222, [256], None, None, 1e-05);  _to_copy_1222 = None
        getitem_2024 = native_layer_norm_default_253[0]
        _to_copy_1223 = torch.ops.aten._to_copy.default(arg674_1, dtype = torch.bfloat16);  arg674_1 = None
        _to_copy_1224 = torch.ops.aten._to_copy.default(getitem_2024, dtype = torch.bfloat16)
        t_447 = torch.ops.aten.t.default(_to_copy_1223);  _to_copy_1223 = None
        view_2155 = torch.ops.aten.view.default(_to_copy_1224, [262144, 256]);  _to_copy_1224 = None
        mm_417 = torch.ops.aten.mm.default(view_2155, t_447);  view_2155 = t_447 = None
        view_2156 = torch.ops.aten.view.default(mm_417, [1, 512, 512, 8]);  mm_417 = None
        view_2157 = torch.ops.aten.view.default(view_2156, [1, 512, 512, 2, 4]);  view_2156 = None
        permute_1186 = torch.ops.aten.permute.default(view_2157, [0, 3, 4, 1, 2]);  view_2157 = None
        view_2158 = torch.ops.aten.view.default(permute_1186, [1, 2, 4, 1, 512, 512]);  permute_1186 = None
        view_2159 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_140 = torch.ops.aten.bitwise_not.default(view_2159);  view_2159 = None
        masked_fill_140 = torch.ops.aten.masked_fill.Scalar(view_2158, bitwise_not_140, -10000);  view_2158 = bitwise_not_140 = None
        view_2160 = torch.ops.aten.view.default(masked_fill_140, [1, 2, 4, 512, 512]);  masked_fill_140 = None
        permute_1187 = torch.ops.aten.permute.default(view_2160, [1, 0, 2, 3, 4]);  view_2160 = None
        view_2161 = torch.ops.aten.view.default(permute_1187, [2, 4, 1, 512, 512]);  permute_1187 = None
        _to_copy_1225 = torch.ops.aten._to_copy.default(arg675_1, dtype = torch.bfloat16);  arg675_1 = None
        _to_copy_1226 = torch.ops.aten._to_copy.default(getitem_2024, dtype = torch.bfloat16)
        t_448 = torch.ops.aten.t.default(_to_copy_1225);  _to_copy_1225 = None
        view_2162 = torch.ops.aten.view.default(_to_copy_1226, [262144, 256]);  _to_copy_1226 = None
        mm_418 = torch.ops.aten.mm.default(view_2162, t_448);  view_2162 = t_448 = None
        view_2163 = torch.ops.aten.view.default(mm_418, [1, 512, 512, 1024]);  mm_418 = None
        select_53 = torch.ops.aten.select.int(view_2161, 0, 0)
        view_2164 = torch.ops.aten.view.default(view_2163, [1, 512, 512, 4, 4, 64]);  view_2163 = None
        permute_1188 = torch.ops.aten.permute.default(view_2164, [4, 0, 3, 1, 2, 5]);  view_2164 = None
        view_2165 = torch.ops.aten.view.default(permute_1188, [4, 4, 512, 512, 64]);  permute_1188 = None
        unbind_int_100 = torch.ops.aten.unbind.int(view_2165);  view_2165 = None
        getitem_2027 = unbind_int_100[0]
        getitem_2028 = unbind_int_100[1]
        getitem_2029 = unbind_int_100[2]
        getitem_2030 = unbind_int_100[3];  unbind_int_100 = None
        expand_127 = torch.ops.aten.expand.default(select_53, [4, 512, 512, 512]);  select_53 = None
        _scaled_dot_product_efficient_attention_default_72 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2027, getitem_2028, getitem_2029, expand_127, False);  getitem_2027 = getitem_2028 = getitem_2029 = expand_127 = None
        getitem_2031 = _scaled_dot_product_efficient_attention_default_72[0]
        sigmoid_165 = torch.ops.aten.sigmoid.default(getitem_2030);  getitem_2030 = None
        mul_270 = torch.ops.aten.mul.Tensor(getitem_2031, sigmoid_165);  getitem_2031 = sigmoid_165 = None
        view_2166 = torch.ops.aten.view.default(mul_270, [1, 4, 512, 512, 64]);  mul_270 = None
        permute_1189 = torch.ops.aten.permute.default(view_2166, [0, 2, 3, 1, 4]);  view_2166 = None
        clone_196 = torch.ops.aten.clone.default(permute_1189, memory_format = torch.contiguous_format);  permute_1189 = None
        _unsafe_view_168 = torch.ops.aten._unsafe_view.default(clone_196, [1, 512, 512, 256]);  clone_196 = None
        transpose_53 = torch.ops.aten.transpose.int(getitem_2024, 1, 2);  getitem_2024 = None
        _to_copy_1227 = torch.ops.aten._to_copy.default(arg676_1, dtype = torch.bfloat16);  arg676_1 = None
        _to_copy_1228 = torch.ops.aten._to_copy.default(transpose_53, dtype = torch.bfloat16);  transpose_53 = None
        t_449 = torch.ops.aten.t.default(_to_copy_1227);  _to_copy_1227 = None
        expand_128 = torch.ops.aten.expand.default(_to_copy_1228, [1, 512, 512, 256]);  _to_copy_1228 = None
        view_2167 = torch.ops.aten.view.default(expand_128, [512, 512, 256]);  expand_128 = None
        expand_129 = torch.ops.aten.expand.default(t_449, [1, 512, 256, 1024]);  t_449 = None
        view_2168 = torch.ops.aten.view.default(expand_129, [512, 256, 1024]);  expand_129 = None
        bmm_180 = torch.ops.aten.bmm.default(view_2167, view_2168);  view_2167 = view_2168 = None
        view_2169 = torch.ops.aten.view.default(bmm_180, [1, 512, 512, 1024]);  bmm_180 = None
        select_54 = torch.ops.aten.select.int(view_2161, 0, 1);  view_2161 = None
        view_2170 = torch.ops.aten.view.default(view_2169, [1, 512, 512, 4, 4, 64]);  view_2169 = None
        permute_1190 = torch.ops.aten.permute.default(view_2170, [4, 0, 3, 1, 2, 5]);  view_2170 = None
        view_2171 = torch.ops.aten.view.default(permute_1190, [4, 4, 512, 512, 64]);  permute_1190 = None
        unbind_int_101 = torch.ops.aten.unbind.int(view_2171);  view_2171 = None
        getitem_2035 = unbind_int_101[0]
        getitem_2036 = unbind_int_101[1]
        getitem_2037 = unbind_int_101[2]
        getitem_2038 = unbind_int_101[3];  unbind_int_101 = None
        expand_130 = torch.ops.aten.expand.default(select_54, [4, 512, 512, 512]);  select_54 = None
        _scaled_dot_product_efficient_attention_default_73 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2035, getitem_2036, getitem_2037, expand_130, False);  getitem_2035 = getitem_2036 = getitem_2037 = expand_130 = None
        getitem_2039 = _scaled_dot_product_efficient_attention_default_73[0]
        sigmoid_166 = torch.ops.aten.sigmoid.default(getitem_2038);  getitem_2038 = None
        mul_271 = torch.ops.aten.mul.Tensor(getitem_2039, sigmoid_166);  getitem_2039 = sigmoid_166 = None
        view_2172 = torch.ops.aten.view.default(mul_271, [1, 4, 512, 512, 64]);  mul_271 = None
        permute_1191 = torch.ops.aten.permute.default(view_2172, [0, 2, 3, 1, 4]);  view_2172 = None
        clone_197 = torch.ops.aten.clone.default(permute_1191, memory_format = torch.contiguous_format);  permute_1191 = None
        _unsafe_view_169 = torch.ops.aten._unsafe_view.default(clone_197, [1, 512, 512, 256]);  clone_197 = None
        cat_32 = torch.ops.aten.cat.default([_unsafe_view_168, _unsafe_view_169], dim = -1);  _unsafe_view_168 = _unsafe_view_169 = None
        slice_225 = torch.ops.aten.slice.Tensor(arg673_1, dim = 0, start = 0, end = 9223372036854775807);  arg673_1 = None
        unsqueeze_740 = torch.ops.aten.unsqueeze.default(slice_225, 1);  slice_225 = None
        mul_272 = torch.ops.aten.mul.Tensor(arg677_1, unsqueeze_740);  arg677_1 = unsqueeze_740 = None
        _to_copy_1229 = torch.ops.aten._to_copy.default(mul_272, dtype = torch.bfloat16);  mul_272 = None
        t_450 = torch.ops.aten.t.default(_to_copy_1229);  _to_copy_1229 = None
        view_2173 = torch.ops.aten.view.default(cat_32, [262144, 512]);  cat_32 = None
        mm_419 = torch.ops.aten.mm.default(view_2173, t_450);  view_2173 = t_450 = None
        view_2174 = torch.ops.aten.view.default(mm_419, [1, 512, 512, 256]);  mm_419 = None
        add_218 = torch.ops.aten.add.Tensor(add_217, view_2174);  add_217 = view_2174 = None
        split_tensor_215 = torch.ops.aten.split.Tensor(add_211, 512, dim = -2)
        getitem_2043 = split_tensor_215[0];  split_tensor_215 = None
        _to_copy_1230 = torch.ops.aten._to_copy.default(getitem_2043, dtype = torch.float32);  getitem_2043 = None
        native_layer_norm_default_254 = torch.ops.aten.native_layer_norm.default(_to_copy_1230, [256], arg664_1, arg665_1, 1e-05);  _to_copy_1230 = arg664_1 = arg665_1 = None
        getitem_2044 = native_layer_norm_default_254[0]
        _to_copy_1231 = torch.ops.aten._to_copy.default(arg666_1, dtype = torch.bfloat16);  arg666_1 = None
        _to_copy_1232 = torch.ops.aten._to_copy.default(getitem_2044, dtype = torch.bfloat16);  getitem_2044 = None
        t_451 = torch.ops.aten.t.default(_to_copy_1231);  _to_copy_1231 = None
        view_2175 = torch.ops.aten.view.default(_to_copy_1232, [262144, 256]);  _to_copy_1232 = None
        mm_420 = torch.ops.aten.mm.default(view_2175, t_451);  view_2175 = t_451 = None
        view_2176 = torch.ops.aten.view.default(mm_420, [1, 512, 512, 1024]);  mm_420 = None
        split_tensor_216 = torch.ops.aten.split.Tensor(view_2176, 512, dim = -1);  view_2176 = None
        getitem_2047 = split_tensor_216[0]
        getitem_2048 = split_tensor_216[1];  split_tensor_216 = None
        silu_58 = torch.ops.aten.silu.default(getitem_2047);  getitem_2047 = None
        mul_273 = torch.ops.aten.mul.Tensor(silu_58, getitem_2048);  silu_58 = getitem_2048 = None
        _to_copy_1233 = torch.ops.aten._to_copy.default(arg667_1, dtype = torch.bfloat16);  arg667_1 = None
        t_452 = torch.ops.aten.t.default(_to_copy_1233);  _to_copy_1233 = None
        view_2178 = torch.ops.aten.view.default(mul_273, [262144, 512]);  mul_273 = None
        mm_421 = torch.ops.aten.mm.default(view_2178, t_452);  view_2178 = t_452 = None
        view_2179 = torch.ops.aten.view.default(mm_421, [1, 512, 512, 256]);  mm_421 = None
        add_219 = torch.ops.aten.add.Tensor(add_218, view_2179);  add_218 = view_2179 = None
        _to_copy_1234 = torch.ops.aten._to_copy.default(add_215, dtype = torch.float32)
        native_layer_norm_default_255 = torch.ops.aten.native_layer_norm.default(_to_copy_1234, [384], arg682_1, arg683_1, 1e-05);  _to_copy_1234 = arg682_1 = arg683_1 = None
        getitem_2049 = native_layer_norm_default_255[0]
        _to_copy_1235 = torch.ops.aten._to_copy.default(add_211, dtype = torch.float32);  add_211 = None
        native_layer_norm_default_256 = torch.ops.aten.native_layer_norm.default(_to_copy_1235, [256], arg684_1, arg685_1, 1e-05);  _to_copy_1235 = arg684_1 = arg685_1 = None
        getitem_2052 = native_layer_norm_default_256[0]
        _to_copy_1236 = torch.ops.aten._to_copy.default(arg686_1, dtype = torch.bfloat16);  arg686_1 = None
        _to_copy_1237 = torch.ops.aten._to_copy.default(getitem_2052, dtype = torch.bfloat16);  getitem_2052 = None
        t_453 = torch.ops.aten.t.default(_to_copy_1236);  _to_copy_1236 = None
        view_2180 = torch.ops.aten.view.default(_to_copy_1237, [262144, 256]);  _to_copy_1237 = None
        mm_422 = torch.ops.aten.mm.default(view_2180, t_453);  view_2180 = t_453 = None
        view_2181 = torch.ops.aten.view.default(mm_422, [1, 512, 512, 16]);  mm_422 = None
        permute_1192 = torch.ops.aten.permute.default(view_2181, [0, 3, 1, 2]);  view_2181 = None
        view_2182 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_141 = torch.ops.aten.bitwise_not.default(view_2182);  view_2182 = None
        masked_fill_141 = torch.ops.aten.masked_fill.Scalar(permute_1192, bitwise_not_141, -10000);  permute_1192 = bitwise_not_141 = None
        _to_copy_1238 = torch.ops.aten._to_copy.default(getitem_2049, dtype = torch.bfloat16);  getitem_2049 = None
        _to_copy_1239 = torch.ops.aten._to_copy.default(arg688_1, dtype = torch.bfloat16);  arg688_1 = None
        unsqueeze_741 = torch.ops.aten.unsqueeze.default(_to_copy_1238, 3);  _to_copy_1238 = None
        unsqueeze_742 = torch.ops.aten.unsqueeze.default(unsqueeze_741, 4);  unsqueeze_741 = None
        unsqueeze_743 = torch.ops.aten.unsqueeze.default(unsqueeze_742, 5);  unsqueeze_742 = None
        permute_1193 = torch.ops.aten.permute.default(unsqueeze_743, [3, 0, 4, 1, 5, 2]);  unsqueeze_743 = None
        unsqueeze_744 = torch.ops.aten.unsqueeze.default(_to_copy_1239, 4);  _to_copy_1239 = None
        unsqueeze_745 = torch.ops.aten.unsqueeze.default(unsqueeze_744, 5);  unsqueeze_744 = None
        permute_1194 = torch.ops.aten.permute.default(unsqueeze_745, [1, 4, 2, 5, 3, 0]);  unsqueeze_745 = None
        permute_1195 = torch.ops.aten.permute.default(permute_1193, [3, 5, 0, 1, 2, 4]);  permute_1193 = None
        view_2183 = torch.ops.aten.view.default(permute_1195, [1, 512, 384]);  permute_1195 = None
        permute_1196 = torch.ops.aten.permute.default(permute_1194, [5, 0, 1, 2, 4, 3]);  permute_1194 = None
        view_2184 = torch.ops.aten.view.default(permute_1196, [1, 384, 1536]);  permute_1196 = None
        bmm_181 = torch.ops.aten.bmm.default(view_2183, view_2184);  view_2183 = view_2184 = None
        view_2185 = torch.ops.aten.view.default(bmm_181, [512, 1, 4, 1, 16, 24]);  bmm_181 = None
        permute_1197 = torch.ops.aten.permute.default(view_2185, [2, 3, 4, 0, 5, 1]);  view_2185 = None
        view_2186 = torch.ops.aten.view.default(permute_1197, [4, 1, 16, 512, 24]);  permute_1197 = None
        unbind_int_102 = torch.ops.aten.unbind.int(view_2186);  view_2186 = None
        getitem_2055 = unbind_int_102[0]
        getitem_2056 = unbind_int_102[1]
        getitem_2057 = unbind_int_102[2]
        getitem_2058 = unbind_int_102[3];  unbind_int_102 = None
        view_2187 = torch.ops.aten.view.default(arg687_1, [1, 16, 1, 24]);  arg687_1 = None
        add_220 = torch.ops.aten.add.Tensor(getitem_2055, view_2187);  getitem_2055 = view_2187 = None
        _to_copy_1240 = torch.ops.aten._to_copy.default(add_220, dtype = torch.bfloat16);  add_220 = None
        expand_131 = torch.ops.aten.expand.default(masked_fill_141, [1, 16, 512, 512]);  masked_fill_141 = None
        _scaled_dot_product_efficient_attention_default_74 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1240, getitem_2056, getitem_2057, expand_131, False);  _to_copy_1240 = getitem_2056 = getitem_2057 = expand_131 = None
        getitem_2059 = _scaled_dot_product_efficient_attention_default_74[0]
        add_221 = torch.ops.aten.add.Tensor(getitem_2058, 1);  getitem_2058 = None
        sigmoid_167 = torch.ops.aten.sigmoid.default(add_221);  add_221 = None
        mul_274 = torch.ops.aten.mul.Tensor(getitem_2059, sigmoid_167);  getitem_2059 = sigmoid_167 = None
        _to_copy_1241 = torch.ops.aten._to_copy.default(arg689_1, dtype = torch.bfloat16);  arg689_1 = None
        unsqueeze_746 = torch.ops.aten.unsqueeze.default(mul_274, 4);  mul_274 = None
        permute_1198 = torch.ops.aten.permute.default(unsqueeze_746, [0, 2, 4, 3, 1]);  unsqueeze_746 = None
        unsqueeze_747 = torch.ops.aten.unsqueeze.default(_to_copy_1241, 3);  _to_copy_1241 = None
        unsqueeze_748 = torch.ops.aten.unsqueeze.default(unsqueeze_747, 4);  unsqueeze_747 = None
        permute_1199 = torch.ops.aten.permute.default(unsqueeze_748, [3, 4, 2, 1, 0]);  unsqueeze_748 = None
        permute_1200 = torch.ops.aten.permute.default(permute_1198, [1, 3, 4, 0, 2]);  permute_1198 = None
        clone_198 = torch.ops.aten.clone.default(permute_1200, memory_format = torch.contiguous_format);  permute_1200 = None
        _unsafe_view_170 = torch.ops.aten._unsafe_view.default(clone_198, [1, 512, 384]);  clone_198 = None
        permute_1201 = torch.ops.aten.permute.default(permute_1199, [3, 4, 0, 2, 1]);  permute_1199 = None
        clone_199 = torch.ops.aten.clone.default(permute_1201, memory_format = torch.contiguous_format);  permute_1201 = None
        _unsafe_view_171 = torch.ops.aten._unsafe_view.default(clone_199, [1, 384, 384]);  clone_199 = None
        bmm_182 = torch.ops.aten.bmm.default(_unsafe_view_170, _unsafe_view_171);  _unsafe_view_170 = _unsafe_view_171 = None
        view_2188 = torch.ops.aten.view.default(bmm_182, [512, 1, 1, 1, 384]);  bmm_182 = None
        permute_1202 = torch.ops.aten.permute.default(view_2188, [3, 0, 4, 1, 2]);  view_2188 = None
        view_2189 = torch.ops.aten.view.default(permute_1202, [1, 512, 384]);  permute_1202 = None
        unsqueeze_749 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_275 = torch.ops.aten.mul.Tensor(view_2189, unsqueeze_749);  view_2189 = unsqueeze_749 = None
        add_222 = torch.ops.aten.add.Tensor(add_215, mul_275);  mul_275 = None
        split_tensor_217 = torch.ops.aten.split.Tensor(add_215, 512, dim = -2);  add_215 = None
        getitem_2063 = split_tensor_217[0];  split_tensor_217 = None
        _to_copy_1242 = torch.ops.aten._to_copy.default(getitem_2063, dtype = torch.float32);  getitem_2063 = None
        native_layer_norm_default_257 = torch.ops.aten.native_layer_norm.default(_to_copy_1242, [384], arg678_1, arg679_1, 1e-05);  _to_copy_1242 = arg678_1 = arg679_1 = None
        getitem_2064 = native_layer_norm_default_257[0]
        _to_copy_1243 = torch.ops.aten._to_copy.default(arg680_1, dtype = torch.bfloat16);  arg680_1 = None
        _to_copy_1244 = torch.ops.aten._to_copy.default(getitem_2064, dtype = torch.bfloat16);  getitem_2064 = None
        t_454 = torch.ops.aten.t.default(_to_copy_1243);  _to_copy_1243 = None
        view_2190 = torch.ops.aten.view.default(_to_copy_1244, [512, 384]);  _to_copy_1244 = None
        mm_423 = torch.ops.aten.mm.default(view_2190, t_454);  view_2190 = t_454 = None
        view_2191 = torch.ops.aten.view.default(mm_423, [1, 512, 1536]);  mm_423 = None
        split_tensor_218 = torch.ops.aten.split.Tensor(view_2191, 768, dim = -1);  view_2191 = None
        getitem_2067 = split_tensor_218[0]
        getitem_2068 = split_tensor_218[1];  split_tensor_218 = None
        silu_59 = torch.ops.aten.silu.default(getitem_2067);  getitem_2067 = None
        mul_276 = torch.ops.aten.mul.Tensor(silu_59, getitem_2068);  silu_59 = getitem_2068 = None
        _to_copy_1245 = torch.ops.aten._to_copy.default(arg681_1, dtype = torch.bfloat16);  arg681_1 = None
        t_455 = torch.ops.aten.t.default(_to_copy_1245);  _to_copy_1245 = None
        view_2193 = torch.ops.aten.view.default(mul_276, [512, 768]);  mul_276 = None
        mm_424 = torch.ops.aten.mm.default(view_2193, t_455);  view_2193 = t_455 = None
        view_2194 = torch.ops.aten.view.default(mm_424, [1, 512, 384]);  mm_424 = None
        add_223 = torch.ops.aten.add.Tensor(add_222, view_2194);  add_222 = view_2194 = None
        _to_copy_1246 = torch.ops.aten._to_copy.default(add_219, dtype = torch.float32)
        native_layer_norm_default_258 = torch.ops.aten.native_layer_norm.default(_to_copy_1246, [256], arg694_1, arg695_1, 1e-05);  _to_copy_1246 = arg694_1 = arg695_1 = None
        getitem_2069 = native_layer_norm_default_258[0]
        split_with_sizes_default_54 = torch.ops.aten.split_with_sizes.default(arg697_1, [512, 512]);  arg697_1 = None
        getitem_2072 = split_with_sizes_default_54[0]
        getitem_2073 = split_with_sizes_default_54[1];  split_with_sizes_default_54 = None
        split_with_sizes_default_55 = torch.ops.aten.split_with_sizes.default(arg698_1, [512, 512, 256]);  arg698_1 = None
        getitem_2074 = split_with_sizes_default_55[0]
        getitem_2075 = split_with_sizes_default_55[1]
        getitem_2076 = split_with_sizes_default_55[2];  split_with_sizes_default_55 = None
        _to_copy_1247 = torch.ops.aten._to_copy.default(getitem_2072, dtype = torch.bfloat16);  getitem_2072 = None
        _to_copy_1248 = torch.ops.aten._to_copy.default(getitem_2069, dtype = torch.bfloat16)
        t_456 = torch.ops.aten.t.default(_to_copy_1247);  _to_copy_1247 = None
        view_2195 = torch.ops.aten.view.default(_to_copy_1248, [262144, 256]);  _to_copy_1248 = None
        mm_425 = torch.ops.aten.mm.default(view_2195, t_456);  view_2195 = t_456 = None
        view_2196 = torch.ops.aten.view.default(mm_425, [1, 512, 512, 512]);  mm_425 = None
        _to_copy_1249 = torch.ops.aten._to_copy.default(getitem_2074, dtype = torch.bfloat16);  getitem_2074 = None
        _to_copy_1250 = torch.ops.aten._to_copy.default(getitem_2069, dtype = torch.bfloat16)
        t_457 = torch.ops.aten.t.default(_to_copy_1249);  _to_copy_1249 = None
        view_2197 = torch.ops.aten.view.default(_to_copy_1250, [262144, 256]);  _to_copy_1250 = None
        mm_426 = torch.ops.aten.mm.default(view_2197, t_457);  view_2197 = t_457 = None
        view_2198 = torch.ops.aten.view.default(mm_426, [1, 512, 512, 512]);  mm_426 = None
        sigmoid_168 = torch.ops.aten.sigmoid.default(view_2198);  view_2198 = None
        mul_277 = torch.ops.aten.mul.Tensor(view_2196, sigmoid_168);  view_2196 = sigmoid_168 = None
        unsqueeze_750 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_142 = torch.ops.aten.bitwise_not.default(unsqueeze_750);  unsqueeze_750 = None
        masked_fill_142 = torch.ops.aten.masked_fill.Scalar(mul_277, bitwise_not_142, 0);  mul_277 = bitwise_not_142 = None
        split_tensor_219 = torch.ops.aten.split.Tensor(masked_fill_142, 256, dim = -1)
        getitem_2079 = split_tensor_219[0]
        unsqueeze_753 = torch.ops.aten.unsqueeze.default(getitem_2079, 4);  getitem_2079 = None
        permute_1207 = torch.ops.aten.permute.default(unsqueeze_753, [0, 1, 4, 3, 2]);  unsqueeze_753 = None
        permute_1208 = torch.ops.aten.permute.default(permute_1207, [3, 1, 4, 0, 2]);  permute_1207 = None
        view_2201 = torch.ops.aten.view.default(permute_1208, [256, 512, 512]);  permute_1208 = None
        split_tensor_220 = torch.ops.aten.split.Tensor(masked_fill_142, 256, dim = -1);  masked_fill_142 = None
        getitem_2082 = split_tensor_220[1];  split_tensor_220 = None
        unsqueeze_754 = torch.ops.aten.unsqueeze.default(getitem_2082, 4);  getitem_2082 = None
        permute_1209 = torch.ops.aten.permute.default(unsqueeze_754, [0, 4, 1, 3, 2]);  unsqueeze_754 = None
        permute_1210 = torch.ops.aten.permute.default(permute_1209, [3, 4, 0, 2, 1]);  permute_1209 = None
        view_2202 = torch.ops.aten.view.default(permute_1210, [256, 512, 512]);  permute_1210 = None
        bmm_183 = torch.ops.aten.bmm.default(view_2201, view_2202);  view_2201 = view_2202 = None
        view_2203 = torch.ops.aten.view.default(bmm_183, [256, 512, 1, 1, 512]);  bmm_183 = None
        permute_1211 = torch.ops.aten.permute.default(view_2203, [3, 1, 4, 0, 2]);  view_2203 = None
        view_2204 = torch.ops.aten.view.default(permute_1211, [1, 512, 512, 256]);  permute_1211 = None
        _to_copy_1251 = torch.ops.aten._to_copy.default(getitem_2073, dtype = torch.bfloat16);  getitem_2073 = None
        _to_copy_1252 = torch.ops.aten._to_copy.default(getitem_2069, dtype = torch.bfloat16)
        t_458 = torch.ops.aten.t.default(_to_copy_1251);  _to_copy_1251 = None
        view_2205 = torch.ops.aten.view.default(_to_copy_1252, [262144, 256]);  _to_copy_1252 = None
        mm_427 = torch.ops.aten.mm.default(view_2205, t_458);  view_2205 = t_458 = None
        view_2206 = torch.ops.aten.view.default(mm_427, [1, 512, 512, 512]);  mm_427 = None
        _to_copy_1253 = torch.ops.aten._to_copy.default(getitem_2075, dtype = torch.bfloat16);  getitem_2075 = None
        _to_copy_1254 = torch.ops.aten._to_copy.default(getitem_2069, dtype = torch.bfloat16)
        t_459 = torch.ops.aten.t.default(_to_copy_1253);  _to_copy_1253 = None
        view_2207 = torch.ops.aten.view.default(_to_copy_1254, [262144, 256]);  _to_copy_1254 = None
        mm_428 = torch.ops.aten.mm.default(view_2207, t_459);  view_2207 = t_459 = None
        view_2208 = torch.ops.aten.view.default(mm_428, [1, 512, 512, 512]);  mm_428 = None
        sigmoid_169 = torch.ops.aten.sigmoid.default(view_2208);  view_2208 = None
        mul_278 = torch.ops.aten.mul.Tensor(view_2206, sigmoid_169);  view_2206 = sigmoid_169 = None
        view_2209 = torch.ops.aten.view.default(mul_278, [262144, 512]);  mul_278 = None
        view_2210 = torch.ops.aten.view.default(view_2209, [1, 512, 512, 512]);  view_2209 = None
        transpose_54 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_755 = torch.ops.aten.unsqueeze.default(transpose_54, 3);  transpose_54 = None
        clone_200 = torch.ops.aten.clone.default(unsqueeze_755, memory_format = torch.contiguous_format);  unsqueeze_755 = None
        bitwise_not_143 = torch.ops.aten.bitwise_not.default(clone_200);  clone_200 = None
        masked_fill_143 = torch.ops.aten.masked_fill.Scalar(view_2210, bitwise_not_143, 0);  view_2210 = bitwise_not_143 = None
        view_2211 = torch.ops.aten.view.default(masked_fill_143, [262144, 512]);  masked_fill_143 = None
        view_2215 = torch.ops.aten.view.default(view_2211, [1, 512, 512, 512])
        split_tensor_221 = torch.ops.aten.split.Tensor(view_2215, 256, dim = -1);  view_2215 = None
        getitem_2085 = split_tensor_221[0]
        unsqueeze_758 = torch.ops.aten.unsqueeze.default(getitem_2085, 4);  getitem_2085 = None
        permute_1216 = torch.ops.aten.permute.default(unsqueeze_758, [0, 2, 4, 3, 1]);  unsqueeze_758 = None
        permute_1217 = torch.ops.aten.permute.default(permute_1216, [3, 1, 4, 0, 2]);  permute_1216 = None
        view_2216 = torch.ops.aten.view.default(permute_1217, [256, 512, 512]);  permute_1217 = None
        view_2217 = torch.ops.aten.view.default(view_2211, [1, 512, 512, 512]);  view_2211 = None
        split_tensor_222 = torch.ops.aten.split.Tensor(view_2217, 256, dim = -1);  view_2217 = None
        getitem_2088 = split_tensor_222[1];  split_tensor_222 = None
        unsqueeze_759 = torch.ops.aten.unsqueeze.default(getitem_2088, 4);  getitem_2088 = None
        permute_1218 = torch.ops.aten.permute.default(unsqueeze_759, [0, 4, 2, 3, 1]);  unsqueeze_759 = None
        permute_1219 = torch.ops.aten.permute.default(permute_1218, [3, 4, 0, 2, 1]);  permute_1218 = None
        view_2218 = torch.ops.aten.view.default(permute_1219, [256, 512, 512]);  permute_1219 = None
        bmm_184 = torch.ops.aten.bmm.default(view_2216, view_2218);  view_2216 = view_2218 = None
        view_2219 = torch.ops.aten.view.default(bmm_184, [256, 512, 1, 1, 512]);  bmm_184 = None
        permute_1220 = torch.ops.aten.permute.default(view_2219, [3, 1, 4, 0, 2]);  view_2219 = None
        view_2220 = torch.ops.aten.view.default(permute_1220, [1, 512, 512, 256]);  permute_1220 = None
        _to_copy_1255 = torch.ops.aten._to_copy.default(view_2204, dtype = torch.float32);  view_2204 = None
        native_layer_norm_default_259 = torch.ops.aten.native_layer_norm.default(_to_copy_1255, [256], None, None, 1e-05);  _to_copy_1255 = None
        getitem_2089 = native_layer_norm_default_259[0]
        _to_copy_1256 = torch.ops.aten._to_copy.default(view_2220, dtype = torch.float32);  view_2220 = None
        native_layer_norm_default_260 = torch.ops.aten.native_layer_norm.default(_to_copy_1256, [256], None, None, 1e-05);  _to_copy_1256 = None
        getitem_2092 = native_layer_norm_default_260[0]
        add_224 = torch.ops.aten.add.Tensor(getitem_2089, getitem_2092);  getitem_2089 = getitem_2092 = None
        _to_copy_1257 = torch.ops.aten._to_copy.default(arg696_1, dtype = torch.bfloat16);  arg696_1 = None
        _to_copy_1258 = torch.ops.aten._to_copy.default(add_224, dtype = torch.bfloat16);  add_224 = None
        t_460 = torch.ops.aten.t.default(_to_copy_1257);  _to_copy_1257 = None
        view_2221 = torch.ops.aten.view.default(_to_copy_1258, [262144, 256]);  _to_copy_1258 = None
        mm_429 = torch.ops.aten.mm.default(view_2221, t_460);  view_2221 = t_460 = None
        view_2222 = torch.ops.aten.view.default(mm_429, [1, 512, 512, 256]);  mm_429 = None
        _to_copy_1259 = torch.ops.aten._to_copy.default(getitem_2076, dtype = torch.bfloat16);  getitem_2076 = None
        _to_copy_1260 = torch.ops.aten._to_copy.default(getitem_2069, dtype = torch.bfloat16);  getitem_2069 = None
        t_461 = torch.ops.aten.t.default(_to_copy_1259);  _to_copy_1259 = None
        view_2223 = torch.ops.aten.view.default(_to_copy_1260, [262144, 256]);  _to_copy_1260 = None
        mm_430 = torch.ops.aten.mm.default(view_2223, t_461);  view_2223 = t_461 = None
        view_2224 = torch.ops.aten.view.default(mm_430, [1, 512, 512, 256]);  mm_430 = None
        sigmoid_170 = torch.ops.aten.sigmoid.default(view_2224);  view_2224 = None
        mul_279 = torch.ops.aten.mul.Tensor(view_2222, sigmoid_170);  view_2222 = sigmoid_170 = None
        add_225 = torch.ops.aten.add.Tensor(add_219, mul_279);  mul_279 = None
        _to_copy_1261 = torch.ops.aten._to_copy.default(add_219, dtype = torch.float32)
        native_layer_norm_default_261 = torch.ops.aten.native_layer_norm.default(_to_copy_1261, [256], None, None, 1e-05);  _to_copy_1261 = None
        getitem_2095 = native_layer_norm_default_261[0]
        _to_copy_1262 = torch.ops.aten._to_copy.default(arg700_1, dtype = torch.bfloat16);  arg700_1 = None
        _to_copy_1263 = torch.ops.aten._to_copy.default(getitem_2095, dtype = torch.bfloat16)
        t_462 = torch.ops.aten.t.default(_to_copy_1262);  _to_copy_1262 = None
        view_2225 = torch.ops.aten.view.default(_to_copy_1263, [262144, 256]);  _to_copy_1263 = None
        mm_431 = torch.ops.aten.mm.default(view_2225, t_462);  view_2225 = t_462 = None
        view_2226 = torch.ops.aten.view.default(mm_431, [1, 512, 512, 8]);  mm_431 = None
        view_2227 = torch.ops.aten.view.default(view_2226, [1, 512, 512, 2, 4]);  view_2226 = None
        permute_1221 = torch.ops.aten.permute.default(view_2227, [0, 3, 4, 1, 2]);  view_2227 = None
        view_2228 = torch.ops.aten.view.default(permute_1221, [1, 2, 4, 1, 512, 512]);  permute_1221 = None
        view_2229 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_144 = torch.ops.aten.bitwise_not.default(view_2229);  view_2229 = None
        masked_fill_144 = torch.ops.aten.masked_fill.Scalar(view_2228, bitwise_not_144, -10000);  view_2228 = bitwise_not_144 = None
        view_2230 = torch.ops.aten.view.default(masked_fill_144, [1, 2, 4, 512, 512]);  masked_fill_144 = None
        permute_1222 = torch.ops.aten.permute.default(view_2230, [1, 0, 2, 3, 4]);  view_2230 = None
        view_2231 = torch.ops.aten.view.default(permute_1222, [2, 4, 1, 512, 512]);  permute_1222 = None
        _to_copy_1264 = torch.ops.aten._to_copy.default(arg701_1, dtype = torch.bfloat16);  arg701_1 = None
        _to_copy_1265 = torch.ops.aten._to_copy.default(getitem_2095, dtype = torch.bfloat16)
        t_463 = torch.ops.aten.t.default(_to_copy_1264);  _to_copy_1264 = None
        view_2232 = torch.ops.aten.view.default(_to_copy_1265, [262144, 256]);  _to_copy_1265 = None
        mm_432 = torch.ops.aten.mm.default(view_2232, t_463);  view_2232 = t_463 = None
        view_2233 = torch.ops.aten.view.default(mm_432, [1, 512, 512, 1024]);  mm_432 = None
        select_55 = torch.ops.aten.select.int(view_2231, 0, 0)
        view_2234 = torch.ops.aten.view.default(view_2233, [1, 512, 512, 4, 4, 64]);  view_2233 = None
        permute_1223 = torch.ops.aten.permute.default(view_2234, [4, 0, 3, 1, 2, 5]);  view_2234 = None
        view_2235 = torch.ops.aten.view.default(permute_1223, [4, 4, 512, 512, 64]);  permute_1223 = None
        unbind_int_103 = torch.ops.aten.unbind.int(view_2235);  view_2235 = None
        getitem_2098 = unbind_int_103[0]
        getitem_2099 = unbind_int_103[1]
        getitem_2100 = unbind_int_103[2]
        getitem_2101 = unbind_int_103[3];  unbind_int_103 = None
        expand_132 = torch.ops.aten.expand.default(select_55, [4, 512, 512, 512]);  select_55 = None
        _scaled_dot_product_efficient_attention_default_75 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2098, getitem_2099, getitem_2100, expand_132, False);  getitem_2098 = getitem_2099 = getitem_2100 = expand_132 = None
        getitem_2102 = _scaled_dot_product_efficient_attention_default_75[0]
        sigmoid_171 = torch.ops.aten.sigmoid.default(getitem_2101);  getitem_2101 = None
        mul_280 = torch.ops.aten.mul.Tensor(getitem_2102, sigmoid_171);  getitem_2102 = sigmoid_171 = None
        view_2236 = torch.ops.aten.view.default(mul_280, [1, 4, 512, 512, 64]);  mul_280 = None
        permute_1224 = torch.ops.aten.permute.default(view_2236, [0, 2, 3, 1, 4]);  view_2236 = None
        clone_201 = torch.ops.aten.clone.default(permute_1224, memory_format = torch.contiguous_format);  permute_1224 = None
        _unsafe_view_172 = torch.ops.aten._unsafe_view.default(clone_201, [1, 512, 512, 256]);  clone_201 = None
        transpose_55 = torch.ops.aten.transpose.int(getitem_2095, 1, 2);  getitem_2095 = None
        _to_copy_1266 = torch.ops.aten._to_copy.default(arg702_1, dtype = torch.bfloat16);  arg702_1 = None
        _to_copy_1267 = torch.ops.aten._to_copy.default(transpose_55, dtype = torch.bfloat16);  transpose_55 = None
        t_464 = torch.ops.aten.t.default(_to_copy_1266);  _to_copy_1266 = None
        expand_133 = torch.ops.aten.expand.default(_to_copy_1267, [1, 512, 512, 256]);  _to_copy_1267 = None
        view_2237 = torch.ops.aten.view.default(expand_133, [512, 512, 256]);  expand_133 = None
        expand_134 = torch.ops.aten.expand.default(t_464, [1, 512, 256, 1024]);  t_464 = None
        view_2238 = torch.ops.aten.view.default(expand_134, [512, 256, 1024]);  expand_134 = None
        bmm_185 = torch.ops.aten.bmm.default(view_2237, view_2238);  view_2237 = view_2238 = None
        view_2239 = torch.ops.aten.view.default(bmm_185, [1, 512, 512, 1024]);  bmm_185 = None
        select_56 = torch.ops.aten.select.int(view_2231, 0, 1);  view_2231 = None
        view_2240 = torch.ops.aten.view.default(view_2239, [1, 512, 512, 4, 4, 64]);  view_2239 = None
        permute_1225 = torch.ops.aten.permute.default(view_2240, [4, 0, 3, 1, 2, 5]);  view_2240 = None
        view_2241 = torch.ops.aten.view.default(permute_1225, [4, 4, 512, 512, 64]);  permute_1225 = None
        unbind_int_104 = torch.ops.aten.unbind.int(view_2241);  view_2241 = None
        getitem_2106 = unbind_int_104[0]
        getitem_2107 = unbind_int_104[1]
        getitem_2108 = unbind_int_104[2]
        getitem_2109 = unbind_int_104[3];  unbind_int_104 = None
        expand_135 = torch.ops.aten.expand.default(select_56, [4, 512, 512, 512]);  select_56 = None
        _scaled_dot_product_efficient_attention_default_76 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2106, getitem_2107, getitem_2108, expand_135, False);  getitem_2106 = getitem_2107 = getitem_2108 = expand_135 = None
        getitem_2110 = _scaled_dot_product_efficient_attention_default_76[0]
        sigmoid_172 = torch.ops.aten.sigmoid.default(getitem_2109);  getitem_2109 = None
        mul_281 = torch.ops.aten.mul.Tensor(getitem_2110, sigmoid_172);  getitem_2110 = sigmoid_172 = None
        view_2242 = torch.ops.aten.view.default(mul_281, [1, 4, 512, 512, 64]);  mul_281 = None
        permute_1226 = torch.ops.aten.permute.default(view_2242, [0, 2, 3, 1, 4]);  view_2242 = None
        clone_202 = torch.ops.aten.clone.default(permute_1226, memory_format = torch.contiguous_format);  permute_1226 = None
        _unsafe_view_173 = torch.ops.aten._unsafe_view.default(clone_202, [1, 512, 512, 256]);  clone_202 = None
        cat_33 = torch.ops.aten.cat.default([_unsafe_view_172, _unsafe_view_173], dim = -1);  _unsafe_view_172 = _unsafe_view_173 = None
        slice_226 = torch.ops.aten.slice.Tensor(arg699_1, dim = 0, start = 0, end = 9223372036854775807);  arg699_1 = None
        unsqueeze_760 = torch.ops.aten.unsqueeze.default(slice_226, 1);  slice_226 = None
        mul_282 = torch.ops.aten.mul.Tensor(arg703_1, unsqueeze_760);  arg703_1 = unsqueeze_760 = None
        _to_copy_1268 = torch.ops.aten._to_copy.default(mul_282, dtype = torch.bfloat16);  mul_282 = None
        t_465 = torch.ops.aten.t.default(_to_copy_1268);  _to_copy_1268 = None
        view_2243 = torch.ops.aten.view.default(cat_33, [262144, 512]);  cat_33 = None
        mm_433 = torch.ops.aten.mm.default(view_2243, t_465);  view_2243 = t_465 = None
        view_2244 = torch.ops.aten.view.default(mm_433, [1, 512, 512, 256]);  mm_433 = None
        add_226 = torch.ops.aten.add.Tensor(add_225, view_2244);  add_225 = view_2244 = None
        split_tensor_223 = torch.ops.aten.split.Tensor(add_219, 512, dim = -2)
        getitem_2114 = split_tensor_223[0];  split_tensor_223 = None
        _to_copy_1269 = torch.ops.aten._to_copy.default(getitem_2114, dtype = torch.float32);  getitem_2114 = None
        native_layer_norm_default_262 = torch.ops.aten.native_layer_norm.default(_to_copy_1269, [256], arg690_1, arg691_1, 1e-05);  _to_copy_1269 = arg690_1 = arg691_1 = None
        getitem_2115 = native_layer_norm_default_262[0]
        _to_copy_1270 = torch.ops.aten._to_copy.default(arg692_1, dtype = torch.bfloat16);  arg692_1 = None
        _to_copy_1271 = torch.ops.aten._to_copy.default(getitem_2115, dtype = torch.bfloat16);  getitem_2115 = None
        t_466 = torch.ops.aten.t.default(_to_copy_1270);  _to_copy_1270 = None
        view_2245 = torch.ops.aten.view.default(_to_copy_1271, [262144, 256]);  _to_copy_1271 = None
        mm_434 = torch.ops.aten.mm.default(view_2245, t_466);  view_2245 = t_466 = None
        view_2246 = torch.ops.aten.view.default(mm_434, [1, 512, 512, 1024]);  mm_434 = None
        split_tensor_224 = torch.ops.aten.split.Tensor(view_2246, 512, dim = -1);  view_2246 = None
        getitem_2118 = split_tensor_224[0]
        getitem_2119 = split_tensor_224[1];  split_tensor_224 = None
        silu_60 = torch.ops.aten.silu.default(getitem_2118);  getitem_2118 = None
        mul_283 = torch.ops.aten.mul.Tensor(silu_60, getitem_2119);  silu_60 = getitem_2119 = None
        _to_copy_1272 = torch.ops.aten._to_copy.default(arg693_1, dtype = torch.bfloat16);  arg693_1 = None
        t_467 = torch.ops.aten.t.default(_to_copy_1272);  _to_copy_1272 = None
        view_2248 = torch.ops.aten.view.default(mul_283, [262144, 512]);  mul_283 = None
        mm_435 = torch.ops.aten.mm.default(view_2248, t_467);  view_2248 = t_467 = None
        view_2249 = torch.ops.aten.view.default(mm_435, [1, 512, 512, 256]);  mm_435 = None
        add_227 = torch.ops.aten.add.Tensor(add_226, view_2249);  add_226 = view_2249 = None
        _to_copy_1273 = torch.ops.aten._to_copy.default(add_223, dtype = torch.float32)
        native_layer_norm_default_263 = torch.ops.aten.native_layer_norm.default(_to_copy_1273, [384], arg708_1, arg709_1, 1e-05);  _to_copy_1273 = arg708_1 = arg709_1 = None
        getitem_2120 = native_layer_norm_default_263[0]
        _to_copy_1274 = torch.ops.aten._to_copy.default(add_219, dtype = torch.float32);  add_219 = None
        native_layer_norm_default_264 = torch.ops.aten.native_layer_norm.default(_to_copy_1274, [256], arg710_1, arg711_1, 1e-05);  _to_copy_1274 = arg710_1 = arg711_1 = None
        getitem_2123 = native_layer_norm_default_264[0]
        _to_copy_1275 = torch.ops.aten._to_copy.default(arg712_1, dtype = torch.bfloat16);  arg712_1 = None
        _to_copy_1276 = torch.ops.aten._to_copy.default(getitem_2123, dtype = torch.bfloat16);  getitem_2123 = None
        t_468 = torch.ops.aten.t.default(_to_copy_1275);  _to_copy_1275 = None
        view_2250 = torch.ops.aten.view.default(_to_copy_1276, [262144, 256]);  _to_copy_1276 = None
        mm_436 = torch.ops.aten.mm.default(view_2250, t_468);  view_2250 = t_468 = None
        view_2251 = torch.ops.aten.view.default(mm_436, [1, 512, 512, 16]);  mm_436 = None
        permute_1227 = torch.ops.aten.permute.default(view_2251, [0, 3, 1, 2]);  view_2251 = None
        view_2252 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_145 = torch.ops.aten.bitwise_not.default(view_2252);  view_2252 = None
        masked_fill_145 = torch.ops.aten.masked_fill.Scalar(permute_1227, bitwise_not_145, -10000);  permute_1227 = bitwise_not_145 = None
        _to_copy_1277 = torch.ops.aten._to_copy.default(getitem_2120, dtype = torch.bfloat16);  getitem_2120 = None
        _to_copy_1278 = torch.ops.aten._to_copy.default(arg714_1, dtype = torch.bfloat16);  arg714_1 = None
        unsqueeze_761 = torch.ops.aten.unsqueeze.default(_to_copy_1277, 3);  _to_copy_1277 = None
        unsqueeze_762 = torch.ops.aten.unsqueeze.default(unsqueeze_761, 4);  unsqueeze_761 = None
        unsqueeze_763 = torch.ops.aten.unsqueeze.default(unsqueeze_762, 5);  unsqueeze_762 = None
        permute_1228 = torch.ops.aten.permute.default(unsqueeze_763, [3, 0, 4, 1, 5, 2]);  unsqueeze_763 = None
        unsqueeze_764 = torch.ops.aten.unsqueeze.default(_to_copy_1278, 4);  _to_copy_1278 = None
        unsqueeze_765 = torch.ops.aten.unsqueeze.default(unsqueeze_764, 5);  unsqueeze_764 = None
        permute_1229 = torch.ops.aten.permute.default(unsqueeze_765, [1, 4, 2, 5, 3, 0]);  unsqueeze_765 = None
        permute_1230 = torch.ops.aten.permute.default(permute_1228, [3, 5, 0, 1, 2, 4]);  permute_1228 = None
        view_2253 = torch.ops.aten.view.default(permute_1230, [1, 512, 384]);  permute_1230 = None
        permute_1231 = torch.ops.aten.permute.default(permute_1229, [5, 0, 1, 2, 4, 3]);  permute_1229 = None
        view_2254 = torch.ops.aten.view.default(permute_1231, [1, 384, 1536]);  permute_1231 = None
        bmm_186 = torch.ops.aten.bmm.default(view_2253, view_2254);  view_2253 = view_2254 = None
        view_2255 = torch.ops.aten.view.default(bmm_186, [512, 1, 4, 1, 16, 24]);  bmm_186 = None
        permute_1232 = torch.ops.aten.permute.default(view_2255, [2, 3, 4, 0, 5, 1]);  view_2255 = None
        view_2256 = torch.ops.aten.view.default(permute_1232, [4, 1, 16, 512, 24]);  permute_1232 = None
        unbind_int_105 = torch.ops.aten.unbind.int(view_2256);  view_2256 = None
        getitem_2126 = unbind_int_105[0]
        getitem_2127 = unbind_int_105[1]
        getitem_2128 = unbind_int_105[2]
        getitem_2129 = unbind_int_105[3];  unbind_int_105 = None
        view_2257 = torch.ops.aten.view.default(arg713_1, [1, 16, 1, 24]);  arg713_1 = None
        add_228 = torch.ops.aten.add.Tensor(getitem_2126, view_2257);  getitem_2126 = view_2257 = None
        _to_copy_1279 = torch.ops.aten._to_copy.default(add_228, dtype = torch.bfloat16);  add_228 = None
        expand_136 = torch.ops.aten.expand.default(masked_fill_145, [1, 16, 512, 512]);  masked_fill_145 = None
        _scaled_dot_product_efficient_attention_default_77 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1279, getitem_2127, getitem_2128, expand_136, False);  _to_copy_1279 = getitem_2127 = getitem_2128 = expand_136 = None
        getitem_2130 = _scaled_dot_product_efficient_attention_default_77[0]
        add_229 = torch.ops.aten.add.Tensor(getitem_2129, 1);  getitem_2129 = None
        sigmoid_173 = torch.ops.aten.sigmoid.default(add_229);  add_229 = None
        mul_284 = torch.ops.aten.mul.Tensor(getitem_2130, sigmoid_173);  getitem_2130 = sigmoid_173 = None
        _to_copy_1280 = torch.ops.aten._to_copy.default(arg715_1, dtype = torch.bfloat16);  arg715_1 = None
        unsqueeze_766 = torch.ops.aten.unsqueeze.default(mul_284, 4);  mul_284 = None
        permute_1233 = torch.ops.aten.permute.default(unsqueeze_766, [0, 2, 4, 3, 1]);  unsqueeze_766 = None
        unsqueeze_767 = torch.ops.aten.unsqueeze.default(_to_copy_1280, 3);  _to_copy_1280 = None
        unsqueeze_768 = torch.ops.aten.unsqueeze.default(unsqueeze_767, 4);  unsqueeze_767 = None
        permute_1234 = torch.ops.aten.permute.default(unsqueeze_768, [3, 4, 2, 1, 0]);  unsqueeze_768 = None
        permute_1235 = torch.ops.aten.permute.default(permute_1233, [1, 3, 4, 0, 2]);  permute_1233 = None
        clone_203 = torch.ops.aten.clone.default(permute_1235, memory_format = torch.contiguous_format);  permute_1235 = None
        _unsafe_view_174 = torch.ops.aten._unsafe_view.default(clone_203, [1, 512, 384]);  clone_203 = None
        permute_1236 = torch.ops.aten.permute.default(permute_1234, [3, 4, 0, 2, 1]);  permute_1234 = None
        clone_204 = torch.ops.aten.clone.default(permute_1236, memory_format = torch.contiguous_format);  permute_1236 = None
        _unsafe_view_175 = torch.ops.aten._unsafe_view.default(clone_204, [1, 384, 384]);  clone_204 = None
        bmm_187 = torch.ops.aten.bmm.default(_unsafe_view_174, _unsafe_view_175);  _unsafe_view_174 = _unsafe_view_175 = None
        view_2258 = torch.ops.aten.view.default(bmm_187, [512, 1, 1, 1, 384]);  bmm_187 = None
        permute_1237 = torch.ops.aten.permute.default(view_2258, [3, 0, 4, 1, 2]);  view_2258 = None
        view_2259 = torch.ops.aten.view.default(permute_1237, [1, 512, 384]);  permute_1237 = None
        unsqueeze_769 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_285 = torch.ops.aten.mul.Tensor(view_2259, unsqueeze_769);  view_2259 = unsqueeze_769 = None
        add_230 = torch.ops.aten.add.Tensor(add_223, mul_285);  mul_285 = None
        split_tensor_225 = torch.ops.aten.split.Tensor(add_223, 512, dim = -2);  add_223 = None
        getitem_2134 = split_tensor_225[0];  split_tensor_225 = None
        _to_copy_1281 = torch.ops.aten._to_copy.default(getitem_2134, dtype = torch.float32);  getitem_2134 = None
        native_layer_norm_default_265 = torch.ops.aten.native_layer_norm.default(_to_copy_1281, [384], arg704_1, arg705_1, 1e-05);  _to_copy_1281 = arg704_1 = arg705_1 = None
        getitem_2135 = native_layer_norm_default_265[0]
        _to_copy_1282 = torch.ops.aten._to_copy.default(arg706_1, dtype = torch.bfloat16);  arg706_1 = None
        _to_copy_1283 = torch.ops.aten._to_copy.default(getitem_2135, dtype = torch.bfloat16);  getitem_2135 = None
        t_469 = torch.ops.aten.t.default(_to_copy_1282);  _to_copy_1282 = None
        view_2260 = torch.ops.aten.view.default(_to_copy_1283, [512, 384]);  _to_copy_1283 = None
        mm_437 = torch.ops.aten.mm.default(view_2260, t_469);  view_2260 = t_469 = None
        view_2261 = torch.ops.aten.view.default(mm_437, [1, 512, 1536]);  mm_437 = None
        split_tensor_226 = torch.ops.aten.split.Tensor(view_2261, 768, dim = -1);  view_2261 = None
        getitem_2138 = split_tensor_226[0]
        getitem_2139 = split_tensor_226[1];  split_tensor_226 = None
        silu_61 = torch.ops.aten.silu.default(getitem_2138);  getitem_2138 = None
        mul_286 = torch.ops.aten.mul.Tensor(silu_61, getitem_2139);  silu_61 = getitem_2139 = None
        _to_copy_1284 = torch.ops.aten._to_copy.default(arg707_1, dtype = torch.bfloat16);  arg707_1 = None
        t_470 = torch.ops.aten.t.default(_to_copy_1284);  _to_copy_1284 = None
        view_2263 = torch.ops.aten.view.default(mul_286, [512, 768]);  mul_286 = None
        mm_438 = torch.ops.aten.mm.default(view_2263, t_470);  view_2263 = t_470 = None
        view_2264 = torch.ops.aten.view.default(mm_438, [1, 512, 384]);  mm_438 = None
        add_231 = torch.ops.aten.add.Tensor(add_230, view_2264);  add_230 = view_2264 = None
        _to_copy_1285 = torch.ops.aten._to_copy.default(add_227, dtype = torch.float32)
        native_layer_norm_default_266 = torch.ops.aten.native_layer_norm.default(_to_copy_1285, [256], arg720_1, arg721_1, 1e-05);  _to_copy_1285 = arg720_1 = arg721_1 = None
        getitem_2140 = native_layer_norm_default_266[0]
        split_with_sizes_default_56 = torch.ops.aten.split_with_sizes.default(arg723_1, [512, 512]);  arg723_1 = None
        getitem_2143 = split_with_sizes_default_56[0]
        getitem_2144 = split_with_sizes_default_56[1];  split_with_sizes_default_56 = None
        split_with_sizes_default_57 = torch.ops.aten.split_with_sizes.default(arg724_1, [512, 512, 256]);  arg724_1 = None
        getitem_2145 = split_with_sizes_default_57[0]
        getitem_2146 = split_with_sizes_default_57[1]
        getitem_2147 = split_with_sizes_default_57[2];  split_with_sizes_default_57 = None
        _to_copy_1286 = torch.ops.aten._to_copy.default(getitem_2143, dtype = torch.bfloat16);  getitem_2143 = None
        _to_copy_1287 = torch.ops.aten._to_copy.default(getitem_2140, dtype = torch.bfloat16)
        t_471 = torch.ops.aten.t.default(_to_copy_1286);  _to_copy_1286 = None
        view_2265 = torch.ops.aten.view.default(_to_copy_1287, [262144, 256]);  _to_copy_1287 = None
        mm_439 = torch.ops.aten.mm.default(view_2265, t_471);  view_2265 = t_471 = None
        view_2266 = torch.ops.aten.view.default(mm_439, [1, 512, 512, 512]);  mm_439 = None
        _to_copy_1288 = torch.ops.aten._to_copy.default(getitem_2145, dtype = torch.bfloat16);  getitem_2145 = None
        _to_copy_1289 = torch.ops.aten._to_copy.default(getitem_2140, dtype = torch.bfloat16)
        t_472 = torch.ops.aten.t.default(_to_copy_1288);  _to_copy_1288 = None
        view_2267 = torch.ops.aten.view.default(_to_copy_1289, [262144, 256]);  _to_copy_1289 = None
        mm_440 = torch.ops.aten.mm.default(view_2267, t_472);  view_2267 = t_472 = None
        view_2268 = torch.ops.aten.view.default(mm_440, [1, 512, 512, 512]);  mm_440 = None
        sigmoid_174 = torch.ops.aten.sigmoid.default(view_2268);  view_2268 = None
        mul_287 = torch.ops.aten.mul.Tensor(view_2266, sigmoid_174);  view_2266 = sigmoid_174 = None
        unsqueeze_770 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_146 = torch.ops.aten.bitwise_not.default(unsqueeze_770);  unsqueeze_770 = None
        masked_fill_146 = torch.ops.aten.masked_fill.Scalar(mul_287, bitwise_not_146, 0);  mul_287 = bitwise_not_146 = None
        split_tensor_227 = torch.ops.aten.split.Tensor(masked_fill_146, 256, dim = -1)
        getitem_2150 = split_tensor_227[0]
        unsqueeze_773 = torch.ops.aten.unsqueeze.default(getitem_2150, 4);  getitem_2150 = None
        permute_1242 = torch.ops.aten.permute.default(unsqueeze_773, [0, 1, 4, 3, 2]);  unsqueeze_773 = None
        permute_1243 = torch.ops.aten.permute.default(permute_1242, [3, 1, 4, 0, 2]);  permute_1242 = None
        view_2271 = torch.ops.aten.view.default(permute_1243, [256, 512, 512]);  permute_1243 = None
        split_tensor_228 = torch.ops.aten.split.Tensor(masked_fill_146, 256, dim = -1);  masked_fill_146 = None
        getitem_2153 = split_tensor_228[1];  split_tensor_228 = None
        unsqueeze_774 = torch.ops.aten.unsqueeze.default(getitem_2153, 4);  getitem_2153 = None
        permute_1244 = torch.ops.aten.permute.default(unsqueeze_774, [0, 4, 1, 3, 2]);  unsqueeze_774 = None
        permute_1245 = torch.ops.aten.permute.default(permute_1244, [3, 4, 0, 2, 1]);  permute_1244 = None
        view_2272 = torch.ops.aten.view.default(permute_1245, [256, 512, 512]);  permute_1245 = None
        bmm_188 = torch.ops.aten.bmm.default(view_2271, view_2272);  view_2271 = view_2272 = None
        view_2273 = torch.ops.aten.view.default(bmm_188, [256, 512, 1, 1, 512]);  bmm_188 = None
        permute_1246 = torch.ops.aten.permute.default(view_2273, [3, 1, 4, 0, 2]);  view_2273 = None
        view_2274 = torch.ops.aten.view.default(permute_1246, [1, 512, 512, 256]);  permute_1246 = None
        _to_copy_1290 = torch.ops.aten._to_copy.default(getitem_2144, dtype = torch.bfloat16);  getitem_2144 = None
        _to_copy_1291 = torch.ops.aten._to_copy.default(getitem_2140, dtype = torch.bfloat16)
        t_473 = torch.ops.aten.t.default(_to_copy_1290);  _to_copy_1290 = None
        view_2275 = torch.ops.aten.view.default(_to_copy_1291, [262144, 256]);  _to_copy_1291 = None
        mm_441 = torch.ops.aten.mm.default(view_2275, t_473);  view_2275 = t_473 = None
        view_2276 = torch.ops.aten.view.default(mm_441, [1, 512, 512, 512]);  mm_441 = None
        _to_copy_1292 = torch.ops.aten._to_copy.default(getitem_2146, dtype = torch.bfloat16);  getitem_2146 = None
        _to_copy_1293 = torch.ops.aten._to_copy.default(getitem_2140, dtype = torch.bfloat16)
        t_474 = torch.ops.aten.t.default(_to_copy_1292);  _to_copy_1292 = None
        view_2277 = torch.ops.aten.view.default(_to_copy_1293, [262144, 256]);  _to_copy_1293 = None
        mm_442 = torch.ops.aten.mm.default(view_2277, t_474);  view_2277 = t_474 = None
        view_2278 = torch.ops.aten.view.default(mm_442, [1, 512, 512, 512]);  mm_442 = None
        sigmoid_175 = torch.ops.aten.sigmoid.default(view_2278);  view_2278 = None
        mul_288 = torch.ops.aten.mul.Tensor(view_2276, sigmoid_175);  view_2276 = sigmoid_175 = None
        view_2279 = torch.ops.aten.view.default(mul_288, [262144, 512]);  mul_288 = None
        view_2280 = torch.ops.aten.view.default(view_2279, [1, 512, 512, 512]);  view_2279 = None
        transpose_56 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_775 = torch.ops.aten.unsqueeze.default(transpose_56, 3);  transpose_56 = None
        clone_205 = torch.ops.aten.clone.default(unsqueeze_775, memory_format = torch.contiguous_format);  unsqueeze_775 = None
        bitwise_not_147 = torch.ops.aten.bitwise_not.default(clone_205);  clone_205 = None
        masked_fill_147 = torch.ops.aten.masked_fill.Scalar(view_2280, bitwise_not_147, 0);  view_2280 = bitwise_not_147 = None
        view_2281 = torch.ops.aten.view.default(masked_fill_147, [262144, 512]);  masked_fill_147 = None
        view_2285 = torch.ops.aten.view.default(view_2281, [1, 512, 512, 512])
        split_tensor_229 = torch.ops.aten.split.Tensor(view_2285, 256, dim = -1);  view_2285 = None
        getitem_2156 = split_tensor_229[0]
        unsqueeze_778 = torch.ops.aten.unsqueeze.default(getitem_2156, 4);  getitem_2156 = None
        permute_1251 = torch.ops.aten.permute.default(unsqueeze_778, [0, 2, 4, 3, 1]);  unsqueeze_778 = None
        permute_1252 = torch.ops.aten.permute.default(permute_1251, [3, 1, 4, 0, 2]);  permute_1251 = None
        view_2286 = torch.ops.aten.view.default(permute_1252, [256, 512, 512]);  permute_1252 = None
        view_2287 = torch.ops.aten.view.default(view_2281, [1, 512, 512, 512]);  view_2281 = None
        split_tensor_230 = torch.ops.aten.split.Tensor(view_2287, 256, dim = -1);  view_2287 = None
        getitem_2159 = split_tensor_230[1];  split_tensor_230 = None
        unsqueeze_779 = torch.ops.aten.unsqueeze.default(getitem_2159, 4);  getitem_2159 = None
        permute_1253 = torch.ops.aten.permute.default(unsqueeze_779, [0, 4, 2, 3, 1]);  unsqueeze_779 = None
        permute_1254 = torch.ops.aten.permute.default(permute_1253, [3, 4, 0, 2, 1]);  permute_1253 = None
        view_2288 = torch.ops.aten.view.default(permute_1254, [256, 512, 512]);  permute_1254 = None
        bmm_189 = torch.ops.aten.bmm.default(view_2286, view_2288);  view_2286 = view_2288 = None
        view_2289 = torch.ops.aten.view.default(bmm_189, [256, 512, 1, 1, 512]);  bmm_189 = None
        permute_1255 = torch.ops.aten.permute.default(view_2289, [3, 1, 4, 0, 2]);  view_2289 = None
        view_2290 = torch.ops.aten.view.default(permute_1255, [1, 512, 512, 256]);  permute_1255 = None
        _to_copy_1294 = torch.ops.aten._to_copy.default(view_2274, dtype = torch.float32);  view_2274 = None
        native_layer_norm_default_267 = torch.ops.aten.native_layer_norm.default(_to_copy_1294, [256], None, None, 1e-05);  _to_copy_1294 = None
        getitem_2160 = native_layer_norm_default_267[0]
        _to_copy_1295 = torch.ops.aten._to_copy.default(view_2290, dtype = torch.float32);  view_2290 = None
        native_layer_norm_default_268 = torch.ops.aten.native_layer_norm.default(_to_copy_1295, [256], None, None, 1e-05);  _to_copy_1295 = None
        getitem_2163 = native_layer_norm_default_268[0]
        add_232 = torch.ops.aten.add.Tensor(getitem_2160, getitem_2163);  getitem_2160 = getitem_2163 = None
        _to_copy_1296 = torch.ops.aten._to_copy.default(arg722_1, dtype = torch.bfloat16);  arg722_1 = None
        _to_copy_1297 = torch.ops.aten._to_copy.default(add_232, dtype = torch.bfloat16);  add_232 = None
        t_475 = torch.ops.aten.t.default(_to_copy_1296);  _to_copy_1296 = None
        view_2291 = torch.ops.aten.view.default(_to_copy_1297, [262144, 256]);  _to_copy_1297 = None
        mm_443 = torch.ops.aten.mm.default(view_2291, t_475);  view_2291 = t_475 = None
        view_2292 = torch.ops.aten.view.default(mm_443, [1, 512, 512, 256]);  mm_443 = None
        _to_copy_1298 = torch.ops.aten._to_copy.default(getitem_2147, dtype = torch.bfloat16);  getitem_2147 = None
        _to_copy_1299 = torch.ops.aten._to_copy.default(getitem_2140, dtype = torch.bfloat16);  getitem_2140 = None
        t_476 = torch.ops.aten.t.default(_to_copy_1298);  _to_copy_1298 = None
        view_2293 = torch.ops.aten.view.default(_to_copy_1299, [262144, 256]);  _to_copy_1299 = None
        mm_444 = torch.ops.aten.mm.default(view_2293, t_476);  view_2293 = t_476 = None
        view_2294 = torch.ops.aten.view.default(mm_444, [1, 512, 512, 256]);  mm_444 = None
        sigmoid_176 = torch.ops.aten.sigmoid.default(view_2294);  view_2294 = None
        mul_289 = torch.ops.aten.mul.Tensor(view_2292, sigmoid_176);  view_2292 = sigmoid_176 = None
        add_233 = torch.ops.aten.add.Tensor(add_227, mul_289);  mul_289 = None
        _to_copy_1300 = torch.ops.aten._to_copy.default(add_227, dtype = torch.float32)
        native_layer_norm_default_269 = torch.ops.aten.native_layer_norm.default(_to_copy_1300, [256], None, None, 1e-05);  _to_copy_1300 = None
        getitem_2166 = native_layer_norm_default_269[0]
        _to_copy_1301 = torch.ops.aten._to_copy.default(arg726_1, dtype = torch.bfloat16);  arg726_1 = None
        _to_copy_1302 = torch.ops.aten._to_copy.default(getitem_2166, dtype = torch.bfloat16)
        t_477 = torch.ops.aten.t.default(_to_copy_1301);  _to_copy_1301 = None
        view_2295 = torch.ops.aten.view.default(_to_copy_1302, [262144, 256]);  _to_copy_1302 = None
        mm_445 = torch.ops.aten.mm.default(view_2295, t_477);  view_2295 = t_477 = None
        view_2296 = torch.ops.aten.view.default(mm_445, [1, 512, 512, 8]);  mm_445 = None
        view_2297 = torch.ops.aten.view.default(view_2296, [1, 512, 512, 2, 4]);  view_2296 = None
        permute_1256 = torch.ops.aten.permute.default(view_2297, [0, 3, 4, 1, 2]);  view_2297 = None
        view_2298 = torch.ops.aten.view.default(permute_1256, [1, 2, 4, 1, 512, 512]);  permute_1256 = None
        view_2299 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_148 = torch.ops.aten.bitwise_not.default(view_2299);  view_2299 = None
        masked_fill_148 = torch.ops.aten.masked_fill.Scalar(view_2298, bitwise_not_148, -10000);  view_2298 = bitwise_not_148 = None
        view_2300 = torch.ops.aten.view.default(masked_fill_148, [1, 2, 4, 512, 512]);  masked_fill_148 = None
        permute_1257 = torch.ops.aten.permute.default(view_2300, [1, 0, 2, 3, 4]);  view_2300 = None
        view_2301 = torch.ops.aten.view.default(permute_1257, [2, 4, 1, 512, 512]);  permute_1257 = None
        _to_copy_1303 = torch.ops.aten._to_copy.default(arg727_1, dtype = torch.bfloat16);  arg727_1 = None
        _to_copy_1304 = torch.ops.aten._to_copy.default(getitem_2166, dtype = torch.bfloat16)
        t_478 = torch.ops.aten.t.default(_to_copy_1303);  _to_copy_1303 = None
        view_2302 = torch.ops.aten.view.default(_to_copy_1304, [262144, 256]);  _to_copy_1304 = None
        mm_446 = torch.ops.aten.mm.default(view_2302, t_478);  view_2302 = t_478 = None
        view_2303 = torch.ops.aten.view.default(mm_446, [1, 512, 512, 1024]);  mm_446 = None
        select_57 = torch.ops.aten.select.int(view_2301, 0, 0)
        view_2304 = torch.ops.aten.view.default(view_2303, [1, 512, 512, 4, 4, 64]);  view_2303 = None
        permute_1258 = torch.ops.aten.permute.default(view_2304, [4, 0, 3, 1, 2, 5]);  view_2304 = None
        view_2305 = torch.ops.aten.view.default(permute_1258, [4, 4, 512, 512, 64]);  permute_1258 = None
        unbind_int_106 = torch.ops.aten.unbind.int(view_2305);  view_2305 = None
        getitem_2169 = unbind_int_106[0]
        getitem_2170 = unbind_int_106[1]
        getitem_2171 = unbind_int_106[2]
        getitem_2172 = unbind_int_106[3];  unbind_int_106 = None
        expand_137 = torch.ops.aten.expand.default(select_57, [4, 512, 512, 512]);  select_57 = None
        _scaled_dot_product_efficient_attention_default_78 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2169, getitem_2170, getitem_2171, expand_137, False);  getitem_2169 = getitem_2170 = getitem_2171 = expand_137 = None
        getitem_2173 = _scaled_dot_product_efficient_attention_default_78[0]
        sigmoid_177 = torch.ops.aten.sigmoid.default(getitem_2172);  getitem_2172 = None
        mul_290 = torch.ops.aten.mul.Tensor(getitem_2173, sigmoid_177);  getitem_2173 = sigmoid_177 = None
        view_2306 = torch.ops.aten.view.default(mul_290, [1, 4, 512, 512, 64]);  mul_290 = None
        permute_1259 = torch.ops.aten.permute.default(view_2306, [0, 2, 3, 1, 4]);  view_2306 = None
        clone_206 = torch.ops.aten.clone.default(permute_1259, memory_format = torch.contiguous_format);  permute_1259 = None
        _unsafe_view_176 = torch.ops.aten._unsafe_view.default(clone_206, [1, 512, 512, 256]);  clone_206 = None
        transpose_57 = torch.ops.aten.transpose.int(getitem_2166, 1, 2);  getitem_2166 = None
        _to_copy_1305 = torch.ops.aten._to_copy.default(arg728_1, dtype = torch.bfloat16);  arg728_1 = None
        _to_copy_1306 = torch.ops.aten._to_copy.default(transpose_57, dtype = torch.bfloat16);  transpose_57 = None
        t_479 = torch.ops.aten.t.default(_to_copy_1305);  _to_copy_1305 = None
        expand_138 = torch.ops.aten.expand.default(_to_copy_1306, [1, 512, 512, 256]);  _to_copy_1306 = None
        view_2307 = torch.ops.aten.view.default(expand_138, [512, 512, 256]);  expand_138 = None
        expand_139 = torch.ops.aten.expand.default(t_479, [1, 512, 256, 1024]);  t_479 = None
        view_2308 = torch.ops.aten.view.default(expand_139, [512, 256, 1024]);  expand_139 = None
        bmm_190 = torch.ops.aten.bmm.default(view_2307, view_2308);  view_2307 = view_2308 = None
        view_2309 = torch.ops.aten.view.default(bmm_190, [1, 512, 512, 1024]);  bmm_190 = None
        select_58 = torch.ops.aten.select.int(view_2301, 0, 1);  view_2301 = None
        view_2310 = torch.ops.aten.view.default(view_2309, [1, 512, 512, 4, 4, 64]);  view_2309 = None
        permute_1260 = torch.ops.aten.permute.default(view_2310, [4, 0, 3, 1, 2, 5]);  view_2310 = None
        view_2311 = torch.ops.aten.view.default(permute_1260, [4, 4, 512, 512, 64]);  permute_1260 = None
        unbind_int_107 = torch.ops.aten.unbind.int(view_2311);  view_2311 = None
        getitem_2177 = unbind_int_107[0]
        getitem_2178 = unbind_int_107[1]
        getitem_2179 = unbind_int_107[2]
        getitem_2180 = unbind_int_107[3];  unbind_int_107 = None
        expand_140 = torch.ops.aten.expand.default(select_58, [4, 512, 512, 512]);  select_58 = None
        _scaled_dot_product_efficient_attention_default_79 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2177, getitem_2178, getitem_2179, expand_140, False);  getitem_2177 = getitem_2178 = getitem_2179 = expand_140 = None
        getitem_2181 = _scaled_dot_product_efficient_attention_default_79[0]
        sigmoid_178 = torch.ops.aten.sigmoid.default(getitem_2180);  getitem_2180 = None
        mul_291 = torch.ops.aten.mul.Tensor(getitem_2181, sigmoid_178);  getitem_2181 = sigmoid_178 = None
        view_2312 = torch.ops.aten.view.default(mul_291, [1, 4, 512, 512, 64]);  mul_291 = None
        permute_1261 = torch.ops.aten.permute.default(view_2312, [0, 2, 3, 1, 4]);  view_2312 = None
        clone_207 = torch.ops.aten.clone.default(permute_1261, memory_format = torch.contiguous_format);  permute_1261 = None
        _unsafe_view_177 = torch.ops.aten._unsafe_view.default(clone_207, [1, 512, 512, 256]);  clone_207 = None
        cat_34 = torch.ops.aten.cat.default([_unsafe_view_176, _unsafe_view_177], dim = -1);  _unsafe_view_176 = _unsafe_view_177 = None
        slice_227 = torch.ops.aten.slice.Tensor(arg725_1, dim = 0, start = 0, end = 9223372036854775807);  arg725_1 = None
        unsqueeze_780 = torch.ops.aten.unsqueeze.default(slice_227, 1);  slice_227 = None
        mul_292 = torch.ops.aten.mul.Tensor(arg729_1, unsqueeze_780);  arg729_1 = unsqueeze_780 = None
        _to_copy_1307 = torch.ops.aten._to_copy.default(mul_292, dtype = torch.bfloat16);  mul_292 = None
        t_480 = torch.ops.aten.t.default(_to_copy_1307);  _to_copy_1307 = None
        view_2313 = torch.ops.aten.view.default(cat_34, [262144, 512]);  cat_34 = None
        mm_447 = torch.ops.aten.mm.default(view_2313, t_480);  view_2313 = t_480 = None
        view_2314 = torch.ops.aten.view.default(mm_447, [1, 512, 512, 256]);  mm_447 = None
        add_234 = torch.ops.aten.add.Tensor(add_233, view_2314);  add_233 = view_2314 = None
        split_tensor_231 = torch.ops.aten.split.Tensor(add_227, 512, dim = -2)
        getitem_2185 = split_tensor_231[0];  split_tensor_231 = None
        _to_copy_1308 = torch.ops.aten._to_copy.default(getitem_2185, dtype = torch.float32);  getitem_2185 = None
        native_layer_norm_default_270 = torch.ops.aten.native_layer_norm.default(_to_copy_1308, [256], arg716_1, arg717_1, 1e-05);  _to_copy_1308 = arg716_1 = arg717_1 = None
        getitem_2186 = native_layer_norm_default_270[0]
        _to_copy_1309 = torch.ops.aten._to_copy.default(arg718_1, dtype = torch.bfloat16);  arg718_1 = None
        _to_copy_1310 = torch.ops.aten._to_copy.default(getitem_2186, dtype = torch.bfloat16);  getitem_2186 = None
        t_481 = torch.ops.aten.t.default(_to_copy_1309);  _to_copy_1309 = None
        view_2315 = torch.ops.aten.view.default(_to_copy_1310, [262144, 256]);  _to_copy_1310 = None
        mm_448 = torch.ops.aten.mm.default(view_2315, t_481);  view_2315 = t_481 = None
        view_2316 = torch.ops.aten.view.default(mm_448, [1, 512, 512, 1024]);  mm_448 = None
        split_tensor_232 = torch.ops.aten.split.Tensor(view_2316, 512, dim = -1);  view_2316 = None
        getitem_2189 = split_tensor_232[0]
        getitem_2190 = split_tensor_232[1];  split_tensor_232 = None
        silu_62 = torch.ops.aten.silu.default(getitem_2189);  getitem_2189 = None
        mul_293 = torch.ops.aten.mul.Tensor(silu_62, getitem_2190);  silu_62 = getitem_2190 = None
        _to_copy_1311 = torch.ops.aten._to_copy.default(arg719_1, dtype = torch.bfloat16);  arg719_1 = None
        t_482 = torch.ops.aten.t.default(_to_copy_1311);  _to_copy_1311 = None
        view_2318 = torch.ops.aten.view.default(mul_293, [262144, 512]);  mul_293 = None
        mm_449 = torch.ops.aten.mm.default(view_2318, t_482);  view_2318 = t_482 = None
        view_2319 = torch.ops.aten.view.default(mm_449, [1, 512, 512, 256]);  mm_449 = None
        add_235 = torch.ops.aten.add.Tensor(add_234, view_2319);  add_234 = view_2319 = None
        _to_copy_1312 = torch.ops.aten._to_copy.default(add_231, dtype = torch.float32)
        native_layer_norm_default_271 = torch.ops.aten.native_layer_norm.default(_to_copy_1312, [384], arg734_1, arg735_1, 1e-05);  _to_copy_1312 = arg734_1 = arg735_1 = None
        getitem_2191 = native_layer_norm_default_271[0]
        _to_copy_1313 = torch.ops.aten._to_copy.default(add_227, dtype = torch.float32);  add_227 = None
        native_layer_norm_default_272 = torch.ops.aten.native_layer_norm.default(_to_copy_1313, [256], arg736_1, arg737_1, 1e-05);  _to_copy_1313 = arg736_1 = arg737_1 = None
        getitem_2194 = native_layer_norm_default_272[0]
        _to_copy_1314 = torch.ops.aten._to_copy.default(arg738_1, dtype = torch.bfloat16);  arg738_1 = None
        _to_copy_1315 = torch.ops.aten._to_copy.default(getitem_2194, dtype = torch.bfloat16);  getitem_2194 = None
        t_483 = torch.ops.aten.t.default(_to_copy_1314);  _to_copy_1314 = None
        view_2320 = torch.ops.aten.view.default(_to_copy_1315, [262144, 256]);  _to_copy_1315 = None
        mm_450 = torch.ops.aten.mm.default(view_2320, t_483);  view_2320 = t_483 = None
        view_2321 = torch.ops.aten.view.default(mm_450, [1, 512, 512, 16]);  mm_450 = None
        permute_1262 = torch.ops.aten.permute.default(view_2321, [0, 3, 1, 2]);  view_2321 = None
        view_2322 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_149 = torch.ops.aten.bitwise_not.default(view_2322);  view_2322 = None
        masked_fill_149 = torch.ops.aten.masked_fill.Scalar(permute_1262, bitwise_not_149, -10000);  permute_1262 = bitwise_not_149 = None
        _to_copy_1316 = torch.ops.aten._to_copy.default(getitem_2191, dtype = torch.bfloat16);  getitem_2191 = None
        _to_copy_1317 = torch.ops.aten._to_copy.default(arg740_1, dtype = torch.bfloat16);  arg740_1 = None
        unsqueeze_781 = torch.ops.aten.unsqueeze.default(_to_copy_1316, 3);  _to_copy_1316 = None
        unsqueeze_782 = torch.ops.aten.unsqueeze.default(unsqueeze_781, 4);  unsqueeze_781 = None
        unsqueeze_783 = torch.ops.aten.unsqueeze.default(unsqueeze_782, 5);  unsqueeze_782 = None
        permute_1263 = torch.ops.aten.permute.default(unsqueeze_783, [3, 0, 4, 1, 5, 2]);  unsqueeze_783 = None
        unsqueeze_784 = torch.ops.aten.unsqueeze.default(_to_copy_1317, 4);  _to_copy_1317 = None
        unsqueeze_785 = torch.ops.aten.unsqueeze.default(unsqueeze_784, 5);  unsqueeze_784 = None
        permute_1264 = torch.ops.aten.permute.default(unsqueeze_785, [1, 4, 2, 5, 3, 0]);  unsqueeze_785 = None
        permute_1265 = torch.ops.aten.permute.default(permute_1263, [3, 5, 0, 1, 2, 4]);  permute_1263 = None
        view_2323 = torch.ops.aten.view.default(permute_1265, [1, 512, 384]);  permute_1265 = None
        permute_1266 = torch.ops.aten.permute.default(permute_1264, [5, 0, 1, 2, 4, 3]);  permute_1264 = None
        view_2324 = torch.ops.aten.view.default(permute_1266, [1, 384, 1536]);  permute_1266 = None
        bmm_191 = torch.ops.aten.bmm.default(view_2323, view_2324);  view_2323 = view_2324 = None
        view_2325 = torch.ops.aten.view.default(bmm_191, [512, 1, 4, 1, 16, 24]);  bmm_191 = None
        permute_1267 = torch.ops.aten.permute.default(view_2325, [2, 3, 4, 0, 5, 1]);  view_2325 = None
        view_2326 = torch.ops.aten.view.default(permute_1267, [4, 1, 16, 512, 24]);  permute_1267 = None
        unbind_int_108 = torch.ops.aten.unbind.int(view_2326);  view_2326 = None
        getitem_2197 = unbind_int_108[0]
        getitem_2198 = unbind_int_108[1]
        getitem_2199 = unbind_int_108[2]
        getitem_2200 = unbind_int_108[3];  unbind_int_108 = None
        view_2327 = torch.ops.aten.view.default(arg739_1, [1, 16, 1, 24]);  arg739_1 = None
        add_236 = torch.ops.aten.add.Tensor(getitem_2197, view_2327);  getitem_2197 = view_2327 = None
        _to_copy_1318 = torch.ops.aten._to_copy.default(add_236, dtype = torch.bfloat16);  add_236 = None
        expand_141 = torch.ops.aten.expand.default(masked_fill_149, [1, 16, 512, 512]);  masked_fill_149 = None
        _scaled_dot_product_efficient_attention_default_80 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1318, getitem_2198, getitem_2199, expand_141, False);  _to_copy_1318 = getitem_2198 = getitem_2199 = expand_141 = None
        getitem_2201 = _scaled_dot_product_efficient_attention_default_80[0]
        add_237 = torch.ops.aten.add.Tensor(getitem_2200, 1);  getitem_2200 = None
        sigmoid_179 = torch.ops.aten.sigmoid.default(add_237);  add_237 = None
        mul_294 = torch.ops.aten.mul.Tensor(getitem_2201, sigmoid_179);  getitem_2201 = sigmoid_179 = None
        _to_copy_1319 = torch.ops.aten._to_copy.default(arg741_1, dtype = torch.bfloat16);  arg741_1 = None
        unsqueeze_786 = torch.ops.aten.unsqueeze.default(mul_294, 4);  mul_294 = None
        permute_1268 = torch.ops.aten.permute.default(unsqueeze_786, [0, 2, 4, 3, 1]);  unsqueeze_786 = None
        unsqueeze_787 = torch.ops.aten.unsqueeze.default(_to_copy_1319, 3);  _to_copy_1319 = None
        unsqueeze_788 = torch.ops.aten.unsqueeze.default(unsqueeze_787, 4);  unsqueeze_787 = None
        permute_1269 = torch.ops.aten.permute.default(unsqueeze_788, [3, 4, 2, 1, 0]);  unsqueeze_788 = None
        permute_1270 = torch.ops.aten.permute.default(permute_1268, [1, 3, 4, 0, 2]);  permute_1268 = None
        clone_208 = torch.ops.aten.clone.default(permute_1270, memory_format = torch.contiguous_format);  permute_1270 = None
        _unsafe_view_178 = torch.ops.aten._unsafe_view.default(clone_208, [1, 512, 384]);  clone_208 = None
        permute_1271 = torch.ops.aten.permute.default(permute_1269, [3, 4, 0, 2, 1]);  permute_1269 = None
        clone_209 = torch.ops.aten.clone.default(permute_1271, memory_format = torch.contiguous_format);  permute_1271 = None
        _unsafe_view_179 = torch.ops.aten._unsafe_view.default(clone_209, [1, 384, 384]);  clone_209 = None
        bmm_192 = torch.ops.aten.bmm.default(_unsafe_view_178, _unsafe_view_179);  _unsafe_view_178 = _unsafe_view_179 = None
        view_2328 = torch.ops.aten.view.default(bmm_192, [512, 1, 1, 1, 384]);  bmm_192 = None
        permute_1272 = torch.ops.aten.permute.default(view_2328, [3, 0, 4, 1, 2]);  view_2328 = None
        view_2329 = torch.ops.aten.view.default(permute_1272, [1, 512, 384]);  permute_1272 = None
        unsqueeze_789 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_295 = torch.ops.aten.mul.Tensor(view_2329, unsqueeze_789);  view_2329 = unsqueeze_789 = None
        add_238 = torch.ops.aten.add.Tensor(add_231, mul_295);  mul_295 = None
        split_tensor_233 = torch.ops.aten.split.Tensor(add_231, 512, dim = -2);  add_231 = None
        getitem_2205 = split_tensor_233[0];  split_tensor_233 = None
        _to_copy_1320 = torch.ops.aten._to_copy.default(getitem_2205, dtype = torch.float32);  getitem_2205 = None
        native_layer_norm_default_273 = torch.ops.aten.native_layer_norm.default(_to_copy_1320, [384], arg730_1, arg731_1, 1e-05);  _to_copy_1320 = arg730_1 = arg731_1 = None
        getitem_2206 = native_layer_norm_default_273[0]
        _to_copy_1321 = torch.ops.aten._to_copy.default(arg732_1, dtype = torch.bfloat16);  arg732_1 = None
        _to_copy_1322 = torch.ops.aten._to_copy.default(getitem_2206, dtype = torch.bfloat16);  getitem_2206 = None
        t_484 = torch.ops.aten.t.default(_to_copy_1321);  _to_copy_1321 = None
        view_2330 = torch.ops.aten.view.default(_to_copy_1322, [512, 384]);  _to_copy_1322 = None
        mm_451 = torch.ops.aten.mm.default(view_2330, t_484);  view_2330 = t_484 = None
        view_2331 = torch.ops.aten.view.default(mm_451, [1, 512, 1536]);  mm_451 = None
        split_tensor_234 = torch.ops.aten.split.Tensor(view_2331, 768, dim = -1);  view_2331 = None
        getitem_2209 = split_tensor_234[0]
        getitem_2210 = split_tensor_234[1];  split_tensor_234 = None
        silu_63 = torch.ops.aten.silu.default(getitem_2209);  getitem_2209 = None
        mul_296 = torch.ops.aten.mul.Tensor(silu_63, getitem_2210);  silu_63 = getitem_2210 = None
        _to_copy_1323 = torch.ops.aten._to_copy.default(arg733_1, dtype = torch.bfloat16);  arg733_1 = None
        t_485 = torch.ops.aten.t.default(_to_copy_1323);  _to_copy_1323 = None
        view_2333 = torch.ops.aten.view.default(mul_296, [512, 768]);  mul_296 = None
        mm_452 = torch.ops.aten.mm.default(view_2333, t_485);  view_2333 = t_485 = None
        view_2334 = torch.ops.aten.view.default(mm_452, [1, 512, 384]);  mm_452 = None
        add_239 = torch.ops.aten.add.Tensor(add_238, view_2334);  add_238 = view_2334 = None
        _to_copy_1324 = torch.ops.aten._to_copy.default(add_235, dtype = torch.float32)
        native_layer_norm_default_274 = torch.ops.aten.native_layer_norm.default(_to_copy_1324, [256], arg746_1, arg747_1, 1e-05);  _to_copy_1324 = arg746_1 = arg747_1 = None
        getitem_2211 = native_layer_norm_default_274[0]
        split_with_sizes_default_58 = torch.ops.aten.split_with_sizes.default(arg749_1, [512, 512]);  arg749_1 = None
        getitem_2214 = split_with_sizes_default_58[0]
        getitem_2215 = split_with_sizes_default_58[1];  split_with_sizes_default_58 = None
        split_with_sizes_default_59 = torch.ops.aten.split_with_sizes.default(arg750_1, [512, 512, 256]);  arg750_1 = None
        getitem_2216 = split_with_sizes_default_59[0]
        getitem_2217 = split_with_sizes_default_59[1]
        getitem_2218 = split_with_sizes_default_59[2];  split_with_sizes_default_59 = None
        _to_copy_1325 = torch.ops.aten._to_copy.default(getitem_2214, dtype = torch.bfloat16);  getitem_2214 = None
        _to_copy_1326 = torch.ops.aten._to_copy.default(getitem_2211, dtype = torch.bfloat16)
        t_486 = torch.ops.aten.t.default(_to_copy_1325);  _to_copy_1325 = None
        view_2335 = torch.ops.aten.view.default(_to_copy_1326, [262144, 256]);  _to_copy_1326 = None
        mm_453 = torch.ops.aten.mm.default(view_2335, t_486);  view_2335 = t_486 = None
        view_2336 = torch.ops.aten.view.default(mm_453, [1, 512, 512, 512]);  mm_453 = None
        _to_copy_1327 = torch.ops.aten._to_copy.default(getitem_2216, dtype = torch.bfloat16);  getitem_2216 = None
        _to_copy_1328 = torch.ops.aten._to_copy.default(getitem_2211, dtype = torch.bfloat16)
        t_487 = torch.ops.aten.t.default(_to_copy_1327);  _to_copy_1327 = None
        view_2337 = torch.ops.aten.view.default(_to_copy_1328, [262144, 256]);  _to_copy_1328 = None
        mm_454 = torch.ops.aten.mm.default(view_2337, t_487);  view_2337 = t_487 = None
        view_2338 = torch.ops.aten.view.default(mm_454, [1, 512, 512, 512]);  mm_454 = None
        sigmoid_180 = torch.ops.aten.sigmoid.default(view_2338);  view_2338 = None
        mul_297 = torch.ops.aten.mul.Tensor(view_2336, sigmoid_180);  view_2336 = sigmoid_180 = None
        unsqueeze_790 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_150 = torch.ops.aten.bitwise_not.default(unsqueeze_790);  unsqueeze_790 = None
        masked_fill_150 = torch.ops.aten.masked_fill.Scalar(mul_297, bitwise_not_150, 0);  mul_297 = bitwise_not_150 = None
        split_tensor_235 = torch.ops.aten.split.Tensor(masked_fill_150, 256, dim = -1)
        getitem_2221 = split_tensor_235[0]
        unsqueeze_793 = torch.ops.aten.unsqueeze.default(getitem_2221, 4);  getitem_2221 = None
        permute_1277 = torch.ops.aten.permute.default(unsqueeze_793, [0, 1, 4, 3, 2]);  unsqueeze_793 = None
        permute_1278 = torch.ops.aten.permute.default(permute_1277, [3, 1, 4, 0, 2]);  permute_1277 = None
        view_2341 = torch.ops.aten.view.default(permute_1278, [256, 512, 512]);  permute_1278 = None
        split_tensor_236 = torch.ops.aten.split.Tensor(masked_fill_150, 256, dim = -1);  masked_fill_150 = None
        getitem_2224 = split_tensor_236[1];  split_tensor_236 = None
        unsqueeze_794 = torch.ops.aten.unsqueeze.default(getitem_2224, 4);  getitem_2224 = None
        permute_1279 = torch.ops.aten.permute.default(unsqueeze_794, [0, 4, 1, 3, 2]);  unsqueeze_794 = None
        permute_1280 = torch.ops.aten.permute.default(permute_1279, [3, 4, 0, 2, 1]);  permute_1279 = None
        view_2342 = torch.ops.aten.view.default(permute_1280, [256, 512, 512]);  permute_1280 = None
        bmm_193 = torch.ops.aten.bmm.default(view_2341, view_2342);  view_2341 = view_2342 = None
        view_2343 = torch.ops.aten.view.default(bmm_193, [256, 512, 1, 1, 512]);  bmm_193 = None
        permute_1281 = torch.ops.aten.permute.default(view_2343, [3, 1, 4, 0, 2]);  view_2343 = None
        view_2344 = torch.ops.aten.view.default(permute_1281, [1, 512, 512, 256]);  permute_1281 = None
        _to_copy_1329 = torch.ops.aten._to_copy.default(getitem_2215, dtype = torch.bfloat16);  getitem_2215 = None
        _to_copy_1330 = torch.ops.aten._to_copy.default(getitem_2211, dtype = torch.bfloat16)
        t_488 = torch.ops.aten.t.default(_to_copy_1329);  _to_copy_1329 = None
        view_2345 = torch.ops.aten.view.default(_to_copy_1330, [262144, 256]);  _to_copy_1330 = None
        mm_455 = torch.ops.aten.mm.default(view_2345, t_488);  view_2345 = t_488 = None
        view_2346 = torch.ops.aten.view.default(mm_455, [1, 512, 512, 512]);  mm_455 = None
        _to_copy_1331 = torch.ops.aten._to_copy.default(getitem_2217, dtype = torch.bfloat16);  getitem_2217 = None
        _to_copy_1332 = torch.ops.aten._to_copy.default(getitem_2211, dtype = torch.bfloat16)
        t_489 = torch.ops.aten.t.default(_to_copy_1331);  _to_copy_1331 = None
        view_2347 = torch.ops.aten.view.default(_to_copy_1332, [262144, 256]);  _to_copy_1332 = None
        mm_456 = torch.ops.aten.mm.default(view_2347, t_489);  view_2347 = t_489 = None
        view_2348 = torch.ops.aten.view.default(mm_456, [1, 512, 512, 512]);  mm_456 = None
        sigmoid_181 = torch.ops.aten.sigmoid.default(view_2348);  view_2348 = None
        mul_298 = torch.ops.aten.mul.Tensor(view_2346, sigmoid_181);  view_2346 = sigmoid_181 = None
        view_2349 = torch.ops.aten.view.default(mul_298, [262144, 512]);  mul_298 = None
        view_2350 = torch.ops.aten.view.default(view_2349, [1, 512, 512, 512]);  view_2349 = None
        transpose_58 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_795 = torch.ops.aten.unsqueeze.default(transpose_58, 3);  transpose_58 = None
        clone_210 = torch.ops.aten.clone.default(unsqueeze_795, memory_format = torch.contiguous_format);  unsqueeze_795 = None
        bitwise_not_151 = torch.ops.aten.bitwise_not.default(clone_210);  clone_210 = None
        masked_fill_151 = torch.ops.aten.masked_fill.Scalar(view_2350, bitwise_not_151, 0);  view_2350 = bitwise_not_151 = None
        view_2351 = torch.ops.aten.view.default(masked_fill_151, [262144, 512]);  masked_fill_151 = None
        view_2355 = torch.ops.aten.view.default(view_2351, [1, 512, 512, 512])
        split_tensor_237 = torch.ops.aten.split.Tensor(view_2355, 256, dim = -1);  view_2355 = None
        getitem_2227 = split_tensor_237[0]
        unsqueeze_798 = torch.ops.aten.unsqueeze.default(getitem_2227, 4);  getitem_2227 = None
        permute_1286 = torch.ops.aten.permute.default(unsqueeze_798, [0, 2, 4, 3, 1]);  unsqueeze_798 = None
        permute_1287 = torch.ops.aten.permute.default(permute_1286, [3, 1, 4, 0, 2]);  permute_1286 = None
        view_2356 = torch.ops.aten.view.default(permute_1287, [256, 512, 512]);  permute_1287 = None
        view_2357 = torch.ops.aten.view.default(view_2351, [1, 512, 512, 512]);  view_2351 = None
        split_tensor_238 = torch.ops.aten.split.Tensor(view_2357, 256, dim = -1);  view_2357 = None
        getitem_2230 = split_tensor_238[1];  split_tensor_238 = None
        unsqueeze_799 = torch.ops.aten.unsqueeze.default(getitem_2230, 4);  getitem_2230 = None
        permute_1288 = torch.ops.aten.permute.default(unsqueeze_799, [0, 4, 2, 3, 1]);  unsqueeze_799 = None
        permute_1289 = torch.ops.aten.permute.default(permute_1288, [3, 4, 0, 2, 1]);  permute_1288 = None
        view_2358 = torch.ops.aten.view.default(permute_1289, [256, 512, 512]);  permute_1289 = None
        bmm_194 = torch.ops.aten.bmm.default(view_2356, view_2358);  view_2356 = view_2358 = None
        view_2359 = torch.ops.aten.view.default(bmm_194, [256, 512, 1, 1, 512]);  bmm_194 = None
        permute_1290 = torch.ops.aten.permute.default(view_2359, [3, 1, 4, 0, 2]);  view_2359 = None
        view_2360 = torch.ops.aten.view.default(permute_1290, [1, 512, 512, 256]);  permute_1290 = None
        _to_copy_1333 = torch.ops.aten._to_copy.default(view_2344, dtype = torch.float32);  view_2344 = None
        native_layer_norm_default_275 = torch.ops.aten.native_layer_norm.default(_to_copy_1333, [256], None, None, 1e-05);  _to_copy_1333 = None
        getitem_2231 = native_layer_norm_default_275[0]
        _to_copy_1334 = torch.ops.aten._to_copy.default(view_2360, dtype = torch.float32);  view_2360 = None
        native_layer_norm_default_276 = torch.ops.aten.native_layer_norm.default(_to_copy_1334, [256], None, None, 1e-05);  _to_copy_1334 = None
        getitem_2234 = native_layer_norm_default_276[0]
        add_240 = torch.ops.aten.add.Tensor(getitem_2231, getitem_2234);  getitem_2231 = getitem_2234 = None
        _to_copy_1335 = torch.ops.aten._to_copy.default(arg748_1, dtype = torch.bfloat16);  arg748_1 = None
        _to_copy_1336 = torch.ops.aten._to_copy.default(add_240, dtype = torch.bfloat16);  add_240 = None
        t_490 = torch.ops.aten.t.default(_to_copy_1335);  _to_copy_1335 = None
        view_2361 = torch.ops.aten.view.default(_to_copy_1336, [262144, 256]);  _to_copy_1336 = None
        mm_457 = torch.ops.aten.mm.default(view_2361, t_490);  view_2361 = t_490 = None
        view_2362 = torch.ops.aten.view.default(mm_457, [1, 512, 512, 256]);  mm_457 = None
        _to_copy_1337 = torch.ops.aten._to_copy.default(getitem_2218, dtype = torch.bfloat16);  getitem_2218 = None
        _to_copy_1338 = torch.ops.aten._to_copy.default(getitem_2211, dtype = torch.bfloat16);  getitem_2211 = None
        t_491 = torch.ops.aten.t.default(_to_copy_1337);  _to_copy_1337 = None
        view_2363 = torch.ops.aten.view.default(_to_copy_1338, [262144, 256]);  _to_copy_1338 = None
        mm_458 = torch.ops.aten.mm.default(view_2363, t_491);  view_2363 = t_491 = None
        view_2364 = torch.ops.aten.view.default(mm_458, [1, 512, 512, 256]);  mm_458 = None
        sigmoid_182 = torch.ops.aten.sigmoid.default(view_2364);  view_2364 = None
        mul_299 = torch.ops.aten.mul.Tensor(view_2362, sigmoid_182);  view_2362 = sigmoid_182 = None
        add_241 = torch.ops.aten.add.Tensor(add_235, mul_299);  mul_299 = None
        _to_copy_1339 = torch.ops.aten._to_copy.default(add_235, dtype = torch.float32)
        native_layer_norm_default_277 = torch.ops.aten.native_layer_norm.default(_to_copy_1339, [256], None, None, 1e-05);  _to_copy_1339 = None
        getitem_2237 = native_layer_norm_default_277[0]
        _to_copy_1340 = torch.ops.aten._to_copy.default(arg752_1, dtype = torch.bfloat16);  arg752_1 = None
        _to_copy_1341 = torch.ops.aten._to_copy.default(getitem_2237, dtype = torch.bfloat16)
        t_492 = torch.ops.aten.t.default(_to_copy_1340);  _to_copy_1340 = None
        view_2365 = torch.ops.aten.view.default(_to_copy_1341, [262144, 256]);  _to_copy_1341 = None
        mm_459 = torch.ops.aten.mm.default(view_2365, t_492);  view_2365 = t_492 = None
        view_2366 = torch.ops.aten.view.default(mm_459, [1, 512, 512, 8]);  mm_459 = None
        view_2367 = torch.ops.aten.view.default(view_2366, [1, 512, 512, 2, 4]);  view_2366 = None
        permute_1291 = torch.ops.aten.permute.default(view_2367, [0, 3, 4, 1, 2]);  view_2367 = None
        view_2368 = torch.ops.aten.view.default(permute_1291, [1, 2, 4, 1, 512, 512]);  permute_1291 = None
        view_2369 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_152 = torch.ops.aten.bitwise_not.default(view_2369);  view_2369 = None
        masked_fill_152 = torch.ops.aten.masked_fill.Scalar(view_2368, bitwise_not_152, -10000);  view_2368 = bitwise_not_152 = None
        view_2370 = torch.ops.aten.view.default(masked_fill_152, [1, 2, 4, 512, 512]);  masked_fill_152 = None
        permute_1292 = torch.ops.aten.permute.default(view_2370, [1, 0, 2, 3, 4]);  view_2370 = None
        view_2371 = torch.ops.aten.view.default(permute_1292, [2, 4, 1, 512, 512]);  permute_1292 = None
        _to_copy_1342 = torch.ops.aten._to_copy.default(arg753_1, dtype = torch.bfloat16);  arg753_1 = None
        _to_copy_1343 = torch.ops.aten._to_copy.default(getitem_2237, dtype = torch.bfloat16)
        t_493 = torch.ops.aten.t.default(_to_copy_1342);  _to_copy_1342 = None
        view_2372 = torch.ops.aten.view.default(_to_copy_1343, [262144, 256]);  _to_copy_1343 = None
        mm_460 = torch.ops.aten.mm.default(view_2372, t_493);  view_2372 = t_493 = None
        view_2373 = torch.ops.aten.view.default(mm_460, [1, 512, 512, 1024]);  mm_460 = None
        select_59 = torch.ops.aten.select.int(view_2371, 0, 0)
        view_2374 = torch.ops.aten.view.default(view_2373, [1, 512, 512, 4, 4, 64]);  view_2373 = None
        permute_1293 = torch.ops.aten.permute.default(view_2374, [4, 0, 3, 1, 2, 5]);  view_2374 = None
        view_2375 = torch.ops.aten.view.default(permute_1293, [4, 4, 512, 512, 64]);  permute_1293 = None
        unbind_int_109 = torch.ops.aten.unbind.int(view_2375);  view_2375 = None
        getitem_2240 = unbind_int_109[0]
        getitem_2241 = unbind_int_109[1]
        getitem_2242 = unbind_int_109[2]
        getitem_2243 = unbind_int_109[3];  unbind_int_109 = None
        expand_142 = torch.ops.aten.expand.default(select_59, [4, 512, 512, 512]);  select_59 = None
        _scaled_dot_product_efficient_attention_default_81 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2240, getitem_2241, getitem_2242, expand_142, False);  getitem_2240 = getitem_2241 = getitem_2242 = expand_142 = None
        getitem_2244 = _scaled_dot_product_efficient_attention_default_81[0]
        sigmoid_183 = torch.ops.aten.sigmoid.default(getitem_2243);  getitem_2243 = None
        mul_300 = torch.ops.aten.mul.Tensor(getitem_2244, sigmoid_183);  getitem_2244 = sigmoid_183 = None
        view_2376 = torch.ops.aten.view.default(mul_300, [1, 4, 512, 512, 64]);  mul_300 = None
        permute_1294 = torch.ops.aten.permute.default(view_2376, [0, 2, 3, 1, 4]);  view_2376 = None
        clone_211 = torch.ops.aten.clone.default(permute_1294, memory_format = torch.contiguous_format);  permute_1294 = None
        _unsafe_view_180 = torch.ops.aten._unsafe_view.default(clone_211, [1, 512, 512, 256]);  clone_211 = None
        transpose_59 = torch.ops.aten.transpose.int(getitem_2237, 1, 2);  getitem_2237 = None
        _to_copy_1344 = torch.ops.aten._to_copy.default(arg754_1, dtype = torch.bfloat16);  arg754_1 = None
        _to_copy_1345 = torch.ops.aten._to_copy.default(transpose_59, dtype = torch.bfloat16);  transpose_59 = None
        t_494 = torch.ops.aten.t.default(_to_copy_1344);  _to_copy_1344 = None
        expand_143 = torch.ops.aten.expand.default(_to_copy_1345, [1, 512, 512, 256]);  _to_copy_1345 = None
        view_2377 = torch.ops.aten.view.default(expand_143, [512, 512, 256]);  expand_143 = None
        expand_144 = torch.ops.aten.expand.default(t_494, [1, 512, 256, 1024]);  t_494 = None
        view_2378 = torch.ops.aten.view.default(expand_144, [512, 256, 1024]);  expand_144 = None
        bmm_195 = torch.ops.aten.bmm.default(view_2377, view_2378);  view_2377 = view_2378 = None
        view_2379 = torch.ops.aten.view.default(bmm_195, [1, 512, 512, 1024]);  bmm_195 = None
        select_60 = torch.ops.aten.select.int(view_2371, 0, 1);  view_2371 = None
        view_2380 = torch.ops.aten.view.default(view_2379, [1, 512, 512, 4, 4, 64]);  view_2379 = None
        permute_1295 = torch.ops.aten.permute.default(view_2380, [4, 0, 3, 1, 2, 5]);  view_2380 = None
        view_2381 = torch.ops.aten.view.default(permute_1295, [4, 4, 512, 512, 64]);  permute_1295 = None
        unbind_int_110 = torch.ops.aten.unbind.int(view_2381);  view_2381 = None
        getitem_2248 = unbind_int_110[0]
        getitem_2249 = unbind_int_110[1]
        getitem_2250 = unbind_int_110[2]
        getitem_2251 = unbind_int_110[3];  unbind_int_110 = None
        expand_145 = torch.ops.aten.expand.default(select_60, [4, 512, 512, 512]);  select_60 = None
        _scaled_dot_product_efficient_attention_default_82 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2248, getitem_2249, getitem_2250, expand_145, False);  getitem_2248 = getitem_2249 = getitem_2250 = expand_145 = None
        getitem_2252 = _scaled_dot_product_efficient_attention_default_82[0]
        sigmoid_184 = torch.ops.aten.sigmoid.default(getitem_2251);  getitem_2251 = None
        mul_301 = torch.ops.aten.mul.Tensor(getitem_2252, sigmoid_184);  getitem_2252 = sigmoid_184 = None
        view_2382 = torch.ops.aten.view.default(mul_301, [1, 4, 512, 512, 64]);  mul_301 = None
        permute_1296 = torch.ops.aten.permute.default(view_2382, [0, 2, 3, 1, 4]);  view_2382 = None
        clone_212 = torch.ops.aten.clone.default(permute_1296, memory_format = torch.contiguous_format);  permute_1296 = None
        _unsafe_view_181 = torch.ops.aten._unsafe_view.default(clone_212, [1, 512, 512, 256]);  clone_212 = None
        cat_35 = torch.ops.aten.cat.default([_unsafe_view_180, _unsafe_view_181], dim = -1);  _unsafe_view_180 = _unsafe_view_181 = None
        slice_228 = torch.ops.aten.slice.Tensor(arg751_1, dim = 0, start = 0, end = 9223372036854775807);  arg751_1 = None
        unsqueeze_800 = torch.ops.aten.unsqueeze.default(slice_228, 1);  slice_228 = None
        mul_302 = torch.ops.aten.mul.Tensor(arg755_1, unsqueeze_800);  arg755_1 = unsqueeze_800 = None
        _to_copy_1346 = torch.ops.aten._to_copy.default(mul_302, dtype = torch.bfloat16);  mul_302 = None
        t_495 = torch.ops.aten.t.default(_to_copy_1346);  _to_copy_1346 = None
        view_2383 = torch.ops.aten.view.default(cat_35, [262144, 512]);  cat_35 = None
        mm_461 = torch.ops.aten.mm.default(view_2383, t_495);  view_2383 = t_495 = None
        view_2384 = torch.ops.aten.view.default(mm_461, [1, 512, 512, 256]);  mm_461 = None
        add_242 = torch.ops.aten.add.Tensor(add_241, view_2384);  add_241 = view_2384 = None
        split_tensor_239 = torch.ops.aten.split.Tensor(add_235, 512, dim = -2)
        getitem_2256 = split_tensor_239[0];  split_tensor_239 = None
        _to_copy_1347 = torch.ops.aten._to_copy.default(getitem_2256, dtype = torch.float32);  getitem_2256 = None
        native_layer_norm_default_278 = torch.ops.aten.native_layer_norm.default(_to_copy_1347, [256], arg742_1, arg743_1, 1e-05);  _to_copy_1347 = arg742_1 = arg743_1 = None
        getitem_2257 = native_layer_norm_default_278[0]
        _to_copy_1348 = torch.ops.aten._to_copy.default(arg744_1, dtype = torch.bfloat16);  arg744_1 = None
        _to_copy_1349 = torch.ops.aten._to_copy.default(getitem_2257, dtype = torch.bfloat16);  getitem_2257 = None
        t_496 = torch.ops.aten.t.default(_to_copy_1348);  _to_copy_1348 = None
        view_2385 = torch.ops.aten.view.default(_to_copy_1349, [262144, 256]);  _to_copy_1349 = None
        mm_462 = torch.ops.aten.mm.default(view_2385, t_496);  view_2385 = t_496 = None
        view_2386 = torch.ops.aten.view.default(mm_462, [1, 512, 512, 1024]);  mm_462 = None
        split_tensor_240 = torch.ops.aten.split.Tensor(view_2386, 512, dim = -1);  view_2386 = None
        getitem_2260 = split_tensor_240[0]
        getitem_2261 = split_tensor_240[1];  split_tensor_240 = None
        silu_64 = torch.ops.aten.silu.default(getitem_2260);  getitem_2260 = None
        mul_303 = torch.ops.aten.mul.Tensor(silu_64, getitem_2261);  silu_64 = getitem_2261 = None
        _to_copy_1350 = torch.ops.aten._to_copy.default(arg745_1, dtype = torch.bfloat16);  arg745_1 = None
        t_497 = torch.ops.aten.t.default(_to_copy_1350);  _to_copy_1350 = None
        view_2388 = torch.ops.aten.view.default(mul_303, [262144, 512]);  mul_303 = None
        mm_463 = torch.ops.aten.mm.default(view_2388, t_497);  view_2388 = t_497 = None
        view_2389 = torch.ops.aten.view.default(mm_463, [1, 512, 512, 256]);  mm_463 = None
        add_243 = torch.ops.aten.add.Tensor(add_242, view_2389);  add_242 = view_2389 = None
        _to_copy_1351 = torch.ops.aten._to_copy.default(add_239, dtype = torch.float32)
        native_layer_norm_default_279 = torch.ops.aten.native_layer_norm.default(_to_copy_1351, [384], arg760_1, arg761_1, 1e-05);  _to_copy_1351 = arg760_1 = arg761_1 = None
        getitem_2262 = native_layer_norm_default_279[0]
        _to_copy_1352 = torch.ops.aten._to_copy.default(add_235, dtype = torch.float32);  add_235 = None
        native_layer_norm_default_280 = torch.ops.aten.native_layer_norm.default(_to_copy_1352, [256], arg762_1, arg763_1, 1e-05);  _to_copy_1352 = arg762_1 = arg763_1 = None
        getitem_2265 = native_layer_norm_default_280[0]
        _to_copy_1353 = torch.ops.aten._to_copy.default(arg764_1, dtype = torch.bfloat16);  arg764_1 = None
        _to_copy_1354 = torch.ops.aten._to_copy.default(getitem_2265, dtype = torch.bfloat16);  getitem_2265 = None
        t_498 = torch.ops.aten.t.default(_to_copy_1353);  _to_copy_1353 = None
        view_2390 = torch.ops.aten.view.default(_to_copy_1354, [262144, 256]);  _to_copy_1354 = None
        mm_464 = torch.ops.aten.mm.default(view_2390, t_498);  view_2390 = t_498 = None
        view_2391 = torch.ops.aten.view.default(mm_464, [1, 512, 512, 16]);  mm_464 = None
        permute_1297 = torch.ops.aten.permute.default(view_2391, [0, 3, 1, 2]);  view_2391 = None
        view_2392 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_153 = torch.ops.aten.bitwise_not.default(view_2392);  view_2392 = None
        masked_fill_153 = torch.ops.aten.masked_fill.Scalar(permute_1297, bitwise_not_153, -10000);  permute_1297 = bitwise_not_153 = None
        _to_copy_1355 = torch.ops.aten._to_copy.default(getitem_2262, dtype = torch.bfloat16);  getitem_2262 = None
        _to_copy_1356 = torch.ops.aten._to_copy.default(arg766_1, dtype = torch.bfloat16);  arg766_1 = None
        unsqueeze_801 = torch.ops.aten.unsqueeze.default(_to_copy_1355, 3);  _to_copy_1355 = None
        unsqueeze_802 = torch.ops.aten.unsqueeze.default(unsqueeze_801, 4);  unsqueeze_801 = None
        unsqueeze_803 = torch.ops.aten.unsqueeze.default(unsqueeze_802, 5);  unsqueeze_802 = None
        permute_1298 = torch.ops.aten.permute.default(unsqueeze_803, [3, 0, 4, 1, 5, 2]);  unsqueeze_803 = None
        unsqueeze_804 = torch.ops.aten.unsqueeze.default(_to_copy_1356, 4);  _to_copy_1356 = None
        unsqueeze_805 = torch.ops.aten.unsqueeze.default(unsqueeze_804, 5);  unsqueeze_804 = None
        permute_1299 = torch.ops.aten.permute.default(unsqueeze_805, [1, 4, 2, 5, 3, 0]);  unsqueeze_805 = None
        permute_1300 = torch.ops.aten.permute.default(permute_1298, [3, 5, 0, 1, 2, 4]);  permute_1298 = None
        view_2393 = torch.ops.aten.view.default(permute_1300, [1, 512, 384]);  permute_1300 = None
        permute_1301 = torch.ops.aten.permute.default(permute_1299, [5, 0, 1, 2, 4, 3]);  permute_1299 = None
        view_2394 = torch.ops.aten.view.default(permute_1301, [1, 384, 1536]);  permute_1301 = None
        bmm_196 = torch.ops.aten.bmm.default(view_2393, view_2394);  view_2393 = view_2394 = None
        view_2395 = torch.ops.aten.view.default(bmm_196, [512, 1, 4, 1, 16, 24]);  bmm_196 = None
        permute_1302 = torch.ops.aten.permute.default(view_2395, [2, 3, 4, 0, 5, 1]);  view_2395 = None
        view_2396 = torch.ops.aten.view.default(permute_1302, [4, 1, 16, 512, 24]);  permute_1302 = None
        unbind_int_111 = torch.ops.aten.unbind.int(view_2396);  view_2396 = None
        getitem_2268 = unbind_int_111[0]
        getitem_2269 = unbind_int_111[1]
        getitem_2270 = unbind_int_111[2]
        getitem_2271 = unbind_int_111[3];  unbind_int_111 = None
        view_2397 = torch.ops.aten.view.default(arg765_1, [1, 16, 1, 24]);  arg765_1 = None
        add_244 = torch.ops.aten.add.Tensor(getitem_2268, view_2397);  getitem_2268 = view_2397 = None
        _to_copy_1357 = torch.ops.aten._to_copy.default(add_244, dtype = torch.bfloat16);  add_244 = None
        expand_146 = torch.ops.aten.expand.default(masked_fill_153, [1, 16, 512, 512]);  masked_fill_153 = None
        _scaled_dot_product_efficient_attention_default_83 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1357, getitem_2269, getitem_2270, expand_146, False);  _to_copy_1357 = getitem_2269 = getitem_2270 = expand_146 = None
        getitem_2272 = _scaled_dot_product_efficient_attention_default_83[0]
        add_245 = torch.ops.aten.add.Tensor(getitem_2271, 1);  getitem_2271 = None
        sigmoid_185 = torch.ops.aten.sigmoid.default(add_245);  add_245 = None
        mul_304 = torch.ops.aten.mul.Tensor(getitem_2272, sigmoid_185);  getitem_2272 = sigmoid_185 = None
        _to_copy_1358 = torch.ops.aten._to_copy.default(arg767_1, dtype = torch.bfloat16);  arg767_1 = None
        unsqueeze_806 = torch.ops.aten.unsqueeze.default(mul_304, 4);  mul_304 = None
        permute_1303 = torch.ops.aten.permute.default(unsqueeze_806, [0, 2, 4, 3, 1]);  unsqueeze_806 = None
        unsqueeze_807 = torch.ops.aten.unsqueeze.default(_to_copy_1358, 3);  _to_copy_1358 = None
        unsqueeze_808 = torch.ops.aten.unsqueeze.default(unsqueeze_807, 4);  unsqueeze_807 = None
        permute_1304 = torch.ops.aten.permute.default(unsqueeze_808, [3, 4, 2, 1, 0]);  unsqueeze_808 = None
        permute_1305 = torch.ops.aten.permute.default(permute_1303, [1, 3, 4, 0, 2]);  permute_1303 = None
        clone_213 = torch.ops.aten.clone.default(permute_1305, memory_format = torch.contiguous_format);  permute_1305 = None
        _unsafe_view_182 = torch.ops.aten._unsafe_view.default(clone_213, [1, 512, 384]);  clone_213 = None
        permute_1306 = torch.ops.aten.permute.default(permute_1304, [3, 4, 0, 2, 1]);  permute_1304 = None
        clone_214 = torch.ops.aten.clone.default(permute_1306, memory_format = torch.contiguous_format);  permute_1306 = None
        _unsafe_view_183 = torch.ops.aten._unsafe_view.default(clone_214, [1, 384, 384]);  clone_214 = None
        bmm_197 = torch.ops.aten.bmm.default(_unsafe_view_182, _unsafe_view_183);  _unsafe_view_182 = _unsafe_view_183 = None
        view_2398 = torch.ops.aten.view.default(bmm_197, [512, 1, 1, 1, 384]);  bmm_197 = None
        permute_1307 = torch.ops.aten.permute.default(view_2398, [3, 0, 4, 1, 2]);  view_2398 = None
        view_2399 = torch.ops.aten.view.default(permute_1307, [1, 512, 384]);  permute_1307 = None
        unsqueeze_809 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_305 = torch.ops.aten.mul.Tensor(view_2399, unsqueeze_809);  view_2399 = unsqueeze_809 = None
        add_246 = torch.ops.aten.add.Tensor(add_239, mul_305);  mul_305 = None
        split_tensor_241 = torch.ops.aten.split.Tensor(add_239, 512, dim = -2);  add_239 = None
        getitem_2276 = split_tensor_241[0];  split_tensor_241 = None
        _to_copy_1359 = torch.ops.aten._to_copy.default(getitem_2276, dtype = torch.float32);  getitem_2276 = None
        native_layer_norm_default_281 = torch.ops.aten.native_layer_norm.default(_to_copy_1359, [384], arg756_1, arg757_1, 1e-05);  _to_copy_1359 = arg756_1 = arg757_1 = None
        getitem_2277 = native_layer_norm_default_281[0]
        _to_copy_1360 = torch.ops.aten._to_copy.default(arg758_1, dtype = torch.bfloat16);  arg758_1 = None
        _to_copy_1361 = torch.ops.aten._to_copy.default(getitem_2277, dtype = torch.bfloat16);  getitem_2277 = None
        t_499 = torch.ops.aten.t.default(_to_copy_1360);  _to_copy_1360 = None
        view_2400 = torch.ops.aten.view.default(_to_copy_1361, [512, 384]);  _to_copy_1361 = None
        mm_465 = torch.ops.aten.mm.default(view_2400, t_499);  view_2400 = t_499 = None
        view_2401 = torch.ops.aten.view.default(mm_465, [1, 512, 1536]);  mm_465 = None
        split_tensor_242 = torch.ops.aten.split.Tensor(view_2401, 768, dim = -1);  view_2401 = None
        getitem_2280 = split_tensor_242[0]
        getitem_2281 = split_tensor_242[1];  split_tensor_242 = None
        silu_65 = torch.ops.aten.silu.default(getitem_2280);  getitem_2280 = None
        mul_306 = torch.ops.aten.mul.Tensor(silu_65, getitem_2281);  silu_65 = getitem_2281 = None
        _to_copy_1362 = torch.ops.aten._to_copy.default(arg759_1, dtype = torch.bfloat16);  arg759_1 = None
        t_500 = torch.ops.aten.t.default(_to_copy_1362);  _to_copy_1362 = None
        view_2403 = torch.ops.aten.view.default(mul_306, [512, 768]);  mul_306 = None
        mm_466 = torch.ops.aten.mm.default(view_2403, t_500);  view_2403 = t_500 = None
        view_2404 = torch.ops.aten.view.default(mm_466, [1, 512, 384]);  mm_466 = None
        add_247 = torch.ops.aten.add.Tensor(add_246, view_2404);  add_246 = view_2404 = None
        _to_copy_1363 = torch.ops.aten._to_copy.default(add_243, dtype = torch.float32)
        native_layer_norm_default_282 = torch.ops.aten.native_layer_norm.default(_to_copy_1363, [256], arg772_1, arg773_1, 1e-05);  _to_copy_1363 = arg772_1 = arg773_1 = None
        getitem_2282 = native_layer_norm_default_282[0]
        split_with_sizes_default_60 = torch.ops.aten.split_with_sizes.default(arg775_1, [512, 512]);  arg775_1 = None
        getitem_2285 = split_with_sizes_default_60[0]
        getitem_2286 = split_with_sizes_default_60[1];  split_with_sizes_default_60 = None
        split_with_sizes_default_61 = torch.ops.aten.split_with_sizes.default(arg776_1, [512, 512, 256]);  arg776_1 = None
        getitem_2287 = split_with_sizes_default_61[0]
        getitem_2288 = split_with_sizes_default_61[1]
        getitem_2289 = split_with_sizes_default_61[2];  split_with_sizes_default_61 = None
        _to_copy_1364 = torch.ops.aten._to_copy.default(getitem_2285, dtype = torch.bfloat16);  getitem_2285 = None
        _to_copy_1365 = torch.ops.aten._to_copy.default(getitem_2282, dtype = torch.bfloat16)
        t_501 = torch.ops.aten.t.default(_to_copy_1364);  _to_copy_1364 = None
        view_2405 = torch.ops.aten.view.default(_to_copy_1365, [262144, 256]);  _to_copy_1365 = None
        mm_467 = torch.ops.aten.mm.default(view_2405, t_501);  view_2405 = t_501 = None
        view_2406 = torch.ops.aten.view.default(mm_467, [1, 512, 512, 512]);  mm_467 = None
        _to_copy_1366 = torch.ops.aten._to_copy.default(getitem_2287, dtype = torch.bfloat16);  getitem_2287 = None
        _to_copy_1367 = torch.ops.aten._to_copy.default(getitem_2282, dtype = torch.bfloat16)
        t_502 = torch.ops.aten.t.default(_to_copy_1366);  _to_copy_1366 = None
        view_2407 = torch.ops.aten.view.default(_to_copy_1367, [262144, 256]);  _to_copy_1367 = None
        mm_468 = torch.ops.aten.mm.default(view_2407, t_502);  view_2407 = t_502 = None
        view_2408 = torch.ops.aten.view.default(mm_468, [1, 512, 512, 512]);  mm_468 = None
        sigmoid_186 = torch.ops.aten.sigmoid.default(view_2408);  view_2408 = None
        mul_307 = torch.ops.aten.mul.Tensor(view_2406, sigmoid_186);  view_2406 = sigmoid_186 = None
        unsqueeze_810 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_154 = torch.ops.aten.bitwise_not.default(unsqueeze_810);  unsqueeze_810 = None
        masked_fill_154 = torch.ops.aten.masked_fill.Scalar(mul_307, bitwise_not_154, 0);  mul_307 = bitwise_not_154 = None
        split_tensor_243 = torch.ops.aten.split.Tensor(masked_fill_154, 256, dim = -1)
        getitem_2292 = split_tensor_243[0]
        unsqueeze_813 = torch.ops.aten.unsqueeze.default(getitem_2292, 4);  getitem_2292 = None
        permute_1312 = torch.ops.aten.permute.default(unsqueeze_813, [0, 1, 4, 3, 2]);  unsqueeze_813 = None
        permute_1313 = torch.ops.aten.permute.default(permute_1312, [3, 1, 4, 0, 2]);  permute_1312 = None
        view_2411 = torch.ops.aten.view.default(permute_1313, [256, 512, 512]);  permute_1313 = None
        split_tensor_244 = torch.ops.aten.split.Tensor(masked_fill_154, 256, dim = -1);  masked_fill_154 = None
        getitem_2295 = split_tensor_244[1];  split_tensor_244 = None
        unsqueeze_814 = torch.ops.aten.unsqueeze.default(getitem_2295, 4);  getitem_2295 = None
        permute_1314 = torch.ops.aten.permute.default(unsqueeze_814, [0, 4, 1, 3, 2]);  unsqueeze_814 = None
        permute_1315 = torch.ops.aten.permute.default(permute_1314, [3, 4, 0, 2, 1]);  permute_1314 = None
        view_2412 = torch.ops.aten.view.default(permute_1315, [256, 512, 512]);  permute_1315 = None
        bmm_198 = torch.ops.aten.bmm.default(view_2411, view_2412);  view_2411 = view_2412 = None
        view_2413 = torch.ops.aten.view.default(bmm_198, [256, 512, 1, 1, 512]);  bmm_198 = None
        permute_1316 = torch.ops.aten.permute.default(view_2413, [3, 1, 4, 0, 2]);  view_2413 = None
        view_2414 = torch.ops.aten.view.default(permute_1316, [1, 512, 512, 256]);  permute_1316 = None
        _to_copy_1368 = torch.ops.aten._to_copy.default(getitem_2286, dtype = torch.bfloat16);  getitem_2286 = None
        _to_copy_1369 = torch.ops.aten._to_copy.default(getitem_2282, dtype = torch.bfloat16)
        t_503 = torch.ops.aten.t.default(_to_copy_1368);  _to_copy_1368 = None
        view_2415 = torch.ops.aten.view.default(_to_copy_1369, [262144, 256]);  _to_copy_1369 = None
        mm_469 = torch.ops.aten.mm.default(view_2415, t_503);  view_2415 = t_503 = None
        view_2416 = torch.ops.aten.view.default(mm_469, [1, 512, 512, 512]);  mm_469 = None
        _to_copy_1370 = torch.ops.aten._to_copy.default(getitem_2288, dtype = torch.bfloat16);  getitem_2288 = None
        _to_copy_1371 = torch.ops.aten._to_copy.default(getitem_2282, dtype = torch.bfloat16)
        t_504 = torch.ops.aten.t.default(_to_copy_1370);  _to_copy_1370 = None
        view_2417 = torch.ops.aten.view.default(_to_copy_1371, [262144, 256]);  _to_copy_1371 = None
        mm_470 = torch.ops.aten.mm.default(view_2417, t_504);  view_2417 = t_504 = None
        view_2418 = torch.ops.aten.view.default(mm_470, [1, 512, 512, 512]);  mm_470 = None
        sigmoid_187 = torch.ops.aten.sigmoid.default(view_2418);  view_2418 = None
        mul_308 = torch.ops.aten.mul.Tensor(view_2416, sigmoid_187);  view_2416 = sigmoid_187 = None
        view_2419 = torch.ops.aten.view.default(mul_308, [262144, 512]);  mul_308 = None
        view_2420 = torch.ops.aten.view.default(view_2419, [1, 512, 512, 512]);  view_2419 = None
        transpose_60 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_815 = torch.ops.aten.unsqueeze.default(transpose_60, 3);  transpose_60 = None
        clone_215 = torch.ops.aten.clone.default(unsqueeze_815, memory_format = torch.contiguous_format);  unsqueeze_815 = None
        bitwise_not_155 = torch.ops.aten.bitwise_not.default(clone_215);  clone_215 = None
        masked_fill_155 = torch.ops.aten.masked_fill.Scalar(view_2420, bitwise_not_155, 0);  view_2420 = bitwise_not_155 = None
        view_2421 = torch.ops.aten.view.default(masked_fill_155, [262144, 512]);  masked_fill_155 = None
        view_2425 = torch.ops.aten.view.default(view_2421, [1, 512, 512, 512])
        split_tensor_245 = torch.ops.aten.split.Tensor(view_2425, 256, dim = -1);  view_2425 = None
        getitem_2298 = split_tensor_245[0]
        unsqueeze_818 = torch.ops.aten.unsqueeze.default(getitem_2298, 4);  getitem_2298 = None
        permute_1321 = torch.ops.aten.permute.default(unsqueeze_818, [0, 2, 4, 3, 1]);  unsqueeze_818 = None
        permute_1322 = torch.ops.aten.permute.default(permute_1321, [3, 1, 4, 0, 2]);  permute_1321 = None
        view_2426 = torch.ops.aten.view.default(permute_1322, [256, 512, 512]);  permute_1322 = None
        view_2427 = torch.ops.aten.view.default(view_2421, [1, 512, 512, 512]);  view_2421 = None
        split_tensor_246 = torch.ops.aten.split.Tensor(view_2427, 256, dim = -1);  view_2427 = None
        getitem_2301 = split_tensor_246[1];  split_tensor_246 = None
        unsqueeze_819 = torch.ops.aten.unsqueeze.default(getitem_2301, 4);  getitem_2301 = None
        permute_1323 = torch.ops.aten.permute.default(unsqueeze_819, [0, 4, 2, 3, 1]);  unsqueeze_819 = None
        permute_1324 = torch.ops.aten.permute.default(permute_1323, [3, 4, 0, 2, 1]);  permute_1323 = None
        view_2428 = torch.ops.aten.view.default(permute_1324, [256, 512, 512]);  permute_1324 = None
        bmm_199 = torch.ops.aten.bmm.default(view_2426, view_2428);  view_2426 = view_2428 = None
        view_2429 = torch.ops.aten.view.default(bmm_199, [256, 512, 1, 1, 512]);  bmm_199 = None
        permute_1325 = torch.ops.aten.permute.default(view_2429, [3, 1, 4, 0, 2]);  view_2429 = None
        view_2430 = torch.ops.aten.view.default(permute_1325, [1, 512, 512, 256]);  permute_1325 = None
        _to_copy_1372 = torch.ops.aten._to_copy.default(view_2414, dtype = torch.float32);  view_2414 = None
        native_layer_norm_default_283 = torch.ops.aten.native_layer_norm.default(_to_copy_1372, [256], None, None, 1e-05);  _to_copy_1372 = None
        getitem_2302 = native_layer_norm_default_283[0]
        _to_copy_1373 = torch.ops.aten._to_copy.default(view_2430, dtype = torch.float32);  view_2430 = None
        native_layer_norm_default_284 = torch.ops.aten.native_layer_norm.default(_to_copy_1373, [256], None, None, 1e-05);  _to_copy_1373 = None
        getitem_2305 = native_layer_norm_default_284[0]
        add_248 = torch.ops.aten.add.Tensor(getitem_2302, getitem_2305);  getitem_2302 = getitem_2305 = None
        _to_copy_1374 = torch.ops.aten._to_copy.default(arg774_1, dtype = torch.bfloat16);  arg774_1 = None
        _to_copy_1375 = torch.ops.aten._to_copy.default(add_248, dtype = torch.bfloat16);  add_248 = None
        t_505 = torch.ops.aten.t.default(_to_copy_1374);  _to_copy_1374 = None
        view_2431 = torch.ops.aten.view.default(_to_copy_1375, [262144, 256]);  _to_copy_1375 = None
        mm_471 = torch.ops.aten.mm.default(view_2431, t_505);  view_2431 = t_505 = None
        view_2432 = torch.ops.aten.view.default(mm_471, [1, 512, 512, 256]);  mm_471 = None
        _to_copy_1376 = torch.ops.aten._to_copy.default(getitem_2289, dtype = torch.bfloat16);  getitem_2289 = None
        _to_copy_1377 = torch.ops.aten._to_copy.default(getitem_2282, dtype = torch.bfloat16);  getitem_2282 = None
        t_506 = torch.ops.aten.t.default(_to_copy_1376);  _to_copy_1376 = None
        view_2433 = torch.ops.aten.view.default(_to_copy_1377, [262144, 256]);  _to_copy_1377 = None
        mm_472 = torch.ops.aten.mm.default(view_2433, t_506);  view_2433 = t_506 = None
        view_2434 = torch.ops.aten.view.default(mm_472, [1, 512, 512, 256]);  mm_472 = None
        sigmoid_188 = torch.ops.aten.sigmoid.default(view_2434);  view_2434 = None
        mul_309 = torch.ops.aten.mul.Tensor(view_2432, sigmoid_188);  view_2432 = sigmoid_188 = None
        add_249 = torch.ops.aten.add.Tensor(add_243, mul_309);  mul_309 = None
        _to_copy_1378 = torch.ops.aten._to_copy.default(add_243, dtype = torch.float32)
        native_layer_norm_default_285 = torch.ops.aten.native_layer_norm.default(_to_copy_1378, [256], None, None, 1e-05);  _to_copy_1378 = None
        getitem_2308 = native_layer_norm_default_285[0]
        _to_copy_1379 = torch.ops.aten._to_copy.default(arg778_1, dtype = torch.bfloat16);  arg778_1 = None
        _to_copy_1380 = torch.ops.aten._to_copy.default(getitem_2308, dtype = torch.bfloat16)
        t_507 = torch.ops.aten.t.default(_to_copy_1379);  _to_copy_1379 = None
        view_2435 = torch.ops.aten.view.default(_to_copy_1380, [262144, 256]);  _to_copy_1380 = None
        mm_473 = torch.ops.aten.mm.default(view_2435, t_507);  view_2435 = t_507 = None
        view_2436 = torch.ops.aten.view.default(mm_473, [1, 512, 512, 8]);  mm_473 = None
        view_2437 = torch.ops.aten.view.default(view_2436, [1, 512, 512, 2, 4]);  view_2436 = None
        permute_1326 = torch.ops.aten.permute.default(view_2437, [0, 3, 4, 1, 2]);  view_2437 = None
        view_2438 = torch.ops.aten.view.default(permute_1326, [1, 2, 4, 1, 512, 512]);  permute_1326 = None
        view_2439 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_156 = torch.ops.aten.bitwise_not.default(view_2439);  view_2439 = None
        masked_fill_156 = torch.ops.aten.masked_fill.Scalar(view_2438, bitwise_not_156, -10000);  view_2438 = bitwise_not_156 = None
        view_2440 = torch.ops.aten.view.default(masked_fill_156, [1, 2, 4, 512, 512]);  masked_fill_156 = None
        permute_1327 = torch.ops.aten.permute.default(view_2440, [1, 0, 2, 3, 4]);  view_2440 = None
        view_2441 = torch.ops.aten.view.default(permute_1327, [2, 4, 1, 512, 512]);  permute_1327 = None
        _to_copy_1381 = torch.ops.aten._to_copy.default(arg779_1, dtype = torch.bfloat16);  arg779_1 = None
        _to_copy_1382 = torch.ops.aten._to_copy.default(getitem_2308, dtype = torch.bfloat16)
        t_508 = torch.ops.aten.t.default(_to_copy_1381);  _to_copy_1381 = None
        view_2442 = torch.ops.aten.view.default(_to_copy_1382, [262144, 256]);  _to_copy_1382 = None
        mm_474 = torch.ops.aten.mm.default(view_2442, t_508);  view_2442 = t_508 = None
        view_2443 = torch.ops.aten.view.default(mm_474, [1, 512, 512, 1024]);  mm_474 = None
        select_61 = torch.ops.aten.select.int(view_2441, 0, 0)
        view_2444 = torch.ops.aten.view.default(view_2443, [1, 512, 512, 4, 4, 64]);  view_2443 = None
        permute_1328 = torch.ops.aten.permute.default(view_2444, [4, 0, 3, 1, 2, 5]);  view_2444 = None
        view_2445 = torch.ops.aten.view.default(permute_1328, [4, 4, 512, 512, 64]);  permute_1328 = None
        unbind_int_112 = torch.ops.aten.unbind.int(view_2445);  view_2445 = None
        getitem_2311 = unbind_int_112[0]
        getitem_2312 = unbind_int_112[1]
        getitem_2313 = unbind_int_112[2]
        getitem_2314 = unbind_int_112[3];  unbind_int_112 = None
        expand_147 = torch.ops.aten.expand.default(select_61, [4, 512, 512, 512]);  select_61 = None
        _scaled_dot_product_efficient_attention_default_84 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2311, getitem_2312, getitem_2313, expand_147, False);  getitem_2311 = getitem_2312 = getitem_2313 = expand_147 = None
        getitem_2315 = _scaled_dot_product_efficient_attention_default_84[0]
        sigmoid_189 = torch.ops.aten.sigmoid.default(getitem_2314);  getitem_2314 = None
        mul_310 = torch.ops.aten.mul.Tensor(getitem_2315, sigmoid_189);  getitem_2315 = sigmoid_189 = None
        view_2446 = torch.ops.aten.view.default(mul_310, [1, 4, 512, 512, 64]);  mul_310 = None
        permute_1329 = torch.ops.aten.permute.default(view_2446, [0, 2, 3, 1, 4]);  view_2446 = None
        clone_216 = torch.ops.aten.clone.default(permute_1329, memory_format = torch.contiguous_format);  permute_1329 = None
        _unsafe_view_184 = torch.ops.aten._unsafe_view.default(clone_216, [1, 512, 512, 256]);  clone_216 = None
        transpose_61 = torch.ops.aten.transpose.int(getitem_2308, 1, 2);  getitem_2308 = None
        _to_copy_1383 = torch.ops.aten._to_copy.default(arg780_1, dtype = torch.bfloat16);  arg780_1 = None
        _to_copy_1384 = torch.ops.aten._to_copy.default(transpose_61, dtype = torch.bfloat16);  transpose_61 = None
        t_509 = torch.ops.aten.t.default(_to_copy_1383);  _to_copy_1383 = None
        expand_148 = torch.ops.aten.expand.default(_to_copy_1384, [1, 512, 512, 256]);  _to_copy_1384 = None
        view_2447 = torch.ops.aten.view.default(expand_148, [512, 512, 256]);  expand_148 = None
        expand_149 = torch.ops.aten.expand.default(t_509, [1, 512, 256, 1024]);  t_509 = None
        view_2448 = torch.ops.aten.view.default(expand_149, [512, 256, 1024]);  expand_149 = None
        bmm_200 = torch.ops.aten.bmm.default(view_2447, view_2448);  view_2447 = view_2448 = None
        view_2449 = torch.ops.aten.view.default(bmm_200, [1, 512, 512, 1024]);  bmm_200 = None
        select_62 = torch.ops.aten.select.int(view_2441, 0, 1);  view_2441 = None
        view_2450 = torch.ops.aten.view.default(view_2449, [1, 512, 512, 4, 4, 64]);  view_2449 = None
        permute_1330 = torch.ops.aten.permute.default(view_2450, [4, 0, 3, 1, 2, 5]);  view_2450 = None
        view_2451 = torch.ops.aten.view.default(permute_1330, [4, 4, 512, 512, 64]);  permute_1330 = None
        unbind_int_113 = torch.ops.aten.unbind.int(view_2451);  view_2451 = None
        getitem_2319 = unbind_int_113[0]
        getitem_2320 = unbind_int_113[1]
        getitem_2321 = unbind_int_113[2]
        getitem_2322 = unbind_int_113[3];  unbind_int_113 = None
        expand_150 = torch.ops.aten.expand.default(select_62, [4, 512, 512, 512]);  select_62 = None
        _scaled_dot_product_efficient_attention_default_85 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2319, getitem_2320, getitem_2321, expand_150, False);  getitem_2319 = getitem_2320 = getitem_2321 = expand_150 = None
        getitem_2323 = _scaled_dot_product_efficient_attention_default_85[0]
        sigmoid_190 = torch.ops.aten.sigmoid.default(getitem_2322);  getitem_2322 = None
        mul_311 = torch.ops.aten.mul.Tensor(getitem_2323, sigmoid_190);  getitem_2323 = sigmoid_190 = None
        view_2452 = torch.ops.aten.view.default(mul_311, [1, 4, 512, 512, 64]);  mul_311 = None
        permute_1331 = torch.ops.aten.permute.default(view_2452, [0, 2, 3, 1, 4]);  view_2452 = None
        clone_217 = torch.ops.aten.clone.default(permute_1331, memory_format = torch.contiguous_format);  permute_1331 = None
        _unsafe_view_185 = torch.ops.aten._unsafe_view.default(clone_217, [1, 512, 512, 256]);  clone_217 = None
        cat_36 = torch.ops.aten.cat.default([_unsafe_view_184, _unsafe_view_185], dim = -1);  _unsafe_view_184 = _unsafe_view_185 = None
        slice_229 = torch.ops.aten.slice.Tensor(arg777_1, dim = 0, start = 0, end = 9223372036854775807);  arg777_1 = None
        unsqueeze_820 = torch.ops.aten.unsqueeze.default(slice_229, 1);  slice_229 = None
        mul_312 = torch.ops.aten.mul.Tensor(arg781_1, unsqueeze_820);  arg781_1 = unsqueeze_820 = None
        _to_copy_1385 = torch.ops.aten._to_copy.default(mul_312, dtype = torch.bfloat16);  mul_312 = None
        t_510 = torch.ops.aten.t.default(_to_copy_1385);  _to_copy_1385 = None
        view_2453 = torch.ops.aten.view.default(cat_36, [262144, 512]);  cat_36 = None
        mm_475 = torch.ops.aten.mm.default(view_2453, t_510);  view_2453 = t_510 = None
        view_2454 = torch.ops.aten.view.default(mm_475, [1, 512, 512, 256]);  mm_475 = None
        add_250 = torch.ops.aten.add.Tensor(add_249, view_2454);  add_249 = view_2454 = None
        split_tensor_247 = torch.ops.aten.split.Tensor(add_243, 512, dim = -2)
        getitem_2327 = split_tensor_247[0];  split_tensor_247 = None
        _to_copy_1386 = torch.ops.aten._to_copy.default(getitem_2327, dtype = torch.float32);  getitem_2327 = None
        native_layer_norm_default_286 = torch.ops.aten.native_layer_norm.default(_to_copy_1386, [256], arg768_1, arg769_1, 1e-05);  _to_copy_1386 = arg768_1 = arg769_1 = None
        getitem_2328 = native_layer_norm_default_286[0]
        _to_copy_1387 = torch.ops.aten._to_copy.default(arg770_1, dtype = torch.bfloat16);  arg770_1 = None
        _to_copy_1388 = torch.ops.aten._to_copy.default(getitem_2328, dtype = torch.bfloat16);  getitem_2328 = None
        t_511 = torch.ops.aten.t.default(_to_copy_1387);  _to_copy_1387 = None
        view_2455 = torch.ops.aten.view.default(_to_copy_1388, [262144, 256]);  _to_copy_1388 = None
        mm_476 = torch.ops.aten.mm.default(view_2455, t_511);  view_2455 = t_511 = None
        view_2456 = torch.ops.aten.view.default(mm_476, [1, 512, 512, 1024]);  mm_476 = None
        split_tensor_248 = torch.ops.aten.split.Tensor(view_2456, 512, dim = -1);  view_2456 = None
        getitem_2331 = split_tensor_248[0]
        getitem_2332 = split_tensor_248[1];  split_tensor_248 = None
        silu_66 = torch.ops.aten.silu.default(getitem_2331);  getitem_2331 = None
        mul_313 = torch.ops.aten.mul.Tensor(silu_66, getitem_2332);  silu_66 = getitem_2332 = None
        _to_copy_1389 = torch.ops.aten._to_copy.default(arg771_1, dtype = torch.bfloat16);  arg771_1 = None
        t_512 = torch.ops.aten.t.default(_to_copy_1389);  _to_copy_1389 = None
        view_2458 = torch.ops.aten.view.default(mul_313, [262144, 512]);  mul_313 = None
        mm_477 = torch.ops.aten.mm.default(view_2458, t_512);  view_2458 = t_512 = None
        view_2459 = torch.ops.aten.view.default(mm_477, [1, 512, 512, 256]);  mm_477 = None
        add_251 = torch.ops.aten.add.Tensor(add_250, view_2459);  add_250 = view_2459 = None
        _to_copy_1390 = torch.ops.aten._to_copy.default(add_247, dtype = torch.float32)
        native_layer_norm_default_287 = torch.ops.aten.native_layer_norm.default(_to_copy_1390, [384], arg786_1, arg787_1, 1e-05);  _to_copy_1390 = arg786_1 = arg787_1 = None
        getitem_2333 = native_layer_norm_default_287[0]
        _to_copy_1391 = torch.ops.aten._to_copy.default(add_243, dtype = torch.float32);  add_243 = None
        native_layer_norm_default_288 = torch.ops.aten.native_layer_norm.default(_to_copy_1391, [256], arg788_1, arg789_1, 1e-05);  _to_copy_1391 = arg788_1 = arg789_1 = None
        getitem_2336 = native_layer_norm_default_288[0]
        _to_copy_1392 = torch.ops.aten._to_copy.default(arg790_1, dtype = torch.bfloat16);  arg790_1 = None
        _to_copy_1393 = torch.ops.aten._to_copy.default(getitem_2336, dtype = torch.bfloat16);  getitem_2336 = None
        t_513 = torch.ops.aten.t.default(_to_copy_1392);  _to_copy_1392 = None
        view_2460 = torch.ops.aten.view.default(_to_copy_1393, [262144, 256]);  _to_copy_1393 = None
        mm_478 = torch.ops.aten.mm.default(view_2460, t_513);  view_2460 = t_513 = None
        view_2461 = torch.ops.aten.view.default(mm_478, [1, 512, 512, 16]);  mm_478 = None
        permute_1332 = torch.ops.aten.permute.default(view_2461, [0, 3, 1, 2]);  view_2461 = None
        view_2462 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_157 = torch.ops.aten.bitwise_not.default(view_2462);  view_2462 = None
        masked_fill_157 = torch.ops.aten.masked_fill.Scalar(permute_1332, bitwise_not_157, -10000);  permute_1332 = bitwise_not_157 = None
        _to_copy_1394 = torch.ops.aten._to_copy.default(getitem_2333, dtype = torch.bfloat16);  getitem_2333 = None
        _to_copy_1395 = torch.ops.aten._to_copy.default(arg792_1, dtype = torch.bfloat16);  arg792_1 = None
        unsqueeze_821 = torch.ops.aten.unsqueeze.default(_to_copy_1394, 3);  _to_copy_1394 = None
        unsqueeze_822 = torch.ops.aten.unsqueeze.default(unsqueeze_821, 4);  unsqueeze_821 = None
        unsqueeze_823 = torch.ops.aten.unsqueeze.default(unsqueeze_822, 5);  unsqueeze_822 = None
        permute_1333 = torch.ops.aten.permute.default(unsqueeze_823, [3, 0, 4, 1, 5, 2]);  unsqueeze_823 = None
        unsqueeze_824 = torch.ops.aten.unsqueeze.default(_to_copy_1395, 4);  _to_copy_1395 = None
        unsqueeze_825 = torch.ops.aten.unsqueeze.default(unsqueeze_824, 5);  unsqueeze_824 = None
        permute_1334 = torch.ops.aten.permute.default(unsqueeze_825, [1, 4, 2, 5, 3, 0]);  unsqueeze_825 = None
        permute_1335 = torch.ops.aten.permute.default(permute_1333, [3, 5, 0, 1, 2, 4]);  permute_1333 = None
        view_2463 = torch.ops.aten.view.default(permute_1335, [1, 512, 384]);  permute_1335 = None
        permute_1336 = torch.ops.aten.permute.default(permute_1334, [5, 0, 1, 2, 4, 3]);  permute_1334 = None
        view_2464 = torch.ops.aten.view.default(permute_1336, [1, 384, 1536]);  permute_1336 = None
        bmm_201 = torch.ops.aten.bmm.default(view_2463, view_2464);  view_2463 = view_2464 = None
        view_2465 = torch.ops.aten.view.default(bmm_201, [512, 1, 4, 1, 16, 24]);  bmm_201 = None
        permute_1337 = torch.ops.aten.permute.default(view_2465, [2, 3, 4, 0, 5, 1]);  view_2465 = None
        view_2466 = torch.ops.aten.view.default(permute_1337, [4, 1, 16, 512, 24]);  permute_1337 = None
        unbind_int_114 = torch.ops.aten.unbind.int(view_2466);  view_2466 = None
        getitem_2339 = unbind_int_114[0]
        getitem_2340 = unbind_int_114[1]
        getitem_2341 = unbind_int_114[2]
        getitem_2342 = unbind_int_114[3];  unbind_int_114 = None
        view_2467 = torch.ops.aten.view.default(arg791_1, [1, 16, 1, 24]);  arg791_1 = None
        add_252 = torch.ops.aten.add.Tensor(getitem_2339, view_2467);  getitem_2339 = view_2467 = None
        _to_copy_1396 = torch.ops.aten._to_copy.default(add_252, dtype = torch.bfloat16);  add_252 = None
        expand_151 = torch.ops.aten.expand.default(masked_fill_157, [1, 16, 512, 512]);  masked_fill_157 = None
        _scaled_dot_product_efficient_attention_default_86 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1396, getitem_2340, getitem_2341, expand_151, False);  _to_copy_1396 = getitem_2340 = getitem_2341 = expand_151 = None
        getitem_2343 = _scaled_dot_product_efficient_attention_default_86[0]
        add_253 = torch.ops.aten.add.Tensor(getitem_2342, 1);  getitem_2342 = None
        sigmoid_191 = torch.ops.aten.sigmoid.default(add_253);  add_253 = None
        mul_314 = torch.ops.aten.mul.Tensor(getitem_2343, sigmoid_191);  getitem_2343 = sigmoid_191 = None
        _to_copy_1397 = torch.ops.aten._to_copy.default(arg793_1, dtype = torch.bfloat16);  arg793_1 = None
        unsqueeze_826 = torch.ops.aten.unsqueeze.default(mul_314, 4);  mul_314 = None
        permute_1338 = torch.ops.aten.permute.default(unsqueeze_826, [0, 2, 4, 3, 1]);  unsqueeze_826 = None
        unsqueeze_827 = torch.ops.aten.unsqueeze.default(_to_copy_1397, 3);  _to_copy_1397 = None
        unsqueeze_828 = torch.ops.aten.unsqueeze.default(unsqueeze_827, 4);  unsqueeze_827 = None
        permute_1339 = torch.ops.aten.permute.default(unsqueeze_828, [3, 4, 2, 1, 0]);  unsqueeze_828 = None
        permute_1340 = torch.ops.aten.permute.default(permute_1338, [1, 3, 4, 0, 2]);  permute_1338 = None
        clone_218 = torch.ops.aten.clone.default(permute_1340, memory_format = torch.contiguous_format);  permute_1340 = None
        _unsafe_view_186 = torch.ops.aten._unsafe_view.default(clone_218, [1, 512, 384]);  clone_218 = None
        permute_1341 = torch.ops.aten.permute.default(permute_1339, [3, 4, 0, 2, 1]);  permute_1339 = None
        clone_219 = torch.ops.aten.clone.default(permute_1341, memory_format = torch.contiguous_format);  permute_1341 = None
        _unsafe_view_187 = torch.ops.aten._unsafe_view.default(clone_219, [1, 384, 384]);  clone_219 = None
        bmm_202 = torch.ops.aten.bmm.default(_unsafe_view_186, _unsafe_view_187);  _unsafe_view_186 = _unsafe_view_187 = None
        view_2468 = torch.ops.aten.view.default(bmm_202, [512, 1, 1, 1, 384]);  bmm_202 = None
        permute_1342 = torch.ops.aten.permute.default(view_2468, [3, 0, 4, 1, 2]);  view_2468 = None
        view_2469 = torch.ops.aten.view.default(permute_1342, [1, 512, 384]);  permute_1342 = None
        unsqueeze_829 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_315 = torch.ops.aten.mul.Tensor(view_2469, unsqueeze_829);  view_2469 = unsqueeze_829 = None
        add_254 = torch.ops.aten.add.Tensor(add_247, mul_315);  mul_315 = None
        split_tensor_249 = torch.ops.aten.split.Tensor(add_247, 512, dim = -2);  add_247 = None
        getitem_2347 = split_tensor_249[0];  split_tensor_249 = None
        _to_copy_1398 = torch.ops.aten._to_copy.default(getitem_2347, dtype = torch.float32);  getitem_2347 = None
        native_layer_norm_default_289 = torch.ops.aten.native_layer_norm.default(_to_copy_1398, [384], arg782_1, arg783_1, 1e-05);  _to_copy_1398 = arg782_1 = arg783_1 = None
        getitem_2348 = native_layer_norm_default_289[0]
        _to_copy_1399 = torch.ops.aten._to_copy.default(arg784_1, dtype = torch.bfloat16);  arg784_1 = None
        _to_copy_1400 = torch.ops.aten._to_copy.default(getitem_2348, dtype = torch.bfloat16);  getitem_2348 = None
        t_514 = torch.ops.aten.t.default(_to_copy_1399);  _to_copy_1399 = None
        view_2470 = torch.ops.aten.view.default(_to_copy_1400, [512, 384]);  _to_copy_1400 = None
        mm_479 = torch.ops.aten.mm.default(view_2470, t_514);  view_2470 = t_514 = None
        view_2471 = torch.ops.aten.view.default(mm_479, [1, 512, 1536]);  mm_479 = None
        split_tensor_250 = torch.ops.aten.split.Tensor(view_2471, 768, dim = -1);  view_2471 = None
        getitem_2351 = split_tensor_250[0]
        getitem_2352 = split_tensor_250[1];  split_tensor_250 = None
        silu_67 = torch.ops.aten.silu.default(getitem_2351);  getitem_2351 = None
        mul_316 = torch.ops.aten.mul.Tensor(silu_67, getitem_2352);  silu_67 = getitem_2352 = None
        _to_copy_1401 = torch.ops.aten._to_copy.default(arg785_1, dtype = torch.bfloat16);  arg785_1 = None
        t_515 = torch.ops.aten.t.default(_to_copy_1401);  _to_copy_1401 = None
        view_2473 = torch.ops.aten.view.default(mul_316, [512, 768]);  mul_316 = None
        mm_480 = torch.ops.aten.mm.default(view_2473, t_515);  view_2473 = t_515 = None
        view_2474 = torch.ops.aten.view.default(mm_480, [1, 512, 384]);  mm_480 = None
        add_255 = torch.ops.aten.add.Tensor(add_254, view_2474);  add_254 = view_2474 = None
        _to_copy_1402 = torch.ops.aten._to_copy.default(add_251, dtype = torch.float32)
        native_layer_norm_default_290 = torch.ops.aten.native_layer_norm.default(_to_copy_1402, [256], arg798_1, arg799_1, 1e-05);  _to_copy_1402 = arg798_1 = arg799_1 = None
        getitem_2353 = native_layer_norm_default_290[0]
        split_with_sizes_default_62 = torch.ops.aten.split_with_sizes.default(arg801_1, [512, 512]);  arg801_1 = None
        getitem_2356 = split_with_sizes_default_62[0]
        getitem_2357 = split_with_sizes_default_62[1];  split_with_sizes_default_62 = None
        split_with_sizes_default_63 = torch.ops.aten.split_with_sizes.default(arg802_1, [512, 512, 256]);  arg802_1 = None
        getitem_2358 = split_with_sizes_default_63[0]
        getitem_2359 = split_with_sizes_default_63[1]
        getitem_2360 = split_with_sizes_default_63[2];  split_with_sizes_default_63 = None
        _to_copy_1403 = torch.ops.aten._to_copy.default(getitem_2356, dtype = torch.bfloat16);  getitem_2356 = None
        _to_copy_1404 = torch.ops.aten._to_copy.default(getitem_2353, dtype = torch.bfloat16)
        t_516 = torch.ops.aten.t.default(_to_copy_1403);  _to_copy_1403 = None
        view_2475 = torch.ops.aten.view.default(_to_copy_1404, [262144, 256]);  _to_copy_1404 = None
        mm_481 = torch.ops.aten.mm.default(view_2475, t_516);  view_2475 = t_516 = None
        view_2476 = torch.ops.aten.view.default(mm_481, [1, 512, 512, 512]);  mm_481 = None
        _to_copy_1405 = torch.ops.aten._to_copy.default(getitem_2358, dtype = torch.bfloat16);  getitem_2358 = None
        _to_copy_1406 = torch.ops.aten._to_copy.default(getitem_2353, dtype = torch.bfloat16)
        t_517 = torch.ops.aten.t.default(_to_copy_1405);  _to_copy_1405 = None
        view_2477 = torch.ops.aten.view.default(_to_copy_1406, [262144, 256]);  _to_copy_1406 = None
        mm_482 = torch.ops.aten.mm.default(view_2477, t_517);  view_2477 = t_517 = None
        view_2478 = torch.ops.aten.view.default(mm_482, [1, 512, 512, 512]);  mm_482 = None
        sigmoid_192 = torch.ops.aten.sigmoid.default(view_2478);  view_2478 = None
        mul_317 = torch.ops.aten.mul.Tensor(view_2476, sigmoid_192);  view_2476 = sigmoid_192 = None
        unsqueeze_830 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_158 = torch.ops.aten.bitwise_not.default(unsqueeze_830);  unsqueeze_830 = None
        masked_fill_158 = torch.ops.aten.masked_fill.Scalar(mul_317, bitwise_not_158, 0);  mul_317 = bitwise_not_158 = None
        split_tensor_251 = torch.ops.aten.split.Tensor(masked_fill_158, 256, dim = -1)
        getitem_2363 = split_tensor_251[0]
        unsqueeze_833 = torch.ops.aten.unsqueeze.default(getitem_2363, 4);  getitem_2363 = None
        permute_1347 = torch.ops.aten.permute.default(unsqueeze_833, [0, 1, 4, 3, 2]);  unsqueeze_833 = None
        permute_1348 = torch.ops.aten.permute.default(permute_1347, [3, 1, 4, 0, 2]);  permute_1347 = None
        view_2481 = torch.ops.aten.view.default(permute_1348, [256, 512, 512]);  permute_1348 = None
        split_tensor_252 = torch.ops.aten.split.Tensor(masked_fill_158, 256, dim = -1);  masked_fill_158 = None
        getitem_2366 = split_tensor_252[1];  split_tensor_252 = None
        unsqueeze_834 = torch.ops.aten.unsqueeze.default(getitem_2366, 4);  getitem_2366 = None
        permute_1349 = torch.ops.aten.permute.default(unsqueeze_834, [0, 4, 1, 3, 2]);  unsqueeze_834 = None
        permute_1350 = torch.ops.aten.permute.default(permute_1349, [3, 4, 0, 2, 1]);  permute_1349 = None
        view_2482 = torch.ops.aten.view.default(permute_1350, [256, 512, 512]);  permute_1350 = None
        bmm_203 = torch.ops.aten.bmm.default(view_2481, view_2482);  view_2481 = view_2482 = None
        view_2483 = torch.ops.aten.view.default(bmm_203, [256, 512, 1, 1, 512]);  bmm_203 = None
        permute_1351 = torch.ops.aten.permute.default(view_2483, [3, 1, 4, 0, 2]);  view_2483 = None
        view_2484 = torch.ops.aten.view.default(permute_1351, [1, 512, 512, 256]);  permute_1351 = None
        _to_copy_1407 = torch.ops.aten._to_copy.default(getitem_2357, dtype = torch.bfloat16);  getitem_2357 = None
        _to_copy_1408 = torch.ops.aten._to_copy.default(getitem_2353, dtype = torch.bfloat16)
        t_518 = torch.ops.aten.t.default(_to_copy_1407);  _to_copy_1407 = None
        view_2485 = torch.ops.aten.view.default(_to_copy_1408, [262144, 256]);  _to_copy_1408 = None
        mm_483 = torch.ops.aten.mm.default(view_2485, t_518);  view_2485 = t_518 = None
        view_2486 = torch.ops.aten.view.default(mm_483, [1, 512, 512, 512]);  mm_483 = None
        _to_copy_1409 = torch.ops.aten._to_copy.default(getitem_2359, dtype = torch.bfloat16);  getitem_2359 = None
        _to_copy_1410 = torch.ops.aten._to_copy.default(getitem_2353, dtype = torch.bfloat16)
        t_519 = torch.ops.aten.t.default(_to_copy_1409);  _to_copy_1409 = None
        view_2487 = torch.ops.aten.view.default(_to_copy_1410, [262144, 256]);  _to_copy_1410 = None
        mm_484 = torch.ops.aten.mm.default(view_2487, t_519);  view_2487 = t_519 = None
        view_2488 = torch.ops.aten.view.default(mm_484, [1, 512, 512, 512]);  mm_484 = None
        sigmoid_193 = torch.ops.aten.sigmoid.default(view_2488);  view_2488 = None
        mul_318 = torch.ops.aten.mul.Tensor(view_2486, sigmoid_193);  view_2486 = sigmoid_193 = None
        view_2489 = torch.ops.aten.view.default(mul_318, [262144, 512]);  mul_318 = None
        view_2490 = torch.ops.aten.view.default(view_2489, [1, 512, 512, 512]);  view_2489 = None
        transpose_62 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_835 = torch.ops.aten.unsqueeze.default(transpose_62, 3);  transpose_62 = None
        clone_220 = torch.ops.aten.clone.default(unsqueeze_835, memory_format = torch.contiguous_format);  unsqueeze_835 = None
        bitwise_not_159 = torch.ops.aten.bitwise_not.default(clone_220);  clone_220 = None
        masked_fill_159 = torch.ops.aten.masked_fill.Scalar(view_2490, bitwise_not_159, 0);  view_2490 = bitwise_not_159 = None
        view_2491 = torch.ops.aten.view.default(masked_fill_159, [262144, 512]);  masked_fill_159 = None
        view_2495 = torch.ops.aten.view.default(view_2491, [1, 512, 512, 512])
        split_tensor_253 = torch.ops.aten.split.Tensor(view_2495, 256, dim = -1);  view_2495 = None
        getitem_2369 = split_tensor_253[0]
        unsqueeze_838 = torch.ops.aten.unsqueeze.default(getitem_2369, 4);  getitem_2369 = None
        permute_1356 = torch.ops.aten.permute.default(unsqueeze_838, [0, 2, 4, 3, 1]);  unsqueeze_838 = None
        permute_1357 = torch.ops.aten.permute.default(permute_1356, [3, 1, 4, 0, 2]);  permute_1356 = None
        view_2496 = torch.ops.aten.view.default(permute_1357, [256, 512, 512]);  permute_1357 = None
        view_2497 = torch.ops.aten.view.default(view_2491, [1, 512, 512, 512]);  view_2491 = None
        split_tensor_254 = torch.ops.aten.split.Tensor(view_2497, 256, dim = -1);  view_2497 = None
        getitem_2372 = split_tensor_254[1];  split_tensor_254 = None
        unsqueeze_839 = torch.ops.aten.unsqueeze.default(getitem_2372, 4);  getitem_2372 = None
        permute_1358 = torch.ops.aten.permute.default(unsqueeze_839, [0, 4, 2, 3, 1]);  unsqueeze_839 = None
        permute_1359 = torch.ops.aten.permute.default(permute_1358, [3, 4, 0, 2, 1]);  permute_1358 = None
        view_2498 = torch.ops.aten.view.default(permute_1359, [256, 512, 512]);  permute_1359 = None
        bmm_204 = torch.ops.aten.bmm.default(view_2496, view_2498);  view_2496 = view_2498 = None
        view_2499 = torch.ops.aten.view.default(bmm_204, [256, 512, 1, 1, 512]);  bmm_204 = None
        permute_1360 = torch.ops.aten.permute.default(view_2499, [3, 1, 4, 0, 2]);  view_2499 = None
        view_2500 = torch.ops.aten.view.default(permute_1360, [1, 512, 512, 256]);  permute_1360 = None
        _to_copy_1411 = torch.ops.aten._to_copy.default(view_2484, dtype = torch.float32);  view_2484 = None
        native_layer_norm_default_291 = torch.ops.aten.native_layer_norm.default(_to_copy_1411, [256], None, None, 1e-05);  _to_copy_1411 = None
        getitem_2373 = native_layer_norm_default_291[0]
        _to_copy_1412 = torch.ops.aten._to_copy.default(view_2500, dtype = torch.float32);  view_2500 = None
        native_layer_norm_default_292 = torch.ops.aten.native_layer_norm.default(_to_copy_1412, [256], None, None, 1e-05);  _to_copy_1412 = None
        getitem_2376 = native_layer_norm_default_292[0]
        add_256 = torch.ops.aten.add.Tensor(getitem_2373, getitem_2376);  getitem_2373 = getitem_2376 = None
        _to_copy_1413 = torch.ops.aten._to_copy.default(arg800_1, dtype = torch.bfloat16);  arg800_1 = None
        _to_copy_1414 = torch.ops.aten._to_copy.default(add_256, dtype = torch.bfloat16);  add_256 = None
        t_520 = torch.ops.aten.t.default(_to_copy_1413);  _to_copy_1413 = None
        view_2501 = torch.ops.aten.view.default(_to_copy_1414, [262144, 256]);  _to_copy_1414 = None
        mm_485 = torch.ops.aten.mm.default(view_2501, t_520);  view_2501 = t_520 = None
        view_2502 = torch.ops.aten.view.default(mm_485, [1, 512, 512, 256]);  mm_485 = None
        _to_copy_1415 = torch.ops.aten._to_copy.default(getitem_2360, dtype = torch.bfloat16);  getitem_2360 = None
        _to_copy_1416 = torch.ops.aten._to_copy.default(getitem_2353, dtype = torch.bfloat16);  getitem_2353 = None
        t_521 = torch.ops.aten.t.default(_to_copy_1415);  _to_copy_1415 = None
        view_2503 = torch.ops.aten.view.default(_to_copy_1416, [262144, 256]);  _to_copy_1416 = None
        mm_486 = torch.ops.aten.mm.default(view_2503, t_521);  view_2503 = t_521 = None
        view_2504 = torch.ops.aten.view.default(mm_486, [1, 512, 512, 256]);  mm_486 = None
        sigmoid_194 = torch.ops.aten.sigmoid.default(view_2504);  view_2504 = None
        mul_319 = torch.ops.aten.mul.Tensor(view_2502, sigmoid_194);  view_2502 = sigmoid_194 = None
        add_257 = torch.ops.aten.add.Tensor(add_251, mul_319);  mul_319 = None
        _to_copy_1417 = torch.ops.aten._to_copy.default(add_251, dtype = torch.float32)
        native_layer_norm_default_293 = torch.ops.aten.native_layer_norm.default(_to_copy_1417, [256], None, None, 1e-05);  _to_copy_1417 = None
        getitem_2379 = native_layer_norm_default_293[0]
        _to_copy_1418 = torch.ops.aten._to_copy.default(arg804_1, dtype = torch.bfloat16);  arg804_1 = None
        _to_copy_1419 = torch.ops.aten._to_copy.default(getitem_2379, dtype = torch.bfloat16)
        t_522 = torch.ops.aten.t.default(_to_copy_1418);  _to_copy_1418 = None
        view_2505 = torch.ops.aten.view.default(_to_copy_1419, [262144, 256]);  _to_copy_1419 = None
        mm_487 = torch.ops.aten.mm.default(view_2505, t_522);  view_2505 = t_522 = None
        view_2506 = torch.ops.aten.view.default(mm_487, [1, 512, 512, 8]);  mm_487 = None
        view_2507 = torch.ops.aten.view.default(view_2506, [1, 512, 512, 2, 4]);  view_2506 = None
        permute_1361 = torch.ops.aten.permute.default(view_2507, [0, 3, 4, 1, 2]);  view_2507 = None
        view_2508 = torch.ops.aten.view.default(permute_1361, [1, 2, 4, 1, 512, 512]);  permute_1361 = None
        view_2509 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_160 = torch.ops.aten.bitwise_not.default(view_2509);  view_2509 = None
        masked_fill_160 = torch.ops.aten.masked_fill.Scalar(view_2508, bitwise_not_160, -10000);  view_2508 = bitwise_not_160 = None
        view_2510 = torch.ops.aten.view.default(masked_fill_160, [1, 2, 4, 512, 512]);  masked_fill_160 = None
        permute_1362 = torch.ops.aten.permute.default(view_2510, [1, 0, 2, 3, 4]);  view_2510 = None
        view_2511 = torch.ops.aten.view.default(permute_1362, [2, 4, 1, 512, 512]);  permute_1362 = None
        _to_copy_1420 = torch.ops.aten._to_copy.default(arg805_1, dtype = torch.bfloat16);  arg805_1 = None
        _to_copy_1421 = torch.ops.aten._to_copy.default(getitem_2379, dtype = torch.bfloat16)
        t_523 = torch.ops.aten.t.default(_to_copy_1420);  _to_copy_1420 = None
        view_2512 = torch.ops.aten.view.default(_to_copy_1421, [262144, 256]);  _to_copy_1421 = None
        mm_488 = torch.ops.aten.mm.default(view_2512, t_523);  view_2512 = t_523 = None
        view_2513 = torch.ops.aten.view.default(mm_488, [1, 512, 512, 1024]);  mm_488 = None
        select_63 = torch.ops.aten.select.int(view_2511, 0, 0)
        view_2514 = torch.ops.aten.view.default(view_2513, [1, 512, 512, 4, 4, 64]);  view_2513 = None
        permute_1363 = torch.ops.aten.permute.default(view_2514, [4, 0, 3, 1, 2, 5]);  view_2514 = None
        view_2515 = torch.ops.aten.view.default(permute_1363, [4, 4, 512, 512, 64]);  permute_1363 = None
        unbind_int_115 = torch.ops.aten.unbind.int(view_2515);  view_2515 = None
        getitem_2382 = unbind_int_115[0]
        getitem_2383 = unbind_int_115[1]
        getitem_2384 = unbind_int_115[2]
        getitem_2385 = unbind_int_115[3];  unbind_int_115 = None
        expand_152 = torch.ops.aten.expand.default(select_63, [4, 512, 512, 512]);  select_63 = None
        _scaled_dot_product_efficient_attention_default_87 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2382, getitem_2383, getitem_2384, expand_152, False);  getitem_2382 = getitem_2383 = getitem_2384 = expand_152 = None
        getitem_2386 = _scaled_dot_product_efficient_attention_default_87[0]
        sigmoid_195 = torch.ops.aten.sigmoid.default(getitem_2385);  getitem_2385 = None
        mul_320 = torch.ops.aten.mul.Tensor(getitem_2386, sigmoid_195);  getitem_2386 = sigmoid_195 = None
        view_2516 = torch.ops.aten.view.default(mul_320, [1, 4, 512, 512, 64]);  mul_320 = None
        permute_1364 = torch.ops.aten.permute.default(view_2516, [0, 2, 3, 1, 4]);  view_2516 = None
        clone_221 = torch.ops.aten.clone.default(permute_1364, memory_format = torch.contiguous_format);  permute_1364 = None
        _unsafe_view_188 = torch.ops.aten._unsafe_view.default(clone_221, [1, 512, 512, 256]);  clone_221 = None
        transpose_63 = torch.ops.aten.transpose.int(getitem_2379, 1, 2);  getitem_2379 = None
        _to_copy_1422 = torch.ops.aten._to_copy.default(arg806_1, dtype = torch.bfloat16);  arg806_1 = None
        _to_copy_1423 = torch.ops.aten._to_copy.default(transpose_63, dtype = torch.bfloat16);  transpose_63 = None
        t_524 = torch.ops.aten.t.default(_to_copy_1422);  _to_copy_1422 = None
        expand_153 = torch.ops.aten.expand.default(_to_copy_1423, [1, 512, 512, 256]);  _to_copy_1423 = None
        view_2517 = torch.ops.aten.view.default(expand_153, [512, 512, 256]);  expand_153 = None
        expand_154 = torch.ops.aten.expand.default(t_524, [1, 512, 256, 1024]);  t_524 = None
        view_2518 = torch.ops.aten.view.default(expand_154, [512, 256, 1024]);  expand_154 = None
        bmm_205 = torch.ops.aten.bmm.default(view_2517, view_2518);  view_2517 = view_2518 = None
        view_2519 = torch.ops.aten.view.default(bmm_205, [1, 512, 512, 1024]);  bmm_205 = None
        select_64 = torch.ops.aten.select.int(view_2511, 0, 1);  view_2511 = None
        view_2520 = torch.ops.aten.view.default(view_2519, [1, 512, 512, 4, 4, 64]);  view_2519 = None
        permute_1365 = torch.ops.aten.permute.default(view_2520, [4, 0, 3, 1, 2, 5]);  view_2520 = None
        view_2521 = torch.ops.aten.view.default(permute_1365, [4, 4, 512, 512, 64]);  permute_1365 = None
        unbind_int_116 = torch.ops.aten.unbind.int(view_2521);  view_2521 = None
        getitem_2390 = unbind_int_116[0]
        getitem_2391 = unbind_int_116[1]
        getitem_2392 = unbind_int_116[2]
        getitem_2393 = unbind_int_116[3];  unbind_int_116 = None
        expand_155 = torch.ops.aten.expand.default(select_64, [4, 512, 512, 512]);  select_64 = None
        _scaled_dot_product_efficient_attention_default_88 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2390, getitem_2391, getitem_2392, expand_155, False);  getitem_2390 = getitem_2391 = getitem_2392 = expand_155 = None
        getitem_2394 = _scaled_dot_product_efficient_attention_default_88[0]
        sigmoid_196 = torch.ops.aten.sigmoid.default(getitem_2393);  getitem_2393 = None
        mul_321 = torch.ops.aten.mul.Tensor(getitem_2394, sigmoid_196);  getitem_2394 = sigmoid_196 = None
        view_2522 = torch.ops.aten.view.default(mul_321, [1, 4, 512, 512, 64]);  mul_321 = None
        permute_1366 = torch.ops.aten.permute.default(view_2522, [0, 2, 3, 1, 4]);  view_2522 = None
        clone_222 = torch.ops.aten.clone.default(permute_1366, memory_format = torch.contiguous_format);  permute_1366 = None
        _unsafe_view_189 = torch.ops.aten._unsafe_view.default(clone_222, [1, 512, 512, 256]);  clone_222 = None
        cat_37 = torch.ops.aten.cat.default([_unsafe_view_188, _unsafe_view_189], dim = -1);  _unsafe_view_188 = _unsafe_view_189 = None
        slice_230 = torch.ops.aten.slice.Tensor(arg803_1, dim = 0, start = 0, end = 9223372036854775807);  arg803_1 = None
        unsqueeze_840 = torch.ops.aten.unsqueeze.default(slice_230, 1);  slice_230 = None
        mul_322 = torch.ops.aten.mul.Tensor(arg807_1, unsqueeze_840);  arg807_1 = unsqueeze_840 = None
        _to_copy_1424 = torch.ops.aten._to_copy.default(mul_322, dtype = torch.bfloat16);  mul_322 = None
        t_525 = torch.ops.aten.t.default(_to_copy_1424);  _to_copy_1424 = None
        view_2523 = torch.ops.aten.view.default(cat_37, [262144, 512]);  cat_37 = None
        mm_489 = torch.ops.aten.mm.default(view_2523, t_525);  view_2523 = t_525 = None
        view_2524 = torch.ops.aten.view.default(mm_489, [1, 512, 512, 256]);  mm_489 = None
        add_258 = torch.ops.aten.add.Tensor(add_257, view_2524);  add_257 = view_2524 = None
        split_tensor_255 = torch.ops.aten.split.Tensor(add_251, 512, dim = -2)
        getitem_2398 = split_tensor_255[0];  split_tensor_255 = None
        _to_copy_1425 = torch.ops.aten._to_copy.default(getitem_2398, dtype = torch.float32);  getitem_2398 = None
        native_layer_norm_default_294 = torch.ops.aten.native_layer_norm.default(_to_copy_1425, [256], arg794_1, arg795_1, 1e-05);  _to_copy_1425 = arg794_1 = arg795_1 = None
        getitem_2399 = native_layer_norm_default_294[0]
        _to_copy_1426 = torch.ops.aten._to_copy.default(arg796_1, dtype = torch.bfloat16);  arg796_1 = None
        _to_copy_1427 = torch.ops.aten._to_copy.default(getitem_2399, dtype = torch.bfloat16);  getitem_2399 = None
        t_526 = torch.ops.aten.t.default(_to_copy_1426);  _to_copy_1426 = None
        view_2525 = torch.ops.aten.view.default(_to_copy_1427, [262144, 256]);  _to_copy_1427 = None
        mm_490 = torch.ops.aten.mm.default(view_2525, t_526);  view_2525 = t_526 = None
        view_2526 = torch.ops.aten.view.default(mm_490, [1, 512, 512, 1024]);  mm_490 = None
        split_tensor_256 = torch.ops.aten.split.Tensor(view_2526, 512, dim = -1);  view_2526 = None
        getitem_2402 = split_tensor_256[0]
        getitem_2403 = split_tensor_256[1];  split_tensor_256 = None
        silu_68 = torch.ops.aten.silu.default(getitem_2402);  getitem_2402 = None
        mul_323 = torch.ops.aten.mul.Tensor(silu_68, getitem_2403);  silu_68 = getitem_2403 = None
        _to_copy_1428 = torch.ops.aten._to_copy.default(arg797_1, dtype = torch.bfloat16);  arg797_1 = None
        t_527 = torch.ops.aten.t.default(_to_copy_1428);  _to_copy_1428 = None
        view_2528 = torch.ops.aten.view.default(mul_323, [262144, 512]);  mul_323 = None
        mm_491 = torch.ops.aten.mm.default(view_2528, t_527);  view_2528 = t_527 = None
        view_2529 = torch.ops.aten.view.default(mm_491, [1, 512, 512, 256]);  mm_491 = None
        add_259 = torch.ops.aten.add.Tensor(add_258, view_2529);  add_258 = view_2529 = None
        _to_copy_1429 = torch.ops.aten._to_copy.default(add_255, dtype = torch.float32)
        native_layer_norm_default_295 = torch.ops.aten.native_layer_norm.default(_to_copy_1429, [384], arg812_1, arg813_1, 1e-05);  _to_copy_1429 = arg812_1 = arg813_1 = None
        getitem_2404 = native_layer_norm_default_295[0]
        _to_copy_1430 = torch.ops.aten._to_copy.default(add_251, dtype = torch.float32);  add_251 = None
        native_layer_norm_default_296 = torch.ops.aten.native_layer_norm.default(_to_copy_1430, [256], arg814_1, arg815_1, 1e-05);  _to_copy_1430 = arg814_1 = arg815_1 = None
        getitem_2407 = native_layer_norm_default_296[0]
        _to_copy_1431 = torch.ops.aten._to_copy.default(arg816_1, dtype = torch.bfloat16);  arg816_1 = None
        _to_copy_1432 = torch.ops.aten._to_copy.default(getitem_2407, dtype = torch.bfloat16);  getitem_2407 = None
        t_528 = torch.ops.aten.t.default(_to_copy_1431);  _to_copy_1431 = None
        view_2530 = torch.ops.aten.view.default(_to_copy_1432, [262144, 256]);  _to_copy_1432 = None
        mm_492 = torch.ops.aten.mm.default(view_2530, t_528);  view_2530 = t_528 = None
        view_2531 = torch.ops.aten.view.default(mm_492, [1, 512, 512, 16]);  mm_492 = None
        permute_1367 = torch.ops.aten.permute.default(view_2531, [0, 3, 1, 2]);  view_2531 = None
        view_2532 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_161 = torch.ops.aten.bitwise_not.default(view_2532);  view_2532 = None
        masked_fill_161 = torch.ops.aten.masked_fill.Scalar(permute_1367, bitwise_not_161, -10000);  permute_1367 = bitwise_not_161 = None
        _to_copy_1433 = torch.ops.aten._to_copy.default(getitem_2404, dtype = torch.bfloat16);  getitem_2404 = None
        _to_copy_1434 = torch.ops.aten._to_copy.default(arg818_1, dtype = torch.bfloat16);  arg818_1 = None
        unsqueeze_841 = torch.ops.aten.unsqueeze.default(_to_copy_1433, 3);  _to_copy_1433 = None
        unsqueeze_842 = torch.ops.aten.unsqueeze.default(unsqueeze_841, 4);  unsqueeze_841 = None
        unsqueeze_843 = torch.ops.aten.unsqueeze.default(unsqueeze_842, 5);  unsqueeze_842 = None
        permute_1368 = torch.ops.aten.permute.default(unsqueeze_843, [3, 0, 4, 1, 5, 2]);  unsqueeze_843 = None
        unsqueeze_844 = torch.ops.aten.unsqueeze.default(_to_copy_1434, 4);  _to_copy_1434 = None
        unsqueeze_845 = torch.ops.aten.unsqueeze.default(unsqueeze_844, 5);  unsqueeze_844 = None
        permute_1369 = torch.ops.aten.permute.default(unsqueeze_845, [1, 4, 2, 5, 3, 0]);  unsqueeze_845 = None
        permute_1370 = torch.ops.aten.permute.default(permute_1368, [3, 5, 0, 1, 2, 4]);  permute_1368 = None
        view_2533 = torch.ops.aten.view.default(permute_1370, [1, 512, 384]);  permute_1370 = None
        permute_1371 = torch.ops.aten.permute.default(permute_1369, [5, 0, 1, 2, 4, 3]);  permute_1369 = None
        view_2534 = torch.ops.aten.view.default(permute_1371, [1, 384, 1536]);  permute_1371 = None
        bmm_206 = torch.ops.aten.bmm.default(view_2533, view_2534);  view_2533 = view_2534 = None
        view_2535 = torch.ops.aten.view.default(bmm_206, [512, 1, 4, 1, 16, 24]);  bmm_206 = None
        permute_1372 = torch.ops.aten.permute.default(view_2535, [2, 3, 4, 0, 5, 1]);  view_2535 = None
        view_2536 = torch.ops.aten.view.default(permute_1372, [4, 1, 16, 512, 24]);  permute_1372 = None
        unbind_int_117 = torch.ops.aten.unbind.int(view_2536);  view_2536 = None
        getitem_2410 = unbind_int_117[0]
        getitem_2411 = unbind_int_117[1]
        getitem_2412 = unbind_int_117[2]
        getitem_2413 = unbind_int_117[3];  unbind_int_117 = None
        view_2537 = torch.ops.aten.view.default(arg817_1, [1, 16, 1, 24]);  arg817_1 = None
        add_260 = torch.ops.aten.add.Tensor(getitem_2410, view_2537);  getitem_2410 = view_2537 = None
        _to_copy_1435 = torch.ops.aten._to_copy.default(add_260, dtype = torch.bfloat16);  add_260 = None
        expand_156 = torch.ops.aten.expand.default(masked_fill_161, [1, 16, 512, 512]);  masked_fill_161 = None
        _scaled_dot_product_efficient_attention_default_89 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1435, getitem_2411, getitem_2412, expand_156, False);  _to_copy_1435 = getitem_2411 = getitem_2412 = expand_156 = None
        getitem_2414 = _scaled_dot_product_efficient_attention_default_89[0]
        add_261 = torch.ops.aten.add.Tensor(getitem_2413, 1);  getitem_2413 = None
        sigmoid_197 = torch.ops.aten.sigmoid.default(add_261);  add_261 = None
        mul_324 = torch.ops.aten.mul.Tensor(getitem_2414, sigmoid_197);  getitem_2414 = sigmoid_197 = None
        _to_copy_1436 = torch.ops.aten._to_copy.default(arg819_1, dtype = torch.bfloat16);  arg819_1 = None
        unsqueeze_846 = torch.ops.aten.unsqueeze.default(mul_324, 4);  mul_324 = None
        permute_1373 = torch.ops.aten.permute.default(unsqueeze_846, [0, 2, 4, 3, 1]);  unsqueeze_846 = None
        unsqueeze_847 = torch.ops.aten.unsqueeze.default(_to_copy_1436, 3);  _to_copy_1436 = None
        unsqueeze_848 = torch.ops.aten.unsqueeze.default(unsqueeze_847, 4);  unsqueeze_847 = None
        permute_1374 = torch.ops.aten.permute.default(unsqueeze_848, [3, 4, 2, 1, 0]);  unsqueeze_848 = None
        permute_1375 = torch.ops.aten.permute.default(permute_1373, [1, 3, 4, 0, 2]);  permute_1373 = None
        clone_223 = torch.ops.aten.clone.default(permute_1375, memory_format = torch.contiguous_format);  permute_1375 = None
        _unsafe_view_190 = torch.ops.aten._unsafe_view.default(clone_223, [1, 512, 384]);  clone_223 = None
        permute_1376 = torch.ops.aten.permute.default(permute_1374, [3, 4, 0, 2, 1]);  permute_1374 = None
        clone_224 = torch.ops.aten.clone.default(permute_1376, memory_format = torch.contiguous_format);  permute_1376 = None
        _unsafe_view_191 = torch.ops.aten._unsafe_view.default(clone_224, [1, 384, 384]);  clone_224 = None
        bmm_207 = torch.ops.aten.bmm.default(_unsafe_view_190, _unsafe_view_191);  _unsafe_view_190 = _unsafe_view_191 = None
        view_2538 = torch.ops.aten.view.default(bmm_207, [512, 1, 1, 1, 384]);  bmm_207 = None
        permute_1377 = torch.ops.aten.permute.default(view_2538, [3, 0, 4, 1, 2]);  view_2538 = None
        view_2539 = torch.ops.aten.view.default(permute_1377, [1, 512, 384]);  permute_1377 = None
        unsqueeze_849 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_325 = torch.ops.aten.mul.Tensor(view_2539, unsqueeze_849);  view_2539 = unsqueeze_849 = None
        add_262 = torch.ops.aten.add.Tensor(add_255, mul_325);  mul_325 = None
        split_tensor_257 = torch.ops.aten.split.Tensor(add_255, 512, dim = -2);  add_255 = None
        getitem_2418 = split_tensor_257[0];  split_tensor_257 = None
        _to_copy_1437 = torch.ops.aten._to_copy.default(getitem_2418, dtype = torch.float32);  getitem_2418 = None
        native_layer_norm_default_297 = torch.ops.aten.native_layer_norm.default(_to_copy_1437, [384], arg808_1, arg809_1, 1e-05);  _to_copy_1437 = arg808_1 = arg809_1 = None
        getitem_2419 = native_layer_norm_default_297[0]
        _to_copy_1438 = torch.ops.aten._to_copy.default(arg810_1, dtype = torch.bfloat16);  arg810_1 = None
        _to_copy_1439 = torch.ops.aten._to_copy.default(getitem_2419, dtype = torch.bfloat16);  getitem_2419 = None
        t_529 = torch.ops.aten.t.default(_to_copy_1438);  _to_copy_1438 = None
        view_2540 = torch.ops.aten.view.default(_to_copy_1439, [512, 384]);  _to_copy_1439 = None
        mm_493 = torch.ops.aten.mm.default(view_2540, t_529);  view_2540 = t_529 = None
        view_2541 = torch.ops.aten.view.default(mm_493, [1, 512, 1536]);  mm_493 = None
        split_tensor_258 = torch.ops.aten.split.Tensor(view_2541, 768, dim = -1);  view_2541 = None
        getitem_2422 = split_tensor_258[0]
        getitem_2423 = split_tensor_258[1];  split_tensor_258 = None
        silu_69 = torch.ops.aten.silu.default(getitem_2422);  getitem_2422 = None
        mul_326 = torch.ops.aten.mul.Tensor(silu_69, getitem_2423);  silu_69 = getitem_2423 = None
        _to_copy_1440 = torch.ops.aten._to_copy.default(arg811_1, dtype = torch.bfloat16);  arg811_1 = None
        t_530 = torch.ops.aten.t.default(_to_copy_1440);  _to_copy_1440 = None
        view_2543 = torch.ops.aten.view.default(mul_326, [512, 768]);  mul_326 = None
        mm_494 = torch.ops.aten.mm.default(view_2543, t_530);  view_2543 = t_530 = None
        view_2544 = torch.ops.aten.view.default(mm_494, [1, 512, 384]);  mm_494 = None
        add_263 = torch.ops.aten.add.Tensor(add_262, view_2544);  add_262 = view_2544 = None
        _to_copy_1441 = torch.ops.aten._to_copy.default(add_259, dtype = torch.float32)
        native_layer_norm_default_298 = torch.ops.aten.native_layer_norm.default(_to_copy_1441, [256], arg824_1, arg825_1, 1e-05);  _to_copy_1441 = arg824_1 = arg825_1 = None
        getitem_2424 = native_layer_norm_default_298[0]
        split_with_sizes_default_64 = torch.ops.aten.split_with_sizes.default(arg827_1, [512, 512]);  arg827_1 = None
        getitem_2427 = split_with_sizes_default_64[0]
        getitem_2428 = split_with_sizes_default_64[1];  split_with_sizes_default_64 = None
        split_with_sizes_default_65 = torch.ops.aten.split_with_sizes.default(arg828_1, [512, 512, 256]);  arg828_1 = None
        getitem_2429 = split_with_sizes_default_65[0]
        getitem_2430 = split_with_sizes_default_65[1]
        getitem_2431 = split_with_sizes_default_65[2];  split_with_sizes_default_65 = None
        _to_copy_1442 = torch.ops.aten._to_copy.default(getitem_2427, dtype = torch.bfloat16);  getitem_2427 = None
        _to_copy_1443 = torch.ops.aten._to_copy.default(getitem_2424, dtype = torch.bfloat16)
        t_531 = torch.ops.aten.t.default(_to_copy_1442);  _to_copy_1442 = None
        view_2545 = torch.ops.aten.view.default(_to_copy_1443, [262144, 256]);  _to_copy_1443 = None
        mm_495 = torch.ops.aten.mm.default(view_2545, t_531);  view_2545 = t_531 = None
        view_2546 = torch.ops.aten.view.default(mm_495, [1, 512, 512, 512]);  mm_495 = None
        _to_copy_1444 = torch.ops.aten._to_copy.default(getitem_2429, dtype = torch.bfloat16);  getitem_2429 = None
        _to_copy_1445 = torch.ops.aten._to_copy.default(getitem_2424, dtype = torch.bfloat16)
        t_532 = torch.ops.aten.t.default(_to_copy_1444);  _to_copy_1444 = None
        view_2547 = torch.ops.aten.view.default(_to_copy_1445, [262144, 256]);  _to_copy_1445 = None
        mm_496 = torch.ops.aten.mm.default(view_2547, t_532);  view_2547 = t_532 = None
        view_2548 = torch.ops.aten.view.default(mm_496, [1, 512, 512, 512]);  mm_496 = None
        sigmoid_198 = torch.ops.aten.sigmoid.default(view_2548);  view_2548 = None
        mul_327 = torch.ops.aten.mul.Tensor(view_2546, sigmoid_198);  view_2546 = sigmoid_198 = None
        unsqueeze_850 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_162 = torch.ops.aten.bitwise_not.default(unsqueeze_850);  unsqueeze_850 = None
        masked_fill_162 = torch.ops.aten.masked_fill.Scalar(mul_327, bitwise_not_162, 0);  mul_327 = bitwise_not_162 = None
        split_tensor_259 = torch.ops.aten.split.Tensor(masked_fill_162, 256, dim = -1)
        getitem_2434 = split_tensor_259[0]
        unsqueeze_853 = torch.ops.aten.unsqueeze.default(getitem_2434, 4);  getitem_2434 = None
        permute_1382 = torch.ops.aten.permute.default(unsqueeze_853, [0, 1, 4, 3, 2]);  unsqueeze_853 = None
        permute_1383 = torch.ops.aten.permute.default(permute_1382, [3, 1, 4, 0, 2]);  permute_1382 = None
        view_2551 = torch.ops.aten.view.default(permute_1383, [256, 512, 512]);  permute_1383 = None
        split_tensor_260 = torch.ops.aten.split.Tensor(masked_fill_162, 256, dim = -1);  masked_fill_162 = None
        getitem_2437 = split_tensor_260[1];  split_tensor_260 = None
        unsqueeze_854 = torch.ops.aten.unsqueeze.default(getitem_2437, 4);  getitem_2437 = None
        permute_1384 = torch.ops.aten.permute.default(unsqueeze_854, [0, 4, 1, 3, 2]);  unsqueeze_854 = None
        permute_1385 = torch.ops.aten.permute.default(permute_1384, [3, 4, 0, 2, 1]);  permute_1384 = None
        view_2552 = torch.ops.aten.view.default(permute_1385, [256, 512, 512]);  permute_1385 = None
        bmm_208 = torch.ops.aten.bmm.default(view_2551, view_2552);  view_2551 = view_2552 = None
        view_2553 = torch.ops.aten.view.default(bmm_208, [256, 512, 1, 1, 512]);  bmm_208 = None
        permute_1386 = torch.ops.aten.permute.default(view_2553, [3, 1, 4, 0, 2]);  view_2553 = None
        view_2554 = torch.ops.aten.view.default(permute_1386, [1, 512, 512, 256]);  permute_1386 = None
        _to_copy_1446 = torch.ops.aten._to_copy.default(getitem_2428, dtype = torch.bfloat16);  getitem_2428 = None
        _to_copy_1447 = torch.ops.aten._to_copy.default(getitem_2424, dtype = torch.bfloat16)
        t_533 = torch.ops.aten.t.default(_to_copy_1446);  _to_copy_1446 = None
        view_2555 = torch.ops.aten.view.default(_to_copy_1447, [262144, 256]);  _to_copy_1447 = None
        mm_497 = torch.ops.aten.mm.default(view_2555, t_533);  view_2555 = t_533 = None
        view_2556 = torch.ops.aten.view.default(mm_497, [1, 512, 512, 512]);  mm_497 = None
        _to_copy_1448 = torch.ops.aten._to_copy.default(getitem_2430, dtype = torch.bfloat16);  getitem_2430 = None
        _to_copy_1449 = torch.ops.aten._to_copy.default(getitem_2424, dtype = torch.bfloat16)
        t_534 = torch.ops.aten.t.default(_to_copy_1448);  _to_copy_1448 = None
        view_2557 = torch.ops.aten.view.default(_to_copy_1449, [262144, 256]);  _to_copy_1449 = None
        mm_498 = torch.ops.aten.mm.default(view_2557, t_534);  view_2557 = t_534 = None
        view_2558 = torch.ops.aten.view.default(mm_498, [1, 512, 512, 512]);  mm_498 = None
        sigmoid_199 = torch.ops.aten.sigmoid.default(view_2558);  view_2558 = None
        mul_328 = torch.ops.aten.mul.Tensor(view_2556, sigmoid_199);  view_2556 = sigmoid_199 = None
        view_2559 = torch.ops.aten.view.default(mul_328, [262144, 512]);  mul_328 = None
        view_2560 = torch.ops.aten.view.default(view_2559, [1, 512, 512, 512]);  view_2559 = None
        transpose_64 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_855 = torch.ops.aten.unsqueeze.default(transpose_64, 3);  transpose_64 = None
        clone_225 = torch.ops.aten.clone.default(unsqueeze_855, memory_format = torch.contiguous_format);  unsqueeze_855 = None
        bitwise_not_163 = torch.ops.aten.bitwise_not.default(clone_225);  clone_225 = None
        masked_fill_163 = torch.ops.aten.masked_fill.Scalar(view_2560, bitwise_not_163, 0);  view_2560 = bitwise_not_163 = None
        view_2561 = torch.ops.aten.view.default(masked_fill_163, [262144, 512]);  masked_fill_163 = None
        view_2565 = torch.ops.aten.view.default(view_2561, [1, 512, 512, 512])
        split_tensor_261 = torch.ops.aten.split.Tensor(view_2565, 256, dim = -1);  view_2565 = None
        getitem_2440 = split_tensor_261[0]
        unsqueeze_858 = torch.ops.aten.unsqueeze.default(getitem_2440, 4);  getitem_2440 = None
        permute_1391 = torch.ops.aten.permute.default(unsqueeze_858, [0, 2, 4, 3, 1]);  unsqueeze_858 = None
        permute_1392 = torch.ops.aten.permute.default(permute_1391, [3, 1, 4, 0, 2]);  permute_1391 = None
        view_2566 = torch.ops.aten.view.default(permute_1392, [256, 512, 512]);  permute_1392 = None
        view_2567 = torch.ops.aten.view.default(view_2561, [1, 512, 512, 512]);  view_2561 = None
        split_tensor_262 = torch.ops.aten.split.Tensor(view_2567, 256, dim = -1);  view_2567 = None
        getitem_2443 = split_tensor_262[1];  split_tensor_262 = None
        unsqueeze_859 = torch.ops.aten.unsqueeze.default(getitem_2443, 4);  getitem_2443 = None
        permute_1393 = torch.ops.aten.permute.default(unsqueeze_859, [0, 4, 2, 3, 1]);  unsqueeze_859 = None
        permute_1394 = torch.ops.aten.permute.default(permute_1393, [3, 4, 0, 2, 1]);  permute_1393 = None
        view_2568 = torch.ops.aten.view.default(permute_1394, [256, 512, 512]);  permute_1394 = None
        bmm_209 = torch.ops.aten.bmm.default(view_2566, view_2568);  view_2566 = view_2568 = None
        view_2569 = torch.ops.aten.view.default(bmm_209, [256, 512, 1, 1, 512]);  bmm_209 = None
        permute_1395 = torch.ops.aten.permute.default(view_2569, [3, 1, 4, 0, 2]);  view_2569 = None
        view_2570 = torch.ops.aten.view.default(permute_1395, [1, 512, 512, 256]);  permute_1395 = None
        _to_copy_1450 = torch.ops.aten._to_copy.default(view_2554, dtype = torch.float32);  view_2554 = None
        native_layer_norm_default_299 = torch.ops.aten.native_layer_norm.default(_to_copy_1450, [256], None, None, 1e-05);  _to_copy_1450 = None
        getitem_2444 = native_layer_norm_default_299[0]
        _to_copy_1451 = torch.ops.aten._to_copy.default(view_2570, dtype = torch.float32);  view_2570 = None
        native_layer_norm_default_300 = torch.ops.aten.native_layer_norm.default(_to_copy_1451, [256], None, None, 1e-05);  _to_copy_1451 = None
        getitem_2447 = native_layer_norm_default_300[0]
        add_264 = torch.ops.aten.add.Tensor(getitem_2444, getitem_2447);  getitem_2444 = getitem_2447 = None
        _to_copy_1452 = torch.ops.aten._to_copy.default(arg826_1, dtype = torch.bfloat16);  arg826_1 = None
        _to_copy_1453 = torch.ops.aten._to_copy.default(add_264, dtype = torch.bfloat16);  add_264 = None
        t_535 = torch.ops.aten.t.default(_to_copy_1452);  _to_copy_1452 = None
        view_2571 = torch.ops.aten.view.default(_to_copy_1453, [262144, 256]);  _to_copy_1453 = None
        mm_499 = torch.ops.aten.mm.default(view_2571, t_535);  view_2571 = t_535 = None
        view_2572 = torch.ops.aten.view.default(mm_499, [1, 512, 512, 256]);  mm_499 = None
        _to_copy_1454 = torch.ops.aten._to_copy.default(getitem_2431, dtype = torch.bfloat16);  getitem_2431 = None
        _to_copy_1455 = torch.ops.aten._to_copy.default(getitem_2424, dtype = torch.bfloat16);  getitem_2424 = None
        t_536 = torch.ops.aten.t.default(_to_copy_1454);  _to_copy_1454 = None
        view_2573 = torch.ops.aten.view.default(_to_copy_1455, [262144, 256]);  _to_copy_1455 = None
        mm_500 = torch.ops.aten.mm.default(view_2573, t_536);  view_2573 = t_536 = None
        view_2574 = torch.ops.aten.view.default(mm_500, [1, 512, 512, 256]);  mm_500 = None
        sigmoid_200 = torch.ops.aten.sigmoid.default(view_2574);  view_2574 = None
        mul_329 = torch.ops.aten.mul.Tensor(view_2572, sigmoid_200);  view_2572 = sigmoid_200 = None
        add_265 = torch.ops.aten.add.Tensor(add_259, mul_329);  mul_329 = None
        _to_copy_1456 = torch.ops.aten._to_copy.default(add_259, dtype = torch.float32)
        native_layer_norm_default_301 = torch.ops.aten.native_layer_norm.default(_to_copy_1456, [256], None, None, 1e-05);  _to_copy_1456 = None
        getitem_2450 = native_layer_norm_default_301[0]
        _to_copy_1457 = torch.ops.aten._to_copy.default(arg830_1, dtype = torch.bfloat16);  arg830_1 = None
        _to_copy_1458 = torch.ops.aten._to_copy.default(getitem_2450, dtype = torch.bfloat16)
        t_537 = torch.ops.aten.t.default(_to_copy_1457);  _to_copy_1457 = None
        view_2575 = torch.ops.aten.view.default(_to_copy_1458, [262144, 256]);  _to_copy_1458 = None
        mm_501 = torch.ops.aten.mm.default(view_2575, t_537);  view_2575 = t_537 = None
        view_2576 = torch.ops.aten.view.default(mm_501, [1, 512, 512, 8]);  mm_501 = None
        view_2577 = torch.ops.aten.view.default(view_2576, [1, 512, 512, 2, 4]);  view_2576 = None
        permute_1396 = torch.ops.aten.permute.default(view_2577, [0, 3, 4, 1, 2]);  view_2577 = None
        view_2578 = torch.ops.aten.view.default(permute_1396, [1, 2, 4, 1, 512, 512]);  permute_1396 = None
        view_2579 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_164 = torch.ops.aten.bitwise_not.default(view_2579);  view_2579 = None
        masked_fill_164 = torch.ops.aten.masked_fill.Scalar(view_2578, bitwise_not_164, -10000);  view_2578 = bitwise_not_164 = None
        view_2580 = torch.ops.aten.view.default(masked_fill_164, [1, 2, 4, 512, 512]);  masked_fill_164 = None
        permute_1397 = torch.ops.aten.permute.default(view_2580, [1, 0, 2, 3, 4]);  view_2580 = None
        view_2581 = torch.ops.aten.view.default(permute_1397, [2, 4, 1, 512, 512]);  permute_1397 = None
        _to_copy_1459 = torch.ops.aten._to_copy.default(arg831_1, dtype = torch.bfloat16);  arg831_1 = None
        _to_copy_1460 = torch.ops.aten._to_copy.default(getitem_2450, dtype = torch.bfloat16)
        t_538 = torch.ops.aten.t.default(_to_copy_1459);  _to_copy_1459 = None
        view_2582 = torch.ops.aten.view.default(_to_copy_1460, [262144, 256]);  _to_copy_1460 = None
        mm_502 = torch.ops.aten.mm.default(view_2582, t_538);  view_2582 = t_538 = None
        view_2583 = torch.ops.aten.view.default(mm_502, [1, 512, 512, 1024]);  mm_502 = None
        select_65 = torch.ops.aten.select.int(view_2581, 0, 0)
        view_2584 = torch.ops.aten.view.default(view_2583, [1, 512, 512, 4, 4, 64]);  view_2583 = None
        permute_1398 = torch.ops.aten.permute.default(view_2584, [4, 0, 3, 1, 2, 5]);  view_2584 = None
        view_2585 = torch.ops.aten.view.default(permute_1398, [4, 4, 512, 512, 64]);  permute_1398 = None
        unbind_int_118 = torch.ops.aten.unbind.int(view_2585);  view_2585 = None
        getitem_2453 = unbind_int_118[0]
        getitem_2454 = unbind_int_118[1]
        getitem_2455 = unbind_int_118[2]
        getitem_2456 = unbind_int_118[3];  unbind_int_118 = None
        expand_157 = torch.ops.aten.expand.default(select_65, [4, 512, 512, 512]);  select_65 = None
        _scaled_dot_product_efficient_attention_default_90 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2453, getitem_2454, getitem_2455, expand_157, False);  getitem_2453 = getitem_2454 = getitem_2455 = expand_157 = None
        getitem_2457 = _scaled_dot_product_efficient_attention_default_90[0]
        sigmoid_201 = torch.ops.aten.sigmoid.default(getitem_2456);  getitem_2456 = None
        mul_330 = torch.ops.aten.mul.Tensor(getitem_2457, sigmoid_201);  getitem_2457 = sigmoid_201 = None
        view_2586 = torch.ops.aten.view.default(mul_330, [1, 4, 512, 512, 64]);  mul_330 = None
        permute_1399 = torch.ops.aten.permute.default(view_2586, [0, 2, 3, 1, 4]);  view_2586 = None
        clone_226 = torch.ops.aten.clone.default(permute_1399, memory_format = torch.contiguous_format);  permute_1399 = None
        _unsafe_view_192 = torch.ops.aten._unsafe_view.default(clone_226, [1, 512, 512, 256]);  clone_226 = None
        transpose_65 = torch.ops.aten.transpose.int(getitem_2450, 1, 2);  getitem_2450 = None
        _to_copy_1461 = torch.ops.aten._to_copy.default(arg832_1, dtype = torch.bfloat16);  arg832_1 = None
        _to_copy_1462 = torch.ops.aten._to_copy.default(transpose_65, dtype = torch.bfloat16);  transpose_65 = None
        t_539 = torch.ops.aten.t.default(_to_copy_1461);  _to_copy_1461 = None
        expand_158 = torch.ops.aten.expand.default(_to_copy_1462, [1, 512, 512, 256]);  _to_copy_1462 = None
        view_2587 = torch.ops.aten.view.default(expand_158, [512, 512, 256]);  expand_158 = None
        expand_159 = torch.ops.aten.expand.default(t_539, [1, 512, 256, 1024]);  t_539 = None
        view_2588 = torch.ops.aten.view.default(expand_159, [512, 256, 1024]);  expand_159 = None
        bmm_210 = torch.ops.aten.bmm.default(view_2587, view_2588);  view_2587 = view_2588 = None
        view_2589 = torch.ops.aten.view.default(bmm_210, [1, 512, 512, 1024]);  bmm_210 = None
        select_66 = torch.ops.aten.select.int(view_2581, 0, 1);  view_2581 = None
        view_2590 = torch.ops.aten.view.default(view_2589, [1, 512, 512, 4, 4, 64]);  view_2589 = None
        permute_1400 = torch.ops.aten.permute.default(view_2590, [4, 0, 3, 1, 2, 5]);  view_2590 = None
        view_2591 = torch.ops.aten.view.default(permute_1400, [4, 4, 512, 512, 64]);  permute_1400 = None
        unbind_int_119 = torch.ops.aten.unbind.int(view_2591);  view_2591 = None
        getitem_2461 = unbind_int_119[0]
        getitem_2462 = unbind_int_119[1]
        getitem_2463 = unbind_int_119[2]
        getitem_2464 = unbind_int_119[3];  unbind_int_119 = None
        expand_160 = torch.ops.aten.expand.default(select_66, [4, 512, 512, 512]);  select_66 = None
        _scaled_dot_product_efficient_attention_default_91 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2461, getitem_2462, getitem_2463, expand_160, False);  getitem_2461 = getitem_2462 = getitem_2463 = expand_160 = None
        getitem_2465 = _scaled_dot_product_efficient_attention_default_91[0]
        sigmoid_202 = torch.ops.aten.sigmoid.default(getitem_2464);  getitem_2464 = None
        mul_331 = torch.ops.aten.mul.Tensor(getitem_2465, sigmoid_202);  getitem_2465 = sigmoid_202 = None
        view_2592 = torch.ops.aten.view.default(mul_331, [1, 4, 512, 512, 64]);  mul_331 = None
        permute_1401 = torch.ops.aten.permute.default(view_2592, [0, 2, 3, 1, 4]);  view_2592 = None
        clone_227 = torch.ops.aten.clone.default(permute_1401, memory_format = torch.contiguous_format);  permute_1401 = None
        _unsafe_view_193 = torch.ops.aten._unsafe_view.default(clone_227, [1, 512, 512, 256]);  clone_227 = None
        cat_38 = torch.ops.aten.cat.default([_unsafe_view_192, _unsafe_view_193], dim = -1);  _unsafe_view_192 = _unsafe_view_193 = None
        slice_231 = torch.ops.aten.slice.Tensor(arg829_1, dim = 0, start = 0, end = 9223372036854775807);  arg829_1 = None
        unsqueeze_860 = torch.ops.aten.unsqueeze.default(slice_231, 1);  slice_231 = None
        mul_332 = torch.ops.aten.mul.Tensor(arg833_1, unsqueeze_860);  arg833_1 = unsqueeze_860 = None
        _to_copy_1463 = torch.ops.aten._to_copy.default(mul_332, dtype = torch.bfloat16);  mul_332 = None
        t_540 = torch.ops.aten.t.default(_to_copy_1463);  _to_copy_1463 = None
        view_2593 = torch.ops.aten.view.default(cat_38, [262144, 512]);  cat_38 = None
        mm_503 = torch.ops.aten.mm.default(view_2593, t_540);  view_2593 = t_540 = None
        view_2594 = torch.ops.aten.view.default(mm_503, [1, 512, 512, 256]);  mm_503 = None
        add_266 = torch.ops.aten.add.Tensor(add_265, view_2594);  add_265 = view_2594 = None
        split_tensor_263 = torch.ops.aten.split.Tensor(add_259, 512, dim = -2)
        getitem_2469 = split_tensor_263[0];  split_tensor_263 = None
        _to_copy_1464 = torch.ops.aten._to_copy.default(getitem_2469, dtype = torch.float32);  getitem_2469 = None
        native_layer_norm_default_302 = torch.ops.aten.native_layer_norm.default(_to_copy_1464, [256], arg820_1, arg821_1, 1e-05);  _to_copy_1464 = arg820_1 = arg821_1 = None
        getitem_2470 = native_layer_norm_default_302[0]
        _to_copy_1465 = torch.ops.aten._to_copy.default(arg822_1, dtype = torch.bfloat16);  arg822_1 = None
        _to_copy_1466 = torch.ops.aten._to_copy.default(getitem_2470, dtype = torch.bfloat16);  getitem_2470 = None
        t_541 = torch.ops.aten.t.default(_to_copy_1465);  _to_copy_1465 = None
        view_2595 = torch.ops.aten.view.default(_to_copy_1466, [262144, 256]);  _to_copy_1466 = None
        mm_504 = torch.ops.aten.mm.default(view_2595, t_541);  view_2595 = t_541 = None
        view_2596 = torch.ops.aten.view.default(mm_504, [1, 512, 512, 1024]);  mm_504 = None
        split_tensor_264 = torch.ops.aten.split.Tensor(view_2596, 512, dim = -1);  view_2596 = None
        getitem_2473 = split_tensor_264[0]
        getitem_2474 = split_tensor_264[1];  split_tensor_264 = None
        silu_70 = torch.ops.aten.silu.default(getitem_2473);  getitem_2473 = None
        mul_333 = torch.ops.aten.mul.Tensor(silu_70, getitem_2474);  silu_70 = getitem_2474 = None
        _to_copy_1467 = torch.ops.aten._to_copy.default(arg823_1, dtype = torch.bfloat16);  arg823_1 = None
        t_542 = torch.ops.aten.t.default(_to_copy_1467);  _to_copy_1467 = None
        view_2598 = torch.ops.aten.view.default(mul_333, [262144, 512]);  mul_333 = None
        mm_505 = torch.ops.aten.mm.default(view_2598, t_542);  view_2598 = t_542 = None
        view_2599 = torch.ops.aten.view.default(mm_505, [1, 512, 512, 256]);  mm_505 = None
        add_267 = torch.ops.aten.add.Tensor(add_266, view_2599);  add_266 = view_2599 = None
        _to_copy_1468 = torch.ops.aten._to_copy.default(add_263, dtype = torch.float32)
        native_layer_norm_default_303 = torch.ops.aten.native_layer_norm.default(_to_copy_1468, [384], arg838_1, arg839_1, 1e-05);  _to_copy_1468 = arg838_1 = arg839_1 = None
        getitem_2475 = native_layer_norm_default_303[0]
        _to_copy_1469 = torch.ops.aten._to_copy.default(add_259, dtype = torch.float32);  add_259 = None
        native_layer_norm_default_304 = torch.ops.aten.native_layer_norm.default(_to_copy_1469, [256], arg840_1, arg841_1, 1e-05);  _to_copy_1469 = arg840_1 = arg841_1 = None
        getitem_2478 = native_layer_norm_default_304[0]
        _to_copy_1470 = torch.ops.aten._to_copy.default(arg842_1, dtype = torch.bfloat16);  arg842_1 = None
        _to_copy_1471 = torch.ops.aten._to_copy.default(getitem_2478, dtype = torch.bfloat16);  getitem_2478 = None
        t_543 = torch.ops.aten.t.default(_to_copy_1470);  _to_copy_1470 = None
        view_2600 = torch.ops.aten.view.default(_to_copy_1471, [262144, 256]);  _to_copy_1471 = None
        mm_506 = torch.ops.aten.mm.default(view_2600, t_543);  view_2600 = t_543 = None
        view_2601 = torch.ops.aten.view.default(mm_506, [1, 512, 512, 16]);  mm_506 = None
        permute_1402 = torch.ops.aten.permute.default(view_2601, [0, 3, 1, 2]);  view_2601 = None
        view_2602 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_165 = torch.ops.aten.bitwise_not.default(view_2602);  view_2602 = None
        masked_fill_165 = torch.ops.aten.masked_fill.Scalar(permute_1402, bitwise_not_165, -10000);  permute_1402 = bitwise_not_165 = None
        _to_copy_1472 = torch.ops.aten._to_copy.default(getitem_2475, dtype = torch.bfloat16);  getitem_2475 = None
        _to_copy_1473 = torch.ops.aten._to_copy.default(arg844_1, dtype = torch.bfloat16);  arg844_1 = None
        unsqueeze_861 = torch.ops.aten.unsqueeze.default(_to_copy_1472, 3);  _to_copy_1472 = None
        unsqueeze_862 = torch.ops.aten.unsqueeze.default(unsqueeze_861, 4);  unsqueeze_861 = None
        unsqueeze_863 = torch.ops.aten.unsqueeze.default(unsqueeze_862, 5);  unsqueeze_862 = None
        permute_1403 = torch.ops.aten.permute.default(unsqueeze_863, [3, 0, 4, 1, 5, 2]);  unsqueeze_863 = None
        unsqueeze_864 = torch.ops.aten.unsqueeze.default(_to_copy_1473, 4);  _to_copy_1473 = None
        unsqueeze_865 = torch.ops.aten.unsqueeze.default(unsqueeze_864, 5);  unsqueeze_864 = None
        permute_1404 = torch.ops.aten.permute.default(unsqueeze_865, [1, 4, 2, 5, 3, 0]);  unsqueeze_865 = None
        permute_1405 = torch.ops.aten.permute.default(permute_1403, [3, 5, 0, 1, 2, 4]);  permute_1403 = None
        view_2603 = torch.ops.aten.view.default(permute_1405, [1, 512, 384]);  permute_1405 = None
        permute_1406 = torch.ops.aten.permute.default(permute_1404, [5, 0, 1, 2, 4, 3]);  permute_1404 = None
        view_2604 = torch.ops.aten.view.default(permute_1406, [1, 384, 1536]);  permute_1406 = None
        bmm_211 = torch.ops.aten.bmm.default(view_2603, view_2604);  view_2603 = view_2604 = None
        view_2605 = torch.ops.aten.view.default(bmm_211, [512, 1, 4, 1, 16, 24]);  bmm_211 = None
        permute_1407 = torch.ops.aten.permute.default(view_2605, [2, 3, 4, 0, 5, 1]);  view_2605 = None
        view_2606 = torch.ops.aten.view.default(permute_1407, [4, 1, 16, 512, 24]);  permute_1407 = None
        unbind_int_120 = torch.ops.aten.unbind.int(view_2606);  view_2606 = None
        getitem_2481 = unbind_int_120[0]
        getitem_2482 = unbind_int_120[1]
        getitem_2483 = unbind_int_120[2]
        getitem_2484 = unbind_int_120[3];  unbind_int_120 = None
        view_2607 = torch.ops.aten.view.default(arg843_1, [1, 16, 1, 24]);  arg843_1 = None
        add_268 = torch.ops.aten.add.Tensor(getitem_2481, view_2607);  getitem_2481 = view_2607 = None
        _to_copy_1474 = torch.ops.aten._to_copy.default(add_268, dtype = torch.bfloat16);  add_268 = None
        expand_161 = torch.ops.aten.expand.default(masked_fill_165, [1, 16, 512, 512]);  masked_fill_165 = None
        _scaled_dot_product_efficient_attention_default_92 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1474, getitem_2482, getitem_2483, expand_161, False);  _to_copy_1474 = getitem_2482 = getitem_2483 = expand_161 = None
        getitem_2485 = _scaled_dot_product_efficient_attention_default_92[0]
        add_269 = torch.ops.aten.add.Tensor(getitem_2484, 1);  getitem_2484 = None
        sigmoid_203 = torch.ops.aten.sigmoid.default(add_269);  add_269 = None
        mul_334 = torch.ops.aten.mul.Tensor(getitem_2485, sigmoid_203);  getitem_2485 = sigmoid_203 = None
        _to_copy_1475 = torch.ops.aten._to_copy.default(arg845_1, dtype = torch.bfloat16);  arg845_1 = None
        unsqueeze_866 = torch.ops.aten.unsqueeze.default(mul_334, 4);  mul_334 = None
        permute_1408 = torch.ops.aten.permute.default(unsqueeze_866, [0, 2, 4, 3, 1]);  unsqueeze_866 = None
        unsqueeze_867 = torch.ops.aten.unsqueeze.default(_to_copy_1475, 3);  _to_copy_1475 = None
        unsqueeze_868 = torch.ops.aten.unsqueeze.default(unsqueeze_867, 4);  unsqueeze_867 = None
        permute_1409 = torch.ops.aten.permute.default(unsqueeze_868, [3, 4, 2, 1, 0]);  unsqueeze_868 = None
        permute_1410 = torch.ops.aten.permute.default(permute_1408, [1, 3, 4, 0, 2]);  permute_1408 = None
        clone_228 = torch.ops.aten.clone.default(permute_1410, memory_format = torch.contiguous_format);  permute_1410 = None
        _unsafe_view_194 = torch.ops.aten._unsafe_view.default(clone_228, [1, 512, 384]);  clone_228 = None
        permute_1411 = torch.ops.aten.permute.default(permute_1409, [3, 4, 0, 2, 1]);  permute_1409 = None
        clone_229 = torch.ops.aten.clone.default(permute_1411, memory_format = torch.contiguous_format);  permute_1411 = None
        _unsafe_view_195 = torch.ops.aten._unsafe_view.default(clone_229, [1, 384, 384]);  clone_229 = None
        bmm_212 = torch.ops.aten.bmm.default(_unsafe_view_194, _unsafe_view_195);  _unsafe_view_194 = _unsafe_view_195 = None
        view_2608 = torch.ops.aten.view.default(bmm_212, [512, 1, 1, 1, 384]);  bmm_212 = None
        permute_1412 = torch.ops.aten.permute.default(view_2608, [3, 0, 4, 1, 2]);  view_2608 = None
        view_2609 = torch.ops.aten.view.default(permute_1412, [1, 512, 384]);  permute_1412 = None
        unsqueeze_869 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_335 = torch.ops.aten.mul.Tensor(view_2609, unsqueeze_869);  view_2609 = unsqueeze_869 = None
        add_270 = torch.ops.aten.add.Tensor(add_263, mul_335);  mul_335 = None
        split_tensor_265 = torch.ops.aten.split.Tensor(add_263, 512, dim = -2);  add_263 = None
        getitem_2489 = split_tensor_265[0];  split_tensor_265 = None
        _to_copy_1476 = torch.ops.aten._to_copy.default(getitem_2489, dtype = torch.float32);  getitem_2489 = None
        native_layer_norm_default_305 = torch.ops.aten.native_layer_norm.default(_to_copy_1476, [384], arg834_1, arg835_1, 1e-05);  _to_copy_1476 = arg834_1 = arg835_1 = None
        getitem_2490 = native_layer_norm_default_305[0]
        _to_copy_1477 = torch.ops.aten._to_copy.default(arg836_1, dtype = torch.bfloat16);  arg836_1 = None
        _to_copy_1478 = torch.ops.aten._to_copy.default(getitem_2490, dtype = torch.bfloat16);  getitem_2490 = None
        t_544 = torch.ops.aten.t.default(_to_copy_1477);  _to_copy_1477 = None
        view_2610 = torch.ops.aten.view.default(_to_copy_1478, [512, 384]);  _to_copy_1478 = None
        mm_507 = torch.ops.aten.mm.default(view_2610, t_544);  view_2610 = t_544 = None
        view_2611 = torch.ops.aten.view.default(mm_507, [1, 512, 1536]);  mm_507 = None
        split_tensor_266 = torch.ops.aten.split.Tensor(view_2611, 768, dim = -1);  view_2611 = None
        getitem_2493 = split_tensor_266[0]
        getitem_2494 = split_tensor_266[1];  split_tensor_266 = None
        silu_71 = torch.ops.aten.silu.default(getitem_2493);  getitem_2493 = None
        mul_336 = torch.ops.aten.mul.Tensor(silu_71, getitem_2494);  silu_71 = getitem_2494 = None
        _to_copy_1479 = torch.ops.aten._to_copy.default(arg837_1, dtype = torch.bfloat16);  arg837_1 = None
        t_545 = torch.ops.aten.t.default(_to_copy_1479);  _to_copy_1479 = None
        view_2613 = torch.ops.aten.view.default(mul_336, [512, 768]);  mul_336 = None
        mm_508 = torch.ops.aten.mm.default(view_2613, t_545);  view_2613 = t_545 = None
        view_2614 = torch.ops.aten.view.default(mm_508, [1, 512, 384]);  mm_508 = None
        add_271 = torch.ops.aten.add.Tensor(add_270, view_2614);  add_270 = view_2614 = None
        _to_copy_1480 = torch.ops.aten._to_copy.default(add_267, dtype = torch.float32)
        native_layer_norm_default_306 = torch.ops.aten.native_layer_norm.default(_to_copy_1480, [256], arg850_1, arg851_1, 1e-05);  _to_copy_1480 = arg850_1 = arg851_1 = None
        getitem_2495 = native_layer_norm_default_306[0]
        split_with_sizes_default_66 = torch.ops.aten.split_with_sizes.default(arg853_1, [512, 512]);  arg853_1 = None
        getitem_2498 = split_with_sizes_default_66[0]
        getitem_2499 = split_with_sizes_default_66[1];  split_with_sizes_default_66 = None
        split_with_sizes_default_67 = torch.ops.aten.split_with_sizes.default(arg854_1, [512, 512, 256]);  arg854_1 = None
        getitem_2500 = split_with_sizes_default_67[0]
        getitem_2501 = split_with_sizes_default_67[1]
        getitem_2502 = split_with_sizes_default_67[2];  split_with_sizes_default_67 = None
        _to_copy_1481 = torch.ops.aten._to_copy.default(getitem_2498, dtype = torch.bfloat16);  getitem_2498 = None
        _to_copy_1482 = torch.ops.aten._to_copy.default(getitem_2495, dtype = torch.bfloat16)
        t_546 = torch.ops.aten.t.default(_to_copy_1481);  _to_copy_1481 = None
        view_2615 = torch.ops.aten.view.default(_to_copy_1482, [262144, 256]);  _to_copy_1482 = None
        mm_509 = torch.ops.aten.mm.default(view_2615, t_546);  view_2615 = t_546 = None
        view_2616 = torch.ops.aten.view.default(mm_509, [1, 512, 512, 512]);  mm_509 = None
        _to_copy_1483 = torch.ops.aten._to_copy.default(getitem_2500, dtype = torch.bfloat16);  getitem_2500 = None
        _to_copy_1484 = torch.ops.aten._to_copy.default(getitem_2495, dtype = torch.bfloat16)
        t_547 = torch.ops.aten.t.default(_to_copy_1483);  _to_copy_1483 = None
        view_2617 = torch.ops.aten.view.default(_to_copy_1484, [262144, 256]);  _to_copy_1484 = None
        mm_510 = torch.ops.aten.mm.default(view_2617, t_547);  view_2617 = t_547 = None
        view_2618 = torch.ops.aten.view.default(mm_510, [1, 512, 512, 512]);  mm_510 = None
        sigmoid_204 = torch.ops.aten.sigmoid.default(view_2618);  view_2618 = None
        mul_337 = torch.ops.aten.mul.Tensor(view_2616, sigmoid_204);  view_2616 = sigmoid_204 = None
        unsqueeze_870 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_166 = torch.ops.aten.bitwise_not.default(unsqueeze_870);  unsqueeze_870 = None
        masked_fill_166 = torch.ops.aten.masked_fill.Scalar(mul_337, bitwise_not_166, 0);  mul_337 = bitwise_not_166 = None
        split_tensor_267 = torch.ops.aten.split.Tensor(masked_fill_166, 256, dim = -1)
        getitem_2505 = split_tensor_267[0]
        unsqueeze_873 = torch.ops.aten.unsqueeze.default(getitem_2505, 4);  getitem_2505 = None
        permute_1417 = torch.ops.aten.permute.default(unsqueeze_873, [0, 1, 4, 3, 2]);  unsqueeze_873 = None
        permute_1418 = torch.ops.aten.permute.default(permute_1417, [3, 1, 4, 0, 2]);  permute_1417 = None
        view_2621 = torch.ops.aten.view.default(permute_1418, [256, 512, 512]);  permute_1418 = None
        split_tensor_268 = torch.ops.aten.split.Tensor(masked_fill_166, 256, dim = -1);  masked_fill_166 = None
        getitem_2508 = split_tensor_268[1];  split_tensor_268 = None
        unsqueeze_874 = torch.ops.aten.unsqueeze.default(getitem_2508, 4);  getitem_2508 = None
        permute_1419 = torch.ops.aten.permute.default(unsqueeze_874, [0, 4, 1, 3, 2]);  unsqueeze_874 = None
        permute_1420 = torch.ops.aten.permute.default(permute_1419, [3, 4, 0, 2, 1]);  permute_1419 = None
        view_2622 = torch.ops.aten.view.default(permute_1420, [256, 512, 512]);  permute_1420 = None
        bmm_213 = torch.ops.aten.bmm.default(view_2621, view_2622);  view_2621 = view_2622 = None
        view_2623 = torch.ops.aten.view.default(bmm_213, [256, 512, 1, 1, 512]);  bmm_213 = None
        permute_1421 = torch.ops.aten.permute.default(view_2623, [3, 1, 4, 0, 2]);  view_2623 = None
        view_2624 = torch.ops.aten.view.default(permute_1421, [1, 512, 512, 256]);  permute_1421 = None
        _to_copy_1485 = torch.ops.aten._to_copy.default(getitem_2499, dtype = torch.bfloat16);  getitem_2499 = None
        _to_copy_1486 = torch.ops.aten._to_copy.default(getitem_2495, dtype = torch.bfloat16)
        t_548 = torch.ops.aten.t.default(_to_copy_1485);  _to_copy_1485 = None
        view_2625 = torch.ops.aten.view.default(_to_copy_1486, [262144, 256]);  _to_copy_1486 = None
        mm_511 = torch.ops.aten.mm.default(view_2625, t_548);  view_2625 = t_548 = None
        view_2626 = torch.ops.aten.view.default(mm_511, [1, 512, 512, 512]);  mm_511 = None
        _to_copy_1487 = torch.ops.aten._to_copy.default(getitem_2501, dtype = torch.bfloat16);  getitem_2501 = None
        _to_copy_1488 = torch.ops.aten._to_copy.default(getitem_2495, dtype = torch.bfloat16)
        t_549 = torch.ops.aten.t.default(_to_copy_1487);  _to_copy_1487 = None
        view_2627 = torch.ops.aten.view.default(_to_copy_1488, [262144, 256]);  _to_copy_1488 = None
        mm_512 = torch.ops.aten.mm.default(view_2627, t_549);  view_2627 = t_549 = None
        view_2628 = torch.ops.aten.view.default(mm_512, [1, 512, 512, 512]);  mm_512 = None
        sigmoid_205 = torch.ops.aten.sigmoid.default(view_2628);  view_2628 = None
        mul_338 = torch.ops.aten.mul.Tensor(view_2626, sigmoid_205);  view_2626 = sigmoid_205 = None
        view_2629 = torch.ops.aten.view.default(mul_338, [262144, 512]);  mul_338 = None
        view_2630 = torch.ops.aten.view.default(view_2629, [1, 512, 512, 512]);  view_2629 = None
        transpose_66 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_875 = torch.ops.aten.unsqueeze.default(transpose_66, 3);  transpose_66 = None
        clone_230 = torch.ops.aten.clone.default(unsqueeze_875, memory_format = torch.contiguous_format);  unsqueeze_875 = None
        bitwise_not_167 = torch.ops.aten.bitwise_not.default(clone_230);  clone_230 = None
        masked_fill_167 = torch.ops.aten.masked_fill.Scalar(view_2630, bitwise_not_167, 0);  view_2630 = bitwise_not_167 = None
        view_2631 = torch.ops.aten.view.default(masked_fill_167, [262144, 512]);  masked_fill_167 = None
        view_2635 = torch.ops.aten.view.default(view_2631, [1, 512, 512, 512])
        split_tensor_269 = torch.ops.aten.split.Tensor(view_2635, 256, dim = -1);  view_2635 = None
        getitem_2511 = split_tensor_269[0]
        unsqueeze_878 = torch.ops.aten.unsqueeze.default(getitem_2511, 4);  getitem_2511 = None
        permute_1426 = torch.ops.aten.permute.default(unsqueeze_878, [0, 2, 4, 3, 1]);  unsqueeze_878 = None
        permute_1427 = torch.ops.aten.permute.default(permute_1426, [3, 1, 4, 0, 2]);  permute_1426 = None
        view_2636 = torch.ops.aten.view.default(permute_1427, [256, 512, 512]);  permute_1427 = None
        view_2637 = torch.ops.aten.view.default(view_2631, [1, 512, 512, 512]);  view_2631 = None
        split_tensor_270 = torch.ops.aten.split.Tensor(view_2637, 256, dim = -1);  view_2637 = None
        getitem_2514 = split_tensor_270[1];  split_tensor_270 = None
        unsqueeze_879 = torch.ops.aten.unsqueeze.default(getitem_2514, 4);  getitem_2514 = None
        permute_1428 = torch.ops.aten.permute.default(unsqueeze_879, [0, 4, 2, 3, 1]);  unsqueeze_879 = None
        permute_1429 = torch.ops.aten.permute.default(permute_1428, [3, 4, 0, 2, 1]);  permute_1428 = None
        view_2638 = torch.ops.aten.view.default(permute_1429, [256, 512, 512]);  permute_1429 = None
        bmm_214 = torch.ops.aten.bmm.default(view_2636, view_2638);  view_2636 = view_2638 = None
        view_2639 = torch.ops.aten.view.default(bmm_214, [256, 512, 1, 1, 512]);  bmm_214 = None
        permute_1430 = torch.ops.aten.permute.default(view_2639, [3, 1, 4, 0, 2]);  view_2639 = None
        view_2640 = torch.ops.aten.view.default(permute_1430, [1, 512, 512, 256]);  permute_1430 = None
        _to_copy_1489 = torch.ops.aten._to_copy.default(view_2624, dtype = torch.float32);  view_2624 = None
        native_layer_norm_default_307 = torch.ops.aten.native_layer_norm.default(_to_copy_1489, [256], None, None, 1e-05);  _to_copy_1489 = None
        getitem_2515 = native_layer_norm_default_307[0]
        _to_copy_1490 = torch.ops.aten._to_copy.default(view_2640, dtype = torch.float32);  view_2640 = None
        native_layer_norm_default_308 = torch.ops.aten.native_layer_norm.default(_to_copy_1490, [256], None, None, 1e-05);  _to_copy_1490 = None
        getitem_2518 = native_layer_norm_default_308[0]
        add_272 = torch.ops.aten.add.Tensor(getitem_2515, getitem_2518);  getitem_2515 = getitem_2518 = None
        _to_copy_1491 = torch.ops.aten._to_copy.default(arg852_1, dtype = torch.bfloat16);  arg852_1 = None
        _to_copy_1492 = torch.ops.aten._to_copy.default(add_272, dtype = torch.bfloat16);  add_272 = None
        t_550 = torch.ops.aten.t.default(_to_copy_1491);  _to_copy_1491 = None
        view_2641 = torch.ops.aten.view.default(_to_copy_1492, [262144, 256]);  _to_copy_1492 = None
        mm_513 = torch.ops.aten.mm.default(view_2641, t_550);  view_2641 = t_550 = None
        view_2642 = torch.ops.aten.view.default(mm_513, [1, 512, 512, 256]);  mm_513 = None
        _to_copy_1493 = torch.ops.aten._to_copy.default(getitem_2502, dtype = torch.bfloat16);  getitem_2502 = None
        _to_copy_1494 = torch.ops.aten._to_copy.default(getitem_2495, dtype = torch.bfloat16);  getitem_2495 = None
        t_551 = torch.ops.aten.t.default(_to_copy_1493);  _to_copy_1493 = None
        view_2643 = torch.ops.aten.view.default(_to_copy_1494, [262144, 256]);  _to_copy_1494 = None
        mm_514 = torch.ops.aten.mm.default(view_2643, t_551);  view_2643 = t_551 = None
        view_2644 = torch.ops.aten.view.default(mm_514, [1, 512, 512, 256]);  mm_514 = None
        sigmoid_206 = torch.ops.aten.sigmoid.default(view_2644);  view_2644 = None
        mul_339 = torch.ops.aten.mul.Tensor(view_2642, sigmoid_206);  view_2642 = sigmoid_206 = None
        add_273 = torch.ops.aten.add.Tensor(add_267, mul_339);  mul_339 = None
        _to_copy_1495 = torch.ops.aten._to_copy.default(add_267, dtype = torch.float32)
        native_layer_norm_default_309 = torch.ops.aten.native_layer_norm.default(_to_copy_1495, [256], None, None, 1e-05);  _to_copy_1495 = None
        getitem_2521 = native_layer_norm_default_309[0]
        _to_copy_1496 = torch.ops.aten._to_copy.default(arg856_1, dtype = torch.bfloat16);  arg856_1 = None
        _to_copy_1497 = torch.ops.aten._to_copy.default(getitem_2521, dtype = torch.bfloat16)
        t_552 = torch.ops.aten.t.default(_to_copy_1496);  _to_copy_1496 = None
        view_2645 = torch.ops.aten.view.default(_to_copy_1497, [262144, 256]);  _to_copy_1497 = None
        mm_515 = torch.ops.aten.mm.default(view_2645, t_552);  view_2645 = t_552 = None
        view_2646 = torch.ops.aten.view.default(mm_515, [1, 512, 512, 8]);  mm_515 = None
        view_2647 = torch.ops.aten.view.default(view_2646, [1, 512, 512, 2, 4]);  view_2646 = None
        permute_1431 = torch.ops.aten.permute.default(view_2647, [0, 3, 4, 1, 2]);  view_2647 = None
        view_2648 = torch.ops.aten.view.default(permute_1431, [1, 2, 4, 1, 512, 512]);  permute_1431 = None
        view_2649 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_168 = torch.ops.aten.bitwise_not.default(view_2649);  view_2649 = None
        masked_fill_168 = torch.ops.aten.masked_fill.Scalar(view_2648, bitwise_not_168, -10000);  view_2648 = bitwise_not_168 = None
        view_2650 = torch.ops.aten.view.default(masked_fill_168, [1, 2, 4, 512, 512]);  masked_fill_168 = None
        permute_1432 = torch.ops.aten.permute.default(view_2650, [1, 0, 2, 3, 4]);  view_2650 = None
        view_2651 = torch.ops.aten.view.default(permute_1432, [2, 4, 1, 512, 512]);  permute_1432 = None
        _to_copy_1498 = torch.ops.aten._to_copy.default(arg857_1, dtype = torch.bfloat16);  arg857_1 = None
        _to_copy_1499 = torch.ops.aten._to_copy.default(getitem_2521, dtype = torch.bfloat16)
        t_553 = torch.ops.aten.t.default(_to_copy_1498);  _to_copy_1498 = None
        view_2652 = torch.ops.aten.view.default(_to_copy_1499, [262144, 256]);  _to_copy_1499 = None
        mm_516 = torch.ops.aten.mm.default(view_2652, t_553);  view_2652 = t_553 = None
        view_2653 = torch.ops.aten.view.default(mm_516, [1, 512, 512, 1024]);  mm_516 = None
        select_67 = torch.ops.aten.select.int(view_2651, 0, 0)
        view_2654 = torch.ops.aten.view.default(view_2653, [1, 512, 512, 4, 4, 64]);  view_2653 = None
        permute_1433 = torch.ops.aten.permute.default(view_2654, [4, 0, 3, 1, 2, 5]);  view_2654 = None
        view_2655 = torch.ops.aten.view.default(permute_1433, [4, 4, 512, 512, 64]);  permute_1433 = None
        unbind_int_121 = torch.ops.aten.unbind.int(view_2655);  view_2655 = None
        getitem_2524 = unbind_int_121[0]
        getitem_2525 = unbind_int_121[1]
        getitem_2526 = unbind_int_121[2]
        getitem_2527 = unbind_int_121[3];  unbind_int_121 = None
        expand_162 = torch.ops.aten.expand.default(select_67, [4, 512, 512, 512]);  select_67 = None
        _scaled_dot_product_efficient_attention_default_93 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2524, getitem_2525, getitem_2526, expand_162, False);  getitem_2524 = getitem_2525 = getitem_2526 = expand_162 = None
        getitem_2528 = _scaled_dot_product_efficient_attention_default_93[0]
        sigmoid_207 = torch.ops.aten.sigmoid.default(getitem_2527);  getitem_2527 = None
        mul_340 = torch.ops.aten.mul.Tensor(getitem_2528, sigmoid_207);  getitem_2528 = sigmoid_207 = None
        view_2656 = torch.ops.aten.view.default(mul_340, [1, 4, 512, 512, 64]);  mul_340 = None
        permute_1434 = torch.ops.aten.permute.default(view_2656, [0, 2, 3, 1, 4]);  view_2656 = None
        clone_231 = torch.ops.aten.clone.default(permute_1434, memory_format = torch.contiguous_format);  permute_1434 = None
        _unsafe_view_196 = torch.ops.aten._unsafe_view.default(clone_231, [1, 512, 512, 256]);  clone_231 = None
        transpose_67 = torch.ops.aten.transpose.int(getitem_2521, 1, 2);  getitem_2521 = None
        _to_copy_1500 = torch.ops.aten._to_copy.default(arg858_1, dtype = torch.bfloat16);  arg858_1 = None
        _to_copy_1501 = torch.ops.aten._to_copy.default(transpose_67, dtype = torch.bfloat16);  transpose_67 = None
        t_554 = torch.ops.aten.t.default(_to_copy_1500);  _to_copy_1500 = None
        expand_163 = torch.ops.aten.expand.default(_to_copy_1501, [1, 512, 512, 256]);  _to_copy_1501 = None
        view_2657 = torch.ops.aten.view.default(expand_163, [512, 512, 256]);  expand_163 = None
        expand_164 = torch.ops.aten.expand.default(t_554, [1, 512, 256, 1024]);  t_554 = None
        view_2658 = torch.ops.aten.view.default(expand_164, [512, 256, 1024]);  expand_164 = None
        bmm_215 = torch.ops.aten.bmm.default(view_2657, view_2658);  view_2657 = view_2658 = None
        view_2659 = torch.ops.aten.view.default(bmm_215, [1, 512, 512, 1024]);  bmm_215 = None
        select_68 = torch.ops.aten.select.int(view_2651, 0, 1);  view_2651 = None
        view_2660 = torch.ops.aten.view.default(view_2659, [1, 512, 512, 4, 4, 64]);  view_2659 = None
        permute_1435 = torch.ops.aten.permute.default(view_2660, [4, 0, 3, 1, 2, 5]);  view_2660 = None
        view_2661 = torch.ops.aten.view.default(permute_1435, [4, 4, 512, 512, 64]);  permute_1435 = None
        unbind_int_122 = torch.ops.aten.unbind.int(view_2661);  view_2661 = None
        getitem_2532 = unbind_int_122[0]
        getitem_2533 = unbind_int_122[1]
        getitem_2534 = unbind_int_122[2]
        getitem_2535 = unbind_int_122[3];  unbind_int_122 = None
        expand_165 = torch.ops.aten.expand.default(select_68, [4, 512, 512, 512]);  select_68 = None
        _scaled_dot_product_efficient_attention_default_94 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2532, getitem_2533, getitem_2534, expand_165, False);  getitem_2532 = getitem_2533 = getitem_2534 = expand_165 = None
        getitem_2536 = _scaled_dot_product_efficient_attention_default_94[0]
        sigmoid_208 = torch.ops.aten.sigmoid.default(getitem_2535);  getitem_2535 = None
        mul_341 = torch.ops.aten.mul.Tensor(getitem_2536, sigmoid_208);  getitem_2536 = sigmoid_208 = None
        view_2662 = torch.ops.aten.view.default(mul_341, [1, 4, 512, 512, 64]);  mul_341 = None
        permute_1436 = torch.ops.aten.permute.default(view_2662, [0, 2, 3, 1, 4]);  view_2662 = None
        clone_232 = torch.ops.aten.clone.default(permute_1436, memory_format = torch.contiguous_format);  permute_1436 = None
        _unsafe_view_197 = torch.ops.aten._unsafe_view.default(clone_232, [1, 512, 512, 256]);  clone_232 = None
        cat_39 = torch.ops.aten.cat.default([_unsafe_view_196, _unsafe_view_197], dim = -1);  _unsafe_view_196 = _unsafe_view_197 = None
        slice_232 = torch.ops.aten.slice.Tensor(arg855_1, dim = 0, start = 0, end = 9223372036854775807);  arg855_1 = None
        unsqueeze_880 = torch.ops.aten.unsqueeze.default(slice_232, 1);  slice_232 = None
        mul_342 = torch.ops.aten.mul.Tensor(arg859_1, unsqueeze_880);  arg859_1 = unsqueeze_880 = None
        _to_copy_1502 = torch.ops.aten._to_copy.default(mul_342, dtype = torch.bfloat16);  mul_342 = None
        t_555 = torch.ops.aten.t.default(_to_copy_1502);  _to_copy_1502 = None
        view_2663 = torch.ops.aten.view.default(cat_39, [262144, 512]);  cat_39 = None
        mm_517 = torch.ops.aten.mm.default(view_2663, t_555);  view_2663 = t_555 = None
        view_2664 = torch.ops.aten.view.default(mm_517, [1, 512, 512, 256]);  mm_517 = None
        add_274 = torch.ops.aten.add.Tensor(add_273, view_2664);  add_273 = view_2664 = None
        split_tensor_271 = torch.ops.aten.split.Tensor(add_267, 512, dim = -2)
        getitem_2540 = split_tensor_271[0];  split_tensor_271 = None
        _to_copy_1503 = torch.ops.aten._to_copy.default(getitem_2540, dtype = torch.float32);  getitem_2540 = None
        native_layer_norm_default_310 = torch.ops.aten.native_layer_norm.default(_to_copy_1503, [256], arg846_1, arg847_1, 1e-05);  _to_copy_1503 = arg846_1 = arg847_1 = None
        getitem_2541 = native_layer_norm_default_310[0]
        _to_copy_1504 = torch.ops.aten._to_copy.default(arg848_1, dtype = torch.bfloat16);  arg848_1 = None
        _to_copy_1505 = torch.ops.aten._to_copy.default(getitem_2541, dtype = torch.bfloat16);  getitem_2541 = None
        t_556 = torch.ops.aten.t.default(_to_copy_1504);  _to_copy_1504 = None
        view_2665 = torch.ops.aten.view.default(_to_copy_1505, [262144, 256]);  _to_copy_1505 = None
        mm_518 = torch.ops.aten.mm.default(view_2665, t_556);  view_2665 = t_556 = None
        view_2666 = torch.ops.aten.view.default(mm_518, [1, 512, 512, 1024]);  mm_518 = None
        split_tensor_272 = torch.ops.aten.split.Tensor(view_2666, 512, dim = -1);  view_2666 = None
        getitem_2544 = split_tensor_272[0]
        getitem_2545 = split_tensor_272[1];  split_tensor_272 = None
        silu_72 = torch.ops.aten.silu.default(getitem_2544);  getitem_2544 = None
        mul_343 = torch.ops.aten.mul.Tensor(silu_72, getitem_2545);  silu_72 = getitem_2545 = None
        _to_copy_1506 = torch.ops.aten._to_copy.default(arg849_1, dtype = torch.bfloat16);  arg849_1 = None
        t_557 = torch.ops.aten.t.default(_to_copy_1506);  _to_copy_1506 = None
        view_2668 = torch.ops.aten.view.default(mul_343, [262144, 512]);  mul_343 = None
        mm_519 = torch.ops.aten.mm.default(view_2668, t_557);  view_2668 = t_557 = None
        view_2669 = torch.ops.aten.view.default(mm_519, [1, 512, 512, 256]);  mm_519 = None
        add_275 = torch.ops.aten.add.Tensor(add_274, view_2669);  add_274 = view_2669 = None
        _to_copy_1507 = torch.ops.aten._to_copy.default(add_271, dtype = torch.float32)
        native_layer_norm_default_311 = torch.ops.aten.native_layer_norm.default(_to_copy_1507, [384], arg864_1, arg865_1, 1e-05);  _to_copy_1507 = arg864_1 = arg865_1 = None
        getitem_2546 = native_layer_norm_default_311[0]
        _to_copy_1508 = torch.ops.aten._to_copy.default(add_267, dtype = torch.float32);  add_267 = None
        native_layer_norm_default_312 = torch.ops.aten.native_layer_norm.default(_to_copy_1508, [256], arg866_1, arg867_1, 1e-05);  _to_copy_1508 = arg866_1 = arg867_1 = None
        getitem_2549 = native_layer_norm_default_312[0]
        _to_copy_1509 = torch.ops.aten._to_copy.default(arg868_1, dtype = torch.bfloat16);  arg868_1 = None
        _to_copy_1510 = torch.ops.aten._to_copy.default(getitem_2549, dtype = torch.bfloat16);  getitem_2549 = None
        t_558 = torch.ops.aten.t.default(_to_copy_1509);  _to_copy_1509 = None
        view_2670 = torch.ops.aten.view.default(_to_copy_1510, [262144, 256]);  _to_copy_1510 = None
        mm_520 = torch.ops.aten.mm.default(view_2670, t_558);  view_2670 = t_558 = None
        view_2671 = torch.ops.aten.view.default(mm_520, [1, 512, 512, 16]);  mm_520 = None
        permute_1437 = torch.ops.aten.permute.default(view_2671, [0, 3, 1, 2]);  view_2671 = None
        view_2672 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_169 = torch.ops.aten.bitwise_not.default(view_2672);  view_2672 = None
        masked_fill_169 = torch.ops.aten.masked_fill.Scalar(permute_1437, bitwise_not_169, -10000);  permute_1437 = bitwise_not_169 = None
        _to_copy_1511 = torch.ops.aten._to_copy.default(getitem_2546, dtype = torch.bfloat16);  getitem_2546 = None
        _to_copy_1512 = torch.ops.aten._to_copy.default(arg870_1, dtype = torch.bfloat16);  arg870_1 = None
        unsqueeze_881 = torch.ops.aten.unsqueeze.default(_to_copy_1511, 3);  _to_copy_1511 = None
        unsqueeze_882 = torch.ops.aten.unsqueeze.default(unsqueeze_881, 4);  unsqueeze_881 = None
        unsqueeze_883 = torch.ops.aten.unsqueeze.default(unsqueeze_882, 5);  unsqueeze_882 = None
        permute_1438 = torch.ops.aten.permute.default(unsqueeze_883, [3, 0, 4, 1, 5, 2]);  unsqueeze_883 = None
        unsqueeze_884 = torch.ops.aten.unsqueeze.default(_to_copy_1512, 4);  _to_copy_1512 = None
        unsqueeze_885 = torch.ops.aten.unsqueeze.default(unsqueeze_884, 5);  unsqueeze_884 = None
        permute_1439 = torch.ops.aten.permute.default(unsqueeze_885, [1, 4, 2, 5, 3, 0]);  unsqueeze_885 = None
        permute_1440 = torch.ops.aten.permute.default(permute_1438, [3, 5, 0, 1, 2, 4]);  permute_1438 = None
        view_2673 = torch.ops.aten.view.default(permute_1440, [1, 512, 384]);  permute_1440 = None
        permute_1441 = torch.ops.aten.permute.default(permute_1439, [5, 0, 1, 2, 4, 3]);  permute_1439 = None
        view_2674 = torch.ops.aten.view.default(permute_1441, [1, 384, 1536]);  permute_1441 = None
        bmm_216 = torch.ops.aten.bmm.default(view_2673, view_2674);  view_2673 = view_2674 = None
        view_2675 = torch.ops.aten.view.default(bmm_216, [512, 1, 4, 1, 16, 24]);  bmm_216 = None
        permute_1442 = torch.ops.aten.permute.default(view_2675, [2, 3, 4, 0, 5, 1]);  view_2675 = None
        view_2676 = torch.ops.aten.view.default(permute_1442, [4, 1, 16, 512, 24]);  permute_1442 = None
        unbind_int_123 = torch.ops.aten.unbind.int(view_2676);  view_2676 = None
        getitem_2552 = unbind_int_123[0]
        getitem_2553 = unbind_int_123[1]
        getitem_2554 = unbind_int_123[2]
        getitem_2555 = unbind_int_123[3];  unbind_int_123 = None
        view_2677 = torch.ops.aten.view.default(arg869_1, [1, 16, 1, 24]);  arg869_1 = None
        add_276 = torch.ops.aten.add.Tensor(getitem_2552, view_2677);  getitem_2552 = view_2677 = None
        _to_copy_1513 = torch.ops.aten._to_copy.default(add_276, dtype = torch.bfloat16);  add_276 = None
        expand_166 = torch.ops.aten.expand.default(masked_fill_169, [1, 16, 512, 512]);  masked_fill_169 = None
        _scaled_dot_product_efficient_attention_default_95 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1513, getitem_2553, getitem_2554, expand_166, False);  _to_copy_1513 = getitem_2553 = getitem_2554 = expand_166 = None
        getitem_2556 = _scaled_dot_product_efficient_attention_default_95[0]
        add_277 = torch.ops.aten.add.Tensor(getitem_2555, 1);  getitem_2555 = None
        sigmoid_209 = torch.ops.aten.sigmoid.default(add_277);  add_277 = None
        mul_344 = torch.ops.aten.mul.Tensor(getitem_2556, sigmoid_209);  getitem_2556 = sigmoid_209 = None
        _to_copy_1514 = torch.ops.aten._to_copy.default(arg871_1, dtype = torch.bfloat16);  arg871_1 = None
        unsqueeze_886 = torch.ops.aten.unsqueeze.default(mul_344, 4);  mul_344 = None
        permute_1443 = torch.ops.aten.permute.default(unsqueeze_886, [0, 2, 4, 3, 1]);  unsqueeze_886 = None
        unsqueeze_887 = torch.ops.aten.unsqueeze.default(_to_copy_1514, 3);  _to_copy_1514 = None
        unsqueeze_888 = torch.ops.aten.unsqueeze.default(unsqueeze_887, 4);  unsqueeze_887 = None
        permute_1444 = torch.ops.aten.permute.default(unsqueeze_888, [3, 4, 2, 1, 0]);  unsqueeze_888 = None
        permute_1445 = torch.ops.aten.permute.default(permute_1443, [1, 3, 4, 0, 2]);  permute_1443 = None
        clone_233 = torch.ops.aten.clone.default(permute_1445, memory_format = torch.contiguous_format);  permute_1445 = None
        _unsafe_view_198 = torch.ops.aten._unsafe_view.default(clone_233, [1, 512, 384]);  clone_233 = None
        permute_1446 = torch.ops.aten.permute.default(permute_1444, [3, 4, 0, 2, 1]);  permute_1444 = None
        clone_234 = torch.ops.aten.clone.default(permute_1446, memory_format = torch.contiguous_format);  permute_1446 = None
        _unsafe_view_199 = torch.ops.aten._unsafe_view.default(clone_234, [1, 384, 384]);  clone_234 = None
        bmm_217 = torch.ops.aten.bmm.default(_unsafe_view_198, _unsafe_view_199);  _unsafe_view_198 = _unsafe_view_199 = None
        view_2678 = torch.ops.aten.view.default(bmm_217, [512, 1, 1, 1, 384]);  bmm_217 = None
        permute_1447 = torch.ops.aten.permute.default(view_2678, [3, 0, 4, 1, 2]);  view_2678 = None
        view_2679 = torch.ops.aten.view.default(permute_1447, [1, 512, 384]);  permute_1447 = None
        unsqueeze_889 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_345 = torch.ops.aten.mul.Tensor(view_2679, unsqueeze_889);  view_2679 = unsqueeze_889 = None
        add_278 = torch.ops.aten.add.Tensor(add_271, mul_345);  mul_345 = None
        split_tensor_273 = torch.ops.aten.split.Tensor(add_271, 512, dim = -2);  add_271 = None
        getitem_2560 = split_tensor_273[0];  split_tensor_273 = None
        _to_copy_1515 = torch.ops.aten._to_copy.default(getitem_2560, dtype = torch.float32);  getitem_2560 = None
        native_layer_norm_default_313 = torch.ops.aten.native_layer_norm.default(_to_copy_1515, [384], arg860_1, arg861_1, 1e-05);  _to_copy_1515 = arg860_1 = arg861_1 = None
        getitem_2561 = native_layer_norm_default_313[0]
        _to_copy_1516 = torch.ops.aten._to_copy.default(arg862_1, dtype = torch.bfloat16);  arg862_1 = None
        _to_copy_1517 = torch.ops.aten._to_copy.default(getitem_2561, dtype = torch.bfloat16);  getitem_2561 = None
        t_559 = torch.ops.aten.t.default(_to_copy_1516);  _to_copy_1516 = None
        view_2680 = torch.ops.aten.view.default(_to_copy_1517, [512, 384]);  _to_copy_1517 = None
        mm_521 = torch.ops.aten.mm.default(view_2680, t_559);  view_2680 = t_559 = None
        view_2681 = torch.ops.aten.view.default(mm_521, [1, 512, 1536]);  mm_521 = None
        split_tensor_274 = torch.ops.aten.split.Tensor(view_2681, 768, dim = -1);  view_2681 = None
        getitem_2564 = split_tensor_274[0]
        getitem_2565 = split_tensor_274[1];  split_tensor_274 = None
        silu_73 = torch.ops.aten.silu.default(getitem_2564);  getitem_2564 = None
        mul_346 = torch.ops.aten.mul.Tensor(silu_73, getitem_2565);  silu_73 = getitem_2565 = None
        _to_copy_1518 = torch.ops.aten._to_copy.default(arg863_1, dtype = torch.bfloat16);  arg863_1 = None
        t_560 = torch.ops.aten.t.default(_to_copy_1518);  _to_copy_1518 = None
        view_2683 = torch.ops.aten.view.default(mul_346, [512, 768]);  mul_346 = None
        mm_522 = torch.ops.aten.mm.default(view_2683, t_560);  view_2683 = t_560 = None
        view_2684 = torch.ops.aten.view.default(mm_522, [1, 512, 384]);  mm_522 = None
        add_279 = torch.ops.aten.add.Tensor(add_278, view_2684);  add_278 = view_2684 = None
        _to_copy_1519 = torch.ops.aten._to_copy.default(add_275, dtype = torch.float32)
        native_layer_norm_default_314 = torch.ops.aten.native_layer_norm.default(_to_copy_1519, [256], arg876_1, arg877_1, 1e-05);  _to_copy_1519 = arg876_1 = arg877_1 = None
        getitem_2566 = native_layer_norm_default_314[0]
        split_with_sizes_default_68 = torch.ops.aten.split_with_sizes.default(arg879_1, [512, 512]);  arg879_1 = None
        getitem_2569 = split_with_sizes_default_68[0]
        getitem_2570 = split_with_sizes_default_68[1];  split_with_sizes_default_68 = None
        split_with_sizes_default_69 = torch.ops.aten.split_with_sizes.default(arg880_1, [512, 512, 256]);  arg880_1 = None
        getitem_2571 = split_with_sizes_default_69[0]
        getitem_2572 = split_with_sizes_default_69[1]
        getitem_2573 = split_with_sizes_default_69[2];  split_with_sizes_default_69 = None
        _to_copy_1520 = torch.ops.aten._to_copy.default(getitem_2569, dtype = torch.bfloat16);  getitem_2569 = None
        _to_copy_1521 = torch.ops.aten._to_copy.default(getitem_2566, dtype = torch.bfloat16)
        t_561 = torch.ops.aten.t.default(_to_copy_1520);  _to_copy_1520 = None
        view_2685 = torch.ops.aten.view.default(_to_copy_1521, [262144, 256]);  _to_copy_1521 = None
        mm_523 = torch.ops.aten.mm.default(view_2685, t_561);  view_2685 = t_561 = None
        view_2686 = torch.ops.aten.view.default(mm_523, [1, 512, 512, 512]);  mm_523 = None
        _to_copy_1522 = torch.ops.aten._to_copy.default(getitem_2571, dtype = torch.bfloat16);  getitem_2571 = None
        _to_copy_1523 = torch.ops.aten._to_copy.default(getitem_2566, dtype = torch.bfloat16)
        t_562 = torch.ops.aten.t.default(_to_copy_1522);  _to_copy_1522 = None
        view_2687 = torch.ops.aten.view.default(_to_copy_1523, [262144, 256]);  _to_copy_1523 = None
        mm_524 = torch.ops.aten.mm.default(view_2687, t_562);  view_2687 = t_562 = None
        view_2688 = torch.ops.aten.view.default(mm_524, [1, 512, 512, 512]);  mm_524 = None
        sigmoid_210 = torch.ops.aten.sigmoid.default(view_2688);  view_2688 = None
        mul_347 = torch.ops.aten.mul.Tensor(view_2686, sigmoid_210);  view_2686 = sigmoid_210 = None
        unsqueeze_890 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_170 = torch.ops.aten.bitwise_not.default(unsqueeze_890);  unsqueeze_890 = None
        masked_fill_170 = torch.ops.aten.masked_fill.Scalar(mul_347, bitwise_not_170, 0);  mul_347 = bitwise_not_170 = None
        split_tensor_275 = torch.ops.aten.split.Tensor(masked_fill_170, 256, dim = -1)
        getitem_2576 = split_tensor_275[0]
        unsqueeze_893 = torch.ops.aten.unsqueeze.default(getitem_2576, 4);  getitem_2576 = None
        permute_1452 = torch.ops.aten.permute.default(unsqueeze_893, [0, 1, 4, 3, 2]);  unsqueeze_893 = None
        permute_1453 = torch.ops.aten.permute.default(permute_1452, [3, 1, 4, 0, 2]);  permute_1452 = None
        view_2691 = torch.ops.aten.view.default(permute_1453, [256, 512, 512]);  permute_1453 = None
        split_tensor_276 = torch.ops.aten.split.Tensor(masked_fill_170, 256, dim = -1);  masked_fill_170 = None
        getitem_2579 = split_tensor_276[1];  split_tensor_276 = None
        unsqueeze_894 = torch.ops.aten.unsqueeze.default(getitem_2579, 4);  getitem_2579 = None
        permute_1454 = torch.ops.aten.permute.default(unsqueeze_894, [0, 4, 1, 3, 2]);  unsqueeze_894 = None
        permute_1455 = torch.ops.aten.permute.default(permute_1454, [3, 4, 0, 2, 1]);  permute_1454 = None
        view_2692 = torch.ops.aten.view.default(permute_1455, [256, 512, 512]);  permute_1455 = None
        bmm_218 = torch.ops.aten.bmm.default(view_2691, view_2692);  view_2691 = view_2692 = None
        view_2693 = torch.ops.aten.view.default(bmm_218, [256, 512, 1, 1, 512]);  bmm_218 = None
        permute_1456 = torch.ops.aten.permute.default(view_2693, [3, 1, 4, 0, 2]);  view_2693 = None
        view_2694 = torch.ops.aten.view.default(permute_1456, [1, 512, 512, 256]);  permute_1456 = None
        _to_copy_1524 = torch.ops.aten._to_copy.default(getitem_2570, dtype = torch.bfloat16);  getitem_2570 = None
        _to_copy_1525 = torch.ops.aten._to_copy.default(getitem_2566, dtype = torch.bfloat16)
        t_563 = torch.ops.aten.t.default(_to_copy_1524);  _to_copy_1524 = None
        view_2695 = torch.ops.aten.view.default(_to_copy_1525, [262144, 256]);  _to_copy_1525 = None
        mm_525 = torch.ops.aten.mm.default(view_2695, t_563);  view_2695 = t_563 = None
        view_2696 = torch.ops.aten.view.default(mm_525, [1, 512, 512, 512]);  mm_525 = None
        _to_copy_1526 = torch.ops.aten._to_copy.default(getitem_2572, dtype = torch.bfloat16);  getitem_2572 = None
        _to_copy_1527 = torch.ops.aten._to_copy.default(getitem_2566, dtype = torch.bfloat16)
        t_564 = torch.ops.aten.t.default(_to_copy_1526);  _to_copy_1526 = None
        view_2697 = torch.ops.aten.view.default(_to_copy_1527, [262144, 256]);  _to_copy_1527 = None
        mm_526 = torch.ops.aten.mm.default(view_2697, t_564);  view_2697 = t_564 = None
        view_2698 = torch.ops.aten.view.default(mm_526, [1, 512, 512, 512]);  mm_526 = None
        sigmoid_211 = torch.ops.aten.sigmoid.default(view_2698);  view_2698 = None
        mul_348 = torch.ops.aten.mul.Tensor(view_2696, sigmoid_211);  view_2696 = sigmoid_211 = None
        view_2699 = torch.ops.aten.view.default(mul_348, [262144, 512]);  mul_348 = None
        view_2700 = torch.ops.aten.view.default(view_2699, [1, 512, 512, 512]);  view_2699 = None
        transpose_68 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_895 = torch.ops.aten.unsqueeze.default(transpose_68, 3);  transpose_68 = None
        clone_235 = torch.ops.aten.clone.default(unsqueeze_895, memory_format = torch.contiguous_format);  unsqueeze_895 = None
        bitwise_not_171 = torch.ops.aten.bitwise_not.default(clone_235);  clone_235 = None
        masked_fill_171 = torch.ops.aten.masked_fill.Scalar(view_2700, bitwise_not_171, 0);  view_2700 = bitwise_not_171 = None
        view_2701 = torch.ops.aten.view.default(masked_fill_171, [262144, 512]);  masked_fill_171 = None
        view_2705 = torch.ops.aten.view.default(view_2701, [1, 512, 512, 512])
        split_tensor_277 = torch.ops.aten.split.Tensor(view_2705, 256, dim = -1);  view_2705 = None
        getitem_2582 = split_tensor_277[0]
        unsqueeze_898 = torch.ops.aten.unsqueeze.default(getitem_2582, 4);  getitem_2582 = None
        permute_1461 = torch.ops.aten.permute.default(unsqueeze_898, [0, 2, 4, 3, 1]);  unsqueeze_898 = None
        permute_1462 = torch.ops.aten.permute.default(permute_1461, [3, 1, 4, 0, 2]);  permute_1461 = None
        view_2706 = torch.ops.aten.view.default(permute_1462, [256, 512, 512]);  permute_1462 = None
        view_2707 = torch.ops.aten.view.default(view_2701, [1, 512, 512, 512]);  view_2701 = None
        split_tensor_278 = torch.ops.aten.split.Tensor(view_2707, 256, dim = -1);  view_2707 = None
        getitem_2585 = split_tensor_278[1];  split_tensor_278 = None
        unsqueeze_899 = torch.ops.aten.unsqueeze.default(getitem_2585, 4);  getitem_2585 = None
        permute_1463 = torch.ops.aten.permute.default(unsqueeze_899, [0, 4, 2, 3, 1]);  unsqueeze_899 = None
        permute_1464 = torch.ops.aten.permute.default(permute_1463, [3, 4, 0, 2, 1]);  permute_1463 = None
        view_2708 = torch.ops.aten.view.default(permute_1464, [256, 512, 512]);  permute_1464 = None
        bmm_219 = torch.ops.aten.bmm.default(view_2706, view_2708);  view_2706 = view_2708 = None
        view_2709 = torch.ops.aten.view.default(bmm_219, [256, 512, 1, 1, 512]);  bmm_219 = None
        permute_1465 = torch.ops.aten.permute.default(view_2709, [3, 1, 4, 0, 2]);  view_2709 = None
        view_2710 = torch.ops.aten.view.default(permute_1465, [1, 512, 512, 256]);  permute_1465 = None
        _to_copy_1528 = torch.ops.aten._to_copy.default(view_2694, dtype = torch.float32);  view_2694 = None
        native_layer_norm_default_315 = torch.ops.aten.native_layer_norm.default(_to_copy_1528, [256], None, None, 1e-05);  _to_copy_1528 = None
        getitem_2586 = native_layer_norm_default_315[0]
        _to_copy_1529 = torch.ops.aten._to_copy.default(view_2710, dtype = torch.float32);  view_2710 = None
        native_layer_norm_default_316 = torch.ops.aten.native_layer_norm.default(_to_copy_1529, [256], None, None, 1e-05);  _to_copy_1529 = None
        getitem_2589 = native_layer_norm_default_316[0]
        add_280 = torch.ops.aten.add.Tensor(getitem_2586, getitem_2589);  getitem_2586 = getitem_2589 = None
        _to_copy_1530 = torch.ops.aten._to_copy.default(arg878_1, dtype = torch.bfloat16);  arg878_1 = None
        _to_copy_1531 = torch.ops.aten._to_copy.default(add_280, dtype = torch.bfloat16);  add_280 = None
        t_565 = torch.ops.aten.t.default(_to_copy_1530);  _to_copy_1530 = None
        view_2711 = torch.ops.aten.view.default(_to_copy_1531, [262144, 256]);  _to_copy_1531 = None
        mm_527 = torch.ops.aten.mm.default(view_2711, t_565);  view_2711 = t_565 = None
        view_2712 = torch.ops.aten.view.default(mm_527, [1, 512, 512, 256]);  mm_527 = None
        _to_copy_1532 = torch.ops.aten._to_copy.default(getitem_2573, dtype = torch.bfloat16);  getitem_2573 = None
        _to_copy_1533 = torch.ops.aten._to_copy.default(getitem_2566, dtype = torch.bfloat16);  getitem_2566 = None
        t_566 = torch.ops.aten.t.default(_to_copy_1532);  _to_copy_1532 = None
        view_2713 = torch.ops.aten.view.default(_to_copy_1533, [262144, 256]);  _to_copy_1533 = None
        mm_528 = torch.ops.aten.mm.default(view_2713, t_566);  view_2713 = t_566 = None
        view_2714 = torch.ops.aten.view.default(mm_528, [1, 512, 512, 256]);  mm_528 = None
        sigmoid_212 = torch.ops.aten.sigmoid.default(view_2714);  view_2714 = None
        mul_349 = torch.ops.aten.mul.Tensor(view_2712, sigmoid_212);  view_2712 = sigmoid_212 = None
        add_281 = torch.ops.aten.add.Tensor(add_275, mul_349);  mul_349 = None
        _to_copy_1534 = torch.ops.aten._to_copy.default(add_275, dtype = torch.float32)
        native_layer_norm_default_317 = torch.ops.aten.native_layer_norm.default(_to_copy_1534, [256], None, None, 1e-05);  _to_copy_1534 = None
        getitem_2592 = native_layer_norm_default_317[0]
        _to_copy_1535 = torch.ops.aten._to_copy.default(arg882_1, dtype = torch.bfloat16);  arg882_1 = None
        _to_copy_1536 = torch.ops.aten._to_copy.default(getitem_2592, dtype = torch.bfloat16)
        t_567 = torch.ops.aten.t.default(_to_copy_1535);  _to_copy_1535 = None
        view_2715 = torch.ops.aten.view.default(_to_copy_1536, [262144, 256]);  _to_copy_1536 = None
        mm_529 = torch.ops.aten.mm.default(view_2715, t_567);  view_2715 = t_567 = None
        view_2716 = torch.ops.aten.view.default(mm_529, [1, 512, 512, 8]);  mm_529 = None
        view_2717 = torch.ops.aten.view.default(view_2716, [1, 512, 512, 2, 4]);  view_2716 = None
        permute_1466 = torch.ops.aten.permute.default(view_2717, [0, 3, 4, 1, 2]);  view_2717 = None
        view_2718 = torch.ops.aten.view.default(permute_1466, [1, 2, 4, 1, 512, 512]);  permute_1466 = None
        view_2719 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_172 = torch.ops.aten.bitwise_not.default(view_2719);  view_2719 = None
        masked_fill_172 = torch.ops.aten.masked_fill.Scalar(view_2718, bitwise_not_172, -10000);  view_2718 = bitwise_not_172 = None
        view_2720 = torch.ops.aten.view.default(masked_fill_172, [1, 2, 4, 512, 512]);  masked_fill_172 = None
        permute_1467 = torch.ops.aten.permute.default(view_2720, [1, 0, 2, 3, 4]);  view_2720 = None
        view_2721 = torch.ops.aten.view.default(permute_1467, [2, 4, 1, 512, 512]);  permute_1467 = None
        _to_copy_1537 = torch.ops.aten._to_copy.default(arg883_1, dtype = torch.bfloat16);  arg883_1 = None
        _to_copy_1538 = torch.ops.aten._to_copy.default(getitem_2592, dtype = torch.bfloat16)
        t_568 = torch.ops.aten.t.default(_to_copy_1537);  _to_copy_1537 = None
        view_2722 = torch.ops.aten.view.default(_to_copy_1538, [262144, 256]);  _to_copy_1538 = None
        mm_530 = torch.ops.aten.mm.default(view_2722, t_568);  view_2722 = t_568 = None
        view_2723 = torch.ops.aten.view.default(mm_530, [1, 512, 512, 1024]);  mm_530 = None
        select_69 = torch.ops.aten.select.int(view_2721, 0, 0)
        view_2724 = torch.ops.aten.view.default(view_2723, [1, 512, 512, 4, 4, 64]);  view_2723 = None
        permute_1468 = torch.ops.aten.permute.default(view_2724, [4, 0, 3, 1, 2, 5]);  view_2724 = None
        view_2725 = torch.ops.aten.view.default(permute_1468, [4, 4, 512, 512, 64]);  permute_1468 = None
        unbind_int_124 = torch.ops.aten.unbind.int(view_2725);  view_2725 = None
        getitem_2595 = unbind_int_124[0]
        getitem_2596 = unbind_int_124[1]
        getitem_2597 = unbind_int_124[2]
        getitem_2598 = unbind_int_124[3];  unbind_int_124 = None
        expand_167 = torch.ops.aten.expand.default(select_69, [4, 512, 512, 512]);  select_69 = None
        _scaled_dot_product_efficient_attention_default_96 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2595, getitem_2596, getitem_2597, expand_167, False);  getitem_2595 = getitem_2596 = getitem_2597 = expand_167 = None
        getitem_2599 = _scaled_dot_product_efficient_attention_default_96[0]
        sigmoid_213 = torch.ops.aten.sigmoid.default(getitem_2598);  getitem_2598 = None
        mul_350 = torch.ops.aten.mul.Tensor(getitem_2599, sigmoid_213);  getitem_2599 = sigmoid_213 = None
        view_2726 = torch.ops.aten.view.default(mul_350, [1, 4, 512, 512, 64]);  mul_350 = None
        permute_1469 = torch.ops.aten.permute.default(view_2726, [0, 2, 3, 1, 4]);  view_2726 = None
        clone_236 = torch.ops.aten.clone.default(permute_1469, memory_format = torch.contiguous_format);  permute_1469 = None
        _unsafe_view_200 = torch.ops.aten._unsafe_view.default(clone_236, [1, 512, 512, 256]);  clone_236 = None
        transpose_69 = torch.ops.aten.transpose.int(getitem_2592, 1, 2);  getitem_2592 = None
        _to_copy_1539 = torch.ops.aten._to_copy.default(arg884_1, dtype = torch.bfloat16);  arg884_1 = None
        _to_copy_1540 = torch.ops.aten._to_copy.default(transpose_69, dtype = torch.bfloat16);  transpose_69 = None
        t_569 = torch.ops.aten.t.default(_to_copy_1539);  _to_copy_1539 = None
        expand_168 = torch.ops.aten.expand.default(_to_copy_1540, [1, 512, 512, 256]);  _to_copy_1540 = None
        view_2727 = torch.ops.aten.view.default(expand_168, [512, 512, 256]);  expand_168 = None
        expand_169 = torch.ops.aten.expand.default(t_569, [1, 512, 256, 1024]);  t_569 = None
        view_2728 = torch.ops.aten.view.default(expand_169, [512, 256, 1024]);  expand_169 = None
        bmm_220 = torch.ops.aten.bmm.default(view_2727, view_2728);  view_2727 = view_2728 = None
        view_2729 = torch.ops.aten.view.default(bmm_220, [1, 512, 512, 1024]);  bmm_220 = None
        select_70 = torch.ops.aten.select.int(view_2721, 0, 1);  view_2721 = None
        view_2730 = torch.ops.aten.view.default(view_2729, [1, 512, 512, 4, 4, 64]);  view_2729 = None
        permute_1470 = torch.ops.aten.permute.default(view_2730, [4, 0, 3, 1, 2, 5]);  view_2730 = None
        view_2731 = torch.ops.aten.view.default(permute_1470, [4, 4, 512, 512, 64]);  permute_1470 = None
        unbind_int_125 = torch.ops.aten.unbind.int(view_2731);  view_2731 = None
        getitem_2603 = unbind_int_125[0]
        getitem_2604 = unbind_int_125[1]
        getitem_2605 = unbind_int_125[2]
        getitem_2606 = unbind_int_125[3];  unbind_int_125 = None
        expand_170 = torch.ops.aten.expand.default(select_70, [4, 512, 512, 512]);  select_70 = None
        _scaled_dot_product_efficient_attention_default_97 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2603, getitem_2604, getitem_2605, expand_170, False);  getitem_2603 = getitem_2604 = getitem_2605 = expand_170 = None
        getitem_2607 = _scaled_dot_product_efficient_attention_default_97[0]
        sigmoid_214 = torch.ops.aten.sigmoid.default(getitem_2606);  getitem_2606 = None
        mul_351 = torch.ops.aten.mul.Tensor(getitem_2607, sigmoid_214);  getitem_2607 = sigmoid_214 = None
        view_2732 = torch.ops.aten.view.default(mul_351, [1, 4, 512, 512, 64]);  mul_351 = None
        permute_1471 = torch.ops.aten.permute.default(view_2732, [0, 2, 3, 1, 4]);  view_2732 = None
        clone_237 = torch.ops.aten.clone.default(permute_1471, memory_format = torch.contiguous_format);  permute_1471 = None
        _unsafe_view_201 = torch.ops.aten._unsafe_view.default(clone_237, [1, 512, 512, 256]);  clone_237 = None
        cat_40 = torch.ops.aten.cat.default([_unsafe_view_200, _unsafe_view_201], dim = -1);  _unsafe_view_200 = _unsafe_view_201 = None
        slice_233 = torch.ops.aten.slice.Tensor(arg881_1, dim = 0, start = 0, end = 9223372036854775807);  arg881_1 = None
        unsqueeze_900 = torch.ops.aten.unsqueeze.default(slice_233, 1);  slice_233 = None
        mul_352 = torch.ops.aten.mul.Tensor(arg885_1, unsqueeze_900);  arg885_1 = unsqueeze_900 = None
        _to_copy_1541 = torch.ops.aten._to_copy.default(mul_352, dtype = torch.bfloat16);  mul_352 = None
        t_570 = torch.ops.aten.t.default(_to_copy_1541);  _to_copy_1541 = None
        view_2733 = torch.ops.aten.view.default(cat_40, [262144, 512]);  cat_40 = None
        mm_531 = torch.ops.aten.mm.default(view_2733, t_570);  view_2733 = t_570 = None
        view_2734 = torch.ops.aten.view.default(mm_531, [1, 512, 512, 256]);  mm_531 = None
        add_282 = torch.ops.aten.add.Tensor(add_281, view_2734);  add_281 = view_2734 = None
        split_tensor_279 = torch.ops.aten.split.Tensor(add_275, 512, dim = -2)
        getitem_2611 = split_tensor_279[0];  split_tensor_279 = None
        _to_copy_1542 = torch.ops.aten._to_copy.default(getitem_2611, dtype = torch.float32);  getitem_2611 = None
        native_layer_norm_default_318 = torch.ops.aten.native_layer_norm.default(_to_copy_1542, [256], arg872_1, arg873_1, 1e-05);  _to_copy_1542 = arg872_1 = arg873_1 = None
        getitem_2612 = native_layer_norm_default_318[0]
        _to_copy_1543 = torch.ops.aten._to_copy.default(arg874_1, dtype = torch.bfloat16);  arg874_1 = None
        _to_copy_1544 = torch.ops.aten._to_copy.default(getitem_2612, dtype = torch.bfloat16);  getitem_2612 = None
        t_571 = torch.ops.aten.t.default(_to_copy_1543);  _to_copy_1543 = None
        view_2735 = torch.ops.aten.view.default(_to_copy_1544, [262144, 256]);  _to_copy_1544 = None
        mm_532 = torch.ops.aten.mm.default(view_2735, t_571);  view_2735 = t_571 = None
        view_2736 = torch.ops.aten.view.default(mm_532, [1, 512, 512, 1024]);  mm_532 = None
        split_tensor_280 = torch.ops.aten.split.Tensor(view_2736, 512, dim = -1);  view_2736 = None
        getitem_2615 = split_tensor_280[0]
        getitem_2616 = split_tensor_280[1];  split_tensor_280 = None
        silu_74 = torch.ops.aten.silu.default(getitem_2615);  getitem_2615 = None
        mul_353 = torch.ops.aten.mul.Tensor(silu_74, getitem_2616);  silu_74 = getitem_2616 = None
        _to_copy_1545 = torch.ops.aten._to_copy.default(arg875_1, dtype = torch.bfloat16);  arg875_1 = None
        t_572 = torch.ops.aten.t.default(_to_copy_1545);  _to_copy_1545 = None
        view_2738 = torch.ops.aten.view.default(mul_353, [262144, 512]);  mul_353 = None
        mm_533 = torch.ops.aten.mm.default(view_2738, t_572);  view_2738 = t_572 = None
        view_2739 = torch.ops.aten.view.default(mm_533, [1, 512, 512, 256]);  mm_533 = None
        add_283 = torch.ops.aten.add.Tensor(add_282, view_2739);  add_282 = view_2739 = None
        _to_copy_1546 = torch.ops.aten._to_copy.default(add_279, dtype = torch.float32)
        native_layer_norm_default_319 = torch.ops.aten.native_layer_norm.default(_to_copy_1546, [384], arg890_1, arg891_1, 1e-05);  _to_copy_1546 = arg890_1 = arg891_1 = None
        getitem_2617 = native_layer_norm_default_319[0]
        _to_copy_1547 = torch.ops.aten._to_copy.default(add_275, dtype = torch.float32);  add_275 = None
        native_layer_norm_default_320 = torch.ops.aten.native_layer_norm.default(_to_copy_1547, [256], arg892_1, arg893_1, 1e-05);  _to_copy_1547 = arg892_1 = arg893_1 = None
        getitem_2620 = native_layer_norm_default_320[0]
        _to_copy_1548 = torch.ops.aten._to_copy.default(arg894_1, dtype = torch.bfloat16);  arg894_1 = None
        _to_copy_1549 = torch.ops.aten._to_copy.default(getitem_2620, dtype = torch.bfloat16);  getitem_2620 = None
        t_573 = torch.ops.aten.t.default(_to_copy_1548);  _to_copy_1548 = None
        view_2740 = torch.ops.aten.view.default(_to_copy_1549, [262144, 256]);  _to_copy_1549 = None
        mm_534 = torch.ops.aten.mm.default(view_2740, t_573);  view_2740 = t_573 = None
        view_2741 = torch.ops.aten.view.default(mm_534, [1, 512, 512, 16]);  mm_534 = None
        permute_1472 = torch.ops.aten.permute.default(view_2741, [0, 3, 1, 2]);  view_2741 = None
        view_2742 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_173 = torch.ops.aten.bitwise_not.default(view_2742);  view_2742 = None
        masked_fill_173 = torch.ops.aten.masked_fill.Scalar(permute_1472, bitwise_not_173, -10000);  permute_1472 = bitwise_not_173 = None
        _to_copy_1550 = torch.ops.aten._to_copy.default(getitem_2617, dtype = torch.bfloat16);  getitem_2617 = None
        _to_copy_1551 = torch.ops.aten._to_copy.default(arg896_1, dtype = torch.bfloat16);  arg896_1 = None
        unsqueeze_901 = torch.ops.aten.unsqueeze.default(_to_copy_1550, 3);  _to_copy_1550 = None
        unsqueeze_902 = torch.ops.aten.unsqueeze.default(unsqueeze_901, 4);  unsqueeze_901 = None
        unsqueeze_903 = torch.ops.aten.unsqueeze.default(unsqueeze_902, 5);  unsqueeze_902 = None
        permute_1473 = torch.ops.aten.permute.default(unsqueeze_903, [3, 0, 4, 1, 5, 2]);  unsqueeze_903 = None
        unsqueeze_904 = torch.ops.aten.unsqueeze.default(_to_copy_1551, 4);  _to_copy_1551 = None
        unsqueeze_905 = torch.ops.aten.unsqueeze.default(unsqueeze_904, 5);  unsqueeze_904 = None
        permute_1474 = torch.ops.aten.permute.default(unsqueeze_905, [1, 4, 2, 5, 3, 0]);  unsqueeze_905 = None
        permute_1475 = torch.ops.aten.permute.default(permute_1473, [3, 5, 0, 1, 2, 4]);  permute_1473 = None
        view_2743 = torch.ops.aten.view.default(permute_1475, [1, 512, 384]);  permute_1475 = None
        permute_1476 = torch.ops.aten.permute.default(permute_1474, [5, 0, 1, 2, 4, 3]);  permute_1474 = None
        view_2744 = torch.ops.aten.view.default(permute_1476, [1, 384, 1536]);  permute_1476 = None
        bmm_221 = torch.ops.aten.bmm.default(view_2743, view_2744);  view_2743 = view_2744 = None
        view_2745 = torch.ops.aten.view.default(bmm_221, [512, 1, 4, 1, 16, 24]);  bmm_221 = None
        permute_1477 = torch.ops.aten.permute.default(view_2745, [2, 3, 4, 0, 5, 1]);  view_2745 = None
        view_2746 = torch.ops.aten.view.default(permute_1477, [4, 1, 16, 512, 24]);  permute_1477 = None
        unbind_int_126 = torch.ops.aten.unbind.int(view_2746);  view_2746 = None
        getitem_2623 = unbind_int_126[0]
        getitem_2624 = unbind_int_126[1]
        getitem_2625 = unbind_int_126[2]
        getitem_2626 = unbind_int_126[3];  unbind_int_126 = None
        view_2747 = torch.ops.aten.view.default(arg895_1, [1, 16, 1, 24]);  arg895_1 = None
        add_284 = torch.ops.aten.add.Tensor(getitem_2623, view_2747);  getitem_2623 = view_2747 = None
        _to_copy_1552 = torch.ops.aten._to_copy.default(add_284, dtype = torch.bfloat16);  add_284 = None
        expand_171 = torch.ops.aten.expand.default(masked_fill_173, [1, 16, 512, 512]);  masked_fill_173 = None
        _scaled_dot_product_efficient_attention_default_98 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1552, getitem_2624, getitem_2625, expand_171, False);  _to_copy_1552 = getitem_2624 = getitem_2625 = expand_171 = None
        getitem_2627 = _scaled_dot_product_efficient_attention_default_98[0]
        add_285 = torch.ops.aten.add.Tensor(getitem_2626, 1);  getitem_2626 = None
        sigmoid_215 = torch.ops.aten.sigmoid.default(add_285);  add_285 = None
        mul_354 = torch.ops.aten.mul.Tensor(getitem_2627, sigmoid_215);  getitem_2627 = sigmoid_215 = None
        _to_copy_1553 = torch.ops.aten._to_copy.default(arg897_1, dtype = torch.bfloat16);  arg897_1 = None
        unsqueeze_906 = torch.ops.aten.unsqueeze.default(mul_354, 4);  mul_354 = None
        permute_1478 = torch.ops.aten.permute.default(unsqueeze_906, [0, 2, 4, 3, 1]);  unsqueeze_906 = None
        unsqueeze_907 = torch.ops.aten.unsqueeze.default(_to_copy_1553, 3);  _to_copy_1553 = None
        unsqueeze_908 = torch.ops.aten.unsqueeze.default(unsqueeze_907, 4);  unsqueeze_907 = None
        permute_1479 = torch.ops.aten.permute.default(unsqueeze_908, [3, 4, 2, 1, 0]);  unsqueeze_908 = None
        permute_1480 = torch.ops.aten.permute.default(permute_1478, [1, 3, 4, 0, 2]);  permute_1478 = None
        clone_238 = torch.ops.aten.clone.default(permute_1480, memory_format = torch.contiguous_format);  permute_1480 = None
        _unsafe_view_202 = torch.ops.aten._unsafe_view.default(clone_238, [1, 512, 384]);  clone_238 = None
        permute_1481 = torch.ops.aten.permute.default(permute_1479, [3, 4, 0, 2, 1]);  permute_1479 = None
        clone_239 = torch.ops.aten.clone.default(permute_1481, memory_format = torch.contiguous_format);  permute_1481 = None
        _unsafe_view_203 = torch.ops.aten._unsafe_view.default(clone_239, [1, 384, 384]);  clone_239 = None
        bmm_222 = torch.ops.aten.bmm.default(_unsafe_view_202, _unsafe_view_203);  _unsafe_view_202 = _unsafe_view_203 = None
        view_2748 = torch.ops.aten.view.default(bmm_222, [512, 1, 1, 1, 384]);  bmm_222 = None
        permute_1482 = torch.ops.aten.permute.default(view_2748, [3, 0, 4, 1, 2]);  view_2748 = None
        view_2749 = torch.ops.aten.view.default(permute_1482, [1, 512, 384]);  permute_1482 = None
        unsqueeze_909 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_355 = torch.ops.aten.mul.Tensor(view_2749, unsqueeze_909);  view_2749 = unsqueeze_909 = None
        add_286 = torch.ops.aten.add.Tensor(add_279, mul_355);  mul_355 = None
        split_tensor_281 = torch.ops.aten.split.Tensor(add_279, 512, dim = -2);  add_279 = None
        getitem_2631 = split_tensor_281[0];  split_tensor_281 = None
        _to_copy_1554 = torch.ops.aten._to_copy.default(getitem_2631, dtype = torch.float32);  getitem_2631 = None
        native_layer_norm_default_321 = torch.ops.aten.native_layer_norm.default(_to_copy_1554, [384], arg886_1, arg887_1, 1e-05);  _to_copy_1554 = arg886_1 = arg887_1 = None
        getitem_2632 = native_layer_norm_default_321[0]
        _to_copy_1555 = torch.ops.aten._to_copy.default(arg888_1, dtype = torch.bfloat16);  arg888_1 = None
        _to_copy_1556 = torch.ops.aten._to_copy.default(getitem_2632, dtype = torch.bfloat16);  getitem_2632 = None
        t_574 = torch.ops.aten.t.default(_to_copy_1555);  _to_copy_1555 = None
        view_2750 = torch.ops.aten.view.default(_to_copy_1556, [512, 384]);  _to_copy_1556 = None
        mm_535 = torch.ops.aten.mm.default(view_2750, t_574);  view_2750 = t_574 = None
        view_2751 = torch.ops.aten.view.default(mm_535, [1, 512, 1536]);  mm_535 = None
        split_tensor_282 = torch.ops.aten.split.Tensor(view_2751, 768, dim = -1);  view_2751 = None
        getitem_2635 = split_tensor_282[0]
        getitem_2636 = split_tensor_282[1];  split_tensor_282 = None
        silu_75 = torch.ops.aten.silu.default(getitem_2635);  getitem_2635 = None
        mul_356 = torch.ops.aten.mul.Tensor(silu_75, getitem_2636);  silu_75 = getitem_2636 = None
        _to_copy_1557 = torch.ops.aten._to_copy.default(arg889_1, dtype = torch.bfloat16);  arg889_1 = None
        t_575 = torch.ops.aten.t.default(_to_copy_1557);  _to_copy_1557 = None
        view_2753 = torch.ops.aten.view.default(mul_356, [512, 768]);  mul_356 = None
        mm_536 = torch.ops.aten.mm.default(view_2753, t_575);  view_2753 = t_575 = None
        view_2754 = torch.ops.aten.view.default(mm_536, [1, 512, 384]);  mm_536 = None
        add_287 = torch.ops.aten.add.Tensor(add_286, view_2754);  add_286 = view_2754 = None
        _to_copy_1558 = torch.ops.aten._to_copy.default(add_283, dtype = torch.float32)
        native_layer_norm_default_322 = torch.ops.aten.native_layer_norm.default(_to_copy_1558, [256], arg902_1, arg903_1, 1e-05);  _to_copy_1558 = arg902_1 = arg903_1 = None
        getitem_2637 = native_layer_norm_default_322[0]
        split_with_sizes_default_70 = torch.ops.aten.split_with_sizes.default(arg905_1, [512, 512]);  arg905_1 = None
        getitem_2640 = split_with_sizes_default_70[0]
        getitem_2641 = split_with_sizes_default_70[1];  split_with_sizes_default_70 = None
        split_with_sizes_default_71 = torch.ops.aten.split_with_sizes.default(arg906_1, [512, 512, 256]);  arg906_1 = None
        getitem_2642 = split_with_sizes_default_71[0]
        getitem_2643 = split_with_sizes_default_71[1]
        getitem_2644 = split_with_sizes_default_71[2];  split_with_sizes_default_71 = None
        _to_copy_1559 = torch.ops.aten._to_copy.default(getitem_2640, dtype = torch.bfloat16);  getitem_2640 = None
        _to_copy_1560 = torch.ops.aten._to_copy.default(getitem_2637, dtype = torch.bfloat16)
        t_576 = torch.ops.aten.t.default(_to_copy_1559);  _to_copy_1559 = None
        view_2755 = torch.ops.aten.view.default(_to_copy_1560, [262144, 256]);  _to_copy_1560 = None
        mm_537 = torch.ops.aten.mm.default(view_2755, t_576);  view_2755 = t_576 = None
        view_2756 = torch.ops.aten.view.default(mm_537, [1, 512, 512, 512]);  mm_537 = None
        _to_copy_1561 = torch.ops.aten._to_copy.default(getitem_2642, dtype = torch.bfloat16);  getitem_2642 = None
        _to_copy_1562 = torch.ops.aten._to_copy.default(getitem_2637, dtype = torch.bfloat16)
        t_577 = torch.ops.aten.t.default(_to_copy_1561);  _to_copy_1561 = None
        view_2757 = torch.ops.aten.view.default(_to_copy_1562, [262144, 256]);  _to_copy_1562 = None
        mm_538 = torch.ops.aten.mm.default(view_2757, t_577);  view_2757 = t_577 = None
        view_2758 = torch.ops.aten.view.default(mm_538, [1, 512, 512, 512]);  mm_538 = None
        sigmoid_216 = torch.ops.aten.sigmoid.default(view_2758);  view_2758 = None
        mul_357 = torch.ops.aten.mul.Tensor(view_2756, sigmoid_216);  view_2756 = sigmoid_216 = None
        unsqueeze_910 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_174 = torch.ops.aten.bitwise_not.default(unsqueeze_910);  unsqueeze_910 = None
        masked_fill_174 = torch.ops.aten.masked_fill.Scalar(mul_357, bitwise_not_174, 0);  mul_357 = bitwise_not_174 = None
        split_tensor_283 = torch.ops.aten.split.Tensor(masked_fill_174, 256, dim = -1)
        getitem_2647 = split_tensor_283[0]
        unsqueeze_913 = torch.ops.aten.unsqueeze.default(getitem_2647, 4);  getitem_2647 = None
        permute_1487 = torch.ops.aten.permute.default(unsqueeze_913, [0, 1, 4, 3, 2]);  unsqueeze_913 = None
        permute_1488 = torch.ops.aten.permute.default(permute_1487, [3, 1, 4, 0, 2]);  permute_1487 = None
        view_2761 = torch.ops.aten.view.default(permute_1488, [256, 512, 512]);  permute_1488 = None
        split_tensor_284 = torch.ops.aten.split.Tensor(masked_fill_174, 256, dim = -1);  masked_fill_174 = None
        getitem_2650 = split_tensor_284[1];  split_tensor_284 = None
        unsqueeze_914 = torch.ops.aten.unsqueeze.default(getitem_2650, 4);  getitem_2650 = None
        permute_1489 = torch.ops.aten.permute.default(unsqueeze_914, [0, 4, 1, 3, 2]);  unsqueeze_914 = None
        permute_1490 = torch.ops.aten.permute.default(permute_1489, [3, 4, 0, 2, 1]);  permute_1489 = None
        view_2762 = torch.ops.aten.view.default(permute_1490, [256, 512, 512]);  permute_1490 = None
        bmm_223 = torch.ops.aten.bmm.default(view_2761, view_2762);  view_2761 = view_2762 = None
        view_2763 = torch.ops.aten.view.default(bmm_223, [256, 512, 1, 1, 512]);  bmm_223 = None
        permute_1491 = torch.ops.aten.permute.default(view_2763, [3, 1, 4, 0, 2]);  view_2763 = None
        view_2764 = torch.ops.aten.view.default(permute_1491, [1, 512, 512, 256]);  permute_1491 = None
        _to_copy_1563 = torch.ops.aten._to_copy.default(getitem_2641, dtype = torch.bfloat16);  getitem_2641 = None
        _to_copy_1564 = torch.ops.aten._to_copy.default(getitem_2637, dtype = torch.bfloat16)
        t_578 = torch.ops.aten.t.default(_to_copy_1563);  _to_copy_1563 = None
        view_2765 = torch.ops.aten.view.default(_to_copy_1564, [262144, 256]);  _to_copy_1564 = None
        mm_539 = torch.ops.aten.mm.default(view_2765, t_578);  view_2765 = t_578 = None
        view_2766 = torch.ops.aten.view.default(mm_539, [1, 512, 512, 512]);  mm_539 = None
        _to_copy_1565 = torch.ops.aten._to_copy.default(getitem_2643, dtype = torch.bfloat16);  getitem_2643 = None
        _to_copy_1566 = torch.ops.aten._to_copy.default(getitem_2637, dtype = torch.bfloat16)
        t_579 = torch.ops.aten.t.default(_to_copy_1565);  _to_copy_1565 = None
        view_2767 = torch.ops.aten.view.default(_to_copy_1566, [262144, 256]);  _to_copy_1566 = None
        mm_540 = torch.ops.aten.mm.default(view_2767, t_579);  view_2767 = t_579 = None
        view_2768 = torch.ops.aten.view.default(mm_540, [1, 512, 512, 512]);  mm_540 = None
        sigmoid_217 = torch.ops.aten.sigmoid.default(view_2768);  view_2768 = None
        mul_358 = torch.ops.aten.mul.Tensor(view_2766, sigmoid_217);  view_2766 = sigmoid_217 = None
        view_2769 = torch.ops.aten.view.default(mul_358, [262144, 512]);  mul_358 = None
        view_2770 = torch.ops.aten.view.default(view_2769, [1, 512, 512, 512]);  view_2769 = None
        transpose_70 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_915 = torch.ops.aten.unsqueeze.default(transpose_70, 3);  transpose_70 = None
        clone_240 = torch.ops.aten.clone.default(unsqueeze_915, memory_format = torch.contiguous_format);  unsqueeze_915 = None
        bitwise_not_175 = torch.ops.aten.bitwise_not.default(clone_240);  clone_240 = None
        masked_fill_175 = torch.ops.aten.masked_fill.Scalar(view_2770, bitwise_not_175, 0);  view_2770 = bitwise_not_175 = None
        view_2771 = torch.ops.aten.view.default(masked_fill_175, [262144, 512]);  masked_fill_175 = None
        view_2775 = torch.ops.aten.view.default(view_2771, [1, 512, 512, 512])
        split_tensor_285 = torch.ops.aten.split.Tensor(view_2775, 256, dim = -1);  view_2775 = None
        getitem_2653 = split_tensor_285[0]
        unsqueeze_918 = torch.ops.aten.unsqueeze.default(getitem_2653, 4);  getitem_2653 = None
        permute_1496 = torch.ops.aten.permute.default(unsqueeze_918, [0, 2, 4, 3, 1]);  unsqueeze_918 = None
        permute_1497 = torch.ops.aten.permute.default(permute_1496, [3, 1, 4, 0, 2]);  permute_1496 = None
        view_2776 = torch.ops.aten.view.default(permute_1497, [256, 512, 512]);  permute_1497 = None
        view_2777 = torch.ops.aten.view.default(view_2771, [1, 512, 512, 512]);  view_2771 = None
        split_tensor_286 = torch.ops.aten.split.Tensor(view_2777, 256, dim = -1);  view_2777 = None
        getitem_2656 = split_tensor_286[1];  split_tensor_286 = None
        unsqueeze_919 = torch.ops.aten.unsqueeze.default(getitem_2656, 4);  getitem_2656 = None
        permute_1498 = torch.ops.aten.permute.default(unsqueeze_919, [0, 4, 2, 3, 1]);  unsqueeze_919 = None
        permute_1499 = torch.ops.aten.permute.default(permute_1498, [3, 4, 0, 2, 1]);  permute_1498 = None
        view_2778 = torch.ops.aten.view.default(permute_1499, [256, 512, 512]);  permute_1499 = None
        bmm_224 = torch.ops.aten.bmm.default(view_2776, view_2778);  view_2776 = view_2778 = None
        view_2779 = torch.ops.aten.view.default(bmm_224, [256, 512, 1, 1, 512]);  bmm_224 = None
        permute_1500 = torch.ops.aten.permute.default(view_2779, [3, 1, 4, 0, 2]);  view_2779 = None
        view_2780 = torch.ops.aten.view.default(permute_1500, [1, 512, 512, 256]);  permute_1500 = None
        _to_copy_1567 = torch.ops.aten._to_copy.default(view_2764, dtype = torch.float32);  view_2764 = None
        native_layer_norm_default_323 = torch.ops.aten.native_layer_norm.default(_to_copy_1567, [256], None, None, 1e-05);  _to_copy_1567 = None
        getitem_2657 = native_layer_norm_default_323[0]
        _to_copy_1568 = torch.ops.aten._to_copy.default(view_2780, dtype = torch.float32);  view_2780 = None
        native_layer_norm_default_324 = torch.ops.aten.native_layer_norm.default(_to_copy_1568, [256], None, None, 1e-05);  _to_copy_1568 = None
        getitem_2660 = native_layer_norm_default_324[0]
        add_288 = torch.ops.aten.add.Tensor(getitem_2657, getitem_2660);  getitem_2657 = getitem_2660 = None
        _to_copy_1569 = torch.ops.aten._to_copy.default(arg904_1, dtype = torch.bfloat16);  arg904_1 = None
        _to_copy_1570 = torch.ops.aten._to_copy.default(add_288, dtype = torch.bfloat16);  add_288 = None
        t_580 = torch.ops.aten.t.default(_to_copy_1569);  _to_copy_1569 = None
        view_2781 = torch.ops.aten.view.default(_to_copy_1570, [262144, 256]);  _to_copy_1570 = None
        mm_541 = torch.ops.aten.mm.default(view_2781, t_580);  view_2781 = t_580 = None
        view_2782 = torch.ops.aten.view.default(mm_541, [1, 512, 512, 256]);  mm_541 = None
        _to_copy_1571 = torch.ops.aten._to_copy.default(getitem_2644, dtype = torch.bfloat16);  getitem_2644 = None
        _to_copy_1572 = torch.ops.aten._to_copy.default(getitem_2637, dtype = torch.bfloat16);  getitem_2637 = None
        t_581 = torch.ops.aten.t.default(_to_copy_1571);  _to_copy_1571 = None
        view_2783 = torch.ops.aten.view.default(_to_copy_1572, [262144, 256]);  _to_copy_1572 = None
        mm_542 = torch.ops.aten.mm.default(view_2783, t_581);  view_2783 = t_581 = None
        view_2784 = torch.ops.aten.view.default(mm_542, [1, 512, 512, 256]);  mm_542 = None
        sigmoid_218 = torch.ops.aten.sigmoid.default(view_2784);  view_2784 = None
        mul_359 = torch.ops.aten.mul.Tensor(view_2782, sigmoid_218);  view_2782 = sigmoid_218 = None
        add_289 = torch.ops.aten.add.Tensor(add_283, mul_359);  mul_359 = None
        _to_copy_1573 = torch.ops.aten._to_copy.default(add_283, dtype = torch.float32)
        native_layer_norm_default_325 = torch.ops.aten.native_layer_norm.default(_to_copy_1573, [256], None, None, 1e-05);  _to_copy_1573 = None
        getitem_2663 = native_layer_norm_default_325[0]
        _to_copy_1574 = torch.ops.aten._to_copy.default(arg908_1, dtype = torch.bfloat16);  arg908_1 = None
        _to_copy_1575 = torch.ops.aten._to_copy.default(getitem_2663, dtype = torch.bfloat16)
        t_582 = torch.ops.aten.t.default(_to_copy_1574);  _to_copy_1574 = None
        view_2785 = torch.ops.aten.view.default(_to_copy_1575, [262144, 256]);  _to_copy_1575 = None
        mm_543 = torch.ops.aten.mm.default(view_2785, t_582);  view_2785 = t_582 = None
        view_2786 = torch.ops.aten.view.default(mm_543, [1, 512, 512, 8]);  mm_543 = None
        view_2787 = torch.ops.aten.view.default(view_2786, [1, 512, 512, 2, 4]);  view_2786 = None
        permute_1501 = torch.ops.aten.permute.default(view_2787, [0, 3, 4, 1, 2]);  view_2787 = None
        view_2788 = torch.ops.aten.view.default(permute_1501, [1, 2, 4, 1, 512, 512]);  permute_1501 = None
        view_2789 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_176 = torch.ops.aten.bitwise_not.default(view_2789);  view_2789 = None
        masked_fill_176 = torch.ops.aten.masked_fill.Scalar(view_2788, bitwise_not_176, -10000);  view_2788 = bitwise_not_176 = None
        view_2790 = torch.ops.aten.view.default(masked_fill_176, [1, 2, 4, 512, 512]);  masked_fill_176 = None
        permute_1502 = torch.ops.aten.permute.default(view_2790, [1, 0, 2, 3, 4]);  view_2790 = None
        view_2791 = torch.ops.aten.view.default(permute_1502, [2, 4, 1, 512, 512]);  permute_1502 = None
        _to_copy_1576 = torch.ops.aten._to_copy.default(arg909_1, dtype = torch.bfloat16);  arg909_1 = None
        _to_copy_1577 = torch.ops.aten._to_copy.default(getitem_2663, dtype = torch.bfloat16)
        t_583 = torch.ops.aten.t.default(_to_copy_1576);  _to_copy_1576 = None
        view_2792 = torch.ops.aten.view.default(_to_copy_1577, [262144, 256]);  _to_copy_1577 = None
        mm_544 = torch.ops.aten.mm.default(view_2792, t_583);  view_2792 = t_583 = None
        view_2793 = torch.ops.aten.view.default(mm_544, [1, 512, 512, 1024]);  mm_544 = None
        select_71 = torch.ops.aten.select.int(view_2791, 0, 0)
        view_2794 = torch.ops.aten.view.default(view_2793, [1, 512, 512, 4, 4, 64]);  view_2793 = None
        permute_1503 = torch.ops.aten.permute.default(view_2794, [4, 0, 3, 1, 2, 5]);  view_2794 = None
        view_2795 = torch.ops.aten.view.default(permute_1503, [4, 4, 512, 512, 64]);  permute_1503 = None
        unbind_int_127 = torch.ops.aten.unbind.int(view_2795);  view_2795 = None
        getitem_2666 = unbind_int_127[0]
        getitem_2667 = unbind_int_127[1]
        getitem_2668 = unbind_int_127[2]
        getitem_2669 = unbind_int_127[3];  unbind_int_127 = None
        expand_172 = torch.ops.aten.expand.default(select_71, [4, 512, 512, 512]);  select_71 = None
        _scaled_dot_product_efficient_attention_default_99 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2666, getitem_2667, getitem_2668, expand_172, False);  getitem_2666 = getitem_2667 = getitem_2668 = expand_172 = None
        getitem_2670 = _scaled_dot_product_efficient_attention_default_99[0]
        sigmoid_219 = torch.ops.aten.sigmoid.default(getitem_2669);  getitem_2669 = None
        mul_360 = torch.ops.aten.mul.Tensor(getitem_2670, sigmoid_219);  getitem_2670 = sigmoid_219 = None
        view_2796 = torch.ops.aten.view.default(mul_360, [1, 4, 512, 512, 64]);  mul_360 = None
        permute_1504 = torch.ops.aten.permute.default(view_2796, [0, 2, 3, 1, 4]);  view_2796 = None
        clone_241 = torch.ops.aten.clone.default(permute_1504, memory_format = torch.contiguous_format);  permute_1504 = None
        _unsafe_view_204 = torch.ops.aten._unsafe_view.default(clone_241, [1, 512, 512, 256]);  clone_241 = None
        transpose_71 = torch.ops.aten.transpose.int(getitem_2663, 1, 2);  getitem_2663 = None
        _to_copy_1578 = torch.ops.aten._to_copy.default(arg910_1, dtype = torch.bfloat16);  arg910_1 = None
        _to_copy_1579 = torch.ops.aten._to_copy.default(transpose_71, dtype = torch.bfloat16);  transpose_71 = None
        t_584 = torch.ops.aten.t.default(_to_copy_1578);  _to_copy_1578 = None
        expand_173 = torch.ops.aten.expand.default(_to_copy_1579, [1, 512, 512, 256]);  _to_copy_1579 = None
        view_2797 = torch.ops.aten.view.default(expand_173, [512, 512, 256]);  expand_173 = None
        expand_174 = torch.ops.aten.expand.default(t_584, [1, 512, 256, 1024]);  t_584 = None
        view_2798 = torch.ops.aten.view.default(expand_174, [512, 256, 1024]);  expand_174 = None
        bmm_225 = torch.ops.aten.bmm.default(view_2797, view_2798);  view_2797 = view_2798 = None
        view_2799 = torch.ops.aten.view.default(bmm_225, [1, 512, 512, 1024]);  bmm_225 = None
        select_72 = torch.ops.aten.select.int(view_2791, 0, 1);  view_2791 = None
        view_2800 = torch.ops.aten.view.default(view_2799, [1, 512, 512, 4, 4, 64]);  view_2799 = None
        permute_1505 = torch.ops.aten.permute.default(view_2800, [4, 0, 3, 1, 2, 5]);  view_2800 = None
        view_2801 = torch.ops.aten.view.default(permute_1505, [4, 4, 512, 512, 64]);  permute_1505 = None
        unbind_int_128 = torch.ops.aten.unbind.int(view_2801);  view_2801 = None
        getitem_2674 = unbind_int_128[0]
        getitem_2675 = unbind_int_128[1]
        getitem_2676 = unbind_int_128[2]
        getitem_2677 = unbind_int_128[3];  unbind_int_128 = None
        expand_175 = torch.ops.aten.expand.default(select_72, [4, 512, 512, 512]);  select_72 = None
        _scaled_dot_product_efficient_attention_default_100 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2674, getitem_2675, getitem_2676, expand_175, False);  getitem_2674 = getitem_2675 = getitem_2676 = expand_175 = None
        getitem_2678 = _scaled_dot_product_efficient_attention_default_100[0]
        sigmoid_220 = torch.ops.aten.sigmoid.default(getitem_2677);  getitem_2677 = None
        mul_361 = torch.ops.aten.mul.Tensor(getitem_2678, sigmoid_220);  getitem_2678 = sigmoid_220 = None
        view_2802 = torch.ops.aten.view.default(mul_361, [1, 4, 512, 512, 64]);  mul_361 = None
        permute_1506 = torch.ops.aten.permute.default(view_2802, [0, 2, 3, 1, 4]);  view_2802 = None
        clone_242 = torch.ops.aten.clone.default(permute_1506, memory_format = torch.contiguous_format);  permute_1506 = None
        _unsafe_view_205 = torch.ops.aten._unsafe_view.default(clone_242, [1, 512, 512, 256]);  clone_242 = None
        cat_41 = torch.ops.aten.cat.default([_unsafe_view_204, _unsafe_view_205], dim = -1);  _unsafe_view_204 = _unsafe_view_205 = None
        slice_234 = torch.ops.aten.slice.Tensor(arg907_1, dim = 0, start = 0, end = 9223372036854775807);  arg907_1 = None
        unsqueeze_920 = torch.ops.aten.unsqueeze.default(slice_234, 1);  slice_234 = None
        mul_362 = torch.ops.aten.mul.Tensor(arg911_1, unsqueeze_920);  arg911_1 = unsqueeze_920 = None
        _to_copy_1580 = torch.ops.aten._to_copy.default(mul_362, dtype = torch.bfloat16);  mul_362 = None
        t_585 = torch.ops.aten.t.default(_to_copy_1580);  _to_copy_1580 = None
        view_2803 = torch.ops.aten.view.default(cat_41, [262144, 512]);  cat_41 = None
        mm_545 = torch.ops.aten.mm.default(view_2803, t_585);  view_2803 = t_585 = None
        view_2804 = torch.ops.aten.view.default(mm_545, [1, 512, 512, 256]);  mm_545 = None
        add_290 = torch.ops.aten.add.Tensor(add_289, view_2804);  add_289 = view_2804 = None
        split_tensor_287 = torch.ops.aten.split.Tensor(add_283, 512, dim = -2)
        getitem_2682 = split_tensor_287[0];  split_tensor_287 = None
        _to_copy_1581 = torch.ops.aten._to_copy.default(getitem_2682, dtype = torch.float32);  getitem_2682 = None
        native_layer_norm_default_326 = torch.ops.aten.native_layer_norm.default(_to_copy_1581, [256], arg898_1, arg899_1, 1e-05);  _to_copy_1581 = arg898_1 = arg899_1 = None
        getitem_2683 = native_layer_norm_default_326[0]
        _to_copy_1582 = torch.ops.aten._to_copy.default(arg900_1, dtype = torch.bfloat16);  arg900_1 = None
        _to_copy_1583 = torch.ops.aten._to_copy.default(getitem_2683, dtype = torch.bfloat16);  getitem_2683 = None
        t_586 = torch.ops.aten.t.default(_to_copy_1582);  _to_copy_1582 = None
        view_2805 = torch.ops.aten.view.default(_to_copy_1583, [262144, 256]);  _to_copy_1583 = None
        mm_546 = torch.ops.aten.mm.default(view_2805, t_586);  view_2805 = t_586 = None
        view_2806 = torch.ops.aten.view.default(mm_546, [1, 512, 512, 1024]);  mm_546 = None
        split_tensor_288 = torch.ops.aten.split.Tensor(view_2806, 512, dim = -1);  view_2806 = None
        getitem_2686 = split_tensor_288[0]
        getitem_2687 = split_tensor_288[1];  split_tensor_288 = None
        silu_76 = torch.ops.aten.silu.default(getitem_2686);  getitem_2686 = None
        mul_363 = torch.ops.aten.mul.Tensor(silu_76, getitem_2687);  silu_76 = getitem_2687 = None
        _to_copy_1584 = torch.ops.aten._to_copy.default(arg901_1, dtype = torch.bfloat16);  arg901_1 = None
        t_587 = torch.ops.aten.t.default(_to_copy_1584);  _to_copy_1584 = None
        view_2808 = torch.ops.aten.view.default(mul_363, [262144, 512]);  mul_363 = None
        mm_547 = torch.ops.aten.mm.default(view_2808, t_587);  view_2808 = t_587 = None
        view_2809 = torch.ops.aten.view.default(mm_547, [1, 512, 512, 256]);  mm_547 = None
        add_291 = torch.ops.aten.add.Tensor(add_290, view_2809);  add_290 = view_2809 = None
        _to_copy_1585 = torch.ops.aten._to_copy.default(add_287, dtype = torch.float32)
        native_layer_norm_default_327 = torch.ops.aten.native_layer_norm.default(_to_copy_1585, [384], arg916_1, arg917_1, 1e-05);  _to_copy_1585 = arg916_1 = arg917_1 = None
        getitem_2688 = native_layer_norm_default_327[0]
        _to_copy_1586 = torch.ops.aten._to_copy.default(add_283, dtype = torch.float32);  add_283 = None
        native_layer_norm_default_328 = torch.ops.aten.native_layer_norm.default(_to_copy_1586, [256], arg918_1, arg919_1, 1e-05);  _to_copy_1586 = arg918_1 = arg919_1 = None
        getitem_2691 = native_layer_norm_default_328[0]
        _to_copy_1587 = torch.ops.aten._to_copy.default(arg920_1, dtype = torch.bfloat16);  arg920_1 = None
        _to_copy_1588 = torch.ops.aten._to_copy.default(getitem_2691, dtype = torch.bfloat16);  getitem_2691 = None
        t_588 = torch.ops.aten.t.default(_to_copy_1587);  _to_copy_1587 = None
        view_2810 = torch.ops.aten.view.default(_to_copy_1588, [262144, 256]);  _to_copy_1588 = None
        mm_548 = torch.ops.aten.mm.default(view_2810, t_588);  view_2810 = t_588 = None
        view_2811 = torch.ops.aten.view.default(mm_548, [1, 512, 512, 16]);  mm_548 = None
        permute_1507 = torch.ops.aten.permute.default(view_2811, [0, 3, 1, 2]);  view_2811 = None
        view_2812 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_177 = torch.ops.aten.bitwise_not.default(view_2812);  view_2812 = None
        masked_fill_177 = torch.ops.aten.masked_fill.Scalar(permute_1507, bitwise_not_177, -10000);  permute_1507 = bitwise_not_177 = None
        _to_copy_1589 = torch.ops.aten._to_copy.default(getitem_2688, dtype = torch.bfloat16);  getitem_2688 = None
        _to_copy_1590 = torch.ops.aten._to_copy.default(arg922_1, dtype = torch.bfloat16);  arg922_1 = None
        unsqueeze_921 = torch.ops.aten.unsqueeze.default(_to_copy_1589, 3);  _to_copy_1589 = None
        unsqueeze_922 = torch.ops.aten.unsqueeze.default(unsqueeze_921, 4);  unsqueeze_921 = None
        unsqueeze_923 = torch.ops.aten.unsqueeze.default(unsqueeze_922, 5);  unsqueeze_922 = None
        permute_1508 = torch.ops.aten.permute.default(unsqueeze_923, [3, 0, 4, 1, 5, 2]);  unsqueeze_923 = None
        unsqueeze_924 = torch.ops.aten.unsqueeze.default(_to_copy_1590, 4);  _to_copy_1590 = None
        unsqueeze_925 = torch.ops.aten.unsqueeze.default(unsqueeze_924, 5);  unsqueeze_924 = None
        permute_1509 = torch.ops.aten.permute.default(unsqueeze_925, [1, 4, 2, 5, 3, 0]);  unsqueeze_925 = None
        permute_1510 = torch.ops.aten.permute.default(permute_1508, [3, 5, 0, 1, 2, 4]);  permute_1508 = None
        view_2813 = torch.ops.aten.view.default(permute_1510, [1, 512, 384]);  permute_1510 = None
        permute_1511 = torch.ops.aten.permute.default(permute_1509, [5, 0, 1, 2, 4, 3]);  permute_1509 = None
        view_2814 = torch.ops.aten.view.default(permute_1511, [1, 384, 1536]);  permute_1511 = None
        bmm_226 = torch.ops.aten.bmm.default(view_2813, view_2814);  view_2813 = view_2814 = None
        view_2815 = torch.ops.aten.view.default(bmm_226, [512, 1, 4, 1, 16, 24]);  bmm_226 = None
        permute_1512 = torch.ops.aten.permute.default(view_2815, [2, 3, 4, 0, 5, 1]);  view_2815 = None
        view_2816 = torch.ops.aten.view.default(permute_1512, [4, 1, 16, 512, 24]);  permute_1512 = None
        unbind_int_129 = torch.ops.aten.unbind.int(view_2816);  view_2816 = None
        getitem_2694 = unbind_int_129[0]
        getitem_2695 = unbind_int_129[1]
        getitem_2696 = unbind_int_129[2]
        getitem_2697 = unbind_int_129[3];  unbind_int_129 = None
        view_2817 = torch.ops.aten.view.default(arg921_1, [1, 16, 1, 24]);  arg921_1 = None
        add_292 = torch.ops.aten.add.Tensor(getitem_2694, view_2817);  getitem_2694 = view_2817 = None
        _to_copy_1591 = torch.ops.aten._to_copy.default(add_292, dtype = torch.bfloat16);  add_292 = None
        expand_176 = torch.ops.aten.expand.default(masked_fill_177, [1, 16, 512, 512]);  masked_fill_177 = None
        _scaled_dot_product_efficient_attention_default_101 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1591, getitem_2695, getitem_2696, expand_176, False);  _to_copy_1591 = getitem_2695 = getitem_2696 = expand_176 = None
        getitem_2698 = _scaled_dot_product_efficient_attention_default_101[0]
        add_293 = torch.ops.aten.add.Tensor(getitem_2697, 1);  getitem_2697 = None
        sigmoid_221 = torch.ops.aten.sigmoid.default(add_293);  add_293 = None
        mul_364 = torch.ops.aten.mul.Tensor(getitem_2698, sigmoid_221);  getitem_2698 = sigmoid_221 = None
        _to_copy_1592 = torch.ops.aten._to_copy.default(arg923_1, dtype = torch.bfloat16);  arg923_1 = None
        unsqueeze_926 = torch.ops.aten.unsqueeze.default(mul_364, 4);  mul_364 = None
        permute_1513 = torch.ops.aten.permute.default(unsqueeze_926, [0, 2, 4, 3, 1]);  unsqueeze_926 = None
        unsqueeze_927 = torch.ops.aten.unsqueeze.default(_to_copy_1592, 3);  _to_copy_1592 = None
        unsqueeze_928 = torch.ops.aten.unsqueeze.default(unsqueeze_927, 4);  unsqueeze_927 = None
        permute_1514 = torch.ops.aten.permute.default(unsqueeze_928, [3, 4, 2, 1, 0]);  unsqueeze_928 = None
        permute_1515 = torch.ops.aten.permute.default(permute_1513, [1, 3, 4, 0, 2]);  permute_1513 = None
        clone_243 = torch.ops.aten.clone.default(permute_1515, memory_format = torch.contiguous_format);  permute_1515 = None
        _unsafe_view_206 = torch.ops.aten._unsafe_view.default(clone_243, [1, 512, 384]);  clone_243 = None
        permute_1516 = torch.ops.aten.permute.default(permute_1514, [3, 4, 0, 2, 1]);  permute_1514 = None
        clone_244 = torch.ops.aten.clone.default(permute_1516, memory_format = torch.contiguous_format);  permute_1516 = None
        _unsafe_view_207 = torch.ops.aten._unsafe_view.default(clone_244, [1, 384, 384]);  clone_244 = None
        bmm_227 = torch.ops.aten.bmm.default(_unsafe_view_206, _unsafe_view_207);  _unsafe_view_206 = _unsafe_view_207 = None
        view_2818 = torch.ops.aten.view.default(bmm_227, [512, 1, 1, 1, 384]);  bmm_227 = None
        permute_1517 = torch.ops.aten.permute.default(view_2818, [3, 0, 4, 1, 2]);  view_2818 = None
        view_2819 = torch.ops.aten.view.default(permute_1517, [1, 512, 384]);  permute_1517 = None
        unsqueeze_929 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_365 = torch.ops.aten.mul.Tensor(view_2819, unsqueeze_929);  view_2819 = unsqueeze_929 = None
        add_294 = torch.ops.aten.add.Tensor(add_287, mul_365);  mul_365 = None
        split_tensor_289 = torch.ops.aten.split.Tensor(add_287, 512, dim = -2);  add_287 = None
        getitem_2702 = split_tensor_289[0];  split_tensor_289 = None
        _to_copy_1593 = torch.ops.aten._to_copy.default(getitem_2702, dtype = torch.float32);  getitem_2702 = None
        native_layer_norm_default_329 = torch.ops.aten.native_layer_norm.default(_to_copy_1593, [384], arg912_1, arg913_1, 1e-05);  _to_copy_1593 = arg912_1 = arg913_1 = None
        getitem_2703 = native_layer_norm_default_329[0]
        _to_copy_1594 = torch.ops.aten._to_copy.default(arg914_1, dtype = torch.bfloat16);  arg914_1 = None
        _to_copy_1595 = torch.ops.aten._to_copy.default(getitem_2703, dtype = torch.bfloat16);  getitem_2703 = None
        t_589 = torch.ops.aten.t.default(_to_copy_1594);  _to_copy_1594 = None
        view_2820 = torch.ops.aten.view.default(_to_copy_1595, [512, 384]);  _to_copy_1595 = None
        mm_549 = torch.ops.aten.mm.default(view_2820, t_589);  view_2820 = t_589 = None
        view_2821 = torch.ops.aten.view.default(mm_549, [1, 512, 1536]);  mm_549 = None
        split_tensor_290 = torch.ops.aten.split.Tensor(view_2821, 768, dim = -1);  view_2821 = None
        getitem_2706 = split_tensor_290[0]
        getitem_2707 = split_tensor_290[1];  split_tensor_290 = None
        silu_77 = torch.ops.aten.silu.default(getitem_2706);  getitem_2706 = None
        mul_366 = torch.ops.aten.mul.Tensor(silu_77, getitem_2707);  silu_77 = getitem_2707 = None
        _to_copy_1596 = torch.ops.aten._to_copy.default(arg915_1, dtype = torch.bfloat16);  arg915_1 = None
        t_590 = torch.ops.aten.t.default(_to_copy_1596);  _to_copy_1596 = None
        view_2823 = torch.ops.aten.view.default(mul_366, [512, 768]);  mul_366 = None
        mm_550 = torch.ops.aten.mm.default(view_2823, t_590);  view_2823 = t_590 = None
        view_2824 = torch.ops.aten.view.default(mm_550, [1, 512, 384]);  mm_550 = None
        add_295 = torch.ops.aten.add.Tensor(add_294, view_2824);  add_294 = view_2824 = None
        _to_copy_1597 = torch.ops.aten._to_copy.default(add_291, dtype = torch.float32)
        native_layer_norm_default_330 = torch.ops.aten.native_layer_norm.default(_to_copy_1597, [256], arg928_1, arg929_1, 1e-05);  _to_copy_1597 = arg928_1 = arg929_1 = None
        getitem_2708 = native_layer_norm_default_330[0]
        split_with_sizes_default_72 = torch.ops.aten.split_with_sizes.default(arg931_1, [512, 512]);  arg931_1 = None
        getitem_2711 = split_with_sizes_default_72[0]
        getitem_2712 = split_with_sizes_default_72[1];  split_with_sizes_default_72 = None
        split_with_sizes_default_73 = torch.ops.aten.split_with_sizes.default(arg932_1, [512, 512, 256]);  arg932_1 = None
        getitem_2713 = split_with_sizes_default_73[0]
        getitem_2714 = split_with_sizes_default_73[1]
        getitem_2715 = split_with_sizes_default_73[2];  split_with_sizes_default_73 = None
        _to_copy_1598 = torch.ops.aten._to_copy.default(getitem_2711, dtype = torch.bfloat16);  getitem_2711 = None
        _to_copy_1599 = torch.ops.aten._to_copy.default(getitem_2708, dtype = torch.bfloat16)
        t_591 = torch.ops.aten.t.default(_to_copy_1598);  _to_copy_1598 = None
        view_2825 = torch.ops.aten.view.default(_to_copy_1599, [262144, 256]);  _to_copy_1599 = None
        mm_551 = torch.ops.aten.mm.default(view_2825, t_591);  view_2825 = t_591 = None
        view_2826 = torch.ops.aten.view.default(mm_551, [1, 512, 512, 512]);  mm_551 = None
        _to_copy_1600 = torch.ops.aten._to_copy.default(getitem_2713, dtype = torch.bfloat16);  getitem_2713 = None
        _to_copy_1601 = torch.ops.aten._to_copy.default(getitem_2708, dtype = torch.bfloat16)
        t_592 = torch.ops.aten.t.default(_to_copy_1600);  _to_copy_1600 = None
        view_2827 = torch.ops.aten.view.default(_to_copy_1601, [262144, 256]);  _to_copy_1601 = None
        mm_552 = torch.ops.aten.mm.default(view_2827, t_592);  view_2827 = t_592 = None
        view_2828 = torch.ops.aten.view.default(mm_552, [1, 512, 512, 512]);  mm_552 = None
        sigmoid_222 = torch.ops.aten.sigmoid.default(view_2828);  view_2828 = None
        mul_367 = torch.ops.aten.mul.Tensor(view_2826, sigmoid_222);  view_2826 = sigmoid_222 = None
        unsqueeze_930 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_178 = torch.ops.aten.bitwise_not.default(unsqueeze_930);  unsqueeze_930 = None
        masked_fill_178 = torch.ops.aten.masked_fill.Scalar(mul_367, bitwise_not_178, 0);  mul_367 = bitwise_not_178 = None
        split_tensor_291 = torch.ops.aten.split.Tensor(masked_fill_178, 256, dim = -1)
        getitem_2718 = split_tensor_291[0]
        unsqueeze_933 = torch.ops.aten.unsqueeze.default(getitem_2718, 4);  getitem_2718 = None
        permute_1522 = torch.ops.aten.permute.default(unsqueeze_933, [0, 1, 4, 3, 2]);  unsqueeze_933 = None
        permute_1523 = torch.ops.aten.permute.default(permute_1522, [3, 1, 4, 0, 2]);  permute_1522 = None
        view_2831 = torch.ops.aten.view.default(permute_1523, [256, 512, 512]);  permute_1523 = None
        split_tensor_292 = torch.ops.aten.split.Tensor(masked_fill_178, 256, dim = -1);  masked_fill_178 = None
        getitem_2721 = split_tensor_292[1];  split_tensor_292 = None
        unsqueeze_934 = torch.ops.aten.unsqueeze.default(getitem_2721, 4);  getitem_2721 = None
        permute_1524 = torch.ops.aten.permute.default(unsqueeze_934, [0, 4, 1, 3, 2]);  unsqueeze_934 = None
        permute_1525 = torch.ops.aten.permute.default(permute_1524, [3, 4, 0, 2, 1]);  permute_1524 = None
        view_2832 = torch.ops.aten.view.default(permute_1525, [256, 512, 512]);  permute_1525 = None
        bmm_228 = torch.ops.aten.bmm.default(view_2831, view_2832);  view_2831 = view_2832 = None
        view_2833 = torch.ops.aten.view.default(bmm_228, [256, 512, 1, 1, 512]);  bmm_228 = None
        permute_1526 = torch.ops.aten.permute.default(view_2833, [3, 1, 4, 0, 2]);  view_2833 = None
        view_2834 = torch.ops.aten.view.default(permute_1526, [1, 512, 512, 256]);  permute_1526 = None
        _to_copy_1602 = torch.ops.aten._to_copy.default(getitem_2712, dtype = torch.bfloat16);  getitem_2712 = None
        _to_copy_1603 = torch.ops.aten._to_copy.default(getitem_2708, dtype = torch.bfloat16)
        t_593 = torch.ops.aten.t.default(_to_copy_1602);  _to_copy_1602 = None
        view_2835 = torch.ops.aten.view.default(_to_copy_1603, [262144, 256]);  _to_copy_1603 = None
        mm_553 = torch.ops.aten.mm.default(view_2835, t_593);  view_2835 = t_593 = None
        view_2836 = torch.ops.aten.view.default(mm_553, [1, 512, 512, 512]);  mm_553 = None
        _to_copy_1604 = torch.ops.aten._to_copy.default(getitem_2714, dtype = torch.bfloat16);  getitem_2714 = None
        _to_copy_1605 = torch.ops.aten._to_copy.default(getitem_2708, dtype = torch.bfloat16)
        t_594 = torch.ops.aten.t.default(_to_copy_1604);  _to_copy_1604 = None
        view_2837 = torch.ops.aten.view.default(_to_copy_1605, [262144, 256]);  _to_copy_1605 = None
        mm_554 = torch.ops.aten.mm.default(view_2837, t_594);  view_2837 = t_594 = None
        view_2838 = torch.ops.aten.view.default(mm_554, [1, 512, 512, 512]);  mm_554 = None
        sigmoid_223 = torch.ops.aten.sigmoid.default(view_2838);  view_2838 = None
        mul_368 = torch.ops.aten.mul.Tensor(view_2836, sigmoid_223);  view_2836 = sigmoid_223 = None
        view_2839 = torch.ops.aten.view.default(mul_368, [262144, 512]);  mul_368 = None
        view_2840 = torch.ops.aten.view.default(view_2839, [1, 512, 512, 512]);  view_2839 = None
        transpose_72 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_935 = torch.ops.aten.unsqueeze.default(transpose_72, 3);  transpose_72 = None
        clone_245 = torch.ops.aten.clone.default(unsqueeze_935, memory_format = torch.contiguous_format);  unsqueeze_935 = None
        bitwise_not_179 = torch.ops.aten.bitwise_not.default(clone_245);  clone_245 = None
        masked_fill_179 = torch.ops.aten.masked_fill.Scalar(view_2840, bitwise_not_179, 0);  view_2840 = bitwise_not_179 = None
        view_2841 = torch.ops.aten.view.default(masked_fill_179, [262144, 512]);  masked_fill_179 = None
        view_2845 = torch.ops.aten.view.default(view_2841, [1, 512, 512, 512])
        split_tensor_293 = torch.ops.aten.split.Tensor(view_2845, 256, dim = -1);  view_2845 = None
        getitem_2724 = split_tensor_293[0]
        unsqueeze_938 = torch.ops.aten.unsqueeze.default(getitem_2724, 4);  getitem_2724 = None
        permute_1531 = torch.ops.aten.permute.default(unsqueeze_938, [0, 2, 4, 3, 1]);  unsqueeze_938 = None
        permute_1532 = torch.ops.aten.permute.default(permute_1531, [3, 1, 4, 0, 2]);  permute_1531 = None
        view_2846 = torch.ops.aten.view.default(permute_1532, [256, 512, 512]);  permute_1532 = None
        view_2847 = torch.ops.aten.view.default(view_2841, [1, 512, 512, 512]);  view_2841 = None
        split_tensor_294 = torch.ops.aten.split.Tensor(view_2847, 256, dim = -1);  view_2847 = None
        getitem_2727 = split_tensor_294[1];  split_tensor_294 = None
        unsqueeze_939 = torch.ops.aten.unsqueeze.default(getitem_2727, 4);  getitem_2727 = None
        permute_1533 = torch.ops.aten.permute.default(unsqueeze_939, [0, 4, 2, 3, 1]);  unsqueeze_939 = None
        permute_1534 = torch.ops.aten.permute.default(permute_1533, [3, 4, 0, 2, 1]);  permute_1533 = None
        view_2848 = torch.ops.aten.view.default(permute_1534, [256, 512, 512]);  permute_1534 = None
        bmm_229 = torch.ops.aten.bmm.default(view_2846, view_2848);  view_2846 = view_2848 = None
        view_2849 = torch.ops.aten.view.default(bmm_229, [256, 512, 1, 1, 512]);  bmm_229 = None
        permute_1535 = torch.ops.aten.permute.default(view_2849, [3, 1, 4, 0, 2]);  view_2849 = None
        view_2850 = torch.ops.aten.view.default(permute_1535, [1, 512, 512, 256]);  permute_1535 = None
        _to_copy_1606 = torch.ops.aten._to_copy.default(view_2834, dtype = torch.float32);  view_2834 = None
        native_layer_norm_default_331 = torch.ops.aten.native_layer_norm.default(_to_copy_1606, [256], None, None, 1e-05);  _to_copy_1606 = None
        getitem_2728 = native_layer_norm_default_331[0]
        _to_copy_1607 = torch.ops.aten._to_copy.default(view_2850, dtype = torch.float32);  view_2850 = None
        native_layer_norm_default_332 = torch.ops.aten.native_layer_norm.default(_to_copy_1607, [256], None, None, 1e-05);  _to_copy_1607 = None
        getitem_2731 = native_layer_norm_default_332[0]
        add_296 = torch.ops.aten.add.Tensor(getitem_2728, getitem_2731);  getitem_2728 = getitem_2731 = None
        _to_copy_1608 = torch.ops.aten._to_copy.default(arg930_1, dtype = torch.bfloat16);  arg930_1 = None
        _to_copy_1609 = torch.ops.aten._to_copy.default(add_296, dtype = torch.bfloat16);  add_296 = None
        t_595 = torch.ops.aten.t.default(_to_copy_1608);  _to_copy_1608 = None
        view_2851 = torch.ops.aten.view.default(_to_copy_1609, [262144, 256]);  _to_copy_1609 = None
        mm_555 = torch.ops.aten.mm.default(view_2851, t_595);  view_2851 = t_595 = None
        view_2852 = torch.ops.aten.view.default(mm_555, [1, 512, 512, 256]);  mm_555 = None
        _to_copy_1610 = torch.ops.aten._to_copy.default(getitem_2715, dtype = torch.bfloat16);  getitem_2715 = None
        _to_copy_1611 = torch.ops.aten._to_copy.default(getitem_2708, dtype = torch.bfloat16);  getitem_2708 = None
        t_596 = torch.ops.aten.t.default(_to_copy_1610);  _to_copy_1610 = None
        view_2853 = torch.ops.aten.view.default(_to_copy_1611, [262144, 256]);  _to_copy_1611 = None
        mm_556 = torch.ops.aten.mm.default(view_2853, t_596);  view_2853 = t_596 = None
        view_2854 = torch.ops.aten.view.default(mm_556, [1, 512, 512, 256]);  mm_556 = None
        sigmoid_224 = torch.ops.aten.sigmoid.default(view_2854);  view_2854 = None
        mul_369 = torch.ops.aten.mul.Tensor(view_2852, sigmoid_224);  view_2852 = sigmoid_224 = None
        add_297 = torch.ops.aten.add.Tensor(add_291, mul_369);  mul_369 = None
        _to_copy_1612 = torch.ops.aten._to_copy.default(add_291, dtype = torch.float32)
        native_layer_norm_default_333 = torch.ops.aten.native_layer_norm.default(_to_copy_1612, [256], None, None, 1e-05);  _to_copy_1612 = None
        getitem_2734 = native_layer_norm_default_333[0]
        _to_copy_1613 = torch.ops.aten._to_copy.default(arg934_1, dtype = torch.bfloat16);  arg934_1 = None
        _to_copy_1614 = torch.ops.aten._to_copy.default(getitem_2734, dtype = torch.bfloat16)
        t_597 = torch.ops.aten.t.default(_to_copy_1613);  _to_copy_1613 = None
        view_2855 = torch.ops.aten.view.default(_to_copy_1614, [262144, 256]);  _to_copy_1614 = None
        mm_557 = torch.ops.aten.mm.default(view_2855, t_597);  view_2855 = t_597 = None
        view_2856 = torch.ops.aten.view.default(mm_557, [1, 512, 512, 8]);  mm_557 = None
        view_2857 = torch.ops.aten.view.default(view_2856, [1, 512, 512, 2, 4]);  view_2856 = None
        permute_1536 = torch.ops.aten.permute.default(view_2857, [0, 3, 4, 1, 2]);  view_2857 = None
        view_2858 = torch.ops.aten.view.default(permute_1536, [1, 2, 4, 1, 512, 512]);  permute_1536 = None
        view_2859 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_180 = torch.ops.aten.bitwise_not.default(view_2859);  view_2859 = None
        masked_fill_180 = torch.ops.aten.masked_fill.Scalar(view_2858, bitwise_not_180, -10000);  view_2858 = bitwise_not_180 = None
        view_2860 = torch.ops.aten.view.default(masked_fill_180, [1, 2, 4, 512, 512]);  masked_fill_180 = None
        permute_1537 = torch.ops.aten.permute.default(view_2860, [1, 0, 2, 3, 4]);  view_2860 = None
        view_2861 = torch.ops.aten.view.default(permute_1537, [2, 4, 1, 512, 512]);  permute_1537 = None
        _to_copy_1615 = torch.ops.aten._to_copy.default(arg935_1, dtype = torch.bfloat16);  arg935_1 = None
        _to_copy_1616 = torch.ops.aten._to_copy.default(getitem_2734, dtype = torch.bfloat16)
        t_598 = torch.ops.aten.t.default(_to_copy_1615);  _to_copy_1615 = None
        view_2862 = torch.ops.aten.view.default(_to_copy_1616, [262144, 256]);  _to_copy_1616 = None
        mm_558 = torch.ops.aten.mm.default(view_2862, t_598);  view_2862 = t_598 = None
        view_2863 = torch.ops.aten.view.default(mm_558, [1, 512, 512, 1024]);  mm_558 = None
        select_73 = torch.ops.aten.select.int(view_2861, 0, 0)
        view_2864 = torch.ops.aten.view.default(view_2863, [1, 512, 512, 4, 4, 64]);  view_2863 = None
        permute_1538 = torch.ops.aten.permute.default(view_2864, [4, 0, 3, 1, 2, 5]);  view_2864 = None
        view_2865 = torch.ops.aten.view.default(permute_1538, [4, 4, 512, 512, 64]);  permute_1538 = None
        unbind_int_130 = torch.ops.aten.unbind.int(view_2865);  view_2865 = None
        getitem_2737 = unbind_int_130[0]
        getitem_2738 = unbind_int_130[1]
        getitem_2739 = unbind_int_130[2]
        getitem_2740 = unbind_int_130[3];  unbind_int_130 = None
        expand_177 = torch.ops.aten.expand.default(select_73, [4, 512, 512, 512]);  select_73 = None
        _scaled_dot_product_efficient_attention_default_102 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2737, getitem_2738, getitem_2739, expand_177, False);  getitem_2737 = getitem_2738 = getitem_2739 = expand_177 = None
        getitem_2741 = _scaled_dot_product_efficient_attention_default_102[0]
        sigmoid_225 = torch.ops.aten.sigmoid.default(getitem_2740);  getitem_2740 = None
        mul_370 = torch.ops.aten.mul.Tensor(getitem_2741, sigmoid_225);  getitem_2741 = sigmoid_225 = None
        view_2866 = torch.ops.aten.view.default(mul_370, [1, 4, 512, 512, 64]);  mul_370 = None
        permute_1539 = torch.ops.aten.permute.default(view_2866, [0, 2, 3, 1, 4]);  view_2866 = None
        clone_246 = torch.ops.aten.clone.default(permute_1539, memory_format = torch.contiguous_format);  permute_1539 = None
        _unsafe_view_208 = torch.ops.aten._unsafe_view.default(clone_246, [1, 512, 512, 256]);  clone_246 = None
        transpose_73 = torch.ops.aten.transpose.int(getitem_2734, 1, 2);  getitem_2734 = None
        _to_copy_1617 = torch.ops.aten._to_copy.default(arg936_1, dtype = torch.bfloat16);  arg936_1 = None
        _to_copy_1618 = torch.ops.aten._to_copy.default(transpose_73, dtype = torch.bfloat16);  transpose_73 = None
        t_599 = torch.ops.aten.t.default(_to_copy_1617);  _to_copy_1617 = None
        expand_178 = torch.ops.aten.expand.default(_to_copy_1618, [1, 512, 512, 256]);  _to_copy_1618 = None
        view_2867 = torch.ops.aten.view.default(expand_178, [512, 512, 256]);  expand_178 = None
        expand_179 = torch.ops.aten.expand.default(t_599, [1, 512, 256, 1024]);  t_599 = None
        view_2868 = torch.ops.aten.view.default(expand_179, [512, 256, 1024]);  expand_179 = None
        bmm_230 = torch.ops.aten.bmm.default(view_2867, view_2868);  view_2867 = view_2868 = None
        view_2869 = torch.ops.aten.view.default(bmm_230, [1, 512, 512, 1024]);  bmm_230 = None
        select_74 = torch.ops.aten.select.int(view_2861, 0, 1);  view_2861 = None
        view_2870 = torch.ops.aten.view.default(view_2869, [1, 512, 512, 4, 4, 64]);  view_2869 = None
        permute_1540 = torch.ops.aten.permute.default(view_2870, [4, 0, 3, 1, 2, 5]);  view_2870 = None
        view_2871 = torch.ops.aten.view.default(permute_1540, [4, 4, 512, 512, 64]);  permute_1540 = None
        unbind_int_131 = torch.ops.aten.unbind.int(view_2871);  view_2871 = None
        getitem_2745 = unbind_int_131[0]
        getitem_2746 = unbind_int_131[1]
        getitem_2747 = unbind_int_131[2]
        getitem_2748 = unbind_int_131[3];  unbind_int_131 = None
        expand_180 = torch.ops.aten.expand.default(select_74, [4, 512, 512, 512]);  select_74 = None
        _scaled_dot_product_efficient_attention_default_103 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2745, getitem_2746, getitem_2747, expand_180, False);  getitem_2745 = getitem_2746 = getitem_2747 = expand_180 = None
        getitem_2749 = _scaled_dot_product_efficient_attention_default_103[0]
        sigmoid_226 = torch.ops.aten.sigmoid.default(getitem_2748);  getitem_2748 = None
        mul_371 = torch.ops.aten.mul.Tensor(getitem_2749, sigmoid_226);  getitem_2749 = sigmoid_226 = None
        view_2872 = torch.ops.aten.view.default(mul_371, [1, 4, 512, 512, 64]);  mul_371 = None
        permute_1541 = torch.ops.aten.permute.default(view_2872, [0, 2, 3, 1, 4]);  view_2872 = None
        clone_247 = torch.ops.aten.clone.default(permute_1541, memory_format = torch.contiguous_format);  permute_1541 = None
        _unsafe_view_209 = torch.ops.aten._unsafe_view.default(clone_247, [1, 512, 512, 256]);  clone_247 = None
        cat_42 = torch.ops.aten.cat.default([_unsafe_view_208, _unsafe_view_209], dim = -1);  _unsafe_view_208 = _unsafe_view_209 = None
        slice_235 = torch.ops.aten.slice.Tensor(arg933_1, dim = 0, start = 0, end = 9223372036854775807);  arg933_1 = None
        unsqueeze_940 = torch.ops.aten.unsqueeze.default(slice_235, 1);  slice_235 = None
        mul_372 = torch.ops.aten.mul.Tensor(arg937_1, unsqueeze_940);  arg937_1 = unsqueeze_940 = None
        _to_copy_1619 = torch.ops.aten._to_copy.default(mul_372, dtype = torch.bfloat16);  mul_372 = None
        t_600 = torch.ops.aten.t.default(_to_copy_1619);  _to_copy_1619 = None
        view_2873 = torch.ops.aten.view.default(cat_42, [262144, 512]);  cat_42 = None
        mm_559 = torch.ops.aten.mm.default(view_2873, t_600);  view_2873 = t_600 = None
        view_2874 = torch.ops.aten.view.default(mm_559, [1, 512, 512, 256]);  mm_559 = None
        add_298 = torch.ops.aten.add.Tensor(add_297, view_2874);  add_297 = view_2874 = None
        split_tensor_295 = torch.ops.aten.split.Tensor(add_291, 512, dim = -2)
        getitem_2753 = split_tensor_295[0];  split_tensor_295 = None
        _to_copy_1620 = torch.ops.aten._to_copy.default(getitem_2753, dtype = torch.float32);  getitem_2753 = None
        native_layer_norm_default_334 = torch.ops.aten.native_layer_norm.default(_to_copy_1620, [256], arg924_1, arg925_1, 1e-05);  _to_copy_1620 = arg924_1 = arg925_1 = None
        getitem_2754 = native_layer_norm_default_334[0]
        _to_copy_1621 = torch.ops.aten._to_copy.default(arg926_1, dtype = torch.bfloat16);  arg926_1 = None
        _to_copy_1622 = torch.ops.aten._to_copy.default(getitem_2754, dtype = torch.bfloat16);  getitem_2754 = None
        t_601 = torch.ops.aten.t.default(_to_copy_1621);  _to_copy_1621 = None
        view_2875 = torch.ops.aten.view.default(_to_copy_1622, [262144, 256]);  _to_copy_1622 = None
        mm_560 = torch.ops.aten.mm.default(view_2875, t_601);  view_2875 = t_601 = None
        view_2876 = torch.ops.aten.view.default(mm_560, [1, 512, 512, 1024]);  mm_560 = None
        split_tensor_296 = torch.ops.aten.split.Tensor(view_2876, 512, dim = -1);  view_2876 = None
        getitem_2757 = split_tensor_296[0]
        getitem_2758 = split_tensor_296[1];  split_tensor_296 = None
        silu_78 = torch.ops.aten.silu.default(getitem_2757);  getitem_2757 = None
        mul_373 = torch.ops.aten.mul.Tensor(silu_78, getitem_2758);  silu_78 = getitem_2758 = None
        _to_copy_1623 = torch.ops.aten._to_copy.default(arg927_1, dtype = torch.bfloat16);  arg927_1 = None
        t_602 = torch.ops.aten.t.default(_to_copy_1623);  _to_copy_1623 = None
        view_2878 = torch.ops.aten.view.default(mul_373, [262144, 512]);  mul_373 = None
        mm_561 = torch.ops.aten.mm.default(view_2878, t_602);  view_2878 = t_602 = None
        view_2879 = torch.ops.aten.view.default(mm_561, [1, 512, 512, 256]);  mm_561 = None
        add_299 = torch.ops.aten.add.Tensor(add_298, view_2879);  add_298 = view_2879 = None
        _to_copy_1624 = torch.ops.aten._to_copy.default(add_295, dtype = torch.float32)
        native_layer_norm_default_335 = torch.ops.aten.native_layer_norm.default(_to_copy_1624, [384], arg942_1, arg943_1, 1e-05);  _to_copy_1624 = arg942_1 = arg943_1 = None
        getitem_2759 = native_layer_norm_default_335[0]
        _to_copy_1625 = torch.ops.aten._to_copy.default(add_291, dtype = torch.float32);  add_291 = None
        native_layer_norm_default_336 = torch.ops.aten.native_layer_norm.default(_to_copy_1625, [256], arg944_1, arg945_1, 1e-05);  _to_copy_1625 = arg944_1 = arg945_1 = None
        getitem_2762 = native_layer_norm_default_336[0]
        _to_copy_1626 = torch.ops.aten._to_copy.default(arg946_1, dtype = torch.bfloat16);  arg946_1 = None
        _to_copy_1627 = torch.ops.aten._to_copy.default(getitem_2762, dtype = torch.bfloat16);  getitem_2762 = None
        t_603 = torch.ops.aten.t.default(_to_copy_1626);  _to_copy_1626 = None
        view_2880 = torch.ops.aten.view.default(_to_copy_1627, [262144, 256]);  _to_copy_1627 = None
        mm_562 = torch.ops.aten.mm.default(view_2880, t_603);  view_2880 = t_603 = None
        view_2881 = torch.ops.aten.view.default(mm_562, [1, 512, 512, 16]);  mm_562 = None
        permute_1542 = torch.ops.aten.permute.default(view_2881, [0, 3, 1, 2]);  view_2881 = None
        view_2882 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_181 = torch.ops.aten.bitwise_not.default(view_2882);  view_2882 = None
        masked_fill_181 = torch.ops.aten.masked_fill.Scalar(permute_1542, bitwise_not_181, -10000);  permute_1542 = bitwise_not_181 = None
        _to_copy_1628 = torch.ops.aten._to_copy.default(getitem_2759, dtype = torch.bfloat16);  getitem_2759 = None
        _to_copy_1629 = torch.ops.aten._to_copy.default(arg948_1, dtype = torch.bfloat16);  arg948_1 = None
        unsqueeze_941 = torch.ops.aten.unsqueeze.default(_to_copy_1628, 3);  _to_copy_1628 = None
        unsqueeze_942 = torch.ops.aten.unsqueeze.default(unsqueeze_941, 4);  unsqueeze_941 = None
        unsqueeze_943 = torch.ops.aten.unsqueeze.default(unsqueeze_942, 5);  unsqueeze_942 = None
        permute_1543 = torch.ops.aten.permute.default(unsqueeze_943, [3, 0, 4, 1, 5, 2]);  unsqueeze_943 = None
        unsqueeze_944 = torch.ops.aten.unsqueeze.default(_to_copy_1629, 4);  _to_copy_1629 = None
        unsqueeze_945 = torch.ops.aten.unsqueeze.default(unsqueeze_944, 5);  unsqueeze_944 = None
        permute_1544 = torch.ops.aten.permute.default(unsqueeze_945, [1, 4, 2, 5, 3, 0]);  unsqueeze_945 = None
        permute_1545 = torch.ops.aten.permute.default(permute_1543, [3, 5, 0, 1, 2, 4]);  permute_1543 = None
        view_2883 = torch.ops.aten.view.default(permute_1545, [1, 512, 384]);  permute_1545 = None
        permute_1546 = torch.ops.aten.permute.default(permute_1544, [5, 0, 1, 2, 4, 3]);  permute_1544 = None
        view_2884 = torch.ops.aten.view.default(permute_1546, [1, 384, 1536]);  permute_1546 = None
        bmm_231 = torch.ops.aten.bmm.default(view_2883, view_2884);  view_2883 = view_2884 = None
        view_2885 = torch.ops.aten.view.default(bmm_231, [512, 1, 4, 1, 16, 24]);  bmm_231 = None
        permute_1547 = torch.ops.aten.permute.default(view_2885, [2, 3, 4, 0, 5, 1]);  view_2885 = None
        view_2886 = torch.ops.aten.view.default(permute_1547, [4, 1, 16, 512, 24]);  permute_1547 = None
        unbind_int_132 = torch.ops.aten.unbind.int(view_2886);  view_2886 = None
        getitem_2765 = unbind_int_132[0]
        getitem_2766 = unbind_int_132[1]
        getitem_2767 = unbind_int_132[2]
        getitem_2768 = unbind_int_132[3];  unbind_int_132 = None
        view_2887 = torch.ops.aten.view.default(arg947_1, [1, 16, 1, 24]);  arg947_1 = None
        add_300 = torch.ops.aten.add.Tensor(getitem_2765, view_2887);  getitem_2765 = view_2887 = None
        _to_copy_1630 = torch.ops.aten._to_copy.default(add_300, dtype = torch.bfloat16);  add_300 = None
        expand_181 = torch.ops.aten.expand.default(masked_fill_181, [1, 16, 512, 512]);  masked_fill_181 = None
        _scaled_dot_product_efficient_attention_default_104 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1630, getitem_2766, getitem_2767, expand_181, False);  _to_copy_1630 = getitem_2766 = getitem_2767 = expand_181 = None
        getitem_2769 = _scaled_dot_product_efficient_attention_default_104[0]
        add_301 = torch.ops.aten.add.Tensor(getitem_2768, 1);  getitem_2768 = None
        sigmoid_227 = torch.ops.aten.sigmoid.default(add_301);  add_301 = None
        mul_374 = torch.ops.aten.mul.Tensor(getitem_2769, sigmoid_227);  getitem_2769 = sigmoid_227 = None
        _to_copy_1631 = torch.ops.aten._to_copy.default(arg949_1, dtype = torch.bfloat16);  arg949_1 = None
        unsqueeze_946 = torch.ops.aten.unsqueeze.default(mul_374, 4);  mul_374 = None
        permute_1548 = torch.ops.aten.permute.default(unsqueeze_946, [0, 2, 4, 3, 1]);  unsqueeze_946 = None
        unsqueeze_947 = torch.ops.aten.unsqueeze.default(_to_copy_1631, 3);  _to_copy_1631 = None
        unsqueeze_948 = torch.ops.aten.unsqueeze.default(unsqueeze_947, 4);  unsqueeze_947 = None
        permute_1549 = torch.ops.aten.permute.default(unsqueeze_948, [3, 4, 2, 1, 0]);  unsqueeze_948 = None
        permute_1550 = torch.ops.aten.permute.default(permute_1548, [1, 3, 4, 0, 2]);  permute_1548 = None
        clone_248 = torch.ops.aten.clone.default(permute_1550, memory_format = torch.contiguous_format);  permute_1550 = None
        _unsafe_view_210 = torch.ops.aten._unsafe_view.default(clone_248, [1, 512, 384]);  clone_248 = None
        permute_1551 = torch.ops.aten.permute.default(permute_1549, [3, 4, 0, 2, 1]);  permute_1549 = None
        clone_249 = torch.ops.aten.clone.default(permute_1551, memory_format = torch.contiguous_format);  permute_1551 = None
        _unsafe_view_211 = torch.ops.aten._unsafe_view.default(clone_249, [1, 384, 384]);  clone_249 = None
        bmm_232 = torch.ops.aten.bmm.default(_unsafe_view_210, _unsafe_view_211);  _unsafe_view_210 = _unsafe_view_211 = None
        view_2888 = torch.ops.aten.view.default(bmm_232, [512, 1, 1, 1, 384]);  bmm_232 = None
        permute_1552 = torch.ops.aten.permute.default(view_2888, [3, 0, 4, 1, 2]);  view_2888 = None
        view_2889 = torch.ops.aten.view.default(permute_1552, [1, 512, 384]);  permute_1552 = None
        unsqueeze_949 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_375 = torch.ops.aten.mul.Tensor(view_2889, unsqueeze_949);  view_2889 = unsqueeze_949 = None
        add_302 = torch.ops.aten.add.Tensor(add_295, mul_375);  mul_375 = None
        split_tensor_297 = torch.ops.aten.split.Tensor(add_295, 512, dim = -2);  add_295 = None
        getitem_2773 = split_tensor_297[0];  split_tensor_297 = None
        _to_copy_1632 = torch.ops.aten._to_copy.default(getitem_2773, dtype = torch.float32);  getitem_2773 = None
        native_layer_norm_default_337 = torch.ops.aten.native_layer_norm.default(_to_copy_1632, [384], arg938_1, arg939_1, 1e-05);  _to_copy_1632 = arg938_1 = arg939_1 = None
        getitem_2774 = native_layer_norm_default_337[0]
        _to_copy_1633 = torch.ops.aten._to_copy.default(arg940_1, dtype = torch.bfloat16);  arg940_1 = None
        _to_copy_1634 = torch.ops.aten._to_copy.default(getitem_2774, dtype = torch.bfloat16);  getitem_2774 = None
        t_604 = torch.ops.aten.t.default(_to_copy_1633);  _to_copy_1633 = None
        view_2890 = torch.ops.aten.view.default(_to_copy_1634, [512, 384]);  _to_copy_1634 = None
        mm_563 = torch.ops.aten.mm.default(view_2890, t_604);  view_2890 = t_604 = None
        view_2891 = torch.ops.aten.view.default(mm_563, [1, 512, 1536]);  mm_563 = None
        split_tensor_298 = torch.ops.aten.split.Tensor(view_2891, 768, dim = -1);  view_2891 = None
        getitem_2777 = split_tensor_298[0]
        getitem_2778 = split_tensor_298[1];  split_tensor_298 = None
        silu_79 = torch.ops.aten.silu.default(getitem_2777);  getitem_2777 = None
        mul_376 = torch.ops.aten.mul.Tensor(silu_79, getitem_2778);  silu_79 = getitem_2778 = None
        _to_copy_1635 = torch.ops.aten._to_copy.default(arg941_1, dtype = torch.bfloat16);  arg941_1 = None
        t_605 = torch.ops.aten.t.default(_to_copy_1635);  _to_copy_1635 = None
        view_2893 = torch.ops.aten.view.default(mul_376, [512, 768]);  mul_376 = None
        mm_564 = torch.ops.aten.mm.default(view_2893, t_605);  view_2893 = t_605 = None
        view_2894 = torch.ops.aten.view.default(mm_564, [1, 512, 384]);  mm_564 = None
        add_303 = torch.ops.aten.add.Tensor(add_302, view_2894);  add_302 = view_2894 = None
        _to_copy_1636 = torch.ops.aten._to_copy.default(add_299, dtype = torch.float32)
        native_layer_norm_default_338 = torch.ops.aten.native_layer_norm.default(_to_copy_1636, [256], arg954_1, arg955_1, 1e-05);  _to_copy_1636 = arg954_1 = arg955_1 = None
        getitem_2779 = native_layer_norm_default_338[0]
        split_with_sizes_default_74 = torch.ops.aten.split_with_sizes.default(arg957_1, [512, 512]);  arg957_1 = None
        getitem_2782 = split_with_sizes_default_74[0]
        getitem_2783 = split_with_sizes_default_74[1];  split_with_sizes_default_74 = None
        split_with_sizes_default_75 = torch.ops.aten.split_with_sizes.default(arg958_1, [512, 512, 256]);  arg958_1 = None
        getitem_2784 = split_with_sizes_default_75[0]
        getitem_2785 = split_with_sizes_default_75[1]
        getitem_2786 = split_with_sizes_default_75[2];  split_with_sizes_default_75 = None
        _to_copy_1637 = torch.ops.aten._to_copy.default(getitem_2782, dtype = torch.bfloat16);  getitem_2782 = None
        _to_copy_1638 = torch.ops.aten._to_copy.default(getitem_2779, dtype = torch.bfloat16)
        t_606 = torch.ops.aten.t.default(_to_copy_1637);  _to_copy_1637 = None
        view_2895 = torch.ops.aten.view.default(_to_copy_1638, [262144, 256]);  _to_copy_1638 = None
        mm_565 = torch.ops.aten.mm.default(view_2895, t_606);  view_2895 = t_606 = None
        view_2896 = torch.ops.aten.view.default(mm_565, [1, 512, 512, 512]);  mm_565 = None
        _to_copy_1639 = torch.ops.aten._to_copy.default(getitem_2784, dtype = torch.bfloat16);  getitem_2784 = None
        _to_copy_1640 = torch.ops.aten._to_copy.default(getitem_2779, dtype = torch.bfloat16)
        t_607 = torch.ops.aten.t.default(_to_copy_1639);  _to_copy_1639 = None
        view_2897 = torch.ops.aten.view.default(_to_copy_1640, [262144, 256]);  _to_copy_1640 = None
        mm_566 = torch.ops.aten.mm.default(view_2897, t_607);  view_2897 = t_607 = None
        view_2898 = torch.ops.aten.view.default(mm_566, [1, 512, 512, 512]);  mm_566 = None
        sigmoid_228 = torch.ops.aten.sigmoid.default(view_2898);  view_2898 = None
        mul_377 = torch.ops.aten.mul.Tensor(view_2896, sigmoid_228);  view_2896 = sigmoid_228 = None
        unsqueeze_950 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_182 = torch.ops.aten.bitwise_not.default(unsqueeze_950);  unsqueeze_950 = None
        masked_fill_182 = torch.ops.aten.masked_fill.Scalar(mul_377, bitwise_not_182, 0);  mul_377 = bitwise_not_182 = None
        split_tensor_299 = torch.ops.aten.split.Tensor(masked_fill_182, 256, dim = -1)
        getitem_2789 = split_tensor_299[0]
        unsqueeze_953 = torch.ops.aten.unsqueeze.default(getitem_2789, 4);  getitem_2789 = None
        permute_1557 = torch.ops.aten.permute.default(unsqueeze_953, [0, 1, 4, 3, 2]);  unsqueeze_953 = None
        permute_1558 = torch.ops.aten.permute.default(permute_1557, [3, 1, 4, 0, 2]);  permute_1557 = None
        view_2901 = torch.ops.aten.view.default(permute_1558, [256, 512, 512]);  permute_1558 = None
        split_tensor_300 = torch.ops.aten.split.Tensor(masked_fill_182, 256, dim = -1);  masked_fill_182 = None
        getitem_2792 = split_tensor_300[1];  split_tensor_300 = None
        unsqueeze_954 = torch.ops.aten.unsqueeze.default(getitem_2792, 4);  getitem_2792 = None
        permute_1559 = torch.ops.aten.permute.default(unsqueeze_954, [0, 4, 1, 3, 2]);  unsqueeze_954 = None
        permute_1560 = torch.ops.aten.permute.default(permute_1559, [3, 4, 0, 2, 1]);  permute_1559 = None
        view_2902 = torch.ops.aten.view.default(permute_1560, [256, 512, 512]);  permute_1560 = None
        bmm_233 = torch.ops.aten.bmm.default(view_2901, view_2902);  view_2901 = view_2902 = None
        view_2903 = torch.ops.aten.view.default(bmm_233, [256, 512, 1, 1, 512]);  bmm_233 = None
        permute_1561 = torch.ops.aten.permute.default(view_2903, [3, 1, 4, 0, 2]);  view_2903 = None
        view_2904 = torch.ops.aten.view.default(permute_1561, [1, 512, 512, 256]);  permute_1561 = None
        _to_copy_1641 = torch.ops.aten._to_copy.default(getitem_2783, dtype = torch.bfloat16);  getitem_2783 = None
        _to_copy_1642 = torch.ops.aten._to_copy.default(getitem_2779, dtype = torch.bfloat16)
        t_608 = torch.ops.aten.t.default(_to_copy_1641);  _to_copy_1641 = None
        view_2905 = torch.ops.aten.view.default(_to_copy_1642, [262144, 256]);  _to_copy_1642 = None
        mm_567 = torch.ops.aten.mm.default(view_2905, t_608);  view_2905 = t_608 = None
        view_2906 = torch.ops.aten.view.default(mm_567, [1, 512, 512, 512]);  mm_567 = None
        _to_copy_1643 = torch.ops.aten._to_copy.default(getitem_2785, dtype = torch.bfloat16);  getitem_2785 = None
        _to_copy_1644 = torch.ops.aten._to_copy.default(getitem_2779, dtype = torch.bfloat16)
        t_609 = torch.ops.aten.t.default(_to_copy_1643);  _to_copy_1643 = None
        view_2907 = torch.ops.aten.view.default(_to_copy_1644, [262144, 256]);  _to_copy_1644 = None
        mm_568 = torch.ops.aten.mm.default(view_2907, t_609);  view_2907 = t_609 = None
        view_2908 = torch.ops.aten.view.default(mm_568, [1, 512, 512, 512]);  mm_568 = None
        sigmoid_229 = torch.ops.aten.sigmoid.default(view_2908);  view_2908 = None
        mul_378 = torch.ops.aten.mul.Tensor(view_2906, sigmoid_229);  view_2906 = sigmoid_229 = None
        view_2909 = torch.ops.aten.view.default(mul_378, [262144, 512]);  mul_378 = None
        view_2910 = torch.ops.aten.view.default(view_2909, [1, 512, 512, 512]);  view_2909 = None
        transpose_74 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_955 = torch.ops.aten.unsqueeze.default(transpose_74, 3);  transpose_74 = None
        clone_250 = torch.ops.aten.clone.default(unsqueeze_955, memory_format = torch.contiguous_format);  unsqueeze_955 = None
        bitwise_not_183 = torch.ops.aten.bitwise_not.default(clone_250);  clone_250 = None
        masked_fill_183 = torch.ops.aten.masked_fill.Scalar(view_2910, bitwise_not_183, 0);  view_2910 = bitwise_not_183 = None
        view_2911 = torch.ops.aten.view.default(masked_fill_183, [262144, 512]);  masked_fill_183 = None
        view_2915 = torch.ops.aten.view.default(view_2911, [1, 512, 512, 512])
        split_tensor_301 = torch.ops.aten.split.Tensor(view_2915, 256, dim = -1);  view_2915 = None
        getitem_2795 = split_tensor_301[0]
        unsqueeze_958 = torch.ops.aten.unsqueeze.default(getitem_2795, 4);  getitem_2795 = None
        permute_1566 = torch.ops.aten.permute.default(unsqueeze_958, [0, 2, 4, 3, 1]);  unsqueeze_958 = None
        permute_1567 = torch.ops.aten.permute.default(permute_1566, [3, 1, 4, 0, 2]);  permute_1566 = None
        view_2916 = torch.ops.aten.view.default(permute_1567, [256, 512, 512]);  permute_1567 = None
        view_2917 = torch.ops.aten.view.default(view_2911, [1, 512, 512, 512]);  view_2911 = None
        split_tensor_302 = torch.ops.aten.split.Tensor(view_2917, 256, dim = -1);  view_2917 = None
        getitem_2798 = split_tensor_302[1];  split_tensor_302 = None
        unsqueeze_959 = torch.ops.aten.unsqueeze.default(getitem_2798, 4);  getitem_2798 = None
        permute_1568 = torch.ops.aten.permute.default(unsqueeze_959, [0, 4, 2, 3, 1]);  unsqueeze_959 = None
        permute_1569 = torch.ops.aten.permute.default(permute_1568, [3, 4, 0, 2, 1]);  permute_1568 = None
        view_2918 = torch.ops.aten.view.default(permute_1569, [256, 512, 512]);  permute_1569 = None
        bmm_234 = torch.ops.aten.bmm.default(view_2916, view_2918);  view_2916 = view_2918 = None
        view_2919 = torch.ops.aten.view.default(bmm_234, [256, 512, 1, 1, 512]);  bmm_234 = None
        permute_1570 = torch.ops.aten.permute.default(view_2919, [3, 1, 4, 0, 2]);  view_2919 = None
        view_2920 = torch.ops.aten.view.default(permute_1570, [1, 512, 512, 256]);  permute_1570 = None
        _to_copy_1645 = torch.ops.aten._to_copy.default(view_2904, dtype = torch.float32);  view_2904 = None
        native_layer_norm_default_339 = torch.ops.aten.native_layer_norm.default(_to_copy_1645, [256], None, None, 1e-05);  _to_copy_1645 = None
        getitem_2799 = native_layer_norm_default_339[0]
        _to_copy_1646 = torch.ops.aten._to_copy.default(view_2920, dtype = torch.float32);  view_2920 = None
        native_layer_norm_default_340 = torch.ops.aten.native_layer_norm.default(_to_copy_1646, [256], None, None, 1e-05);  _to_copy_1646 = None
        getitem_2802 = native_layer_norm_default_340[0]
        add_304 = torch.ops.aten.add.Tensor(getitem_2799, getitem_2802);  getitem_2799 = getitem_2802 = None
        _to_copy_1647 = torch.ops.aten._to_copy.default(arg956_1, dtype = torch.bfloat16);  arg956_1 = None
        _to_copy_1648 = torch.ops.aten._to_copy.default(add_304, dtype = torch.bfloat16);  add_304 = None
        t_610 = torch.ops.aten.t.default(_to_copy_1647);  _to_copy_1647 = None
        view_2921 = torch.ops.aten.view.default(_to_copy_1648, [262144, 256]);  _to_copy_1648 = None
        mm_569 = torch.ops.aten.mm.default(view_2921, t_610);  view_2921 = t_610 = None
        view_2922 = torch.ops.aten.view.default(mm_569, [1, 512, 512, 256]);  mm_569 = None
        _to_copy_1649 = torch.ops.aten._to_copy.default(getitem_2786, dtype = torch.bfloat16);  getitem_2786 = None
        _to_copy_1650 = torch.ops.aten._to_copy.default(getitem_2779, dtype = torch.bfloat16);  getitem_2779 = None
        t_611 = torch.ops.aten.t.default(_to_copy_1649);  _to_copy_1649 = None
        view_2923 = torch.ops.aten.view.default(_to_copy_1650, [262144, 256]);  _to_copy_1650 = None
        mm_570 = torch.ops.aten.mm.default(view_2923, t_611);  view_2923 = t_611 = None
        view_2924 = torch.ops.aten.view.default(mm_570, [1, 512, 512, 256]);  mm_570 = None
        sigmoid_230 = torch.ops.aten.sigmoid.default(view_2924);  view_2924 = None
        mul_379 = torch.ops.aten.mul.Tensor(view_2922, sigmoid_230);  view_2922 = sigmoid_230 = None
        add_305 = torch.ops.aten.add.Tensor(add_299, mul_379);  mul_379 = None
        _to_copy_1651 = torch.ops.aten._to_copy.default(add_299, dtype = torch.float32)
        native_layer_norm_default_341 = torch.ops.aten.native_layer_norm.default(_to_copy_1651, [256], None, None, 1e-05);  _to_copy_1651 = None
        getitem_2805 = native_layer_norm_default_341[0]
        _to_copy_1652 = torch.ops.aten._to_copy.default(arg960_1, dtype = torch.bfloat16);  arg960_1 = None
        _to_copy_1653 = torch.ops.aten._to_copy.default(getitem_2805, dtype = torch.bfloat16)
        t_612 = torch.ops.aten.t.default(_to_copy_1652);  _to_copy_1652 = None
        view_2925 = torch.ops.aten.view.default(_to_copy_1653, [262144, 256]);  _to_copy_1653 = None
        mm_571 = torch.ops.aten.mm.default(view_2925, t_612);  view_2925 = t_612 = None
        view_2926 = torch.ops.aten.view.default(mm_571, [1, 512, 512, 8]);  mm_571 = None
        view_2927 = torch.ops.aten.view.default(view_2926, [1, 512, 512, 2, 4]);  view_2926 = None
        permute_1571 = torch.ops.aten.permute.default(view_2927, [0, 3, 4, 1, 2]);  view_2927 = None
        view_2928 = torch.ops.aten.view.default(permute_1571, [1, 2, 4, 1, 512, 512]);  permute_1571 = None
        view_2929 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_184 = torch.ops.aten.bitwise_not.default(view_2929);  view_2929 = None
        masked_fill_184 = torch.ops.aten.masked_fill.Scalar(view_2928, bitwise_not_184, -10000);  view_2928 = bitwise_not_184 = None
        view_2930 = torch.ops.aten.view.default(masked_fill_184, [1, 2, 4, 512, 512]);  masked_fill_184 = None
        permute_1572 = torch.ops.aten.permute.default(view_2930, [1, 0, 2, 3, 4]);  view_2930 = None
        view_2931 = torch.ops.aten.view.default(permute_1572, [2, 4, 1, 512, 512]);  permute_1572 = None
        _to_copy_1654 = torch.ops.aten._to_copy.default(arg961_1, dtype = torch.bfloat16);  arg961_1 = None
        _to_copy_1655 = torch.ops.aten._to_copy.default(getitem_2805, dtype = torch.bfloat16)
        t_613 = torch.ops.aten.t.default(_to_copy_1654);  _to_copy_1654 = None
        view_2932 = torch.ops.aten.view.default(_to_copy_1655, [262144, 256]);  _to_copy_1655 = None
        mm_572 = torch.ops.aten.mm.default(view_2932, t_613);  view_2932 = t_613 = None
        view_2933 = torch.ops.aten.view.default(mm_572, [1, 512, 512, 1024]);  mm_572 = None
        select_75 = torch.ops.aten.select.int(view_2931, 0, 0)
        view_2934 = torch.ops.aten.view.default(view_2933, [1, 512, 512, 4, 4, 64]);  view_2933 = None
        permute_1573 = torch.ops.aten.permute.default(view_2934, [4, 0, 3, 1, 2, 5]);  view_2934 = None
        view_2935 = torch.ops.aten.view.default(permute_1573, [4, 4, 512, 512, 64]);  permute_1573 = None
        unbind_int_133 = torch.ops.aten.unbind.int(view_2935);  view_2935 = None
        getitem_2808 = unbind_int_133[0]
        getitem_2809 = unbind_int_133[1]
        getitem_2810 = unbind_int_133[2]
        getitem_2811 = unbind_int_133[3];  unbind_int_133 = None
        expand_182 = torch.ops.aten.expand.default(select_75, [4, 512, 512, 512]);  select_75 = None
        _scaled_dot_product_efficient_attention_default_105 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2808, getitem_2809, getitem_2810, expand_182, False);  getitem_2808 = getitem_2809 = getitem_2810 = expand_182 = None
        getitem_2812 = _scaled_dot_product_efficient_attention_default_105[0]
        sigmoid_231 = torch.ops.aten.sigmoid.default(getitem_2811);  getitem_2811 = None
        mul_380 = torch.ops.aten.mul.Tensor(getitem_2812, sigmoid_231);  getitem_2812 = sigmoid_231 = None
        view_2936 = torch.ops.aten.view.default(mul_380, [1, 4, 512, 512, 64]);  mul_380 = None
        permute_1574 = torch.ops.aten.permute.default(view_2936, [0, 2, 3, 1, 4]);  view_2936 = None
        clone_251 = torch.ops.aten.clone.default(permute_1574, memory_format = torch.contiguous_format);  permute_1574 = None
        _unsafe_view_212 = torch.ops.aten._unsafe_view.default(clone_251, [1, 512, 512, 256]);  clone_251 = None
        transpose_75 = torch.ops.aten.transpose.int(getitem_2805, 1, 2);  getitem_2805 = None
        _to_copy_1656 = torch.ops.aten._to_copy.default(arg962_1, dtype = torch.bfloat16);  arg962_1 = None
        _to_copy_1657 = torch.ops.aten._to_copy.default(transpose_75, dtype = torch.bfloat16);  transpose_75 = None
        t_614 = torch.ops.aten.t.default(_to_copy_1656);  _to_copy_1656 = None
        expand_183 = torch.ops.aten.expand.default(_to_copy_1657, [1, 512, 512, 256]);  _to_copy_1657 = None
        view_2937 = torch.ops.aten.view.default(expand_183, [512, 512, 256]);  expand_183 = None
        expand_184 = torch.ops.aten.expand.default(t_614, [1, 512, 256, 1024]);  t_614 = None
        view_2938 = torch.ops.aten.view.default(expand_184, [512, 256, 1024]);  expand_184 = None
        bmm_235 = torch.ops.aten.bmm.default(view_2937, view_2938);  view_2937 = view_2938 = None
        view_2939 = torch.ops.aten.view.default(bmm_235, [1, 512, 512, 1024]);  bmm_235 = None
        select_76 = torch.ops.aten.select.int(view_2931, 0, 1);  view_2931 = None
        view_2940 = torch.ops.aten.view.default(view_2939, [1, 512, 512, 4, 4, 64]);  view_2939 = None
        permute_1575 = torch.ops.aten.permute.default(view_2940, [4, 0, 3, 1, 2, 5]);  view_2940 = None
        view_2941 = torch.ops.aten.view.default(permute_1575, [4, 4, 512, 512, 64]);  permute_1575 = None
        unbind_int_134 = torch.ops.aten.unbind.int(view_2941);  view_2941 = None
        getitem_2816 = unbind_int_134[0]
        getitem_2817 = unbind_int_134[1]
        getitem_2818 = unbind_int_134[2]
        getitem_2819 = unbind_int_134[3];  unbind_int_134 = None
        expand_185 = torch.ops.aten.expand.default(select_76, [4, 512, 512, 512]);  select_76 = None
        _scaled_dot_product_efficient_attention_default_106 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2816, getitem_2817, getitem_2818, expand_185, False);  getitem_2816 = getitem_2817 = getitem_2818 = expand_185 = None
        getitem_2820 = _scaled_dot_product_efficient_attention_default_106[0]
        sigmoid_232 = torch.ops.aten.sigmoid.default(getitem_2819);  getitem_2819 = None
        mul_381 = torch.ops.aten.mul.Tensor(getitem_2820, sigmoid_232);  getitem_2820 = sigmoid_232 = None
        view_2942 = torch.ops.aten.view.default(mul_381, [1, 4, 512, 512, 64]);  mul_381 = None
        permute_1576 = torch.ops.aten.permute.default(view_2942, [0, 2, 3, 1, 4]);  view_2942 = None
        clone_252 = torch.ops.aten.clone.default(permute_1576, memory_format = torch.contiguous_format);  permute_1576 = None
        _unsafe_view_213 = torch.ops.aten._unsafe_view.default(clone_252, [1, 512, 512, 256]);  clone_252 = None
        cat_43 = torch.ops.aten.cat.default([_unsafe_view_212, _unsafe_view_213], dim = -1);  _unsafe_view_212 = _unsafe_view_213 = None
        slice_236 = torch.ops.aten.slice.Tensor(arg959_1, dim = 0, start = 0, end = 9223372036854775807);  arg959_1 = None
        unsqueeze_960 = torch.ops.aten.unsqueeze.default(slice_236, 1);  slice_236 = None
        mul_382 = torch.ops.aten.mul.Tensor(arg963_1, unsqueeze_960);  arg963_1 = unsqueeze_960 = None
        _to_copy_1658 = torch.ops.aten._to_copy.default(mul_382, dtype = torch.bfloat16);  mul_382 = None
        t_615 = torch.ops.aten.t.default(_to_copy_1658);  _to_copy_1658 = None
        view_2943 = torch.ops.aten.view.default(cat_43, [262144, 512]);  cat_43 = None
        mm_573 = torch.ops.aten.mm.default(view_2943, t_615);  view_2943 = t_615 = None
        view_2944 = torch.ops.aten.view.default(mm_573, [1, 512, 512, 256]);  mm_573 = None
        add_306 = torch.ops.aten.add.Tensor(add_305, view_2944);  add_305 = view_2944 = None
        split_tensor_303 = torch.ops.aten.split.Tensor(add_299, 512, dim = -2)
        getitem_2824 = split_tensor_303[0];  split_tensor_303 = None
        _to_copy_1659 = torch.ops.aten._to_copy.default(getitem_2824, dtype = torch.float32);  getitem_2824 = None
        native_layer_norm_default_342 = torch.ops.aten.native_layer_norm.default(_to_copy_1659, [256], arg950_1, arg951_1, 1e-05);  _to_copy_1659 = arg950_1 = arg951_1 = None
        getitem_2825 = native_layer_norm_default_342[0]
        _to_copy_1660 = torch.ops.aten._to_copy.default(arg952_1, dtype = torch.bfloat16);  arg952_1 = None
        _to_copy_1661 = torch.ops.aten._to_copy.default(getitem_2825, dtype = torch.bfloat16);  getitem_2825 = None
        t_616 = torch.ops.aten.t.default(_to_copy_1660);  _to_copy_1660 = None
        view_2945 = torch.ops.aten.view.default(_to_copy_1661, [262144, 256]);  _to_copy_1661 = None
        mm_574 = torch.ops.aten.mm.default(view_2945, t_616);  view_2945 = t_616 = None
        view_2946 = torch.ops.aten.view.default(mm_574, [1, 512, 512, 1024]);  mm_574 = None
        split_tensor_304 = torch.ops.aten.split.Tensor(view_2946, 512, dim = -1);  view_2946 = None
        getitem_2828 = split_tensor_304[0]
        getitem_2829 = split_tensor_304[1];  split_tensor_304 = None
        silu_80 = torch.ops.aten.silu.default(getitem_2828);  getitem_2828 = None
        mul_383 = torch.ops.aten.mul.Tensor(silu_80, getitem_2829);  silu_80 = getitem_2829 = None
        _to_copy_1662 = torch.ops.aten._to_copy.default(arg953_1, dtype = torch.bfloat16);  arg953_1 = None
        t_617 = torch.ops.aten.t.default(_to_copy_1662);  _to_copy_1662 = None
        view_2948 = torch.ops.aten.view.default(mul_383, [262144, 512]);  mul_383 = None
        mm_575 = torch.ops.aten.mm.default(view_2948, t_617);  view_2948 = t_617 = None
        view_2949 = torch.ops.aten.view.default(mm_575, [1, 512, 512, 256]);  mm_575 = None
        add_307 = torch.ops.aten.add.Tensor(add_306, view_2949);  add_306 = view_2949 = None
        _to_copy_1663 = torch.ops.aten._to_copy.default(add_303, dtype = torch.float32)
        native_layer_norm_default_343 = torch.ops.aten.native_layer_norm.default(_to_copy_1663, [384], arg968_1, arg969_1, 1e-05);  _to_copy_1663 = arg968_1 = arg969_1 = None
        getitem_2830 = native_layer_norm_default_343[0]
        _to_copy_1664 = torch.ops.aten._to_copy.default(add_299, dtype = torch.float32);  add_299 = None
        native_layer_norm_default_344 = torch.ops.aten.native_layer_norm.default(_to_copy_1664, [256], arg970_1, arg971_1, 1e-05);  _to_copy_1664 = arg970_1 = arg971_1 = None
        getitem_2833 = native_layer_norm_default_344[0]
        _to_copy_1665 = torch.ops.aten._to_copy.default(arg972_1, dtype = torch.bfloat16);  arg972_1 = None
        _to_copy_1666 = torch.ops.aten._to_copy.default(getitem_2833, dtype = torch.bfloat16);  getitem_2833 = None
        t_618 = torch.ops.aten.t.default(_to_copy_1665);  _to_copy_1665 = None
        view_2950 = torch.ops.aten.view.default(_to_copy_1666, [262144, 256]);  _to_copy_1666 = None
        mm_576 = torch.ops.aten.mm.default(view_2950, t_618);  view_2950 = t_618 = None
        view_2951 = torch.ops.aten.view.default(mm_576, [1, 512, 512, 16]);  mm_576 = None
        permute_1577 = torch.ops.aten.permute.default(view_2951, [0, 3, 1, 2]);  view_2951 = None
        view_2952 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_185 = torch.ops.aten.bitwise_not.default(view_2952);  view_2952 = None
        masked_fill_185 = torch.ops.aten.masked_fill.Scalar(permute_1577, bitwise_not_185, -10000);  permute_1577 = bitwise_not_185 = None
        _to_copy_1667 = torch.ops.aten._to_copy.default(getitem_2830, dtype = torch.bfloat16);  getitem_2830 = None
        _to_copy_1668 = torch.ops.aten._to_copy.default(arg974_1, dtype = torch.bfloat16);  arg974_1 = None
        unsqueeze_961 = torch.ops.aten.unsqueeze.default(_to_copy_1667, 3);  _to_copy_1667 = None
        unsqueeze_962 = torch.ops.aten.unsqueeze.default(unsqueeze_961, 4);  unsqueeze_961 = None
        unsqueeze_963 = torch.ops.aten.unsqueeze.default(unsqueeze_962, 5);  unsqueeze_962 = None
        permute_1578 = torch.ops.aten.permute.default(unsqueeze_963, [3, 0, 4, 1, 5, 2]);  unsqueeze_963 = None
        unsqueeze_964 = torch.ops.aten.unsqueeze.default(_to_copy_1668, 4);  _to_copy_1668 = None
        unsqueeze_965 = torch.ops.aten.unsqueeze.default(unsqueeze_964, 5);  unsqueeze_964 = None
        permute_1579 = torch.ops.aten.permute.default(unsqueeze_965, [1, 4, 2, 5, 3, 0]);  unsqueeze_965 = None
        permute_1580 = torch.ops.aten.permute.default(permute_1578, [3, 5, 0, 1, 2, 4]);  permute_1578 = None
        view_2953 = torch.ops.aten.view.default(permute_1580, [1, 512, 384]);  permute_1580 = None
        permute_1581 = torch.ops.aten.permute.default(permute_1579, [5, 0, 1, 2, 4, 3]);  permute_1579 = None
        view_2954 = torch.ops.aten.view.default(permute_1581, [1, 384, 1536]);  permute_1581 = None
        bmm_236 = torch.ops.aten.bmm.default(view_2953, view_2954);  view_2953 = view_2954 = None
        view_2955 = torch.ops.aten.view.default(bmm_236, [512, 1, 4, 1, 16, 24]);  bmm_236 = None
        permute_1582 = torch.ops.aten.permute.default(view_2955, [2, 3, 4, 0, 5, 1]);  view_2955 = None
        view_2956 = torch.ops.aten.view.default(permute_1582, [4, 1, 16, 512, 24]);  permute_1582 = None
        unbind_int_135 = torch.ops.aten.unbind.int(view_2956);  view_2956 = None
        getitem_2836 = unbind_int_135[0]
        getitem_2837 = unbind_int_135[1]
        getitem_2838 = unbind_int_135[2]
        getitem_2839 = unbind_int_135[3];  unbind_int_135 = None
        view_2957 = torch.ops.aten.view.default(arg973_1, [1, 16, 1, 24]);  arg973_1 = None
        add_308 = torch.ops.aten.add.Tensor(getitem_2836, view_2957);  getitem_2836 = view_2957 = None
        _to_copy_1669 = torch.ops.aten._to_copy.default(add_308, dtype = torch.bfloat16);  add_308 = None
        expand_186 = torch.ops.aten.expand.default(masked_fill_185, [1, 16, 512, 512]);  masked_fill_185 = None
        _scaled_dot_product_efficient_attention_default_107 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1669, getitem_2837, getitem_2838, expand_186, False);  _to_copy_1669 = getitem_2837 = getitem_2838 = expand_186 = None
        getitem_2840 = _scaled_dot_product_efficient_attention_default_107[0]
        add_309 = torch.ops.aten.add.Tensor(getitem_2839, 1);  getitem_2839 = None
        sigmoid_233 = torch.ops.aten.sigmoid.default(add_309);  add_309 = None
        mul_384 = torch.ops.aten.mul.Tensor(getitem_2840, sigmoid_233);  getitem_2840 = sigmoid_233 = None
        _to_copy_1670 = torch.ops.aten._to_copy.default(arg975_1, dtype = torch.bfloat16);  arg975_1 = None
        unsqueeze_966 = torch.ops.aten.unsqueeze.default(mul_384, 4);  mul_384 = None
        permute_1583 = torch.ops.aten.permute.default(unsqueeze_966, [0, 2, 4, 3, 1]);  unsqueeze_966 = None
        unsqueeze_967 = torch.ops.aten.unsqueeze.default(_to_copy_1670, 3);  _to_copy_1670 = None
        unsqueeze_968 = torch.ops.aten.unsqueeze.default(unsqueeze_967, 4);  unsqueeze_967 = None
        permute_1584 = torch.ops.aten.permute.default(unsqueeze_968, [3, 4, 2, 1, 0]);  unsqueeze_968 = None
        permute_1585 = torch.ops.aten.permute.default(permute_1583, [1, 3, 4, 0, 2]);  permute_1583 = None
        clone_253 = torch.ops.aten.clone.default(permute_1585, memory_format = torch.contiguous_format);  permute_1585 = None
        _unsafe_view_214 = torch.ops.aten._unsafe_view.default(clone_253, [1, 512, 384]);  clone_253 = None
        permute_1586 = torch.ops.aten.permute.default(permute_1584, [3, 4, 0, 2, 1]);  permute_1584 = None
        clone_254 = torch.ops.aten.clone.default(permute_1586, memory_format = torch.contiguous_format);  permute_1586 = None
        _unsafe_view_215 = torch.ops.aten._unsafe_view.default(clone_254, [1, 384, 384]);  clone_254 = None
        bmm_237 = torch.ops.aten.bmm.default(_unsafe_view_214, _unsafe_view_215);  _unsafe_view_214 = _unsafe_view_215 = None
        view_2958 = torch.ops.aten.view.default(bmm_237, [512, 1, 1, 1, 384]);  bmm_237 = None
        permute_1587 = torch.ops.aten.permute.default(view_2958, [3, 0, 4, 1, 2]);  view_2958 = None
        view_2959 = torch.ops.aten.view.default(permute_1587, [1, 512, 384]);  permute_1587 = None
        unsqueeze_969 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_385 = torch.ops.aten.mul.Tensor(view_2959, unsqueeze_969);  view_2959 = unsqueeze_969 = None
        add_310 = torch.ops.aten.add.Tensor(add_303, mul_385);  mul_385 = None
        split_tensor_305 = torch.ops.aten.split.Tensor(add_303, 512, dim = -2);  add_303 = None
        getitem_2844 = split_tensor_305[0];  split_tensor_305 = None
        _to_copy_1671 = torch.ops.aten._to_copy.default(getitem_2844, dtype = torch.float32);  getitem_2844 = None
        native_layer_norm_default_345 = torch.ops.aten.native_layer_norm.default(_to_copy_1671, [384], arg964_1, arg965_1, 1e-05);  _to_copy_1671 = arg964_1 = arg965_1 = None
        getitem_2845 = native_layer_norm_default_345[0]
        _to_copy_1672 = torch.ops.aten._to_copy.default(arg966_1, dtype = torch.bfloat16);  arg966_1 = None
        _to_copy_1673 = torch.ops.aten._to_copy.default(getitem_2845, dtype = torch.bfloat16);  getitem_2845 = None
        t_619 = torch.ops.aten.t.default(_to_copy_1672);  _to_copy_1672 = None
        view_2960 = torch.ops.aten.view.default(_to_copy_1673, [512, 384]);  _to_copy_1673 = None
        mm_577 = torch.ops.aten.mm.default(view_2960, t_619);  view_2960 = t_619 = None
        view_2961 = torch.ops.aten.view.default(mm_577, [1, 512, 1536]);  mm_577 = None
        split_tensor_306 = torch.ops.aten.split.Tensor(view_2961, 768, dim = -1);  view_2961 = None
        getitem_2848 = split_tensor_306[0]
        getitem_2849 = split_tensor_306[1];  split_tensor_306 = None
        silu_81 = torch.ops.aten.silu.default(getitem_2848);  getitem_2848 = None
        mul_386 = torch.ops.aten.mul.Tensor(silu_81, getitem_2849);  silu_81 = getitem_2849 = None
        _to_copy_1674 = torch.ops.aten._to_copy.default(arg967_1, dtype = torch.bfloat16);  arg967_1 = None
        t_620 = torch.ops.aten.t.default(_to_copy_1674);  _to_copy_1674 = None
        view_2963 = torch.ops.aten.view.default(mul_386, [512, 768]);  mul_386 = None
        mm_578 = torch.ops.aten.mm.default(view_2963, t_620);  view_2963 = t_620 = None
        view_2964 = torch.ops.aten.view.default(mm_578, [1, 512, 384]);  mm_578 = None
        add_311 = torch.ops.aten.add.Tensor(add_310, view_2964);  add_310 = view_2964 = None
        _to_copy_1675 = torch.ops.aten._to_copy.default(add_307, dtype = torch.float32)
        native_layer_norm_default_346 = torch.ops.aten.native_layer_norm.default(_to_copy_1675, [256], arg980_1, arg981_1, 1e-05);  _to_copy_1675 = arg980_1 = arg981_1 = None
        getitem_2850 = native_layer_norm_default_346[0]
        split_with_sizes_default_76 = torch.ops.aten.split_with_sizes.default(arg983_1, [512, 512]);  arg983_1 = None
        getitem_2853 = split_with_sizes_default_76[0]
        getitem_2854 = split_with_sizes_default_76[1];  split_with_sizes_default_76 = None
        split_with_sizes_default_77 = torch.ops.aten.split_with_sizes.default(arg984_1, [512, 512, 256]);  arg984_1 = None
        getitem_2855 = split_with_sizes_default_77[0]
        getitem_2856 = split_with_sizes_default_77[1]
        getitem_2857 = split_with_sizes_default_77[2];  split_with_sizes_default_77 = None
        _to_copy_1676 = torch.ops.aten._to_copy.default(getitem_2853, dtype = torch.bfloat16);  getitem_2853 = None
        _to_copy_1677 = torch.ops.aten._to_copy.default(getitem_2850, dtype = torch.bfloat16)
        t_621 = torch.ops.aten.t.default(_to_copy_1676);  _to_copy_1676 = None
        view_2965 = torch.ops.aten.view.default(_to_copy_1677, [262144, 256]);  _to_copy_1677 = None
        mm_579 = torch.ops.aten.mm.default(view_2965, t_621);  view_2965 = t_621 = None
        view_2966 = torch.ops.aten.view.default(mm_579, [1, 512, 512, 512]);  mm_579 = None
        _to_copy_1678 = torch.ops.aten._to_copy.default(getitem_2855, dtype = torch.bfloat16);  getitem_2855 = None
        _to_copy_1679 = torch.ops.aten._to_copy.default(getitem_2850, dtype = torch.bfloat16)
        t_622 = torch.ops.aten.t.default(_to_copy_1678);  _to_copy_1678 = None
        view_2967 = torch.ops.aten.view.default(_to_copy_1679, [262144, 256]);  _to_copy_1679 = None
        mm_580 = torch.ops.aten.mm.default(view_2967, t_622);  view_2967 = t_622 = None
        view_2968 = torch.ops.aten.view.default(mm_580, [1, 512, 512, 512]);  mm_580 = None
        sigmoid_234 = torch.ops.aten.sigmoid.default(view_2968);  view_2968 = None
        mul_387 = torch.ops.aten.mul.Tensor(view_2966, sigmoid_234);  view_2966 = sigmoid_234 = None
        unsqueeze_970 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_186 = torch.ops.aten.bitwise_not.default(unsqueeze_970);  unsqueeze_970 = None
        masked_fill_186 = torch.ops.aten.masked_fill.Scalar(mul_387, bitwise_not_186, 0);  mul_387 = bitwise_not_186 = None
        split_tensor_307 = torch.ops.aten.split.Tensor(masked_fill_186, 256, dim = -1)
        getitem_2860 = split_tensor_307[0]
        unsqueeze_973 = torch.ops.aten.unsqueeze.default(getitem_2860, 4);  getitem_2860 = None
        permute_1592 = torch.ops.aten.permute.default(unsqueeze_973, [0, 1, 4, 3, 2]);  unsqueeze_973 = None
        permute_1593 = torch.ops.aten.permute.default(permute_1592, [3, 1, 4, 0, 2]);  permute_1592 = None
        view_2971 = torch.ops.aten.view.default(permute_1593, [256, 512, 512]);  permute_1593 = None
        split_tensor_308 = torch.ops.aten.split.Tensor(masked_fill_186, 256, dim = -1);  masked_fill_186 = None
        getitem_2863 = split_tensor_308[1];  split_tensor_308 = None
        unsqueeze_974 = torch.ops.aten.unsqueeze.default(getitem_2863, 4);  getitem_2863 = None
        permute_1594 = torch.ops.aten.permute.default(unsqueeze_974, [0, 4, 1, 3, 2]);  unsqueeze_974 = None
        permute_1595 = torch.ops.aten.permute.default(permute_1594, [3, 4, 0, 2, 1]);  permute_1594 = None
        view_2972 = torch.ops.aten.view.default(permute_1595, [256, 512, 512]);  permute_1595 = None
        bmm_238 = torch.ops.aten.bmm.default(view_2971, view_2972);  view_2971 = view_2972 = None
        view_2973 = torch.ops.aten.view.default(bmm_238, [256, 512, 1, 1, 512]);  bmm_238 = None
        permute_1596 = torch.ops.aten.permute.default(view_2973, [3, 1, 4, 0, 2]);  view_2973 = None
        view_2974 = torch.ops.aten.view.default(permute_1596, [1, 512, 512, 256]);  permute_1596 = None
        _to_copy_1680 = torch.ops.aten._to_copy.default(getitem_2854, dtype = torch.bfloat16);  getitem_2854 = None
        _to_copy_1681 = torch.ops.aten._to_copy.default(getitem_2850, dtype = torch.bfloat16)
        t_623 = torch.ops.aten.t.default(_to_copy_1680);  _to_copy_1680 = None
        view_2975 = torch.ops.aten.view.default(_to_copy_1681, [262144, 256]);  _to_copy_1681 = None
        mm_581 = torch.ops.aten.mm.default(view_2975, t_623);  view_2975 = t_623 = None
        view_2976 = torch.ops.aten.view.default(mm_581, [1, 512, 512, 512]);  mm_581 = None
        _to_copy_1682 = torch.ops.aten._to_copy.default(getitem_2856, dtype = torch.bfloat16);  getitem_2856 = None
        _to_copy_1683 = torch.ops.aten._to_copy.default(getitem_2850, dtype = torch.bfloat16)
        t_624 = torch.ops.aten.t.default(_to_copy_1682);  _to_copy_1682 = None
        view_2977 = torch.ops.aten.view.default(_to_copy_1683, [262144, 256]);  _to_copy_1683 = None
        mm_582 = torch.ops.aten.mm.default(view_2977, t_624);  view_2977 = t_624 = None
        view_2978 = torch.ops.aten.view.default(mm_582, [1, 512, 512, 512]);  mm_582 = None
        sigmoid_235 = torch.ops.aten.sigmoid.default(view_2978);  view_2978 = None
        mul_388 = torch.ops.aten.mul.Tensor(view_2976, sigmoid_235);  view_2976 = sigmoid_235 = None
        view_2979 = torch.ops.aten.view.default(mul_388, [262144, 512]);  mul_388 = None
        view_2980 = torch.ops.aten.view.default(view_2979, [1, 512, 512, 512]);  view_2979 = None
        transpose_76 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_975 = torch.ops.aten.unsqueeze.default(transpose_76, 3);  transpose_76 = None
        clone_255 = torch.ops.aten.clone.default(unsqueeze_975, memory_format = torch.contiguous_format);  unsqueeze_975 = None
        bitwise_not_187 = torch.ops.aten.bitwise_not.default(clone_255);  clone_255 = None
        masked_fill_187 = torch.ops.aten.masked_fill.Scalar(view_2980, bitwise_not_187, 0);  view_2980 = bitwise_not_187 = None
        view_2981 = torch.ops.aten.view.default(masked_fill_187, [262144, 512]);  masked_fill_187 = None
        view_2985 = torch.ops.aten.view.default(view_2981, [1, 512, 512, 512])
        split_tensor_309 = torch.ops.aten.split.Tensor(view_2985, 256, dim = -1);  view_2985 = None
        getitem_2866 = split_tensor_309[0]
        unsqueeze_978 = torch.ops.aten.unsqueeze.default(getitem_2866, 4);  getitem_2866 = None
        permute_1601 = torch.ops.aten.permute.default(unsqueeze_978, [0, 2, 4, 3, 1]);  unsqueeze_978 = None
        permute_1602 = torch.ops.aten.permute.default(permute_1601, [3, 1, 4, 0, 2]);  permute_1601 = None
        view_2986 = torch.ops.aten.view.default(permute_1602, [256, 512, 512]);  permute_1602 = None
        view_2987 = torch.ops.aten.view.default(view_2981, [1, 512, 512, 512]);  view_2981 = None
        split_tensor_310 = torch.ops.aten.split.Tensor(view_2987, 256, dim = -1);  view_2987 = None
        getitem_2869 = split_tensor_310[1];  split_tensor_310 = None
        unsqueeze_979 = torch.ops.aten.unsqueeze.default(getitem_2869, 4);  getitem_2869 = None
        permute_1603 = torch.ops.aten.permute.default(unsqueeze_979, [0, 4, 2, 3, 1]);  unsqueeze_979 = None
        permute_1604 = torch.ops.aten.permute.default(permute_1603, [3, 4, 0, 2, 1]);  permute_1603 = None
        view_2988 = torch.ops.aten.view.default(permute_1604, [256, 512, 512]);  permute_1604 = None
        bmm_239 = torch.ops.aten.bmm.default(view_2986, view_2988);  view_2986 = view_2988 = None
        view_2989 = torch.ops.aten.view.default(bmm_239, [256, 512, 1, 1, 512]);  bmm_239 = None
        permute_1605 = torch.ops.aten.permute.default(view_2989, [3, 1, 4, 0, 2]);  view_2989 = None
        view_2990 = torch.ops.aten.view.default(permute_1605, [1, 512, 512, 256]);  permute_1605 = None
        _to_copy_1684 = torch.ops.aten._to_copy.default(view_2974, dtype = torch.float32);  view_2974 = None
        native_layer_norm_default_347 = torch.ops.aten.native_layer_norm.default(_to_copy_1684, [256], None, None, 1e-05);  _to_copy_1684 = None
        getitem_2870 = native_layer_norm_default_347[0]
        _to_copy_1685 = torch.ops.aten._to_copy.default(view_2990, dtype = torch.float32);  view_2990 = None
        native_layer_norm_default_348 = torch.ops.aten.native_layer_norm.default(_to_copy_1685, [256], None, None, 1e-05);  _to_copy_1685 = None
        getitem_2873 = native_layer_norm_default_348[0]
        add_312 = torch.ops.aten.add.Tensor(getitem_2870, getitem_2873);  getitem_2870 = getitem_2873 = None
        _to_copy_1686 = torch.ops.aten._to_copy.default(arg982_1, dtype = torch.bfloat16);  arg982_1 = None
        _to_copy_1687 = torch.ops.aten._to_copy.default(add_312, dtype = torch.bfloat16);  add_312 = None
        t_625 = torch.ops.aten.t.default(_to_copy_1686);  _to_copy_1686 = None
        view_2991 = torch.ops.aten.view.default(_to_copy_1687, [262144, 256]);  _to_copy_1687 = None
        mm_583 = torch.ops.aten.mm.default(view_2991, t_625);  view_2991 = t_625 = None
        view_2992 = torch.ops.aten.view.default(mm_583, [1, 512, 512, 256]);  mm_583 = None
        _to_copy_1688 = torch.ops.aten._to_copy.default(getitem_2857, dtype = torch.bfloat16);  getitem_2857 = None
        _to_copy_1689 = torch.ops.aten._to_copy.default(getitem_2850, dtype = torch.bfloat16);  getitem_2850 = None
        t_626 = torch.ops.aten.t.default(_to_copy_1688);  _to_copy_1688 = None
        view_2993 = torch.ops.aten.view.default(_to_copy_1689, [262144, 256]);  _to_copy_1689 = None
        mm_584 = torch.ops.aten.mm.default(view_2993, t_626);  view_2993 = t_626 = None
        view_2994 = torch.ops.aten.view.default(mm_584, [1, 512, 512, 256]);  mm_584 = None
        sigmoid_236 = torch.ops.aten.sigmoid.default(view_2994);  view_2994 = None
        mul_389 = torch.ops.aten.mul.Tensor(view_2992, sigmoid_236);  view_2992 = sigmoid_236 = None
        add_313 = torch.ops.aten.add.Tensor(add_307, mul_389);  mul_389 = None
        _to_copy_1690 = torch.ops.aten._to_copy.default(add_307, dtype = torch.float32)
        native_layer_norm_default_349 = torch.ops.aten.native_layer_norm.default(_to_copy_1690, [256], None, None, 1e-05);  _to_copy_1690 = None
        getitem_2876 = native_layer_norm_default_349[0]
        _to_copy_1691 = torch.ops.aten._to_copy.default(arg986_1, dtype = torch.bfloat16);  arg986_1 = None
        _to_copy_1692 = torch.ops.aten._to_copy.default(getitem_2876, dtype = torch.bfloat16)
        t_627 = torch.ops.aten.t.default(_to_copy_1691);  _to_copy_1691 = None
        view_2995 = torch.ops.aten.view.default(_to_copy_1692, [262144, 256]);  _to_copy_1692 = None
        mm_585 = torch.ops.aten.mm.default(view_2995, t_627);  view_2995 = t_627 = None
        view_2996 = torch.ops.aten.view.default(mm_585, [1, 512, 512, 8]);  mm_585 = None
        view_2997 = torch.ops.aten.view.default(view_2996, [1, 512, 512, 2, 4]);  view_2996 = None
        permute_1606 = torch.ops.aten.permute.default(view_2997, [0, 3, 4, 1, 2]);  view_2997 = None
        view_2998 = torch.ops.aten.view.default(permute_1606, [1, 2, 4, 1, 512, 512]);  permute_1606 = None
        view_2999 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_188 = torch.ops.aten.bitwise_not.default(view_2999);  view_2999 = None
        masked_fill_188 = torch.ops.aten.masked_fill.Scalar(view_2998, bitwise_not_188, -10000);  view_2998 = bitwise_not_188 = None
        view_3000 = torch.ops.aten.view.default(masked_fill_188, [1, 2, 4, 512, 512]);  masked_fill_188 = None
        permute_1607 = torch.ops.aten.permute.default(view_3000, [1, 0, 2, 3, 4]);  view_3000 = None
        view_3001 = torch.ops.aten.view.default(permute_1607, [2, 4, 1, 512, 512]);  permute_1607 = None
        _to_copy_1693 = torch.ops.aten._to_copy.default(arg987_1, dtype = torch.bfloat16);  arg987_1 = None
        _to_copy_1694 = torch.ops.aten._to_copy.default(getitem_2876, dtype = torch.bfloat16)
        t_628 = torch.ops.aten.t.default(_to_copy_1693);  _to_copy_1693 = None
        view_3002 = torch.ops.aten.view.default(_to_copy_1694, [262144, 256]);  _to_copy_1694 = None
        mm_586 = torch.ops.aten.mm.default(view_3002, t_628);  view_3002 = t_628 = None
        view_3003 = torch.ops.aten.view.default(mm_586, [1, 512, 512, 1024]);  mm_586 = None
        select_77 = torch.ops.aten.select.int(view_3001, 0, 0)
        view_3004 = torch.ops.aten.view.default(view_3003, [1, 512, 512, 4, 4, 64]);  view_3003 = None
        permute_1608 = torch.ops.aten.permute.default(view_3004, [4, 0, 3, 1, 2, 5]);  view_3004 = None
        view_3005 = torch.ops.aten.view.default(permute_1608, [4, 4, 512, 512, 64]);  permute_1608 = None
        unbind_int_136 = torch.ops.aten.unbind.int(view_3005);  view_3005 = None
        getitem_2879 = unbind_int_136[0]
        getitem_2880 = unbind_int_136[1]
        getitem_2881 = unbind_int_136[2]
        getitem_2882 = unbind_int_136[3];  unbind_int_136 = None
        expand_187 = torch.ops.aten.expand.default(select_77, [4, 512, 512, 512]);  select_77 = None
        _scaled_dot_product_efficient_attention_default_108 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2879, getitem_2880, getitem_2881, expand_187, False);  getitem_2879 = getitem_2880 = getitem_2881 = expand_187 = None
        getitem_2883 = _scaled_dot_product_efficient_attention_default_108[0]
        sigmoid_237 = torch.ops.aten.sigmoid.default(getitem_2882);  getitem_2882 = None
        mul_390 = torch.ops.aten.mul.Tensor(getitem_2883, sigmoid_237);  getitem_2883 = sigmoid_237 = None
        view_3006 = torch.ops.aten.view.default(mul_390, [1, 4, 512, 512, 64]);  mul_390 = None
        permute_1609 = torch.ops.aten.permute.default(view_3006, [0, 2, 3, 1, 4]);  view_3006 = None
        clone_256 = torch.ops.aten.clone.default(permute_1609, memory_format = torch.contiguous_format);  permute_1609 = None
        _unsafe_view_216 = torch.ops.aten._unsafe_view.default(clone_256, [1, 512, 512, 256]);  clone_256 = None
        transpose_77 = torch.ops.aten.transpose.int(getitem_2876, 1, 2);  getitem_2876 = None
        _to_copy_1695 = torch.ops.aten._to_copy.default(arg988_1, dtype = torch.bfloat16);  arg988_1 = None
        _to_copy_1696 = torch.ops.aten._to_copy.default(transpose_77, dtype = torch.bfloat16);  transpose_77 = None
        t_629 = torch.ops.aten.t.default(_to_copy_1695);  _to_copy_1695 = None
        expand_188 = torch.ops.aten.expand.default(_to_copy_1696, [1, 512, 512, 256]);  _to_copy_1696 = None
        view_3007 = torch.ops.aten.view.default(expand_188, [512, 512, 256]);  expand_188 = None
        expand_189 = torch.ops.aten.expand.default(t_629, [1, 512, 256, 1024]);  t_629 = None
        view_3008 = torch.ops.aten.view.default(expand_189, [512, 256, 1024]);  expand_189 = None
        bmm_240 = torch.ops.aten.bmm.default(view_3007, view_3008);  view_3007 = view_3008 = None
        view_3009 = torch.ops.aten.view.default(bmm_240, [1, 512, 512, 1024]);  bmm_240 = None
        select_78 = torch.ops.aten.select.int(view_3001, 0, 1);  view_3001 = None
        view_3010 = torch.ops.aten.view.default(view_3009, [1, 512, 512, 4, 4, 64]);  view_3009 = None
        permute_1610 = torch.ops.aten.permute.default(view_3010, [4, 0, 3, 1, 2, 5]);  view_3010 = None
        view_3011 = torch.ops.aten.view.default(permute_1610, [4, 4, 512, 512, 64]);  permute_1610 = None
        unbind_int_137 = torch.ops.aten.unbind.int(view_3011);  view_3011 = None
        getitem_2887 = unbind_int_137[0]
        getitem_2888 = unbind_int_137[1]
        getitem_2889 = unbind_int_137[2]
        getitem_2890 = unbind_int_137[3];  unbind_int_137 = None
        expand_190 = torch.ops.aten.expand.default(select_78, [4, 512, 512, 512]);  select_78 = None
        _scaled_dot_product_efficient_attention_default_109 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2887, getitem_2888, getitem_2889, expand_190, False);  getitem_2887 = getitem_2888 = getitem_2889 = expand_190 = None
        getitem_2891 = _scaled_dot_product_efficient_attention_default_109[0]
        sigmoid_238 = torch.ops.aten.sigmoid.default(getitem_2890);  getitem_2890 = None
        mul_391 = torch.ops.aten.mul.Tensor(getitem_2891, sigmoid_238);  getitem_2891 = sigmoid_238 = None
        view_3012 = torch.ops.aten.view.default(mul_391, [1, 4, 512, 512, 64]);  mul_391 = None
        permute_1611 = torch.ops.aten.permute.default(view_3012, [0, 2, 3, 1, 4]);  view_3012 = None
        clone_257 = torch.ops.aten.clone.default(permute_1611, memory_format = torch.contiguous_format);  permute_1611 = None
        _unsafe_view_217 = torch.ops.aten._unsafe_view.default(clone_257, [1, 512, 512, 256]);  clone_257 = None
        cat_44 = torch.ops.aten.cat.default([_unsafe_view_216, _unsafe_view_217], dim = -1);  _unsafe_view_216 = _unsafe_view_217 = None
        slice_237 = torch.ops.aten.slice.Tensor(arg985_1, dim = 0, start = 0, end = 9223372036854775807);  arg985_1 = None
        unsqueeze_980 = torch.ops.aten.unsqueeze.default(slice_237, 1);  slice_237 = None
        mul_392 = torch.ops.aten.mul.Tensor(arg989_1, unsqueeze_980);  arg989_1 = unsqueeze_980 = None
        _to_copy_1697 = torch.ops.aten._to_copy.default(mul_392, dtype = torch.bfloat16);  mul_392 = None
        t_630 = torch.ops.aten.t.default(_to_copy_1697);  _to_copy_1697 = None
        view_3013 = torch.ops.aten.view.default(cat_44, [262144, 512]);  cat_44 = None
        mm_587 = torch.ops.aten.mm.default(view_3013, t_630);  view_3013 = t_630 = None
        view_3014 = torch.ops.aten.view.default(mm_587, [1, 512, 512, 256]);  mm_587 = None
        add_314 = torch.ops.aten.add.Tensor(add_313, view_3014);  add_313 = view_3014 = None
        split_tensor_311 = torch.ops.aten.split.Tensor(add_307, 512, dim = -2)
        getitem_2895 = split_tensor_311[0];  split_tensor_311 = None
        _to_copy_1698 = torch.ops.aten._to_copy.default(getitem_2895, dtype = torch.float32);  getitem_2895 = None
        native_layer_norm_default_350 = torch.ops.aten.native_layer_norm.default(_to_copy_1698, [256], arg976_1, arg977_1, 1e-05);  _to_copy_1698 = arg976_1 = arg977_1 = None
        getitem_2896 = native_layer_norm_default_350[0]
        _to_copy_1699 = torch.ops.aten._to_copy.default(arg978_1, dtype = torch.bfloat16);  arg978_1 = None
        _to_copy_1700 = torch.ops.aten._to_copy.default(getitem_2896, dtype = torch.bfloat16);  getitem_2896 = None
        t_631 = torch.ops.aten.t.default(_to_copy_1699);  _to_copy_1699 = None
        view_3015 = torch.ops.aten.view.default(_to_copy_1700, [262144, 256]);  _to_copy_1700 = None
        mm_588 = torch.ops.aten.mm.default(view_3015, t_631);  view_3015 = t_631 = None
        view_3016 = torch.ops.aten.view.default(mm_588, [1, 512, 512, 1024]);  mm_588 = None
        split_tensor_312 = torch.ops.aten.split.Tensor(view_3016, 512, dim = -1);  view_3016 = None
        getitem_2899 = split_tensor_312[0]
        getitem_2900 = split_tensor_312[1];  split_tensor_312 = None
        silu_82 = torch.ops.aten.silu.default(getitem_2899);  getitem_2899 = None
        mul_393 = torch.ops.aten.mul.Tensor(silu_82, getitem_2900);  silu_82 = getitem_2900 = None
        _to_copy_1701 = torch.ops.aten._to_copy.default(arg979_1, dtype = torch.bfloat16);  arg979_1 = None
        t_632 = torch.ops.aten.t.default(_to_copy_1701);  _to_copy_1701 = None
        view_3018 = torch.ops.aten.view.default(mul_393, [262144, 512]);  mul_393 = None
        mm_589 = torch.ops.aten.mm.default(view_3018, t_632);  view_3018 = t_632 = None
        view_3019 = torch.ops.aten.view.default(mm_589, [1, 512, 512, 256]);  mm_589 = None
        add_315 = torch.ops.aten.add.Tensor(add_314, view_3019);  add_314 = view_3019 = None
        _to_copy_1702 = torch.ops.aten._to_copy.default(add_311, dtype = torch.float32)
        native_layer_norm_default_351 = torch.ops.aten.native_layer_norm.default(_to_copy_1702, [384], arg994_1, arg995_1, 1e-05);  _to_copy_1702 = arg994_1 = arg995_1 = None
        getitem_2901 = native_layer_norm_default_351[0]
        _to_copy_1703 = torch.ops.aten._to_copy.default(add_307, dtype = torch.float32);  add_307 = None
        native_layer_norm_default_352 = torch.ops.aten.native_layer_norm.default(_to_copy_1703, [256], arg996_1, arg997_1, 1e-05);  _to_copy_1703 = arg996_1 = arg997_1 = None
        getitem_2904 = native_layer_norm_default_352[0]
        _to_copy_1704 = torch.ops.aten._to_copy.default(arg998_1, dtype = torch.bfloat16);  arg998_1 = None
        _to_copy_1705 = torch.ops.aten._to_copy.default(getitem_2904, dtype = torch.bfloat16);  getitem_2904 = None
        t_633 = torch.ops.aten.t.default(_to_copy_1704);  _to_copy_1704 = None
        view_3020 = torch.ops.aten.view.default(_to_copy_1705, [262144, 256]);  _to_copy_1705 = None
        mm_590 = torch.ops.aten.mm.default(view_3020, t_633);  view_3020 = t_633 = None
        view_3021 = torch.ops.aten.view.default(mm_590, [1, 512, 512, 16]);  mm_590 = None
        permute_1612 = torch.ops.aten.permute.default(view_3021, [0, 3, 1, 2]);  view_3021 = None
        view_3022 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_189 = torch.ops.aten.bitwise_not.default(view_3022);  view_3022 = None
        masked_fill_189 = torch.ops.aten.masked_fill.Scalar(permute_1612, bitwise_not_189, -10000);  permute_1612 = bitwise_not_189 = None
        _to_copy_1706 = torch.ops.aten._to_copy.default(getitem_2901, dtype = torch.bfloat16);  getitem_2901 = None
        _to_copy_1707 = torch.ops.aten._to_copy.default(arg1000_1, dtype = torch.bfloat16);  arg1000_1 = None
        unsqueeze_981 = torch.ops.aten.unsqueeze.default(_to_copy_1706, 3);  _to_copy_1706 = None
        unsqueeze_982 = torch.ops.aten.unsqueeze.default(unsqueeze_981, 4);  unsqueeze_981 = None
        unsqueeze_983 = torch.ops.aten.unsqueeze.default(unsqueeze_982, 5);  unsqueeze_982 = None
        permute_1613 = torch.ops.aten.permute.default(unsqueeze_983, [3, 0, 4, 1, 5, 2]);  unsqueeze_983 = None
        unsqueeze_984 = torch.ops.aten.unsqueeze.default(_to_copy_1707, 4);  _to_copy_1707 = None
        unsqueeze_985 = torch.ops.aten.unsqueeze.default(unsqueeze_984, 5);  unsqueeze_984 = None
        permute_1614 = torch.ops.aten.permute.default(unsqueeze_985, [1, 4, 2, 5, 3, 0]);  unsqueeze_985 = None
        permute_1615 = torch.ops.aten.permute.default(permute_1613, [3, 5, 0, 1, 2, 4]);  permute_1613 = None
        view_3023 = torch.ops.aten.view.default(permute_1615, [1, 512, 384]);  permute_1615 = None
        permute_1616 = torch.ops.aten.permute.default(permute_1614, [5, 0, 1, 2, 4, 3]);  permute_1614 = None
        view_3024 = torch.ops.aten.view.default(permute_1616, [1, 384, 1536]);  permute_1616 = None
        bmm_241 = torch.ops.aten.bmm.default(view_3023, view_3024);  view_3023 = view_3024 = None
        view_3025 = torch.ops.aten.view.default(bmm_241, [512, 1, 4, 1, 16, 24]);  bmm_241 = None
        permute_1617 = torch.ops.aten.permute.default(view_3025, [2, 3, 4, 0, 5, 1]);  view_3025 = None
        view_3026 = torch.ops.aten.view.default(permute_1617, [4, 1, 16, 512, 24]);  permute_1617 = None
        unbind_int_138 = torch.ops.aten.unbind.int(view_3026);  view_3026 = None
        getitem_2907 = unbind_int_138[0]
        getitem_2908 = unbind_int_138[1]
        getitem_2909 = unbind_int_138[2]
        getitem_2910 = unbind_int_138[3];  unbind_int_138 = None
        view_3027 = torch.ops.aten.view.default(arg999_1, [1, 16, 1, 24]);  arg999_1 = None
        add_316 = torch.ops.aten.add.Tensor(getitem_2907, view_3027);  getitem_2907 = view_3027 = None
        _to_copy_1708 = torch.ops.aten._to_copy.default(add_316, dtype = torch.bfloat16);  add_316 = None
        expand_191 = torch.ops.aten.expand.default(masked_fill_189, [1, 16, 512, 512]);  masked_fill_189 = None
        _scaled_dot_product_efficient_attention_default_110 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1708, getitem_2908, getitem_2909, expand_191, False);  _to_copy_1708 = getitem_2908 = getitem_2909 = expand_191 = None
        getitem_2911 = _scaled_dot_product_efficient_attention_default_110[0]
        add_317 = torch.ops.aten.add.Tensor(getitem_2910, 1);  getitem_2910 = None
        sigmoid_239 = torch.ops.aten.sigmoid.default(add_317);  add_317 = None
        mul_394 = torch.ops.aten.mul.Tensor(getitem_2911, sigmoid_239);  getitem_2911 = sigmoid_239 = None
        _to_copy_1709 = torch.ops.aten._to_copy.default(arg1001_1, dtype = torch.bfloat16);  arg1001_1 = None
        unsqueeze_986 = torch.ops.aten.unsqueeze.default(mul_394, 4);  mul_394 = None
        permute_1618 = torch.ops.aten.permute.default(unsqueeze_986, [0, 2, 4, 3, 1]);  unsqueeze_986 = None
        unsqueeze_987 = torch.ops.aten.unsqueeze.default(_to_copy_1709, 3);  _to_copy_1709 = None
        unsqueeze_988 = torch.ops.aten.unsqueeze.default(unsqueeze_987, 4);  unsqueeze_987 = None
        permute_1619 = torch.ops.aten.permute.default(unsqueeze_988, [3, 4, 2, 1, 0]);  unsqueeze_988 = None
        permute_1620 = torch.ops.aten.permute.default(permute_1618, [1, 3, 4, 0, 2]);  permute_1618 = None
        clone_258 = torch.ops.aten.clone.default(permute_1620, memory_format = torch.contiguous_format);  permute_1620 = None
        _unsafe_view_218 = torch.ops.aten._unsafe_view.default(clone_258, [1, 512, 384]);  clone_258 = None
        permute_1621 = torch.ops.aten.permute.default(permute_1619, [3, 4, 0, 2, 1]);  permute_1619 = None
        clone_259 = torch.ops.aten.clone.default(permute_1621, memory_format = torch.contiguous_format);  permute_1621 = None
        _unsafe_view_219 = torch.ops.aten._unsafe_view.default(clone_259, [1, 384, 384]);  clone_259 = None
        bmm_242 = torch.ops.aten.bmm.default(_unsafe_view_218, _unsafe_view_219);  _unsafe_view_218 = _unsafe_view_219 = None
        view_3028 = torch.ops.aten.view.default(bmm_242, [512, 1, 1, 1, 384]);  bmm_242 = None
        permute_1622 = torch.ops.aten.permute.default(view_3028, [3, 0, 4, 1, 2]);  view_3028 = None
        view_3029 = torch.ops.aten.view.default(permute_1622, [1, 512, 384]);  permute_1622 = None
        unsqueeze_989 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_395 = torch.ops.aten.mul.Tensor(view_3029, unsqueeze_989);  view_3029 = unsqueeze_989 = None
        add_318 = torch.ops.aten.add.Tensor(add_311, mul_395);  mul_395 = None
        split_tensor_313 = torch.ops.aten.split.Tensor(add_311, 512, dim = -2);  add_311 = None
        getitem_2915 = split_tensor_313[0];  split_tensor_313 = None
        _to_copy_1710 = torch.ops.aten._to_copy.default(getitem_2915, dtype = torch.float32);  getitem_2915 = None
        native_layer_norm_default_353 = torch.ops.aten.native_layer_norm.default(_to_copy_1710, [384], arg990_1, arg991_1, 1e-05);  _to_copy_1710 = arg990_1 = arg991_1 = None
        getitem_2916 = native_layer_norm_default_353[0]
        _to_copy_1711 = torch.ops.aten._to_copy.default(arg992_1, dtype = torch.bfloat16);  arg992_1 = None
        _to_copy_1712 = torch.ops.aten._to_copy.default(getitem_2916, dtype = torch.bfloat16);  getitem_2916 = None
        t_634 = torch.ops.aten.t.default(_to_copy_1711);  _to_copy_1711 = None
        view_3030 = torch.ops.aten.view.default(_to_copy_1712, [512, 384]);  _to_copy_1712 = None
        mm_591 = torch.ops.aten.mm.default(view_3030, t_634);  view_3030 = t_634 = None
        view_3031 = torch.ops.aten.view.default(mm_591, [1, 512, 1536]);  mm_591 = None
        split_tensor_314 = torch.ops.aten.split.Tensor(view_3031, 768, dim = -1);  view_3031 = None
        getitem_2919 = split_tensor_314[0]
        getitem_2920 = split_tensor_314[1];  split_tensor_314 = None
        silu_83 = torch.ops.aten.silu.default(getitem_2919);  getitem_2919 = None
        mul_396 = torch.ops.aten.mul.Tensor(silu_83, getitem_2920);  silu_83 = getitem_2920 = None
        _to_copy_1713 = torch.ops.aten._to_copy.default(arg993_1, dtype = torch.bfloat16);  arg993_1 = None
        t_635 = torch.ops.aten.t.default(_to_copy_1713);  _to_copy_1713 = None
        view_3033 = torch.ops.aten.view.default(mul_396, [512, 768]);  mul_396 = None
        mm_592 = torch.ops.aten.mm.default(view_3033, t_635);  view_3033 = t_635 = None
        view_3034 = torch.ops.aten.view.default(mm_592, [1, 512, 384]);  mm_592 = None
        add_319 = torch.ops.aten.add.Tensor(add_318, view_3034);  add_318 = view_3034 = None
        _to_copy_1714 = torch.ops.aten._to_copy.default(add_315, dtype = torch.float32)
        native_layer_norm_default_354 = torch.ops.aten.native_layer_norm.default(_to_copy_1714, [256], arg1006_1, arg1007_1, 1e-05);  _to_copy_1714 = arg1006_1 = arg1007_1 = None
        getitem_2921 = native_layer_norm_default_354[0]
        split_with_sizes_default_78 = torch.ops.aten.split_with_sizes.default(arg1009_1, [512, 512]);  arg1009_1 = None
        getitem_2924 = split_with_sizes_default_78[0]
        getitem_2925 = split_with_sizes_default_78[1];  split_with_sizes_default_78 = None
        split_with_sizes_default_79 = torch.ops.aten.split_with_sizes.default(arg1010_1, [512, 512, 256]);  arg1010_1 = None
        getitem_2926 = split_with_sizes_default_79[0]
        getitem_2927 = split_with_sizes_default_79[1]
        getitem_2928 = split_with_sizes_default_79[2];  split_with_sizes_default_79 = None
        _to_copy_1715 = torch.ops.aten._to_copy.default(getitem_2924, dtype = torch.bfloat16);  getitem_2924 = None
        _to_copy_1716 = torch.ops.aten._to_copy.default(getitem_2921, dtype = torch.bfloat16)
        t_636 = torch.ops.aten.t.default(_to_copy_1715);  _to_copy_1715 = None
        view_3035 = torch.ops.aten.view.default(_to_copy_1716, [262144, 256]);  _to_copy_1716 = None
        mm_593 = torch.ops.aten.mm.default(view_3035, t_636);  view_3035 = t_636 = None
        view_3036 = torch.ops.aten.view.default(mm_593, [1, 512, 512, 512]);  mm_593 = None
        _to_copy_1717 = torch.ops.aten._to_copy.default(getitem_2926, dtype = torch.bfloat16);  getitem_2926 = None
        _to_copy_1718 = torch.ops.aten._to_copy.default(getitem_2921, dtype = torch.bfloat16)
        t_637 = torch.ops.aten.t.default(_to_copy_1717);  _to_copy_1717 = None
        view_3037 = torch.ops.aten.view.default(_to_copy_1718, [262144, 256]);  _to_copy_1718 = None
        mm_594 = torch.ops.aten.mm.default(view_3037, t_637);  view_3037 = t_637 = None
        view_3038 = torch.ops.aten.view.default(mm_594, [1, 512, 512, 512]);  mm_594 = None
        sigmoid_240 = torch.ops.aten.sigmoid.default(view_3038);  view_3038 = None
        mul_397 = torch.ops.aten.mul.Tensor(view_3036, sigmoid_240);  view_3036 = sigmoid_240 = None
        unsqueeze_990 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_190 = torch.ops.aten.bitwise_not.default(unsqueeze_990);  unsqueeze_990 = None
        masked_fill_190 = torch.ops.aten.masked_fill.Scalar(mul_397, bitwise_not_190, 0);  mul_397 = bitwise_not_190 = None
        split_tensor_315 = torch.ops.aten.split.Tensor(masked_fill_190, 256, dim = -1)
        getitem_2931 = split_tensor_315[0]
        unsqueeze_993 = torch.ops.aten.unsqueeze.default(getitem_2931, 4);  getitem_2931 = None
        permute_1627 = torch.ops.aten.permute.default(unsqueeze_993, [0, 1, 4, 3, 2]);  unsqueeze_993 = None
        permute_1628 = torch.ops.aten.permute.default(permute_1627, [3, 1, 4, 0, 2]);  permute_1627 = None
        view_3041 = torch.ops.aten.view.default(permute_1628, [256, 512, 512]);  permute_1628 = None
        split_tensor_316 = torch.ops.aten.split.Tensor(masked_fill_190, 256, dim = -1);  masked_fill_190 = None
        getitem_2934 = split_tensor_316[1];  split_tensor_316 = None
        unsqueeze_994 = torch.ops.aten.unsqueeze.default(getitem_2934, 4);  getitem_2934 = None
        permute_1629 = torch.ops.aten.permute.default(unsqueeze_994, [0, 4, 1, 3, 2]);  unsqueeze_994 = None
        permute_1630 = torch.ops.aten.permute.default(permute_1629, [3, 4, 0, 2, 1]);  permute_1629 = None
        view_3042 = torch.ops.aten.view.default(permute_1630, [256, 512, 512]);  permute_1630 = None
        bmm_243 = torch.ops.aten.bmm.default(view_3041, view_3042);  view_3041 = view_3042 = None
        view_3043 = torch.ops.aten.view.default(bmm_243, [256, 512, 1, 1, 512]);  bmm_243 = None
        permute_1631 = torch.ops.aten.permute.default(view_3043, [3, 1, 4, 0, 2]);  view_3043 = None
        view_3044 = torch.ops.aten.view.default(permute_1631, [1, 512, 512, 256]);  permute_1631 = None
        _to_copy_1719 = torch.ops.aten._to_copy.default(getitem_2925, dtype = torch.bfloat16);  getitem_2925 = None
        _to_copy_1720 = torch.ops.aten._to_copy.default(getitem_2921, dtype = torch.bfloat16)
        t_638 = torch.ops.aten.t.default(_to_copy_1719);  _to_copy_1719 = None
        view_3045 = torch.ops.aten.view.default(_to_copy_1720, [262144, 256]);  _to_copy_1720 = None
        mm_595 = torch.ops.aten.mm.default(view_3045, t_638);  view_3045 = t_638 = None
        view_3046 = torch.ops.aten.view.default(mm_595, [1, 512, 512, 512]);  mm_595 = None
        _to_copy_1721 = torch.ops.aten._to_copy.default(getitem_2927, dtype = torch.bfloat16);  getitem_2927 = None
        _to_copy_1722 = torch.ops.aten._to_copy.default(getitem_2921, dtype = torch.bfloat16)
        t_639 = torch.ops.aten.t.default(_to_copy_1721);  _to_copy_1721 = None
        view_3047 = torch.ops.aten.view.default(_to_copy_1722, [262144, 256]);  _to_copy_1722 = None
        mm_596 = torch.ops.aten.mm.default(view_3047, t_639);  view_3047 = t_639 = None
        view_3048 = torch.ops.aten.view.default(mm_596, [1, 512, 512, 512]);  mm_596 = None
        sigmoid_241 = torch.ops.aten.sigmoid.default(view_3048);  view_3048 = None
        mul_398 = torch.ops.aten.mul.Tensor(view_3046, sigmoid_241);  view_3046 = sigmoid_241 = None
        view_3049 = torch.ops.aten.view.default(mul_398, [262144, 512]);  mul_398 = None
        view_3050 = torch.ops.aten.view.default(view_3049, [1, 512, 512, 512]);  view_3049 = None
        transpose_78 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_995 = torch.ops.aten.unsqueeze.default(transpose_78, 3);  transpose_78 = None
        clone_260 = torch.ops.aten.clone.default(unsqueeze_995, memory_format = torch.contiguous_format);  unsqueeze_995 = None
        bitwise_not_191 = torch.ops.aten.bitwise_not.default(clone_260);  clone_260 = None
        masked_fill_191 = torch.ops.aten.masked_fill.Scalar(view_3050, bitwise_not_191, 0);  view_3050 = bitwise_not_191 = None
        view_3051 = torch.ops.aten.view.default(masked_fill_191, [262144, 512]);  masked_fill_191 = None
        view_3055 = torch.ops.aten.view.default(view_3051, [1, 512, 512, 512])
        split_tensor_317 = torch.ops.aten.split.Tensor(view_3055, 256, dim = -1);  view_3055 = None
        getitem_2937 = split_tensor_317[0]
        unsqueeze_998 = torch.ops.aten.unsqueeze.default(getitem_2937, 4);  getitem_2937 = None
        permute_1636 = torch.ops.aten.permute.default(unsqueeze_998, [0, 2, 4, 3, 1]);  unsqueeze_998 = None
        permute_1637 = torch.ops.aten.permute.default(permute_1636, [3, 1, 4, 0, 2]);  permute_1636 = None
        view_3056 = torch.ops.aten.view.default(permute_1637, [256, 512, 512]);  permute_1637 = None
        view_3057 = torch.ops.aten.view.default(view_3051, [1, 512, 512, 512]);  view_3051 = None
        split_tensor_318 = torch.ops.aten.split.Tensor(view_3057, 256, dim = -1);  view_3057 = None
        getitem_2940 = split_tensor_318[1];  split_tensor_318 = None
        unsqueeze_999 = torch.ops.aten.unsqueeze.default(getitem_2940, 4);  getitem_2940 = None
        permute_1638 = torch.ops.aten.permute.default(unsqueeze_999, [0, 4, 2, 3, 1]);  unsqueeze_999 = None
        permute_1639 = torch.ops.aten.permute.default(permute_1638, [3, 4, 0, 2, 1]);  permute_1638 = None
        view_3058 = torch.ops.aten.view.default(permute_1639, [256, 512, 512]);  permute_1639 = None
        bmm_244 = torch.ops.aten.bmm.default(view_3056, view_3058);  view_3056 = view_3058 = None
        view_3059 = torch.ops.aten.view.default(bmm_244, [256, 512, 1, 1, 512]);  bmm_244 = None
        permute_1640 = torch.ops.aten.permute.default(view_3059, [3, 1, 4, 0, 2]);  view_3059 = None
        view_3060 = torch.ops.aten.view.default(permute_1640, [1, 512, 512, 256]);  permute_1640 = None
        _to_copy_1723 = torch.ops.aten._to_copy.default(view_3044, dtype = torch.float32);  view_3044 = None
        native_layer_norm_default_355 = torch.ops.aten.native_layer_norm.default(_to_copy_1723, [256], None, None, 1e-05);  _to_copy_1723 = None
        getitem_2941 = native_layer_norm_default_355[0]
        _to_copy_1724 = torch.ops.aten._to_copy.default(view_3060, dtype = torch.float32);  view_3060 = None
        native_layer_norm_default_356 = torch.ops.aten.native_layer_norm.default(_to_copy_1724, [256], None, None, 1e-05);  _to_copy_1724 = None
        getitem_2944 = native_layer_norm_default_356[0]
        add_320 = torch.ops.aten.add.Tensor(getitem_2941, getitem_2944);  getitem_2941 = getitem_2944 = None
        _to_copy_1725 = torch.ops.aten._to_copy.default(arg1008_1, dtype = torch.bfloat16);  arg1008_1 = None
        _to_copy_1726 = torch.ops.aten._to_copy.default(add_320, dtype = torch.bfloat16);  add_320 = None
        t_640 = torch.ops.aten.t.default(_to_copy_1725);  _to_copy_1725 = None
        view_3061 = torch.ops.aten.view.default(_to_copy_1726, [262144, 256]);  _to_copy_1726 = None
        mm_597 = torch.ops.aten.mm.default(view_3061, t_640);  view_3061 = t_640 = None
        view_3062 = torch.ops.aten.view.default(mm_597, [1, 512, 512, 256]);  mm_597 = None
        _to_copy_1727 = torch.ops.aten._to_copy.default(getitem_2928, dtype = torch.bfloat16);  getitem_2928 = None
        _to_copy_1728 = torch.ops.aten._to_copy.default(getitem_2921, dtype = torch.bfloat16);  getitem_2921 = None
        t_641 = torch.ops.aten.t.default(_to_copy_1727);  _to_copy_1727 = None
        view_3063 = torch.ops.aten.view.default(_to_copy_1728, [262144, 256]);  _to_copy_1728 = None
        mm_598 = torch.ops.aten.mm.default(view_3063, t_641);  view_3063 = t_641 = None
        view_3064 = torch.ops.aten.view.default(mm_598, [1, 512, 512, 256]);  mm_598 = None
        sigmoid_242 = torch.ops.aten.sigmoid.default(view_3064);  view_3064 = None
        mul_399 = torch.ops.aten.mul.Tensor(view_3062, sigmoid_242);  view_3062 = sigmoid_242 = None
        add_321 = torch.ops.aten.add.Tensor(add_315, mul_399);  mul_399 = None
        _to_copy_1729 = torch.ops.aten._to_copy.default(add_315, dtype = torch.float32)
        native_layer_norm_default_357 = torch.ops.aten.native_layer_norm.default(_to_copy_1729, [256], None, None, 1e-05);  _to_copy_1729 = None
        getitem_2947 = native_layer_norm_default_357[0]
        _to_copy_1730 = torch.ops.aten._to_copy.default(arg1012_1, dtype = torch.bfloat16);  arg1012_1 = None
        _to_copy_1731 = torch.ops.aten._to_copy.default(getitem_2947, dtype = torch.bfloat16)
        t_642 = torch.ops.aten.t.default(_to_copy_1730);  _to_copy_1730 = None
        view_3065 = torch.ops.aten.view.default(_to_copy_1731, [262144, 256]);  _to_copy_1731 = None
        mm_599 = torch.ops.aten.mm.default(view_3065, t_642);  view_3065 = t_642 = None
        view_3066 = torch.ops.aten.view.default(mm_599, [1, 512, 512, 8]);  mm_599 = None
        view_3067 = torch.ops.aten.view.default(view_3066, [1, 512, 512, 2, 4]);  view_3066 = None
        permute_1641 = torch.ops.aten.permute.default(view_3067, [0, 3, 4, 1, 2]);  view_3067 = None
        view_3068 = torch.ops.aten.view.default(permute_1641, [1, 2, 4, 1, 512, 512]);  permute_1641 = None
        view_3069 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_192 = torch.ops.aten.bitwise_not.default(view_3069);  view_3069 = None
        masked_fill_192 = torch.ops.aten.masked_fill.Scalar(view_3068, bitwise_not_192, -10000);  view_3068 = bitwise_not_192 = None
        view_3070 = torch.ops.aten.view.default(masked_fill_192, [1, 2, 4, 512, 512]);  masked_fill_192 = None
        permute_1642 = torch.ops.aten.permute.default(view_3070, [1, 0, 2, 3, 4]);  view_3070 = None
        view_3071 = torch.ops.aten.view.default(permute_1642, [2, 4, 1, 512, 512]);  permute_1642 = None
        _to_copy_1732 = torch.ops.aten._to_copy.default(arg1013_1, dtype = torch.bfloat16);  arg1013_1 = None
        _to_copy_1733 = torch.ops.aten._to_copy.default(getitem_2947, dtype = torch.bfloat16)
        t_643 = torch.ops.aten.t.default(_to_copy_1732);  _to_copy_1732 = None
        view_3072 = torch.ops.aten.view.default(_to_copy_1733, [262144, 256]);  _to_copy_1733 = None
        mm_600 = torch.ops.aten.mm.default(view_3072, t_643);  view_3072 = t_643 = None
        view_3073 = torch.ops.aten.view.default(mm_600, [1, 512, 512, 1024]);  mm_600 = None
        select_79 = torch.ops.aten.select.int(view_3071, 0, 0)
        view_3074 = torch.ops.aten.view.default(view_3073, [1, 512, 512, 4, 4, 64]);  view_3073 = None
        permute_1643 = torch.ops.aten.permute.default(view_3074, [4, 0, 3, 1, 2, 5]);  view_3074 = None
        view_3075 = torch.ops.aten.view.default(permute_1643, [4, 4, 512, 512, 64]);  permute_1643 = None
        unbind_int_139 = torch.ops.aten.unbind.int(view_3075);  view_3075 = None
        getitem_2950 = unbind_int_139[0]
        getitem_2951 = unbind_int_139[1]
        getitem_2952 = unbind_int_139[2]
        getitem_2953 = unbind_int_139[3];  unbind_int_139 = None
        expand_192 = torch.ops.aten.expand.default(select_79, [4, 512, 512, 512]);  select_79 = None
        _scaled_dot_product_efficient_attention_default_111 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2950, getitem_2951, getitem_2952, expand_192, False);  getitem_2950 = getitem_2951 = getitem_2952 = expand_192 = None
        getitem_2954 = _scaled_dot_product_efficient_attention_default_111[0]
        sigmoid_243 = torch.ops.aten.sigmoid.default(getitem_2953);  getitem_2953 = None
        mul_400 = torch.ops.aten.mul.Tensor(getitem_2954, sigmoid_243);  getitem_2954 = sigmoid_243 = None
        view_3076 = torch.ops.aten.view.default(mul_400, [1, 4, 512, 512, 64]);  mul_400 = None
        permute_1644 = torch.ops.aten.permute.default(view_3076, [0, 2, 3, 1, 4]);  view_3076 = None
        clone_261 = torch.ops.aten.clone.default(permute_1644, memory_format = torch.contiguous_format);  permute_1644 = None
        _unsafe_view_220 = torch.ops.aten._unsafe_view.default(clone_261, [1, 512, 512, 256]);  clone_261 = None
        transpose_79 = torch.ops.aten.transpose.int(getitem_2947, 1, 2);  getitem_2947 = None
        _to_copy_1734 = torch.ops.aten._to_copy.default(arg1014_1, dtype = torch.bfloat16);  arg1014_1 = None
        _to_copy_1735 = torch.ops.aten._to_copy.default(transpose_79, dtype = torch.bfloat16);  transpose_79 = None
        t_644 = torch.ops.aten.t.default(_to_copy_1734);  _to_copy_1734 = None
        expand_193 = torch.ops.aten.expand.default(_to_copy_1735, [1, 512, 512, 256]);  _to_copy_1735 = None
        view_3077 = torch.ops.aten.view.default(expand_193, [512, 512, 256]);  expand_193 = None
        expand_194 = torch.ops.aten.expand.default(t_644, [1, 512, 256, 1024]);  t_644 = None
        view_3078 = torch.ops.aten.view.default(expand_194, [512, 256, 1024]);  expand_194 = None
        bmm_245 = torch.ops.aten.bmm.default(view_3077, view_3078);  view_3077 = view_3078 = None
        view_3079 = torch.ops.aten.view.default(bmm_245, [1, 512, 512, 1024]);  bmm_245 = None
        select_80 = torch.ops.aten.select.int(view_3071, 0, 1);  view_3071 = None
        view_3080 = torch.ops.aten.view.default(view_3079, [1, 512, 512, 4, 4, 64]);  view_3079 = None
        permute_1645 = torch.ops.aten.permute.default(view_3080, [4, 0, 3, 1, 2, 5]);  view_3080 = None
        view_3081 = torch.ops.aten.view.default(permute_1645, [4, 4, 512, 512, 64]);  permute_1645 = None
        unbind_int_140 = torch.ops.aten.unbind.int(view_3081);  view_3081 = None
        getitem_2958 = unbind_int_140[0]
        getitem_2959 = unbind_int_140[1]
        getitem_2960 = unbind_int_140[2]
        getitem_2961 = unbind_int_140[3];  unbind_int_140 = None
        expand_195 = torch.ops.aten.expand.default(select_80, [4, 512, 512, 512]);  select_80 = None
        _scaled_dot_product_efficient_attention_default_112 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_2958, getitem_2959, getitem_2960, expand_195, False);  getitem_2958 = getitem_2959 = getitem_2960 = expand_195 = None
        getitem_2962 = _scaled_dot_product_efficient_attention_default_112[0]
        sigmoid_244 = torch.ops.aten.sigmoid.default(getitem_2961);  getitem_2961 = None
        mul_401 = torch.ops.aten.mul.Tensor(getitem_2962, sigmoid_244);  getitem_2962 = sigmoid_244 = None
        view_3082 = torch.ops.aten.view.default(mul_401, [1, 4, 512, 512, 64]);  mul_401 = None
        permute_1646 = torch.ops.aten.permute.default(view_3082, [0, 2, 3, 1, 4]);  view_3082 = None
        clone_262 = torch.ops.aten.clone.default(permute_1646, memory_format = torch.contiguous_format);  permute_1646 = None
        _unsafe_view_221 = torch.ops.aten._unsafe_view.default(clone_262, [1, 512, 512, 256]);  clone_262 = None
        cat_45 = torch.ops.aten.cat.default([_unsafe_view_220, _unsafe_view_221], dim = -1);  _unsafe_view_220 = _unsafe_view_221 = None
        slice_238 = torch.ops.aten.slice.Tensor(arg1011_1, dim = 0, start = 0, end = 9223372036854775807);  arg1011_1 = None
        unsqueeze_1000 = torch.ops.aten.unsqueeze.default(slice_238, 1);  slice_238 = None
        mul_402 = torch.ops.aten.mul.Tensor(arg1015_1, unsqueeze_1000);  arg1015_1 = unsqueeze_1000 = None
        _to_copy_1736 = torch.ops.aten._to_copy.default(mul_402, dtype = torch.bfloat16);  mul_402 = None
        t_645 = torch.ops.aten.t.default(_to_copy_1736);  _to_copy_1736 = None
        view_3083 = torch.ops.aten.view.default(cat_45, [262144, 512]);  cat_45 = None
        mm_601 = torch.ops.aten.mm.default(view_3083, t_645);  view_3083 = t_645 = None
        view_3084 = torch.ops.aten.view.default(mm_601, [1, 512, 512, 256]);  mm_601 = None
        add_322 = torch.ops.aten.add.Tensor(add_321, view_3084);  add_321 = view_3084 = None
        split_tensor_319 = torch.ops.aten.split.Tensor(add_315, 512, dim = -2)
        getitem_2966 = split_tensor_319[0];  split_tensor_319 = None
        _to_copy_1737 = torch.ops.aten._to_copy.default(getitem_2966, dtype = torch.float32);  getitem_2966 = None
        native_layer_norm_default_358 = torch.ops.aten.native_layer_norm.default(_to_copy_1737, [256], arg1002_1, arg1003_1, 1e-05);  _to_copy_1737 = arg1002_1 = arg1003_1 = None
        getitem_2967 = native_layer_norm_default_358[0]
        _to_copy_1738 = torch.ops.aten._to_copy.default(arg1004_1, dtype = torch.bfloat16);  arg1004_1 = None
        _to_copy_1739 = torch.ops.aten._to_copy.default(getitem_2967, dtype = torch.bfloat16);  getitem_2967 = None
        t_646 = torch.ops.aten.t.default(_to_copy_1738);  _to_copy_1738 = None
        view_3085 = torch.ops.aten.view.default(_to_copy_1739, [262144, 256]);  _to_copy_1739 = None
        mm_602 = torch.ops.aten.mm.default(view_3085, t_646);  view_3085 = t_646 = None
        view_3086 = torch.ops.aten.view.default(mm_602, [1, 512, 512, 1024]);  mm_602 = None
        split_tensor_320 = torch.ops.aten.split.Tensor(view_3086, 512, dim = -1);  view_3086 = None
        getitem_2970 = split_tensor_320[0]
        getitem_2971 = split_tensor_320[1];  split_tensor_320 = None
        silu_84 = torch.ops.aten.silu.default(getitem_2970);  getitem_2970 = None
        mul_403 = torch.ops.aten.mul.Tensor(silu_84, getitem_2971);  silu_84 = getitem_2971 = None
        _to_copy_1740 = torch.ops.aten._to_copy.default(arg1005_1, dtype = torch.bfloat16);  arg1005_1 = None
        t_647 = torch.ops.aten.t.default(_to_copy_1740);  _to_copy_1740 = None
        view_3088 = torch.ops.aten.view.default(mul_403, [262144, 512]);  mul_403 = None
        mm_603 = torch.ops.aten.mm.default(view_3088, t_647);  view_3088 = t_647 = None
        view_3089 = torch.ops.aten.view.default(mm_603, [1, 512, 512, 256]);  mm_603 = None
        add_323 = torch.ops.aten.add.Tensor(add_322, view_3089);  add_322 = view_3089 = None
        _to_copy_1741 = torch.ops.aten._to_copy.default(add_319, dtype = torch.float32)
        native_layer_norm_default_359 = torch.ops.aten.native_layer_norm.default(_to_copy_1741, [384], arg1020_1, arg1021_1, 1e-05);  _to_copy_1741 = arg1020_1 = arg1021_1 = None
        getitem_2972 = native_layer_norm_default_359[0]
        _to_copy_1742 = torch.ops.aten._to_copy.default(add_315, dtype = torch.float32);  add_315 = None
        native_layer_norm_default_360 = torch.ops.aten.native_layer_norm.default(_to_copy_1742, [256], arg1022_1, arg1023_1, 1e-05);  _to_copy_1742 = arg1022_1 = arg1023_1 = None
        getitem_2975 = native_layer_norm_default_360[0]
        _to_copy_1743 = torch.ops.aten._to_copy.default(arg1024_1, dtype = torch.bfloat16);  arg1024_1 = None
        _to_copy_1744 = torch.ops.aten._to_copy.default(getitem_2975, dtype = torch.bfloat16);  getitem_2975 = None
        t_648 = torch.ops.aten.t.default(_to_copy_1743);  _to_copy_1743 = None
        view_3090 = torch.ops.aten.view.default(_to_copy_1744, [262144, 256]);  _to_copy_1744 = None
        mm_604 = torch.ops.aten.mm.default(view_3090, t_648);  view_3090 = t_648 = None
        view_3091 = torch.ops.aten.view.default(mm_604, [1, 512, 512, 16]);  mm_604 = None
        permute_1647 = torch.ops.aten.permute.default(view_3091, [0, 3, 1, 2]);  view_3091 = None
        view_3092 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_193 = torch.ops.aten.bitwise_not.default(view_3092);  view_3092 = None
        masked_fill_193 = torch.ops.aten.masked_fill.Scalar(permute_1647, bitwise_not_193, -10000);  permute_1647 = bitwise_not_193 = None
        _to_copy_1745 = torch.ops.aten._to_copy.default(getitem_2972, dtype = torch.bfloat16);  getitem_2972 = None
        _to_copy_1746 = torch.ops.aten._to_copy.default(arg1026_1, dtype = torch.bfloat16);  arg1026_1 = None
        unsqueeze_1001 = torch.ops.aten.unsqueeze.default(_to_copy_1745, 3);  _to_copy_1745 = None
        unsqueeze_1002 = torch.ops.aten.unsqueeze.default(unsqueeze_1001, 4);  unsqueeze_1001 = None
        unsqueeze_1003 = torch.ops.aten.unsqueeze.default(unsqueeze_1002, 5);  unsqueeze_1002 = None
        permute_1648 = torch.ops.aten.permute.default(unsqueeze_1003, [3, 0, 4, 1, 5, 2]);  unsqueeze_1003 = None
        unsqueeze_1004 = torch.ops.aten.unsqueeze.default(_to_copy_1746, 4);  _to_copy_1746 = None
        unsqueeze_1005 = torch.ops.aten.unsqueeze.default(unsqueeze_1004, 5);  unsqueeze_1004 = None
        permute_1649 = torch.ops.aten.permute.default(unsqueeze_1005, [1, 4, 2, 5, 3, 0]);  unsqueeze_1005 = None
        permute_1650 = torch.ops.aten.permute.default(permute_1648, [3, 5, 0, 1, 2, 4]);  permute_1648 = None
        view_3093 = torch.ops.aten.view.default(permute_1650, [1, 512, 384]);  permute_1650 = None
        permute_1651 = torch.ops.aten.permute.default(permute_1649, [5, 0, 1, 2, 4, 3]);  permute_1649 = None
        view_3094 = torch.ops.aten.view.default(permute_1651, [1, 384, 1536]);  permute_1651 = None
        bmm_246 = torch.ops.aten.bmm.default(view_3093, view_3094);  view_3093 = view_3094 = None
        view_3095 = torch.ops.aten.view.default(bmm_246, [512, 1, 4, 1, 16, 24]);  bmm_246 = None
        permute_1652 = torch.ops.aten.permute.default(view_3095, [2, 3, 4, 0, 5, 1]);  view_3095 = None
        view_3096 = torch.ops.aten.view.default(permute_1652, [4, 1, 16, 512, 24]);  permute_1652 = None
        unbind_int_141 = torch.ops.aten.unbind.int(view_3096);  view_3096 = None
        getitem_2978 = unbind_int_141[0]
        getitem_2979 = unbind_int_141[1]
        getitem_2980 = unbind_int_141[2]
        getitem_2981 = unbind_int_141[3];  unbind_int_141 = None
        view_3097 = torch.ops.aten.view.default(arg1025_1, [1, 16, 1, 24]);  arg1025_1 = None
        add_324 = torch.ops.aten.add.Tensor(getitem_2978, view_3097);  getitem_2978 = view_3097 = None
        _to_copy_1747 = torch.ops.aten._to_copy.default(add_324, dtype = torch.bfloat16);  add_324 = None
        expand_196 = torch.ops.aten.expand.default(masked_fill_193, [1, 16, 512, 512]);  masked_fill_193 = None
        _scaled_dot_product_efficient_attention_default_113 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1747, getitem_2979, getitem_2980, expand_196, False);  _to_copy_1747 = getitem_2979 = getitem_2980 = expand_196 = None
        getitem_2982 = _scaled_dot_product_efficient_attention_default_113[0]
        add_325 = torch.ops.aten.add.Tensor(getitem_2981, 1);  getitem_2981 = None
        sigmoid_245 = torch.ops.aten.sigmoid.default(add_325);  add_325 = None
        mul_404 = torch.ops.aten.mul.Tensor(getitem_2982, sigmoid_245);  getitem_2982 = sigmoid_245 = None
        _to_copy_1748 = torch.ops.aten._to_copy.default(arg1027_1, dtype = torch.bfloat16);  arg1027_1 = None
        unsqueeze_1006 = torch.ops.aten.unsqueeze.default(mul_404, 4);  mul_404 = None
        permute_1653 = torch.ops.aten.permute.default(unsqueeze_1006, [0, 2, 4, 3, 1]);  unsqueeze_1006 = None
        unsqueeze_1007 = torch.ops.aten.unsqueeze.default(_to_copy_1748, 3);  _to_copy_1748 = None
        unsqueeze_1008 = torch.ops.aten.unsqueeze.default(unsqueeze_1007, 4);  unsqueeze_1007 = None
        permute_1654 = torch.ops.aten.permute.default(unsqueeze_1008, [3, 4, 2, 1, 0]);  unsqueeze_1008 = None
        permute_1655 = torch.ops.aten.permute.default(permute_1653, [1, 3, 4, 0, 2]);  permute_1653 = None
        clone_263 = torch.ops.aten.clone.default(permute_1655, memory_format = torch.contiguous_format);  permute_1655 = None
        _unsafe_view_222 = torch.ops.aten._unsafe_view.default(clone_263, [1, 512, 384]);  clone_263 = None
        permute_1656 = torch.ops.aten.permute.default(permute_1654, [3, 4, 0, 2, 1]);  permute_1654 = None
        clone_264 = torch.ops.aten.clone.default(permute_1656, memory_format = torch.contiguous_format);  permute_1656 = None
        _unsafe_view_223 = torch.ops.aten._unsafe_view.default(clone_264, [1, 384, 384]);  clone_264 = None
        bmm_247 = torch.ops.aten.bmm.default(_unsafe_view_222, _unsafe_view_223);  _unsafe_view_222 = _unsafe_view_223 = None
        view_3098 = torch.ops.aten.view.default(bmm_247, [512, 1, 1, 1, 384]);  bmm_247 = None
        permute_1657 = torch.ops.aten.permute.default(view_3098, [3, 0, 4, 1, 2]);  view_3098 = None
        view_3099 = torch.ops.aten.view.default(permute_1657, [1, 512, 384]);  permute_1657 = None
        unsqueeze_1009 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_405 = torch.ops.aten.mul.Tensor(view_3099, unsqueeze_1009);  view_3099 = unsqueeze_1009 = None
        add_326 = torch.ops.aten.add.Tensor(add_319, mul_405);  mul_405 = None
        split_tensor_321 = torch.ops.aten.split.Tensor(add_319, 512, dim = -2);  add_319 = None
        getitem_2986 = split_tensor_321[0];  split_tensor_321 = None
        _to_copy_1749 = torch.ops.aten._to_copy.default(getitem_2986, dtype = torch.float32);  getitem_2986 = None
        native_layer_norm_default_361 = torch.ops.aten.native_layer_norm.default(_to_copy_1749, [384], arg1016_1, arg1017_1, 1e-05);  _to_copy_1749 = arg1016_1 = arg1017_1 = None
        getitem_2987 = native_layer_norm_default_361[0]
        _to_copy_1750 = torch.ops.aten._to_copy.default(arg1018_1, dtype = torch.bfloat16);  arg1018_1 = None
        _to_copy_1751 = torch.ops.aten._to_copy.default(getitem_2987, dtype = torch.bfloat16);  getitem_2987 = None
        t_649 = torch.ops.aten.t.default(_to_copy_1750);  _to_copy_1750 = None
        view_3100 = torch.ops.aten.view.default(_to_copy_1751, [512, 384]);  _to_copy_1751 = None
        mm_605 = torch.ops.aten.mm.default(view_3100, t_649);  view_3100 = t_649 = None
        view_3101 = torch.ops.aten.view.default(mm_605, [1, 512, 1536]);  mm_605 = None
        split_tensor_322 = torch.ops.aten.split.Tensor(view_3101, 768, dim = -1);  view_3101 = None
        getitem_2990 = split_tensor_322[0]
        getitem_2991 = split_tensor_322[1];  split_tensor_322 = None
        silu_85 = torch.ops.aten.silu.default(getitem_2990);  getitem_2990 = None
        mul_406 = torch.ops.aten.mul.Tensor(silu_85, getitem_2991);  silu_85 = getitem_2991 = None
        _to_copy_1752 = torch.ops.aten._to_copy.default(arg1019_1, dtype = torch.bfloat16);  arg1019_1 = None
        t_650 = torch.ops.aten.t.default(_to_copy_1752);  _to_copy_1752 = None
        view_3103 = torch.ops.aten.view.default(mul_406, [512, 768]);  mul_406 = None
        mm_606 = torch.ops.aten.mm.default(view_3103, t_650);  view_3103 = t_650 = None
        view_3104 = torch.ops.aten.view.default(mm_606, [1, 512, 384]);  mm_606 = None
        add_327 = torch.ops.aten.add.Tensor(add_326, view_3104);  add_326 = view_3104 = None
        _to_copy_1753 = torch.ops.aten._to_copy.default(add_323, dtype = torch.float32)
        native_layer_norm_default_362 = torch.ops.aten.native_layer_norm.default(_to_copy_1753, [256], arg1032_1, arg1033_1, 1e-05);  _to_copy_1753 = arg1032_1 = arg1033_1 = None
        getitem_2992 = native_layer_norm_default_362[0]
        split_with_sizes_default_80 = torch.ops.aten.split_with_sizes.default(arg1035_1, [512, 512]);  arg1035_1 = None
        getitem_2995 = split_with_sizes_default_80[0]
        getitem_2996 = split_with_sizes_default_80[1];  split_with_sizes_default_80 = None
        split_with_sizes_default_81 = torch.ops.aten.split_with_sizes.default(arg1036_1, [512, 512, 256]);  arg1036_1 = None
        getitem_2997 = split_with_sizes_default_81[0]
        getitem_2998 = split_with_sizes_default_81[1]
        getitem_2999 = split_with_sizes_default_81[2];  split_with_sizes_default_81 = None
        _to_copy_1754 = torch.ops.aten._to_copy.default(getitem_2995, dtype = torch.bfloat16);  getitem_2995 = None
        _to_copy_1755 = torch.ops.aten._to_copy.default(getitem_2992, dtype = torch.bfloat16)
        t_651 = torch.ops.aten.t.default(_to_copy_1754);  _to_copy_1754 = None
        view_3105 = torch.ops.aten.view.default(_to_copy_1755, [262144, 256]);  _to_copy_1755 = None
        mm_607 = torch.ops.aten.mm.default(view_3105, t_651);  view_3105 = t_651 = None
        view_3106 = torch.ops.aten.view.default(mm_607, [1, 512, 512, 512]);  mm_607 = None
        _to_copy_1756 = torch.ops.aten._to_copy.default(getitem_2997, dtype = torch.bfloat16);  getitem_2997 = None
        _to_copy_1757 = torch.ops.aten._to_copy.default(getitem_2992, dtype = torch.bfloat16)
        t_652 = torch.ops.aten.t.default(_to_copy_1756);  _to_copy_1756 = None
        view_3107 = torch.ops.aten.view.default(_to_copy_1757, [262144, 256]);  _to_copy_1757 = None
        mm_608 = torch.ops.aten.mm.default(view_3107, t_652);  view_3107 = t_652 = None
        view_3108 = torch.ops.aten.view.default(mm_608, [1, 512, 512, 512]);  mm_608 = None
        sigmoid_246 = torch.ops.aten.sigmoid.default(view_3108);  view_3108 = None
        mul_407 = torch.ops.aten.mul.Tensor(view_3106, sigmoid_246);  view_3106 = sigmoid_246 = None
        unsqueeze_1010 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_194 = torch.ops.aten.bitwise_not.default(unsqueeze_1010);  unsqueeze_1010 = None
        masked_fill_194 = torch.ops.aten.masked_fill.Scalar(mul_407, bitwise_not_194, 0);  mul_407 = bitwise_not_194 = None
        split_tensor_323 = torch.ops.aten.split.Tensor(masked_fill_194, 256, dim = -1)
        getitem_3002 = split_tensor_323[0]
        unsqueeze_1013 = torch.ops.aten.unsqueeze.default(getitem_3002, 4);  getitem_3002 = None
        permute_1662 = torch.ops.aten.permute.default(unsqueeze_1013, [0, 1, 4, 3, 2]);  unsqueeze_1013 = None
        permute_1663 = torch.ops.aten.permute.default(permute_1662, [3, 1, 4, 0, 2]);  permute_1662 = None
        view_3111 = torch.ops.aten.view.default(permute_1663, [256, 512, 512]);  permute_1663 = None
        split_tensor_324 = torch.ops.aten.split.Tensor(masked_fill_194, 256, dim = -1);  masked_fill_194 = None
        getitem_3005 = split_tensor_324[1];  split_tensor_324 = None
        unsqueeze_1014 = torch.ops.aten.unsqueeze.default(getitem_3005, 4);  getitem_3005 = None
        permute_1664 = torch.ops.aten.permute.default(unsqueeze_1014, [0, 4, 1, 3, 2]);  unsqueeze_1014 = None
        permute_1665 = torch.ops.aten.permute.default(permute_1664, [3, 4, 0, 2, 1]);  permute_1664 = None
        view_3112 = torch.ops.aten.view.default(permute_1665, [256, 512, 512]);  permute_1665 = None
        bmm_248 = torch.ops.aten.bmm.default(view_3111, view_3112);  view_3111 = view_3112 = None
        view_3113 = torch.ops.aten.view.default(bmm_248, [256, 512, 1, 1, 512]);  bmm_248 = None
        permute_1666 = torch.ops.aten.permute.default(view_3113, [3, 1, 4, 0, 2]);  view_3113 = None
        view_3114 = torch.ops.aten.view.default(permute_1666, [1, 512, 512, 256]);  permute_1666 = None
        _to_copy_1758 = torch.ops.aten._to_copy.default(getitem_2996, dtype = torch.bfloat16);  getitem_2996 = None
        _to_copy_1759 = torch.ops.aten._to_copy.default(getitem_2992, dtype = torch.bfloat16)
        t_653 = torch.ops.aten.t.default(_to_copy_1758);  _to_copy_1758 = None
        view_3115 = torch.ops.aten.view.default(_to_copy_1759, [262144, 256]);  _to_copy_1759 = None
        mm_609 = torch.ops.aten.mm.default(view_3115, t_653);  view_3115 = t_653 = None
        view_3116 = torch.ops.aten.view.default(mm_609, [1, 512, 512, 512]);  mm_609 = None
        _to_copy_1760 = torch.ops.aten._to_copy.default(getitem_2998, dtype = torch.bfloat16);  getitem_2998 = None
        _to_copy_1761 = torch.ops.aten._to_copy.default(getitem_2992, dtype = torch.bfloat16)
        t_654 = torch.ops.aten.t.default(_to_copy_1760);  _to_copy_1760 = None
        view_3117 = torch.ops.aten.view.default(_to_copy_1761, [262144, 256]);  _to_copy_1761 = None
        mm_610 = torch.ops.aten.mm.default(view_3117, t_654);  view_3117 = t_654 = None
        view_3118 = torch.ops.aten.view.default(mm_610, [1, 512, 512, 512]);  mm_610 = None
        sigmoid_247 = torch.ops.aten.sigmoid.default(view_3118);  view_3118 = None
        mul_408 = torch.ops.aten.mul.Tensor(view_3116, sigmoid_247);  view_3116 = sigmoid_247 = None
        view_3119 = torch.ops.aten.view.default(mul_408, [262144, 512]);  mul_408 = None
        view_3120 = torch.ops.aten.view.default(view_3119, [1, 512, 512, 512]);  view_3119 = None
        transpose_80 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_1015 = torch.ops.aten.unsqueeze.default(transpose_80, 3);  transpose_80 = None
        clone_265 = torch.ops.aten.clone.default(unsqueeze_1015, memory_format = torch.contiguous_format);  unsqueeze_1015 = None
        bitwise_not_195 = torch.ops.aten.bitwise_not.default(clone_265);  clone_265 = None
        masked_fill_195 = torch.ops.aten.masked_fill.Scalar(view_3120, bitwise_not_195, 0);  view_3120 = bitwise_not_195 = None
        view_3121 = torch.ops.aten.view.default(masked_fill_195, [262144, 512]);  masked_fill_195 = None
        view_3125 = torch.ops.aten.view.default(view_3121, [1, 512, 512, 512])
        split_tensor_325 = torch.ops.aten.split.Tensor(view_3125, 256, dim = -1);  view_3125 = None
        getitem_3008 = split_tensor_325[0]
        unsqueeze_1018 = torch.ops.aten.unsqueeze.default(getitem_3008, 4);  getitem_3008 = None
        permute_1671 = torch.ops.aten.permute.default(unsqueeze_1018, [0, 2, 4, 3, 1]);  unsqueeze_1018 = None
        permute_1672 = torch.ops.aten.permute.default(permute_1671, [3, 1, 4, 0, 2]);  permute_1671 = None
        view_3126 = torch.ops.aten.view.default(permute_1672, [256, 512, 512]);  permute_1672 = None
        view_3127 = torch.ops.aten.view.default(view_3121, [1, 512, 512, 512]);  view_3121 = None
        split_tensor_326 = torch.ops.aten.split.Tensor(view_3127, 256, dim = -1);  view_3127 = None
        getitem_3011 = split_tensor_326[1];  split_tensor_326 = None
        unsqueeze_1019 = torch.ops.aten.unsqueeze.default(getitem_3011, 4);  getitem_3011 = None
        permute_1673 = torch.ops.aten.permute.default(unsqueeze_1019, [0, 4, 2, 3, 1]);  unsqueeze_1019 = None
        permute_1674 = torch.ops.aten.permute.default(permute_1673, [3, 4, 0, 2, 1]);  permute_1673 = None
        view_3128 = torch.ops.aten.view.default(permute_1674, [256, 512, 512]);  permute_1674 = None
        bmm_249 = torch.ops.aten.bmm.default(view_3126, view_3128);  view_3126 = view_3128 = None
        view_3129 = torch.ops.aten.view.default(bmm_249, [256, 512, 1, 1, 512]);  bmm_249 = None
        permute_1675 = torch.ops.aten.permute.default(view_3129, [3, 1, 4, 0, 2]);  view_3129 = None
        view_3130 = torch.ops.aten.view.default(permute_1675, [1, 512, 512, 256]);  permute_1675 = None
        _to_copy_1762 = torch.ops.aten._to_copy.default(view_3114, dtype = torch.float32);  view_3114 = None
        native_layer_norm_default_363 = torch.ops.aten.native_layer_norm.default(_to_copy_1762, [256], None, None, 1e-05);  _to_copy_1762 = None
        getitem_3012 = native_layer_norm_default_363[0]
        _to_copy_1763 = torch.ops.aten._to_copy.default(view_3130, dtype = torch.float32);  view_3130 = None
        native_layer_norm_default_364 = torch.ops.aten.native_layer_norm.default(_to_copy_1763, [256], None, None, 1e-05);  _to_copy_1763 = None
        getitem_3015 = native_layer_norm_default_364[0]
        add_328 = torch.ops.aten.add.Tensor(getitem_3012, getitem_3015);  getitem_3012 = getitem_3015 = None
        _to_copy_1764 = torch.ops.aten._to_copy.default(arg1034_1, dtype = torch.bfloat16);  arg1034_1 = None
        _to_copy_1765 = torch.ops.aten._to_copy.default(add_328, dtype = torch.bfloat16);  add_328 = None
        t_655 = torch.ops.aten.t.default(_to_copy_1764);  _to_copy_1764 = None
        view_3131 = torch.ops.aten.view.default(_to_copy_1765, [262144, 256]);  _to_copy_1765 = None
        mm_611 = torch.ops.aten.mm.default(view_3131, t_655);  view_3131 = t_655 = None
        view_3132 = torch.ops.aten.view.default(mm_611, [1, 512, 512, 256]);  mm_611 = None
        _to_copy_1766 = torch.ops.aten._to_copy.default(getitem_2999, dtype = torch.bfloat16);  getitem_2999 = None
        _to_copy_1767 = torch.ops.aten._to_copy.default(getitem_2992, dtype = torch.bfloat16);  getitem_2992 = None
        t_656 = torch.ops.aten.t.default(_to_copy_1766);  _to_copy_1766 = None
        view_3133 = torch.ops.aten.view.default(_to_copy_1767, [262144, 256]);  _to_copy_1767 = None
        mm_612 = torch.ops.aten.mm.default(view_3133, t_656);  view_3133 = t_656 = None
        view_3134 = torch.ops.aten.view.default(mm_612, [1, 512, 512, 256]);  mm_612 = None
        sigmoid_248 = torch.ops.aten.sigmoid.default(view_3134);  view_3134 = None
        mul_409 = torch.ops.aten.mul.Tensor(view_3132, sigmoid_248);  view_3132 = sigmoid_248 = None
        add_329 = torch.ops.aten.add.Tensor(add_323, mul_409);  mul_409 = None
        _to_copy_1768 = torch.ops.aten._to_copy.default(add_323, dtype = torch.float32)
        native_layer_norm_default_365 = torch.ops.aten.native_layer_norm.default(_to_copy_1768, [256], None, None, 1e-05);  _to_copy_1768 = None
        getitem_3018 = native_layer_norm_default_365[0]
        _to_copy_1769 = torch.ops.aten._to_copy.default(arg1038_1, dtype = torch.bfloat16);  arg1038_1 = None
        _to_copy_1770 = torch.ops.aten._to_copy.default(getitem_3018, dtype = torch.bfloat16)
        t_657 = torch.ops.aten.t.default(_to_copy_1769);  _to_copy_1769 = None
        view_3135 = torch.ops.aten.view.default(_to_copy_1770, [262144, 256]);  _to_copy_1770 = None
        mm_613 = torch.ops.aten.mm.default(view_3135, t_657);  view_3135 = t_657 = None
        view_3136 = torch.ops.aten.view.default(mm_613, [1, 512, 512, 8]);  mm_613 = None
        view_3137 = torch.ops.aten.view.default(view_3136, [1, 512, 512, 2, 4]);  view_3136 = None
        permute_1676 = torch.ops.aten.permute.default(view_3137, [0, 3, 4, 1, 2]);  view_3137 = None
        view_3138 = torch.ops.aten.view.default(permute_1676, [1, 2, 4, 1, 512, 512]);  permute_1676 = None
        view_3139 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_196 = torch.ops.aten.bitwise_not.default(view_3139);  view_3139 = None
        masked_fill_196 = torch.ops.aten.masked_fill.Scalar(view_3138, bitwise_not_196, -10000);  view_3138 = bitwise_not_196 = None
        view_3140 = torch.ops.aten.view.default(masked_fill_196, [1, 2, 4, 512, 512]);  masked_fill_196 = None
        permute_1677 = torch.ops.aten.permute.default(view_3140, [1, 0, 2, 3, 4]);  view_3140 = None
        view_3141 = torch.ops.aten.view.default(permute_1677, [2, 4, 1, 512, 512]);  permute_1677 = None
        _to_copy_1771 = torch.ops.aten._to_copy.default(arg1039_1, dtype = torch.bfloat16);  arg1039_1 = None
        _to_copy_1772 = torch.ops.aten._to_copy.default(getitem_3018, dtype = torch.bfloat16)
        t_658 = torch.ops.aten.t.default(_to_copy_1771);  _to_copy_1771 = None
        view_3142 = torch.ops.aten.view.default(_to_copy_1772, [262144, 256]);  _to_copy_1772 = None
        mm_614 = torch.ops.aten.mm.default(view_3142, t_658);  view_3142 = t_658 = None
        view_3143 = torch.ops.aten.view.default(mm_614, [1, 512, 512, 1024]);  mm_614 = None
        select_81 = torch.ops.aten.select.int(view_3141, 0, 0)
        view_3144 = torch.ops.aten.view.default(view_3143, [1, 512, 512, 4, 4, 64]);  view_3143 = None
        permute_1678 = torch.ops.aten.permute.default(view_3144, [4, 0, 3, 1, 2, 5]);  view_3144 = None
        view_3145 = torch.ops.aten.view.default(permute_1678, [4, 4, 512, 512, 64]);  permute_1678 = None
        unbind_int_142 = torch.ops.aten.unbind.int(view_3145);  view_3145 = None
        getitem_3021 = unbind_int_142[0]
        getitem_3022 = unbind_int_142[1]
        getitem_3023 = unbind_int_142[2]
        getitem_3024 = unbind_int_142[3];  unbind_int_142 = None
        expand_197 = torch.ops.aten.expand.default(select_81, [4, 512, 512, 512]);  select_81 = None
        _scaled_dot_product_efficient_attention_default_114 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3021, getitem_3022, getitem_3023, expand_197, False);  getitem_3021 = getitem_3022 = getitem_3023 = expand_197 = None
        getitem_3025 = _scaled_dot_product_efficient_attention_default_114[0]
        sigmoid_249 = torch.ops.aten.sigmoid.default(getitem_3024);  getitem_3024 = None
        mul_410 = torch.ops.aten.mul.Tensor(getitem_3025, sigmoid_249);  getitem_3025 = sigmoid_249 = None
        view_3146 = torch.ops.aten.view.default(mul_410, [1, 4, 512, 512, 64]);  mul_410 = None
        permute_1679 = torch.ops.aten.permute.default(view_3146, [0, 2, 3, 1, 4]);  view_3146 = None
        clone_266 = torch.ops.aten.clone.default(permute_1679, memory_format = torch.contiguous_format);  permute_1679 = None
        _unsafe_view_224 = torch.ops.aten._unsafe_view.default(clone_266, [1, 512, 512, 256]);  clone_266 = None
        transpose_81 = torch.ops.aten.transpose.int(getitem_3018, 1, 2);  getitem_3018 = None
        _to_copy_1773 = torch.ops.aten._to_copy.default(arg1040_1, dtype = torch.bfloat16);  arg1040_1 = None
        _to_copy_1774 = torch.ops.aten._to_copy.default(transpose_81, dtype = torch.bfloat16);  transpose_81 = None
        t_659 = torch.ops.aten.t.default(_to_copy_1773);  _to_copy_1773 = None
        expand_198 = torch.ops.aten.expand.default(_to_copy_1774, [1, 512, 512, 256]);  _to_copy_1774 = None
        view_3147 = torch.ops.aten.view.default(expand_198, [512, 512, 256]);  expand_198 = None
        expand_199 = torch.ops.aten.expand.default(t_659, [1, 512, 256, 1024]);  t_659 = None
        view_3148 = torch.ops.aten.view.default(expand_199, [512, 256, 1024]);  expand_199 = None
        bmm_250 = torch.ops.aten.bmm.default(view_3147, view_3148);  view_3147 = view_3148 = None
        view_3149 = torch.ops.aten.view.default(bmm_250, [1, 512, 512, 1024]);  bmm_250 = None
        select_82 = torch.ops.aten.select.int(view_3141, 0, 1);  view_3141 = None
        view_3150 = torch.ops.aten.view.default(view_3149, [1, 512, 512, 4, 4, 64]);  view_3149 = None
        permute_1680 = torch.ops.aten.permute.default(view_3150, [4, 0, 3, 1, 2, 5]);  view_3150 = None
        view_3151 = torch.ops.aten.view.default(permute_1680, [4, 4, 512, 512, 64]);  permute_1680 = None
        unbind_int_143 = torch.ops.aten.unbind.int(view_3151);  view_3151 = None
        getitem_3029 = unbind_int_143[0]
        getitem_3030 = unbind_int_143[1]
        getitem_3031 = unbind_int_143[2]
        getitem_3032 = unbind_int_143[3];  unbind_int_143 = None
        expand_200 = torch.ops.aten.expand.default(select_82, [4, 512, 512, 512]);  select_82 = None
        _scaled_dot_product_efficient_attention_default_115 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3029, getitem_3030, getitem_3031, expand_200, False);  getitem_3029 = getitem_3030 = getitem_3031 = expand_200 = None
        getitem_3033 = _scaled_dot_product_efficient_attention_default_115[0]
        sigmoid_250 = torch.ops.aten.sigmoid.default(getitem_3032);  getitem_3032 = None
        mul_411 = torch.ops.aten.mul.Tensor(getitem_3033, sigmoid_250);  getitem_3033 = sigmoid_250 = None
        view_3152 = torch.ops.aten.view.default(mul_411, [1, 4, 512, 512, 64]);  mul_411 = None
        permute_1681 = torch.ops.aten.permute.default(view_3152, [0, 2, 3, 1, 4]);  view_3152 = None
        clone_267 = torch.ops.aten.clone.default(permute_1681, memory_format = torch.contiguous_format);  permute_1681 = None
        _unsafe_view_225 = torch.ops.aten._unsafe_view.default(clone_267, [1, 512, 512, 256]);  clone_267 = None
        cat_46 = torch.ops.aten.cat.default([_unsafe_view_224, _unsafe_view_225], dim = -1);  _unsafe_view_224 = _unsafe_view_225 = None
        slice_239 = torch.ops.aten.slice.Tensor(arg1037_1, dim = 0, start = 0, end = 9223372036854775807);  arg1037_1 = None
        unsqueeze_1020 = torch.ops.aten.unsqueeze.default(slice_239, 1);  slice_239 = None
        mul_412 = torch.ops.aten.mul.Tensor(arg1041_1, unsqueeze_1020);  arg1041_1 = unsqueeze_1020 = None
        _to_copy_1775 = torch.ops.aten._to_copy.default(mul_412, dtype = torch.bfloat16);  mul_412 = None
        t_660 = torch.ops.aten.t.default(_to_copy_1775);  _to_copy_1775 = None
        view_3153 = torch.ops.aten.view.default(cat_46, [262144, 512]);  cat_46 = None
        mm_615 = torch.ops.aten.mm.default(view_3153, t_660);  view_3153 = t_660 = None
        view_3154 = torch.ops.aten.view.default(mm_615, [1, 512, 512, 256]);  mm_615 = None
        add_330 = torch.ops.aten.add.Tensor(add_329, view_3154);  add_329 = view_3154 = None
        split_tensor_327 = torch.ops.aten.split.Tensor(add_323, 512, dim = -2)
        getitem_3037 = split_tensor_327[0];  split_tensor_327 = None
        _to_copy_1776 = torch.ops.aten._to_copy.default(getitem_3037, dtype = torch.float32);  getitem_3037 = None
        native_layer_norm_default_366 = torch.ops.aten.native_layer_norm.default(_to_copy_1776, [256], arg1028_1, arg1029_1, 1e-05);  _to_copy_1776 = arg1028_1 = arg1029_1 = None
        getitem_3038 = native_layer_norm_default_366[0]
        _to_copy_1777 = torch.ops.aten._to_copy.default(arg1030_1, dtype = torch.bfloat16);  arg1030_1 = None
        _to_copy_1778 = torch.ops.aten._to_copy.default(getitem_3038, dtype = torch.bfloat16);  getitem_3038 = None
        t_661 = torch.ops.aten.t.default(_to_copy_1777);  _to_copy_1777 = None
        view_3155 = torch.ops.aten.view.default(_to_copy_1778, [262144, 256]);  _to_copy_1778 = None
        mm_616 = torch.ops.aten.mm.default(view_3155, t_661);  view_3155 = t_661 = None
        view_3156 = torch.ops.aten.view.default(mm_616, [1, 512, 512, 1024]);  mm_616 = None
        split_tensor_328 = torch.ops.aten.split.Tensor(view_3156, 512, dim = -1);  view_3156 = None
        getitem_3041 = split_tensor_328[0]
        getitem_3042 = split_tensor_328[1];  split_tensor_328 = None
        silu_86 = torch.ops.aten.silu.default(getitem_3041);  getitem_3041 = None
        mul_413 = torch.ops.aten.mul.Tensor(silu_86, getitem_3042);  silu_86 = getitem_3042 = None
        _to_copy_1779 = torch.ops.aten._to_copy.default(arg1031_1, dtype = torch.bfloat16);  arg1031_1 = None
        t_662 = torch.ops.aten.t.default(_to_copy_1779);  _to_copy_1779 = None
        view_3158 = torch.ops.aten.view.default(mul_413, [262144, 512]);  mul_413 = None
        mm_617 = torch.ops.aten.mm.default(view_3158, t_662);  view_3158 = t_662 = None
        view_3159 = torch.ops.aten.view.default(mm_617, [1, 512, 512, 256]);  mm_617 = None
        add_331 = torch.ops.aten.add.Tensor(add_330, view_3159);  add_330 = view_3159 = None
        _to_copy_1780 = torch.ops.aten._to_copy.default(add_327, dtype = torch.float32)
        native_layer_norm_default_367 = torch.ops.aten.native_layer_norm.default(_to_copy_1780, [384], arg1046_1, arg1047_1, 1e-05);  _to_copy_1780 = arg1046_1 = arg1047_1 = None
        getitem_3043 = native_layer_norm_default_367[0]
        _to_copy_1781 = torch.ops.aten._to_copy.default(add_323, dtype = torch.float32);  add_323 = None
        native_layer_norm_default_368 = torch.ops.aten.native_layer_norm.default(_to_copy_1781, [256], arg1048_1, arg1049_1, 1e-05);  _to_copy_1781 = arg1048_1 = arg1049_1 = None
        getitem_3046 = native_layer_norm_default_368[0]
        _to_copy_1782 = torch.ops.aten._to_copy.default(arg1050_1, dtype = torch.bfloat16);  arg1050_1 = None
        _to_copy_1783 = torch.ops.aten._to_copy.default(getitem_3046, dtype = torch.bfloat16);  getitem_3046 = None
        t_663 = torch.ops.aten.t.default(_to_copy_1782);  _to_copy_1782 = None
        view_3160 = torch.ops.aten.view.default(_to_copy_1783, [262144, 256]);  _to_copy_1783 = None
        mm_618 = torch.ops.aten.mm.default(view_3160, t_663);  view_3160 = t_663 = None
        view_3161 = torch.ops.aten.view.default(mm_618, [1, 512, 512, 16]);  mm_618 = None
        permute_1682 = torch.ops.aten.permute.default(view_3161, [0, 3, 1, 2]);  view_3161 = None
        view_3162 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_197 = torch.ops.aten.bitwise_not.default(view_3162);  view_3162 = None
        masked_fill_197 = torch.ops.aten.masked_fill.Scalar(permute_1682, bitwise_not_197, -10000);  permute_1682 = bitwise_not_197 = None
        _to_copy_1784 = torch.ops.aten._to_copy.default(getitem_3043, dtype = torch.bfloat16);  getitem_3043 = None
        _to_copy_1785 = torch.ops.aten._to_copy.default(arg1052_1, dtype = torch.bfloat16);  arg1052_1 = None
        unsqueeze_1021 = torch.ops.aten.unsqueeze.default(_to_copy_1784, 3);  _to_copy_1784 = None
        unsqueeze_1022 = torch.ops.aten.unsqueeze.default(unsqueeze_1021, 4);  unsqueeze_1021 = None
        unsqueeze_1023 = torch.ops.aten.unsqueeze.default(unsqueeze_1022, 5);  unsqueeze_1022 = None
        permute_1683 = torch.ops.aten.permute.default(unsqueeze_1023, [3, 0, 4, 1, 5, 2]);  unsqueeze_1023 = None
        unsqueeze_1024 = torch.ops.aten.unsqueeze.default(_to_copy_1785, 4);  _to_copy_1785 = None
        unsqueeze_1025 = torch.ops.aten.unsqueeze.default(unsqueeze_1024, 5);  unsqueeze_1024 = None
        permute_1684 = torch.ops.aten.permute.default(unsqueeze_1025, [1, 4, 2, 5, 3, 0]);  unsqueeze_1025 = None
        permute_1685 = torch.ops.aten.permute.default(permute_1683, [3, 5, 0, 1, 2, 4]);  permute_1683 = None
        view_3163 = torch.ops.aten.view.default(permute_1685, [1, 512, 384]);  permute_1685 = None
        permute_1686 = torch.ops.aten.permute.default(permute_1684, [5, 0, 1, 2, 4, 3]);  permute_1684 = None
        view_3164 = torch.ops.aten.view.default(permute_1686, [1, 384, 1536]);  permute_1686 = None
        bmm_251 = torch.ops.aten.bmm.default(view_3163, view_3164);  view_3163 = view_3164 = None
        view_3165 = torch.ops.aten.view.default(bmm_251, [512, 1, 4, 1, 16, 24]);  bmm_251 = None
        permute_1687 = torch.ops.aten.permute.default(view_3165, [2, 3, 4, 0, 5, 1]);  view_3165 = None
        view_3166 = torch.ops.aten.view.default(permute_1687, [4, 1, 16, 512, 24]);  permute_1687 = None
        unbind_int_144 = torch.ops.aten.unbind.int(view_3166);  view_3166 = None
        getitem_3049 = unbind_int_144[0]
        getitem_3050 = unbind_int_144[1]
        getitem_3051 = unbind_int_144[2]
        getitem_3052 = unbind_int_144[3];  unbind_int_144 = None
        view_3167 = torch.ops.aten.view.default(arg1051_1, [1, 16, 1, 24]);  arg1051_1 = None
        add_332 = torch.ops.aten.add.Tensor(getitem_3049, view_3167);  getitem_3049 = view_3167 = None
        _to_copy_1786 = torch.ops.aten._to_copy.default(add_332, dtype = torch.bfloat16);  add_332 = None
        expand_201 = torch.ops.aten.expand.default(masked_fill_197, [1, 16, 512, 512]);  masked_fill_197 = None
        _scaled_dot_product_efficient_attention_default_116 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1786, getitem_3050, getitem_3051, expand_201, False);  _to_copy_1786 = getitem_3050 = getitem_3051 = expand_201 = None
        getitem_3053 = _scaled_dot_product_efficient_attention_default_116[0]
        add_333 = torch.ops.aten.add.Tensor(getitem_3052, 1);  getitem_3052 = None
        sigmoid_251 = torch.ops.aten.sigmoid.default(add_333);  add_333 = None
        mul_414 = torch.ops.aten.mul.Tensor(getitem_3053, sigmoid_251);  getitem_3053 = sigmoid_251 = None
        _to_copy_1787 = torch.ops.aten._to_copy.default(arg1053_1, dtype = torch.bfloat16);  arg1053_1 = None
        unsqueeze_1026 = torch.ops.aten.unsqueeze.default(mul_414, 4);  mul_414 = None
        permute_1688 = torch.ops.aten.permute.default(unsqueeze_1026, [0, 2, 4, 3, 1]);  unsqueeze_1026 = None
        unsqueeze_1027 = torch.ops.aten.unsqueeze.default(_to_copy_1787, 3);  _to_copy_1787 = None
        unsqueeze_1028 = torch.ops.aten.unsqueeze.default(unsqueeze_1027, 4);  unsqueeze_1027 = None
        permute_1689 = torch.ops.aten.permute.default(unsqueeze_1028, [3, 4, 2, 1, 0]);  unsqueeze_1028 = None
        permute_1690 = torch.ops.aten.permute.default(permute_1688, [1, 3, 4, 0, 2]);  permute_1688 = None
        clone_268 = torch.ops.aten.clone.default(permute_1690, memory_format = torch.contiguous_format);  permute_1690 = None
        _unsafe_view_226 = torch.ops.aten._unsafe_view.default(clone_268, [1, 512, 384]);  clone_268 = None
        permute_1691 = torch.ops.aten.permute.default(permute_1689, [3, 4, 0, 2, 1]);  permute_1689 = None
        clone_269 = torch.ops.aten.clone.default(permute_1691, memory_format = torch.contiguous_format);  permute_1691 = None
        _unsafe_view_227 = torch.ops.aten._unsafe_view.default(clone_269, [1, 384, 384]);  clone_269 = None
        bmm_252 = torch.ops.aten.bmm.default(_unsafe_view_226, _unsafe_view_227);  _unsafe_view_226 = _unsafe_view_227 = None
        view_3168 = torch.ops.aten.view.default(bmm_252, [512, 1, 1, 1, 384]);  bmm_252 = None
        permute_1692 = torch.ops.aten.permute.default(view_3168, [3, 0, 4, 1, 2]);  view_3168 = None
        view_3169 = torch.ops.aten.view.default(permute_1692, [1, 512, 384]);  permute_1692 = None
        unsqueeze_1029 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_415 = torch.ops.aten.mul.Tensor(view_3169, unsqueeze_1029);  view_3169 = unsqueeze_1029 = None
        add_334 = torch.ops.aten.add.Tensor(add_327, mul_415);  mul_415 = None
        split_tensor_329 = torch.ops.aten.split.Tensor(add_327, 512, dim = -2);  add_327 = None
        getitem_3057 = split_tensor_329[0];  split_tensor_329 = None
        _to_copy_1788 = torch.ops.aten._to_copy.default(getitem_3057, dtype = torch.float32);  getitem_3057 = None
        native_layer_norm_default_369 = torch.ops.aten.native_layer_norm.default(_to_copy_1788, [384], arg1042_1, arg1043_1, 1e-05);  _to_copy_1788 = arg1042_1 = arg1043_1 = None
        getitem_3058 = native_layer_norm_default_369[0]
        _to_copy_1789 = torch.ops.aten._to_copy.default(arg1044_1, dtype = torch.bfloat16);  arg1044_1 = None
        _to_copy_1790 = torch.ops.aten._to_copy.default(getitem_3058, dtype = torch.bfloat16);  getitem_3058 = None
        t_664 = torch.ops.aten.t.default(_to_copy_1789);  _to_copy_1789 = None
        view_3170 = torch.ops.aten.view.default(_to_copy_1790, [512, 384]);  _to_copy_1790 = None
        mm_619 = torch.ops.aten.mm.default(view_3170, t_664);  view_3170 = t_664 = None
        view_3171 = torch.ops.aten.view.default(mm_619, [1, 512, 1536]);  mm_619 = None
        split_tensor_330 = torch.ops.aten.split.Tensor(view_3171, 768, dim = -1);  view_3171 = None
        getitem_3061 = split_tensor_330[0]
        getitem_3062 = split_tensor_330[1];  split_tensor_330 = None
        silu_87 = torch.ops.aten.silu.default(getitem_3061);  getitem_3061 = None
        mul_416 = torch.ops.aten.mul.Tensor(silu_87, getitem_3062);  silu_87 = getitem_3062 = None
        _to_copy_1791 = torch.ops.aten._to_copy.default(arg1045_1, dtype = torch.bfloat16);  arg1045_1 = None
        t_665 = torch.ops.aten.t.default(_to_copy_1791);  _to_copy_1791 = None
        view_3173 = torch.ops.aten.view.default(mul_416, [512, 768]);  mul_416 = None
        mm_620 = torch.ops.aten.mm.default(view_3173, t_665);  view_3173 = t_665 = None
        view_3174 = torch.ops.aten.view.default(mm_620, [1, 512, 384]);  mm_620 = None
        add_335 = torch.ops.aten.add.Tensor(add_334, view_3174);  add_334 = view_3174 = None
        _to_copy_1792 = torch.ops.aten._to_copy.default(add_331, dtype = torch.float32)
        native_layer_norm_default_370 = torch.ops.aten.native_layer_norm.default(_to_copy_1792, [256], arg1058_1, arg1059_1, 1e-05);  _to_copy_1792 = arg1058_1 = arg1059_1 = None
        getitem_3063 = native_layer_norm_default_370[0]
        split_with_sizes_default_82 = torch.ops.aten.split_with_sizes.default(arg1061_1, [512, 512]);  arg1061_1 = None
        getitem_3066 = split_with_sizes_default_82[0]
        getitem_3067 = split_with_sizes_default_82[1];  split_with_sizes_default_82 = None
        split_with_sizes_default_83 = torch.ops.aten.split_with_sizes.default(arg1062_1, [512, 512, 256]);  arg1062_1 = None
        getitem_3068 = split_with_sizes_default_83[0]
        getitem_3069 = split_with_sizes_default_83[1]
        getitem_3070 = split_with_sizes_default_83[2];  split_with_sizes_default_83 = None
        _to_copy_1793 = torch.ops.aten._to_copy.default(getitem_3066, dtype = torch.bfloat16);  getitem_3066 = None
        _to_copy_1794 = torch.ops.aten._to_copy.default(getitem_3063, dtype = torch.bfloat16)
        t_666 = torch.ops.aten.t.default(_to_copy_1793);  _to_copy_1793 = None
        view_3175 = torch.ops.aten.view.default(_to_copy_1794, [262144, 256]);  _to_copy_1794 = None
        mm_621 = torch.ops.aten.mm.default(view_3175, t_666);  view_3175 = t_666 = None
        view_3176 = torch.ops.aten.view.default(mm_621, [1, 512, 512, 512]);  mm_621 = None
        _to_copy_1795 = torch.ops.aten._to_copy.default(getitem_3068, dtype = torch.bfloat16);  getitem_3068 = None
        _to_copy_1796 = torch.ops.aten._to_copy.default(getitem_3063, dtype = torch.bfloat16)
        t_667 = torch.ops.aten.t.default(_to_copy_1795);  _to_copy_1795 = None
        view_3177 = torch.ops.aten.view.default(_to_copy_1796, [262144, 256]);  _to_copy_1796 = None
        mm_622 = torch.ops.aten.mm.default(view_3177, t_667);  view_3177 = t_667 = None
        view_3178 = torch.ops.aten.view.default(mm_622, [1, 512, 512, 512]);  mm_622 = None
        sigmoid_252 = torch.ops.aten.sigmoid.default(view_3178);  view_3178 = None
        mul_417 = torch.ops.aten.mul.Tensor(view_3176, sigmoid_252);  view_3176 = sigmoid_252 = None
        unsqueeze_1030 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_198 = torch.ops.aten.bitwise_not.default(unsqueeze_1030);  unsqueeze_1030 = None
        masked_fill_198 = torch.ops.aten.masked_fill.Scalar(mul_417, bitwise_not_198, 0);  mul_417 = bitwise_not_198 = None
        split_tensor_331 = torch.ops.aten.split.Tensor(masked_fill_198, 256, dim = -1)
        getitem_3073 = split_tensor_331[0]
        unsqueeze_1033 = torch.ops.aten.unsqueeze.default(getitem_3073, 4);  getitem_3073 = None
        permute_1697 = torch.ops.aten.permute.default(unsqueeze_1033, [0, 1, 4, 3, 2]);  unsqueeze_1033 = None
        permute_1698 = torch.ops.aten.permute.default(permute_1697, [3, 1, 4, 0, 2]);  permute_1697 = None
        view_3181 = torch.ops.aten.view.default(permute_1698, [256, 512, 512]);  permute_1698 = None
        split_tensor_332 = torch.ops.aten.split.Tensor(masked_fill_198, 256, dim = -1);  masked_fill_198 = None
        getitem_3076 = split_tensor_332[1];  split_tensor_332 = None
        unsqueeze_1034 = torch.ops.aten.unsqueeze.default(getitem_3076, 4);  getitem_3076 = None
        permute_1699 = torch.ops.aten.permute.default(unsqueeze_1034, [0, 4, 1, 3, 2]);  unsqueeze_1034 = None
        permute_1700 = torch.ops.aten.permute.default(permute_1699, [3, 4, 0, 2, 1]);  permute_1699 = None
        view_3182 = torch.ops.aten.view.default(permute_1700, [256, 512, 512]);  permute_1700 = None
        bmm_253 = torch.ops.aten.bmm.default(view_3181, view_3182);  view_3181 = view_3182 = None
        view_3183 = torch.ops.aten.view.default(bmm_253, [256, 512, 1, 1, 512]);  bmm_253 = None
        permute_1701 = torch.ops.aten.permute.default(view_3183, [3, 1, 4, 0, 2]);  view_3183 = None
        view_3184 = torch.ops.aten.view.default(permute_1701, [1, 512, 512, 256]);  permute_1701 = None
        _to_copy_1797 = torch.ops.aten._to_copy.default(getitem_3067, dtype = torch.bfloat16);  getitem_3067 = None
        _to_copy_1798 = torch.ops.aten._to_copy.default(getitem_3063, dtype = torch.bfloat16)
        t_668 = torch.ops.aten.t.default(_to_copy_1797);  _to_copy_1797 = None
        view_3185 = torch.ops.aten.view.default(_to_copy_1798, [262144, 256]);  _to_copy_1798 = None
        mm_623 = torch.ops.aten.mm.default(view_3185, t_668);  view_3185 = t_668 = None
        view_3186 = torch.ops.aten.view.default(mm_623, [1, 512, 512, 512]);  mm_623 = None
        _to_copy_1799 = torch.ops.aten._to_copy.default(getitem_3069, dtype = torch.bfloat16);  getitem_3069 = None
        _to_copy_1800 = torch.ops.aten._to_copy.default(getitem_3063, dtype = torch.bfloat16)
        t_669 = torch.ops.aten.t.default(_to_copy_1799);  _to_copy_1799 = None
        view_3187 = torch.ops.aten.view.default(_to_copy_1800, [262144, 256]);  _to_copy_1800 = None
        mm_624 = torch.ops.aten.mm.default(view_3187, t_669);  view_3187 = t_669 = None
        view_3188 = torch.ops.aten.view.default(mm_624, [1, 512, 512, 512]);  mm_624 = None
        sigmoid_253 = torch.ops.aten.sigmoid.default(view_3188);  view_3188 = None
        mul_418 = torch.ops.aten.mul.Tensor(view_3186, sigmoid_253);  view_3186 = sigmoid_253 = None
        view_3189 = torch.ops.aten.view.default(mul_418, [262144, 512]);  mul_418 = None
        view_3190 = torch.ops.aten.view.default(view_3189, [1, 512, 512, 512]);  view_3189 = None
        transpose_82 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_1035 = torch.ops.aten.unsqueeze.default(transpose_82, 3);  transpose_82 = None
        clone_270 = torch.ops.aten.clone.default(unsqueeze_1035, memory_format = torch.contiguous_format);  unsqueeze_1035 = None
        bitwise_not_199 = torch.ops.aten.bitwise_not.default(clone_270);  clone_270 = None
        masked_fill_199 = torch.ops.aten.masked_fill.Scalar(view_3190, bitwise_not_199, 0);  view_3190 = bitwise_not_199 = None
        view_3191 = torch.ops.aten.view.default(masked_fill_199, [262144, 512]);  masked_fill_199 = None
        view_3195 = torch.ops.aten.view.default(view_3191, [1, 512, 512, 512])
        split_tensor_333 = torch.ops.aten.split.Tensor(view_3195, 256, dim = -1);  view_3195 = None
        getitem_3079 = split_tensor_333[0]
        unsqueeze_1038 = torch.ops.aten.unsqueeze.default(getitem_3079, 4);  getitem_3079 = None
        permute_1706 = torch.ops.aten.permute.default(unsqueeze_1038, [0, 2, 4, 3, 1]);  unsqueeze_1038 = None
        permute_1707 = torch.ops.aten.permute.default(permute_1706, [3, 1, 4, 0, 2]);  permute_1706 = None
        view_3196 = torch.ops.aten.view.default(permute_1707, [256, 512, 512]);  permute_1707 = None
        view_3197 = torch.ops.aten.view.default(view_3191, [1, 512, 512, 512]);  view_3191 = None
        split_tensor_334 = torch.ops.aten.split.Tensor(view_3197, 256, dim = -1);  view_3197 = None
        getitem_3082 = split_tensor_334[1];  split_tensor_334 = None
        unsqueeze_1039 = torch.ops.aten.unsqueeze.default(getitem_3082, 4);  getitem_3082 = None
        permute_1708 = torch.ops.aten.permute.default(unsqueeze_1039, [0, 4, 2, 3, 1]);  unsqueeze_1039 = None
        permute_1709 = torch.ops.aten.permute.default(permute_1708, [3, 4, 0, 2, 1]);  permute_1708 = None
        view_3198 = torch.ops.aten.view.default(permute_1709, [256, 512, 512]);  permute_1709 = None
        bmm_254 = torch.ops.aten.bmm.default(view_3196, view_3198);  view_3196 = view_3198 = None
        view_3199 = torch.ops.aten.view.default(bmm_254, [256, 512, 1, 1, 512]);  bmm_254 = None
        permute_1710 = torch.ops.aten.permute.default(view_3199, [3, 1, 4, 0, 2]);  view_3199 = None
        view_3200 = torch.ops.aten.view.default(permute_1710, [1, 512, 512, 256]);  permute_1710 = None
        _to_copy_1801 = torch.ops.aten._to_copy.default(view_3184, dtype = torch.float32);  view_3184 = None
        native_layer_norm_default_371 = torch.ops.aten.native_layer_norm.default(_to_copy_1801, [256], None, None, 1e-05);  _to_copy_1801 = None
        getitem_3083 = native_layer_norm_default_371[0]
        _to_copy_1802 = torch.ops.aten._to_copy.default(view_3200, dtype = torch.float32);  view_3200 = None
        native_layer_norm_default_372 = torch.ops.aten.native_layer_norm.default(_to_copy_1802, [256], None, None, 1e-05);  _to_copy_1802 = None
        getitem_3086 = native_layer_norm_default_372[0]
        add_336 = torch.ops.aten.add.Tensor(getitem_3083, getitem_3086);  getitem_3083 = getitem_3086 = None
        _to_copy_1803 = torch.ops.aten._to_copy.default(arg1060_1, dtype = torch.bfloat16);  arg1060_1 = None
        _to_copy_1804 = torch.ops.aten._to_copy.default(add_336, dtype = torch.bfloat16);  add_336 = None
        t_670 = torch.ops.aten.t.default(_to_copy_1803);  _to_copy_1803 = None
        view_3201 = torch.ops.aten.view.default(_to_copy_1804, [262144, 256]);  _to_copy_1804 = None
        mm_625 = torch.ops.aten.mm.default(view_3201, t_670);  view_3201 = t_670 = None
        view_3202 = torch.ops.aten.view.default(mm_625, [1, 512, 512, 256]);  mm_625 = None
        _to_copy_1805 = torch.ops.aten._to_copy.default(getitem_3070, dtype = torch.bfloat16);  getitem_3070 = None
        _to_copy_1806 = torch.ops.aten._to_copy.default(getitem_3063, dtype = torch.bfloat16);  getitem_3063 = None
        t_671 = torch.ops.aten.t.default(_to_copy_1805);  _to_copy_1805 = None
        view_3203 = torch.ops.aten.view.default(_to_copy_1806, [262144, 256]);  _to_copy_1806 = None
        mm_626 = torch.ops.aten.mm.default(view_3203, t_671);  view_3203 = t_671 = None
        view_3204 = torch.ops.aten.view.default(mm_626, [1, 512, 512, 256]);  mm_626 = None
        sigmoid_254 = torch.ops.aten.sigmoid.default(view_3204);  view_3204 = None
        mul_419 = torch.ops.aten.mul.Tensor(view_3202, sigmoid_254);  view_3202 = sigmoid_254 = None
        add_337 = torch.ops.aten.add.Tensor(add_331, mul_419);  mul_419 = None
        _to_copy_1807 = torch.ops.aten._to_copy.default(add_331, dtype = torch.float32)
        native_layer_norm_default_373 = torch.ops.aten.native_layer_norm.default(_to_copy_1807, [256], None, None, 1e-05);  _to_copy_1807 = None
        getitem_3089 = native_layer_norm_default_373[0]
        _to_copy_1808 = torch.ops.aten._to_copy.default(arg1064_1, dtype = torch.bfloat16);  arg1064_1 = None
        _to_copy_1809 = torch.ops.aten._to_copy.default(getitem_3089, dtype = torch.bfloat16)
        t_672 = torch.ops.aten.t.default(_to_copy_1808);  _to_copy_1808 = None
        view_3205 = torch.ops.aten.view.default(_to_copy_1809, [262144, 256]);  _to_copy_1809 = None
        mm_627 = torch.ops.aten.mm.default(view_3205, t_672);  view_3205 = t_672 = None
        view_3206 = torch.ops.aten.view.default(mm_627, [1, 512, 512, 8]);  mm_627 = None
        view_3207 = torch.ops.aten.view.default(view_3206, [1, 512, 512, 2, 4]);  view_3206 = None
        permute_1711 = torch.ops.aten.permute.default(view_3207, [0, 3, 4, 1, 2]);  view_3207 = None
        view_3208 = torch.ops.aten.view.default(permute_1711, [1, 2, 4, 1, 512, 512]);  permute_1711 = None
        view_3209 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_200 = torch.ops.aten.bitwise_not.default(view_3209);  view_3209 = None
        masked_fill_200 = torch.ops.aten.masked_fill.Scalar(view_3208, bitwise_not_200, -10000);  view_3208 = bitwise_not_200 = None
        view_3210 = torch.ops.aten.view.default(masked_fill_200, [1, 2, 4, 512, 512]);  masked_fill_200 = None
        permute_1712 = torch.ops.aten.permute.default(view_3210, [1, 0, 2, 3, 4]);  view_3210 = None
        view_3211 = torch.ops.aten.view.default(permute_1712, [2, 4, 1, 512, 512]);  permute_1712 = None
        _to_copy_1810 = torch.ops.aten._to_copy.default(arg1065_1, dtype = torch.bfloat16);  arg1065_1 = None
        _to_copy_1811 = torch.ops.aten._to_copy.default(getitem_3089, dtype = torch.bfloat16)
        t_673 = torch.ops.aten.t.default(_to_copy_1810);  _to_copy_1810 = None
        view_3212 = torch.ops.aten.view.default(_to_copy_1811, [262144, 256]);  _to_copy_1811 = None
        mm_628 = torch.ops.aten.mm.default(view_3212, t_673);  view_3212 = t_673 = None
        view_3213 = torch.ops.aten.view.default(mm_628, [1, 512, 512, 1024]);  mm_628 = None
        select_83 = torch.ops.aten.select.int(view_3211, 0, 0)
        view_3214 = torch.ops.aten.view.default(view_3213, [1, 512, 512, 4, 4, 64]);  view_3213 = None
        permute_1713 = torch.ops.aten.permute.default(view_3214, [4, 0, 3, 1, 2, 5]);  view_3214 = None
        view_3215 = torch.ops.aten.view.default(permute_1713, [4, 4, 512, 512, 64]);  permute_1713 = None
        unbind_int_145 = torch.ops.aten.unbind.int(view_3215);  view_3215 = None
        getitem_3092 = unbind_int_145[0]
        getitem_3093 = unbind_int_145[1]
        getitem_3094 = unbind_int_145[2]
        getitem_3095 = unbind_int_145[3];  unbind_int_145 = None
        expand_202 = torch.ops.aten.expand.default(select_83, [4, 512, 512, 512]);  select_83 = None
        _scaled_dot_product_efficient_attention_default_117 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3092, getitem_3093, getitem_3094, expand_202, False);  getitem_3092 = getitem_3093 = getitem_3094 = expand_202 = None
        getitem_3096 = _scaled_dot_product_efficient_attention_default_117[0]
        sigmoid_255 = torch.ops.aten.sigmoid.default(getitem_3095);  getitem_3095 = None
        mul_420 = torch.ops.aten.mul.Tensor(getitem_3096, sigmoid_255);  getitem_3096 = sigmoid_255 = None
        view_3216 = torch.ops.aten.view.default(mul_420, [1, 4, 512, 512, 64]);  mul_420 = None
        permute_1714 = torch.ops.aten.permute.default(view_3216, [0, 2, 3, 1, 4]);  view_3216 = None
        clone_271 = torch.ops.aten.clone.default(permute_1714, memory_format = torch.contiguous_format);  permute_1714 = None
        _unsafe_view_228 = torch.ops.aten._unsafe_view.default(clone_271, [1, 512, 512, 256]);  clone_271 = None
        transpose_83 = torch.ops.aten.transpose.int(getitem_3089, 1, 2);  getitem_3089 = None
        _to_copy_1812 = torch.ops.aten._to_copy.default(arg1066_1, dtype = torch.bfloat16);  arg1066_1 = None
        _to_copy_1813 = torch.ops.aten._to_copy.default(transpose_83, dtype = torch.bfloat16);  transpose_83 = None
        t_674 = torch.ops.aten.t.default(_to_copy_1812);  _to_copy_1812 = None
        expand_203 = torch.ops.aten.expand.default(_to_copy_1813, [1, 512, 512, 256]);  _to_copy_1813 = None
        view_3217 = torch.ops.aten.view.default(expand_203, [512, 512, 256]);  expand_203 = None
        expand_204 = torch.ops.aten.expand.default(t_674, [1, 512, 256, 1024]);  t_674 = None
        view_3218 = torch.ops.aten.view.default(expand_204, [512, 256, 1024]);  expand_204 = None
        bmm_255 = torch.ops.aten.bmm.default(view_3217, view_3218);  view_3217 = view_3218 = None
        view_3219 = torch.ops.aten.view.default(bmm_255, [1, 512, 512, 1024]);  bmm_255 = None
        select_84 = torch.ops.aten.select.int(view_3211, 0, 1);  view_3211 = None
        view_3220 = torch.ops.aten.view.default(view_3219, [1, 512, 512, 4, 4, 64]);  view_3219 = None
        permute_1715 = torch.ops.aten.permute.default(view_3220, [4, 0, 3, 1, 2, 5]);  view_3220 = None
        view_3221 = torch.ops.aten.view.default(permute_1715, [4, 4, 512, 512, 64]);  permute_1715 = None
        unbind_int_146 = torch.ops.aten.unbind.int(view_3221);  view_3221 = None
        getitem_3100 = unbind_int_146[0]
        getitem_3101 = unbind_int_146[1]
        getitem_3102 = unbind_int_146[2]
        getitem_3103 = unbind_int_146[3];  unbind_int_146 = None
        expand_205 = torch.ops.aten.expand.default(select_84, [4, 512, 512, 512]);  select_84 = None
        _scaled_dot_product_efficient_attention_default_118 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3100, getitem_3101, getitem_3102, expand_205, False);  getitem_3100 = getitem_3101 = getitem_3102 = expand_205 = None
        getitem_3104 = _scaled_dot_product_efficient_attention_default_118[0]
        sigmoid_256 = torch.ops.aten.sigmoid.default(getitem_3103);  getitem_3103 = None
        mul_421 = torch.ops.aten.mul.Tensor(getitem_3104, sigmoid_256);  getitem_3104 = sigmoid_256 = None
        view_3222 = torch.ops.aten.view.default(mul_421, [1, 4, 512, 512, 64]);  mul_421 = None
        permute_1716 = torch.ops.aten.permute.default(view_3222, [0, 2, 3, 1, 4]);  view_3222 = None
        clone_272 = torch.ops.aten.clone.default(permute_1716, memory_format = torch.contiguous_format);  permute_1716 = None
        _unsafe_view_229 = torch.ops.aten._unsafe_view.default(clone_272, [1, 512, 512, 256]);  clone_272 = None
        cat_47 = torch.ops.aten.cat.default([_unsafe_view_228, _unsafe_view_229], dim = -1);  _unsafe_view_228 = _unsafe_view_229 = None
        slice_240 = torch.ops.aten.slice.Tensor(arg1063_1, dim = 0, start = 0, end = 9223372036854775807);  arg1063_1 = None
        unsqueeze_1040 = torch.ops.aten.unsqueeze.default(slice_240, 1);  slice_240 = None
        mul_422 = torch.ops.aten.mul.Tensor(arg1067_1, unsqueeze_1040);  arg1067_1 = unsqueeze_1040 = None
        _to_copy_1814 = torch.ops.aten._to_copy.default(mul_422, dtype = torch.bfloat16);  mul_422 = None
        t_675 = torch.ops.aten.t.default(_to_copy_1814);  _to_copy_1814 = None
        view_3223 = torch.ops.aten.view.default(cat_47, [262144, 512]);  cat_47 = None
        mm_629 = torch.ops.aten.mm.default(view_3223, t_675);  view_3223 = t_675 = None
        view_3224 = torch.ops.aten.view.default(mm_629, [1, 512, 512, 256]);  mm_629 = None
        add_338 = torch.ops.aten.add.Tensor(add_337, view_3224);  add_337 = view_3224 = None
        split_tensor_335 = torch.ops.aten.split.Tensor(add_331, 512, dim = -2)
        getitem_3108 = split_tensor_335[0];  split_tensor_335 = None
        _to_copy_1815 = torch.ops.aten._to_copy.default(getitem_3108, dtype = torch.float32);  getitem_3108 = None
        native_layer_norm_default_374 = torch.ops.aten.native_layer_norm.default(_to_copy_1815, [256], arg1054_1, arg1055_1, 1e-05);  _to_copy_1815 = arg1054_1 = arg1055_1 = None
        getitem_3109 = native_layer_norm_default_374[0]
        _to_copy_1816 = torch.ops.aten._to_copy.default(arg1056_1, dtype = torch.bfloat16);  arg1056_1 = None
        _to_copy_1817 = torch.ops.aten._to_copy.default(getitem_3109, dtype = torch.bfloat16);  getitem_3109 = None
        t_676 = torch.ops.aten.t.default(_to_copy_1816);  _to_copy_1816 = None
        view_3225 = torch.ops.aten.view.default(_to_copy_1817, [262144, 256]);  _to_copy_1817 = None
        mm_630 = torch.ops.aten.mm.default(view_3225, t_676);  view_3225 = t_676 = None
        view_3226 = torch.ops.aten.view.default(mm_630, [1, 512, 512, 1024]);  mm_630 = None
        split_tensor_336 = torch.ops.aten.split.Tensor(view_3226, 512, dim = -1);  view_3226 = None
        getitem_3112 = split_tensor_336[0]
        getitem_3113 = split_tensor_336[1];  split_tensor_336 = None
        silu_88 = torch.ops.aten.silu.default(getitem_3112);  getitem_3112 = None
        mul_423 = torch.ops.aten.mul.Tensor(silu_88, getitem_3113);  silu_88 = getitem_3113 = None
        _to_copy_1818 = torch.ops.aten._to_copy.default(arg1057_1, dtype = torch.bfloat16);  arg1057_1 = None
        t_677 = torch.ops.aten.t.default(_to_copy_1818);  _to_copy_1818 = None
        view_3228 = torch.ops.aten.view.default(mul_423, [262144, 512]);  mul_423 = None
        mm_631 = torch.ops.aten.mm.default(view_3228, t_677);  view_3228 = t_677 = None
        view_3229 = torch.ops.aten.view.default(mm_631, [1, 512, 512, 256]);  mm_631 = None
        add_339 = torch.ops.aten.add.Tensor(add_338, view_3229);  add_338 = view_3229 = None
        _to_copy_1819 = torch.ops.aten._to_copy.default(add_335, dtype = torch.float32)
        native_layer_norm_default_375 = torch.ops.aten.native_layer_norm.default(_to_copy_1819, [384], arg1072_1, arg1073_1, 1e-05);  _to_copy_1819 = arg1072_1 = arg1073_1 = None
        getitem_3114 = native_layer_norm_default_375[0]
        _to_copy_1820 = torch.ops.aten._to_copy.default(add_331, dtype = torch.float32);  add_331 = None
        native_layer_norm_default_376 = torch.ops.aten.native_layer_norm.default(_to_copy_1820, [256], arg1074_1, arg1075_1, 1e-05);  _to_copy_1820 = arg1074_1 = arg1075_1 = None
        getitem_3117 = native_layer_norm_default_376[0]
        _to_copy_1821 = torch.ops.aten._to_copy.default(arg1076_1, dtype = torch.bfloat16);  arg1076_1 = None
        _to_copy_1822 = torch.ops.aten._to_copy.default(getitem_3117, dtype = torch.bfloat16);  getitem_3117 = None
        t_678 = torch.ops.aten.t.default(_to_copy_1821);  _to_copy_1821 = None
        view_3230 = torch.ops.aten.view.default(_to_copy_1822, [262144, 256]);  _to_copy_1822 = None
        mm_632 = torch.ops.aten.mm.default(view_3230, t_678);  view_3230 = t_678 = None
        view_3231 = torch.ops.aten.view.default(mm_632, [1, 512, 512, 16]);  mm_632 = None
        permute_1717 = torch.ops.aten.permute.default(view_3231, [0, 3, 1, 2]);  view_3231 = None
        view_3232 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_201 = torch.ops.aten.bitwise_not.default(view_3232);  view_3232 = None
        masked_fill_201 = torch.ops.aten.masked_fill.Scalar(permute_1717, bitwise_not_201, -10000);  permute_1717 = bitwise_not_201 = None
        _to_copy_1823 = torch.ops.aten._to_copy.default(getitem_3114, dtype = torch.bfloat16);  getitem_3114 = None
        _to_copy_1824 = torch.ops.aten._to_copy.default(arg1078_1, dtype = torch.bfloat16);  arg1078_1 = None
        unsqueeze_1041 = torch.ops.aten.unsqueeze.default(_to_copy_1823, 3);  _to_copy_1823 = None
        unsqueeze_1042 = torch.ops.aten.unsqueeze.default(unsqueeze_1041, 4);  unsqueeze_1041 = None
        unsqueeze_1043 = torch.ops.aten.unsqueeze.default(unsqueeze_1042, 5);  unsqueeze_1042 = None
        permute_1718 = torch.ops.aten.permute.default(unsqueeze_1043, [3, 0, 4, 1, 5, 2]);  unsqueeze_1043 = None
        unsqueeze_1044 = torch.ops.aten.unsqueeze.default(_to_copy_1824, 4);  _to_copy_1824 = None
        unsqueeze_1045 = torch.ops.aten.unsqueeze.default(unsqueeze_1044, 5);  unsqueeze_1044 = None
        permute_1719 = torch.ops.aten.permute.default(unsqueeze_1045, [1, 4, 2, 5, 3, 0]);  unsqueeze_1045 = None
        permute_1720 = torch.ops.aten.permute.default(permute_1718, [3, 5, 0, 1, 2, 4]);  permute_1718 = None
        view_3233 = torch.ops.aten.view.default(permute_1720, [1, 512, 384]);  permute_1720 = None
        permute_1721 = torch.ops.aten.permute.default(permute_1719, [5, 0, 1, 2, 4, 3]);  permute_1719 = None
        view_3234 = torch.ops.aten.view.default(permute_1721, [1, 384, 1536]);  permute_1721 = None
        bmm_256 = torch.ops.aten.bmm.default(view_3233, view_3234);  view_3233 = view_3234 = None
        view_3235 = torch.ops.aten.view.default(bmm_256, [512, 1, 4, 1, 16, 24]);  bmm_256 = None
        permute_1722 = torch.ops.aten.permute.default(view_3235, [2, 3, 4, 0, 5, 1]);  view_3235 = None
        view_3236 = torch.ops.aten.view.default(permute_1722, [4, 1, 16, 512, 24]);  permute_1722 = None
        unbind_int_147 = torch.ops.aten.unbind.int(view_3236);  view_3236 = None
        getitem_3120 = unbind_int_147[0]
        getitem_3121 = unbind_int_147[1]
        getitem_3122 = unbind_int_147[2]
        getitem_3123 = unbind_int_147[3];  unbind_int_147 = None
        view_3237 = torch.ops.aten.view.default(arg1077_1, [1, 16, 1, 24]);  arg1077_1 = None
        add_340 = torch.ops.aten.add.Tensor(getitem_3120, view_3237);  getitem_3120 = view_3237 = None
        _to_copy_1825 = torch.ops.aten._to_copy.default(add_340, dtype = torch.bfloat16);  add_340 = None
        expand_206 = torch.ops.aten.expand.default(masked_fill_201, [1, 16, 512, 512]);  masked_fill_201 = None
        _scaled_dot_product_efficient_attention_default_119 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1825, getitem_3121, getitem_3122, expand_206, False);  _to_copy_1825 = getitem_3121 = getitem_3122 = expand_206 = None
        getitem_3124 = _scaled_dot_product_efficient_attention_default_119[0]
        add_341 = torch.ops.aten.add.Tensor(getitem_3123, 1);  getitem_3123 = None
        sigmoid_257 = torch.ops.aten.sigmoid.default(add_341);  add_341 = None
        mul_424 = torch.ops.aten.mul.Tensor(getitem_3124, sigmoid_257);  getitem_3124 = sigmoid_257 = None
        _to_copy_1826 = torch.ops.aten._to_copy.default(arg1079_1, dtype = torch.bfloat16);  arg1079_1 = None
        unsqueeze_1046 = torch.ops.aten.unsqueeze.default(mul_424, 4);  mul_424 = None
        permute_1723 = torch.ops.aten.permute.default(unsqueeze_1046, [0, 2, 4, 3, 1]);  unsqueeze_1046 = None
        unsqueeze_1047 = torch.ops.aten.unsqueeze.default(_to_copy_1826, 3);  _to_copy_1826 = None
        unsqueeze_1048 = torch.ops.aten.unsqueeze.default(unsqueeze_1047, 4);  unsqueeze_1047 = None
        permute_1724 = torch.ops.aten.permute.default(unsqueeze_1048, [3, 4, 2, 1, 0]);  unsqueeze_1048 = None
        permute_1725 = torch.ops.aten.permute.default(permute_1723, [1, 3, 4, 0, 2]);  permute_1723 = None
        clone_273 = torch.ops.aten.clone.default(permute_1725, memory_format = torch.contiguous_format);  permute_1725 = None
        _unsafe_view_230 = torch.ops.aten._unsafe_view.default(clone_273, [1, 512, 384]);  clone_273 = None
        permute_1726 = torch.ops.aten.permute.default(permute_1724, [3, 4, 0, 2, 1]);  permute_1724 = None
        clone_274 = torch.ops.aten.clone.default(permute_1726, memory_format = torch.contiguous_format);  permute_1726 = None
        _unsafe_view_231 = torch.ops.aten._unsafe_view.default(clone_274, [1, 384, 384]);  clone_274 = None
        bmm_257 = torch.ops.aten.bmm.default(_unsafe_view_230, _unsafe_view_231);  _unsafe_view_230 = _unsafe_view_231 = None
        view_3238 = torch.ops.aten.view.default(bmm_257, [512, 1, 1, 1, 384]);  bmm_257 = None
        permute_1727 = torch.ops.aten.permute.default(view_3238, [3, 0, 4, 1, 2]);  view_3238 = None
        view_3239 = torch.ops.aten.view.default(permute_1727, [1, 512, 384]);  permute_1727 = None
        unsqueeze_1049 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_425 = torch.ops.aten.mul.Tensor(view_3239, unsqueeze_1049);  view_3239 = unsqueeze_1049 = None
        add_342 = torch.ops.aten.add.Tensor(add_335, mul_425);  mul_425 = None
        split_tensor_337 = torch.ops.aten.split.Tensor(add_335, 512, dim = -2);  add_335 = None
        getitem_3128 = split_tensor_337[0];  split_tensor_337 = None
        _to_copy_1827 = torch.ops.aten._to_copy.default(getitem_3128, dtype = torch.float32);  getitem_3128 = None
        native_layer_norm_default_377 = torch.ops.aten.native_layer_norm.default(_to_copy_1827, [384], arg1068_1, arg1069_1, 1e-05);  _to_copy_1827 = arg1068_1 = arg1069_1 = None
        getitem_3129 = native_layer_norm_default_377[0]
        _to_copy_1828 = torch.ops.aten._to_copy.default(arg1070_1, dtype = torch.bfloat16);  arg1070_1 = None
        _to_copy_1829 = torch.ops.aten._to_copy.default(getitem_3129, dtype = torch.bfloat16);  getitem_3129 = None
        t_679 = torch.ops.aten.t.default(_to_copy_1828);  _to_copy_1828 = None
        view_3240 = torch.ops.aten.view.default(_to_copy_1829, [512, 384]);  _to_copy_1829 = None
        mm_633 = torch.ops.aten.mm.default(view_3240, t_679);  view_3240 = t_679 = None
        view_3241 = torch.ops.aten.view.default(mm_633, [1, 512, 1536]);  mm_633 = None
        split_tensor_338 = torch.ops.aten.split.Tensor(view_3241, 768, dim = -1);  view_3241 = None
        getitem_3132 = split_tensor_338[0]
        getitem_3133 = split_tensor_338[1];  split_tensor_338 = None
        silu_89 = torch.ops.aten.silu.default(getitem_3132);  getitem_3132 = None
        mul_426 = torch.ops.aten.mul.Tensor(silu_89, getitem_3133);  silu_89 = getitem_3133 = None
        _to_copy_1830 = torch.ops.aten._to_copy.default(arg1071_1, dtype = torch.bfloat16);  arg1071_1 = None
        t_680 = torch.ops.aten.t.default(_to_copy_1830);  _to_copy_1830 = None
        view_3243 = torch.ops.aten.view.default(mul_426, [512, 768]);  mul_426 = None
        mm_634 = torch.ops.aten.mm.default(view_3243, t_680);  view_3243 = t_680 = None
        view_3244 = torch.ops.aten.view.default(mm_634, [1, 512, 384]);  mm_634 = None
        add_343 = torch.ops.aten.add.Tensor(add_342, view_3244);  add_342 = view_3244 = None
        _to_copy_1831 = torch.ops.aten._to_copy.default(add_339, dtype = torch.float32)
        native_layer_norm_default_378 = torch.ops.aten.native_layer_norm.default(_to_copy_1831, [256], arg1084_1, arg1085_1, 1e-05);  _to_copy_1831 = arg1084_1 = arg1085_1 = None
        getitem_3134 = native_layer_norm_default_378[0]
        split_with_sizes_default_84 = torch.ops.aten.split_with_sizes.default(arg1087_1, [512, 512]);  arg1087_1 = None
        getitem_3137 = split_with_sizes_default_84[0]
        getitem_3138 = split_with_sizes_default_84[1];  split_with_sizes_default_84 = None
        split_with_sizes_default_85 = torch.ops.aten.split_with_sizes.default(arg1088_1, [512, 512, 256]);  arg1088_1 = None
        getitem_3139 = split_with_sizes_default_85[0]
        getitem_3140 = split_with_sizes_default_85[1]
        getitem_3141 = split_with_sizes_default_85[2];  split_with_sizes_default_85 = None
        _to_copy_1832 = torch.ops.aten._to_copy.default(getitem_3137, dtype = torch.bfloat16);  getitem_3137 = None
        _to_copy_1833 = torch.ops.aten._to_copy.default(getitem_3134, dtype = torch.bfloat16)
        t_681 = torch.ops.aten.t.default(_to_copy_1832);  _to_copy_1832 = None
        view_3245 = torch.ops.aten.view.default(_to_copy_1833, [262144, 256]);  _to_copy_1833 = None
        mm_635 = torch.ops.aten.mm.default(view_3245, t_681);  view_3245 = t_681 = None
        view_3246 = torch.ops.aten.view.default(mm_635, [1, 512, 512, 512]);  mm_635 = None
        _to_copy_1834 = torch.ops.aten._to_copy.default(getitem_3139, dtype = torch.bfloat16);  getitem_3139 = None
        _to_copy_1835 = torch.ops.aten._to_copy.default(getitem_3134, dtype = torch.bfloat16)
        t_682 = torch.ops.aten.t.default(_to_copy_1834);  _to_copy_1834 = None
        view_3247 = torch.ops.aten.view.default(_to_copy_1835, [262144, 256]);  _to_copy_1835 = None
        mm_636 = torch.ops.aten.mm.default(view_3247, t_682);  view_3247 = t_682 = None
        view_3248 = torch.ops.aten.view.default(mm_636, [1, 512, 512, 512]);  mm_636 = None
        sigmoid_258 = torch.ops.aten.sigmoid.default(view_3248);  view_3248 = None
        mul_427 = torch.ops.aten.mul.Tensor(view_3246, sigmoid_258);  view_3246 = sigmoid_258 = None
        unsqueeze_1050 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_202 = torch.ops.aten.bitwise_not.default(unsqueeze_1050);  unsqueeze_1050 = None
        masked_fill_202 = torch.ops.aten.masked_fill.Scalar(mul_427, bitwise_not_202, 0);  mul_427 = bitwise_not_202 = None
        split_tensor_339 = torch.ops.aten.split.Tensor(masked_fill_202, 256, dim = -1)
        getitem_3144 = split_tensor_339[0]
        unsqueeze_1053 = torch.ops.aten.unsqueeze.default(getitem_3144, 4);  getitem_3144 = None
        permute_1732 = torch.ops.aten.permute.default(unsqueeze_1053, [0, 1, 4, 3, 2]);  unsqueeze_1053 = None
        permute_1733 = torch.ops.aten.permute.default(permute_1732, [3, 1, 4, 0, 2]);  permute_1732 = None
        view_3251 = torch.ops.aten.view.default(permute_1733, [256, 512, 512]);  permute_1733 = None
        split_tensor_340 = torch.ops.aten.split.Tensor(masked_fill_202, 256, dim = -1);  masked_fill_202 = None
        getitem_3147 = split_tensor_340[1];  split_tensor_340 = None
        unsqueeze_1054 = torch.ops.aten.unsqueeze.default(getitem_3147, 4);  getitem_3147 = None
        permute_1734 = torch.ops.aten.permute.default(unsqueeze_1054, [0, 4, 1, 3, 2]);  unsqueeze_1054 = None
        permute_1735 = torch.ops.aten.permute.default(permute_1734, [3, 4, 0, 2, 1]);  permute_1734 = None
        view_3252 = torch.ops.aten.view.default(permute_1735, [256, 512, 512]);  permute_1735 = None
        bmm_258 = torch.ops.aten.bmm.default(view_3251, view_3252);  view_3251 = view_3252 = None
        view_3253 = torch.ops.aten.view.default(bmm_258, [256, 512, 1, 1, 512]);  bmm_258 = None
        permute_1736 = torch.ops.aten.permute.default(view_3253, [3, 1, 4, 0, 2]);  view_3253 = None
        view_3254 = torch.ops.aten.view.default(permute_1736, [1, 512, 512, 256]);  permute_1736 = None
        _to_copy_1836 = torch.ops.aten._to_copy.default(getitem_3138, dtype = torch.bfloat16);  getitem_3138 = None
        _to_copy_1837 = torch.ops.aten._to_copy.default(getitem_3134, dtype = torch.bfloat16)
        t_683 = torch.ops.aten.t.default(_to_copy_1836);  _to_copy_1836 = None
        view_3255 = torch.ops.aten.view.default(_to_copy_1837, [262144, 256]);  _to_copy_1837 = None
        mm_637 = torch.ops.aten.mm.default(view_3255, t_683);  view_3255 = t_683 = None
        view_3256 = torch.ops.aten.view.default(mm_637, [1, 512, 512, 512]);  mm_637 = None
        _to_copy_1838 = torch.ops.aten._to_copy.default(getitem_3140, dtype = torch.bfloat16);  getitem_3140 = None
        _to_copy_1839 = torch.ops.aten._to_copy.default(getitem_3134, dtype = torch.bfloat16)
        t_684 = torch.ops.aten.t.default(_to_copy_1838);  _to_copy_1838 = None
        view_3257 = torch.ops.aten.view.default(_to_copy_1839, [262144, 256]);  _to_copy_1839 = None
        mm_638 = torch.ops.aten.mm.default(view_3257, t_684);  view_3257 = t_684 = None
        view_3258 = torch.ops.aten.view.default(mm_638, [1, 512, 512, 512]);  mm_638 = None
        sigmoid_259 = torch.ops.aten.sigmoid.default(view_3258);  view_3258 = None
        mul_428 = torch.ops.aten.mul.Tensor(view_3256, sigmoid_259);  view_3256 = sigmoid_259 = None
        view_3259 = torch.ops.aten.view.default(mul_428, [262144, 512]);  mul_428 = None
        view_3260 = torch.ops.aten.view.default(view_3259, [1, 512, 512, 512]);  view_3259 = None
        transpose_84 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_1055 = torch.ops.aten.unsqueeze.default(transpose_84, 3);  transpose_84 = None
        clone_275 = torch.ops.aten.clone.default(unsqueeze_1055, memory_format = torch.contiguous_format);  unsqueeze_1055 = None
        bitwise_not_203 = torch.ops.aten.bitwise_not.default(clone_275);  clone_275 = None
        masked_fill_203 = torch.ops.aten.masked_fill.Scalar(view_3260, bitwise_not_203, 0);  view_3260 = bitwise_not_203 = None
        view_3261 = torch.ops.aten.view.default(masked_fill_203, [262144, 512]);  masked_fill_203 = None
        view_3265 = torch.ops.aten.view.default(view_3261, [1, 512, 512, 512])
        split_tensor_341 = torch.ops.aten.split.Tensor(view_3265, 256, dim = -1);  view_3265 = None
        getitem_3150 = split_tensor_341[0]
        unsqueeze_1058 = torch.ops.aten.unsqueeze.default(getitem_3150, 4);  getitem_3150 = None
        permute_1741 = torch.ops.aten.permute.default(unsqueeze_1058, [0, 2, 4, 3, 1]);  unsqueeze_1058 = None
        permute_1742 = torch.ops.aten.permute.default(permute_1741, [3, 1, 4, 0, 2]);  permute_1741 = None
        view_3266 = torch.ops.aten.view.default(permute_1742, [256, 512, 512]);  permute_1742 = None
        view_3267 = torch.ops.aten.view.default(view_3261, [1, 512, 512, 512]);  view_3261 = None
        split_tensor_342 = torch.ops.aten.split.Tensor(view_3267, 256, dim = -1);  view_3267 = None
        getitem_3153 = split_tensor_342[1];  split_tensor_342 = None
        unsqueeze_1059 = torch.ops.aten.unsqueeze.default(getitem_3153, 4);  getitem_3153 = None
        permute_1743 = torch.ops.aten.permute.default(unsqueeze_1059, [0, 4, 2, 3, 1]);  unsqueeze_1059 = None
        permute_1744 = torch.ops.aten.permute.default(permute_1743, [3, 4, 0, 2, 1]);  permute_1743 = None
        view_3268 = torch.ops.aten.view.default(permute_1744, [256, 512, 512]);  permute_1744 = None
        bmm_259 = torch.ops.aten.bmm.default(view_3266, view_3268);  view_3266 = view_3268 = None
        view_3269 = torch.ops.aten.view.default(bmm_259, [256, 512, 1, 1, 512]);  bmm_259 = None
        permute_1745 = torch.ops.aten.permute.default(view_3269, [3, 1, 4, 0, 2]);  view_3269 = None
        view_3270 = torch.ops.aten.view.default(permute_1745, [1, 512, 512, 256]);  permute_1745 = None
        _to_copy_1840 = torch.ops.aten._to_copy.default(view_3254, dtype = torch.float32);  view_3254 = None
        native_layer_norm_default_379 = torch.ops.aten.native_layer_norm.default(_to_copy_1840, [256], None, None, 1e-05);  _to_copy_1840 = None
        getitem_3154 = native_layer_norm_default_379[0]
        _to_copy_1841 = torch.ops.aten._to_copy.default(view_3270, dtype = torch.float32);  view_3270 = None
        native_layer_norm_default_380 = torch.ops.aten.native_layer_norm.default(_to_copy_1841, [256], None, None, 1e-05);  _to_copy_1841 = None
        getitem_3157 = native_layer_norm_default_380[0]
        add_344 = torch.ops.aten.add.Tensor(getitem_3154, getitem_3157);  getitem_3154 = getitem_3157 = None
        _to_copy_1842 = torch.ops.aten._to_copy.default(arg1086_1, dtype = torch.bfloat16);  arg1086_1 = None
        _to_copy_1843 = torch.ops.aten._to_copy.default(add_344, dtype = torch.bfloat16);  add_344 = None
        t_685 = torch.ops.aten.t.default(_to_copy_1842);  _to_copy_1842 = None
        view_3271 = torch.ops.aten.view.default(_to_copy_1843, [262144, 256]);  _to_copy_1843 = None
        mm_639 = torch.ops.aten.mm.default(view_3271, t_685);  view_3271 = t_685 = None
        view_3272 = torch.ops.aten.view.default(mm_639, [1, 512, 512, 256]);  mm_639 = None
        _to_copy_1844 = torch.ops.aten._to_copy.default(getitem_3141, dtype = torch.bfloat16);  getitem_3141 = None
        _to_copy_1845 = torch.ops.aten._to_copy.default(getitem_3134, dtype = torch.bfloat16);  getitem_3134 = None
        t_686 = torch.ops.aten.t.default(_to_copy_1844);  _to_copy_1844 = None
        view_3273 = torch.ops.aten.view.default(_to_copy_1845, [262144, 256]);  _to_copy_1845 = None
        mm_640 = torch.ops.aten.mm.default(view_3273, t_686);  view_3273 = t_686 = None
        view_3274 = torch.ops.aten.view.default(mm_640, [1, 512, 512, 256]);  mm_640 = None
        sigmoid_260 = torch.ops.aten.sigmoid.default(view_3274);  view_3274 = None
        mul_429 = torch.ops.aten.mul.Tensor(view_3272, sigmoid_260);  view_3272 = sigmoid_260 = None
        add_345 = torch.ops.aten.add.Tensor(add_339, mul_429);  mul_429 = None
        _to_copy_1846 = torch.ops.aten._to_copy.default(add_339, dtype = torch.float32)
        native_layer_norm_default_381 = torch.ops.aten.native_layer_norm.default(_to_copy_1846, [256], None, None, 1e-05);  _to_copy_1846 = None
        getitem_3160 = native_layer_norm_default_381[0]
        _to_copy_1847 = torch.ops.aten._to_copy.default(arg1090_1, dtype = torch.bfloat16);  arg1090_1 = None
        _to_copy_1848 = torch.ops.aten._to_copy.default(getitem_3160, dtype = torch.bfloat16)
        t_687 = torch.ops.aten.t.default(_to_copy_1847);  _to_copy_1847 = None
        view_3275 = torch.ops.aten.view.default(_to_copy_1848, [262144, 256]);  _to_copy_1848 = None
        mm_641 = torch.ops.aten.mm.default(view_3275, t_687);  view_3275 = t_687 = None
        view_3276 = torch.ops.aten.view.default(mm_641, [1, 512, 512, 8]);  mm_641 = None
        view_3277 = torch.ops.aten.view.default(view_3276, [1, 512, 512, 2, 4]);  view_3276 = None
        permute_1746 = torch.ops.aten.permute.default(view_3277, [0, 3, 4, 1, 2]);  view_3277 = None
        view_3278 = torch.ops.aten.view.default(permute_1746, [1, 2, 4, 1, 512, 512]);  permute_1746 = None
        view_3279 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_204 = torch.ops.aten.bitwise_not.default(view_3279);  view_3279 = None
        masked_fill_204 = torch.ops.aten.masked_fill.Scalar(view_3278, bitwise_not_204, -10000);  view_3278 = bitwise_not_204 = None
        view_3280 = torch.ops.aten.view.default(masked_fill_204, [1, 2, 4, 512, 512]);  masked_fill_204 = None
        permute_1747 = torch.ops.aten.permute.default(view_3280, [1, 0, 2, 3, 4]);  view_3280 = None
        view_3281 = torch.ops.aten.view.default(permute_1747, [2, 4, 1, 512, 512]);  permute_1747 = None
        _to_copy_1849 = torch.ops.aten._to_copy.default(arg1091_1, dtype = torch.bfloat16);  arg1091_1 = None
        _to_copy_1850 = torch.ops.aten._to_copy.default(getitem_3160, dtype = torch.bfloat16)
        t_688 = torch.ops.aten.t.default(_to_copy_1849);  _to_copy_1849 = None
        view_3282 = torch.ops.aten.view.default(_to_copy_1850, [262144, 256]);  _to_copy_1850 = None
        mm_642 = torch.ops.aten.mm.default(view_3282, t_688);  view_3282 = t_688 = None
        view_3283 = torch.ops.aten.view.default(mm_642, [1, 512, 512, 1024]);  mm_642 = None
        select_85 = torch.ops.aten.select.int(view_3281, 0, 0)
        view_3284 = torch.ops.aten.view.default(view_3283, [1, 512, 512, 4, 4, 64]);  view_3283 = None
        permute_1748 = torch.ops.aten.permute.default(view_3284, [4, 0, 3, 1, 2, 5]);  view_3284 = None
        view_3285 = torch.ops.aten.view.default(permute_1748, [4, 4, 512, 512, 64]);  permute_1748 = None
        unbind_int_148 = torch.ops.aten.unbind.int(view_3285);  view_3285 = None
        getitem_3163 = unbind_int_148[0]
        getitem_3164 = unbind_int_148[1]
        getitem_3165 = unbind_int_148[2]
        getitem_3166 = unbind_int_148[3];  unbind_int_148 = None
        expand_207 = torch.ops.aten.expand.default(select_85, [4, 512, 512, 512]);  select_85 = None
        _scaled_dot_product_efficient_attention_default_120 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3163, getitem_3164, getitem_3165, expand_207, False);  getitem_3163 = getitem_3164 = getitem_3165 = expand_207 = None
        getitem_3167 = _scaled_dot_product_efficient_attention_default_120[0]
        sigmoid_261 = torch.ops.aten.sigmoid.default(getitem_3166);  getitem_3166 = None
        mul_430 = torch.ops.aten.mul.Tensor(getitem_3167, sigmoid_261);  getitem_3167 = sigmoid_261 = None
        view_3286 = torch.ops.aten.view.default(mul_430, [1, 4, 512, 512, 64]);  mul_430 = None
        permute_1749 = torch.ops.aten.permute.default(view_3286, [0, 2, 3, 1, 4]);  view_3286 = None
        clone_276 = torch.ops.aten.clone.default(permute_1749, memory_format = torch.contiguous_format);  permute_1749 = None
        _unsafe_view_232 = torch.ops.aten._unsafe_view.default(clone_276, [1, 512, 512, 256]);  clone_276 = None
        transpose_85 = torch.ops.aten.transpose.int(getitem_3160, 1, 2);  getitem_3160 = None
        _to_copy_1851 = torch.ops.aten._to_copy.default(arg1092_1, dtype = torch.bfloat16);  arg1092_1 = None
        _to_copy_1852 = torch.ops.aten._to_copy.default(transpose_85, dtype = torch.bfloat16);  transpose_85 = None
        t_689 = torch.ops.aten.t.default(_to_copy_1851);  _to_copy_1851 = None
        expand_208 = torch.ops.aten.expand.default(_to_copy_1852, [1, 512, 512, 256]);  _to_copy_1852 = None
        view_3287 = torch.ops.aten.view.default(expand_208, [512, 512, 256]);  expand_208 = None
        expand_209 = torch.ops.aten.expand.default(t_689, [1, 512, 256, 1024]);  t_689 = None
        view_3288 = torch.ops.aten.view.default(expand_209, [512, 256, 1024]);  expand_209 = None
        bmm_260 = torch.ops.aten.bmm.default(view_3287, view_3288);  view_3287 = view_3288 = None
        view_3289 = torch.ops.aten.view.default(bmm_260, [1, 512, 512, 1024]);  bmm_260 = None
        select_86 = torch.ops.aten.select.int(view_3281, 0, 1);  view_3281 = None
        view_3290 = torch.ops.aten.view.default(view_3289, [1, 512, 512, 4, 4, 64]);  view_3289 = None
        permute_1750 = torch.ops.aten.permute.default(view_3290, [4, 0, 3, 1, 2, 5]);  view_3290 = None
        view_3291 = torch.ops.aten.view.default(permute_1750, [4, 4, 512, 512, 64]);  permute_1750 = None
        unbind_int_149 = torch.ops.aten.unbind.int(view_3291);  view_3291 = None
        getitem_3171 = unbind_int_149[0]
        getitem_3172 = unbind_int_149[1]
        getitem_3173 = unbind_int_149[2]
        getitem_3174 = unbind_int_149[3];  unbind_int_149 = None
        expand_210 = torch.ops.aten.expand.default(select_86, [4, 512, 512, 512]);  select_86 = None
        _scaled_dot_product_efficient_attention_default_121 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3171, getitem_3172, getitem_3173, expand_210, False);  getitem_3171 = getitem_3172 = getitem_3173 = expand_210 = None
        getitem_3175 = _scaled_dot_product_efficient_attention_default_121[0]
        sigmoid_262 = torch.ops.aten.sigmoid.default(getitem_3174);  getitem_3174 = None
        mul_431 = torch.ops.aten.mul.Tensor(getitem_3175, sigmoid_262);  getitem_3175 = sigmoid_262 = None
        view_3292 = torch.ops.aten.view.default(mul_431, [1, 4, 512, 512, 64]);  mul_431 = None
        permute_1751 = torch.ops.aten.permute.default(view_3292, [0, 2, 3, 1, 4]);  view_3292 = None
        clone_277 = torch.ops.aten.clone.default(permute_1751, memory_format = torch.contiguous_format);  permute_1751 = None
        _unsafe_view_233 = torch.ops.aten._unsafe_view.default(clone_277, [1, 512, 512, 256]);  clone_277 = None
        cat_48 = torch.ops.aten.cat.default([_unsafe_view_232, _unsafe_view_233], dim = -1);  _unsafe_view_232 = _unsafe_view_233 = None
        slice_241 = torch.ops.aten.slice.Tensor(arg1089_1, dim = 0, start = 0, end = 9223372036854775807);  arg1089_1 = None
        unsqueeze_1060 = torch.ops.aten.unsqueeze.default(slice_241, 1);  slice_241 = None
        mul_432 = torch.ops.aten.mul.Tensor(arg1093_1, unsqueeze_1060);  arg1093_1 = unsqueeze_1060 = None
        _to_copy_1853 = torch.ops.aten._to_copy.default(mul_432, dtype = torch.bfloat16);  mul_432 = None
        t_690 = torch.ops.aten.t.default(_to_copy_1853);  _to_copy_1853 = None
        view_3293 = torch.ops.aten.view.default(cat_48, [262144, 512]);  cat_48 = None
        mm_643 = torch.ops.aten.mm.default(view_3293, t_690);  view_3293 = t_690 = None
        view_3294 = torch.ops.aten.view.default(mm_643, [1, 512, 512, 256]);  mm_643 = None
        add_346 = torch.ops.aten.add.Tensor(add_345, view_3294);  add_345 = view_3294 = None
        split_tensor_343 = torch.ops.aten.split.Tensor(add_339, 512, dim = -2)
        getitem_3179 = split_tensor_343[0];  split_tensor_343 = None
        _to_copy_1854 = torch.ops.aten._to_copy.default(getitem_3179, dtype = torch.float32);  getitem_3179 = None
        native_layer_norm_default_382 = torch.ops.aten.native_layer_norm.default(_to_copy_1854, [256], arg1080_1, arg1081_1, 1e-05);  _to_copy_1854 = arg1080_1 = arg1081_1 = None
        getitem_3180 = native_layer_norm_default_382[0]
        _to_copy_1855 = torch.ops.aten._to_copy.default(arg1082_1, dtype = torch.bfloat16);  arg1082_1 = None
        _to_copy_1856 = torch.ops.aten._to_copy.default(getitem_3180, dtype = torch.bfloat16);  getitem_3180 = None
        t_691 = torch.ops.aten.t.default(_to_copy_1855);  _to_copy_1855 = None
        view_3295 = torch.ops.aten.view.default(_to_copy_1856, [262144, 256]);  _to_copy_1856 = None
        mm_644 = torch.ops.aten.mm.default(view_3295, t_691);  view_3295 = t_691 = None
        view_3296 = torch.ops.aten.view.default(mm_644, [1, 512, 512, 1024]);  mm_644 = None
        split_tensor_344 = torch.ops.aten.split.Tensor(view_3296, 512, dim = -1);  view_3296 = None
        getitem_3183 = split_tensor_344[0]
        getitem_3184 = split_tensor_344[1];  split_tensor_344 = None
        silu_90 = torch.ops.aten.silu.default(getitem_3183);  getitem_3183 = None
        mul_433 = torch.ops.aten.mul.Tensor(silu_90, getitem_3184);  silu_90 = getitem_3184 = None
        _to_copy_1857 = torch.ops.aten._to_copy.default(arg1083_1, dtype = torch.bfloat16);  arg1083_1 = None
        t_692 = torch.ops.aten.t.default(_to_copy_1857);  _to_copy_1857 = None
        view_3298 = torch.ops.aten.view.default(mul_433, [262144, 512]);  mul_433 = None
        mm_645 = torch.ops.aten.mm.default(view_3298, t_692);  view_3298 = t_692 = None
        view_3299 = torch.ops.aten.view.default(mm_645, [1, 512, 512, 256]);  mm_645 = None
        add_347 = torch.ops.aten.add.Tensor(add_346, view_3299);  add_346 = view_3299 = None
        _to_copy_1858 = torch.ops.aten._to_copy.default(add_343, dtype = torch.float32)
        native_layer_norm_default_383 = torch.ops.aten.native_layer_norm.default(_to_copy_1858, [384], arg1098_1, arg1099_1, 1e-05);  _to_copy_1858 = arg1098_1 = arg1099_1 = None
        getitem_3185 = native_layer_norm_default_383[0]
        _to_copy_1859 = torch.ops.aten._to_copy.default(add_339, dtype = torch.float32);  add_339 = None
        native_layer_norm_default_384 = torch.ops.aten.native_layer_norm.default(_to_copy_1859, [256], arg1100_1, arg1101_1, 1e-05);  _to_copy_1859 = arg1100_1 = arg1101_1 = None
        getitem_3188 = native_layer_norm_default_384[0]
        _to_copy_1860 = torch.ops.aten._to_copy.default(arg1102_1, dtype = torch.bfloat16);  arg1102_1 = None
        _to_copy_1861 = torch.ops.aten._to_copy.default(getitem_3188, dtype = torch.bfloat16);  getitem_3188 = None
        t_693 = torch.ops.aten.t.default(_to_copy_1860);  _to_copy_1860 = None
        view_3300 = torch.ops.aten.view.default(_to_copy_1861, [262144, 256]);  _to_copy_1861 = None
        mm_646 = torch.ops.aten.mm.default(view_3300, t_693);  view_3300 = t_693 = None
        view_3301 = torch.ops.aten.view.default(mm_646, [1, 512, 512, 16]);  mm_646 = None
        permute_1752 = torch.ops.aten.permute.default(view_3301, [0, 3, 1, 2]);  view_3301 = None
        view_3302 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_205 = torch.ops.aten.bitwise_not.default(view_3302);  view_3302 = None
        masked_fill_205 = torch.ops.aten.masked_fill.Scalar(permute_1752, bitwise_not_205, -10000);  permute_1752 = bitwise_not_205 = None
        _to_copy_1862 = torch.ops.aten._to_copy.default(getitem_3185, dtype = torch.bfloat16);  getitem_3185 = None
        _to_copy_1863 = torch.ops.aten._to_copy.default(arg1104_1, dtype = torch.bfloat16);  arg1104_1 = None
        unsqueeze_1061 = torch.ops.aten.unsqueeze.default(_to_copy_1862, 3);  _to_copy_1862 = None
        unsqueeze_1062 = torch.ops.aten.unsqueeze.default(unsqueeze_1061, 4);  unsqueeze_1061 = None
        unsqueeze_1063 = torch.ops.aten.unsqueeze.default(unsqueeze_1062, 5);  unsqueeze_1062 = None
        permute_1753 = torch.ops.aten.permute.default(unsqueeze_1063, [3, 0, 4, 1, 5, 2]);  unsqueeze_1063 = None
        unsqueeze_1064 = torch.ops.aten.unsqueeze.default(_to_copy_1863, 4);  _to_copy_1863 = None
        unsqueeze_1065 = torch.ops.aten.unsqueeze.default(unsqueeze_1064, 5);  unsqueeze_1064 = None
        permute_1754 = torch.ops.aten.permute.default(unsqueeze_1065, [1, 4, 2, 5, 3, 0]);  unsqueeze_1065 = None
        permute_1755 = torch.ops.aten.permute.default(permute_1753, [3, 5, 0, 1, 2, 4]);  permute_1753 = None
        view_3303 = torch.ops.aten.view.default(permute_1755, [1, 512, 384]);  permute_1755 = None
        permute_1756 = torch.ops.aten.permute.default(permute_1754, [5, 0, 1, 2, 4, 3]);  permute_1754 = None
        view_3304 = torch.ops.aten.view.default(permute_1756, [1, 384, 1536]);  permute_1756 = None
        bmm_261 = torch.ops.aten.bmm.default(view_3303, view_3304);  view_3303 = view_3304 = None
        view_3305 = torch.ops.aten.view.default(bmm_261, [512, 1, 4, 1, 16, 24]);  bmm_261 = None
        permute_1757 = torch.ops.aten.permute.default(view_3305, [2, 3, 4, 0, 5, 1]);  view_3305 = None
        view_3306 = torch.ops.aten.view.default(permute_1757, [4, 1, 16, 512, 24]);  permute_1757 = None
        unbind_int_150 = torch.ops.aten.unbind.int(view_3306);  view_3306 = None
        getitem_3191 = unbind_int_150[0]
        getitem_3192 = unbind_int_150[1]
        getitem_3193 = unbind_int_150[2]
        getitem_3194 = unbind_int_150[3];  unbind_int_150 = None
        view_3307 = torch.ops.aten.view.default(arg1103_1, [1, 16, 1, 24]);  arg1103_1 = None
        add_348 = torch.ops.aten.add.Tensor(getitem_3191, view_3307);  getitem_3191 = view_3307 = None
        _to_copy_1864 = torch.ops.aten._to_copy.default(add_348, dtype = torch.bfloat16);  add_348 = None
        expand_211 = torch.ops.aten.expand.default(masked_fill_205, [1, 16, 512, 512]);  masked_fill_205 = None
        _scaled_dot_product_efficient_attention_default_122 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1864, getitem_3192, getitem_3193, expand_211, False);  _to_copy_1864 = getitem_3192 = getitem_3193 = expand_211 = None
        getitem_3195 = _scaled_dot_product_efficient_attention_default_122[0]
        add_349 = torch.ops.aten.add.Tensor(getitem_3194, 1);  getitem_3194 = None
        sigmoid_263 = torch.ops.aten.sigmoid.default(add_349);  add_349 = None
        mul_434 = torch.ops.aten.mul.Tensor(getitem_3195, sigmoid_263);  getitem_3195 = sigmoid_263 = None
        _to_copy_1865 = torch.ops.aten._to_copy.default(arg1105_1, dtype = torch.bfloat16);  arg1105_1 = None
        unsqueeze_1066 = torch.ops.aten.unsqueeze.default(mul_434, 4);  mul_434 = None
        permute_1758 = torch.ops.aten.permute.default(unsqueeze_1066, [0, 2, 4, 3, 1]);  unsqueeze_1066 = None
        unsqueeze_1067 = torch.ops.aten.unsqueeze.default(_to_copy_1865, 3);  _to_copy_1865 = None
        unsqueeze_1068 = torch.ops.aten.unsqueeze.default(unsqueeze_1067, 4);  unsqueeze_1067 = None
        permute_1759 = torch.ops.aten.permute.default(unsqueeze_1068, [3, 4, 2, 1, 0]);  unsqueeze_1068 = None
        permute_1760 = torch.ops.aten.permute.default(permute_1758, [1, 3, 4, 0, 2]);  permute_1758 = None
        clone_278 = torch.ops.aten.clone.default(permute_1760, memory_format = torch.contiguous_format);  permute_1760 = None
        _unsafe_view_234 = torch.ops.aten._unsafe_view.default(clone_278, [1, 512, 384]);  clone_278 = None
        permute_1761 = torch.ops.aten.permute.default(permute_1759, [3, 4, 0, 2, 1]);  permute_1759 = None
        clone_279 = torch.ops.aten.clone.default(permute_1761, memory_format = torch.contiguous_format);  permute_1761 = None
        _unsafe_view_235 = torch.ops.aten._unsafe_view.default(clone_279, [1, 384, 384]);  clone_279 = None
        bmm_262 = torch.ops.aten.bmm.default(_unsafe_view_234, _unsafe_view_235);  _unsafe_view_234 = _unsafe_view_235 = None
        view_3308 = torch.ops.aten.view.default(bmm_262, [512, 1, 1, 1, 384]);  bmm_262 = None
        permute_1762 = torch.ops.aten.permute.default(view_3308, [3, 0, 4, 1, 2]);  view_3308 = None
        view_3309 = torch.ops.aten.view.default(permute_1762, [1, 512, 384]);  permute_1762 = None
        unsqueeze_1069 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_435 = torch.ops.aten.mul.Tensor(view_3309, unsqueeze_1069);  view_3309 = unsqueeze_1069 = None
        add_350 = torch.ops.aten.add.Tensor(add_343, mul_435);  mul_435 = None
        split_tensor_345 = torch.ops.aten.split.Tensor(add_343, 512, dim = -2);  add_343 = None
        getitem_3199 = split_tensor_345[0];  split_tensor_345 = None
        _to_copy_1866 = torch.ops.aten._to_copy.default(getitem_3199, dtype = torch.float32);  getitem_3199 = None
        native_layer_norm_default_385 = torch.ops.aten.native_layer_norm.default(_to_copy_1866, [384], arg1094_1, arg1095_1, 1e-05);  _to_copy_1866 = arg1094_1 = arg1095_1 = None
        getitem_3200 = native_layer_norm_default_385[0]
        _to_copy_1867 = torch.ops.aten._to_copy.default(arg1096_1, dtype = torch.bfloat16);  arg1096_1 = None
        _to_copy_1868 = torch.ops.aten._to_copy.default(getitem_3200, dtype = torch.bfloat16);  getitem_3200 = None
        t_694 = torch.ops.aten.t.default(_to_copy_1867);  _to_copy_1867 = None
        view_3310 = torch.ops.aten.view.default(_to_copy_1868, [512, 384]);  _to_copy_1868 = None
        mm_647 = torch.ops.aten.mm.default(view_3310, t_694);  view_3310 = t_694 = None
        view_3311 = torch.ops.aten.view.default(mm_647, [1, 512, 1536]);  mm_647 = None
        split_tensor_346 = torch.ops.aten.split.Tensor(view_3311, 768, dim = -1);  view_3311 = None
        getitem_3203 = split_tensor_346[0]
        getitem_3204 = split_tensor_346[1];  split_tensor_346 = None
        silu_91 = torch.ops.aten.silu.default(getitem_3203);  getitem_3203 = None
        mul_436 = torch.ops.aten.mul.Tensor(silu_91, getitem_3204);  silu_91 = getitem_3204 = None
        _to_copy_1869 = torch.ops.aten._to_copy.default(arg1097_1, dtype = torch.bfloat16);  arg1097_1 = None
        t_695 = torch.ops.aten.t.default(_to_copy_1869);  _to_copy_1869 = None
        view_3313 = torch.ops.aten.view.default(mul_436, [512, 768]);  mul_436 = None
        mm_648 = torch.ops.aten.mm.default(view_3313, t_695);  view_3313 = t_695 = None
        view_3314 = torch.ops.aten.view.default(mm_648, [1, 512, 384]);  mm_648 = None
        add_351 = torch.ops.aten.add.Tensor(add_350, view_3314);  add_350 = view_3314 = None
        _to_copy_1870 = torch.ops.aten._to_copy.default(add_347, dtype = torch.float32)
        native_layer_norm_default_386 = torch.ops.aten.native_layer_norm.default(_to_copy_1870, [256], arg1110_1, arg1111_1, 1e-05);  _to_copy_1870 = arg1110_1 = arg1111_1 = None
        getitem_3205 = native_layer_norm_default_386[0]
        split_with_sizes_default_86 = torch.ops.aten.split_with_sizes.default(arg1113_1, [512, 512]);  arg1113_1 = None
        getitem_3208 = split_with_sizes_default_86[0]
        getitem_3209 = split_with_sizes_default_86[1];  split_with_sizes_default_86 = None
        split_with_sizes_default_87 = torch.ops.aten.split_with_sizes.default(arg1114_1, [512, 512, 256]);  arg1114_1 = None
        getitem_3210 = split_with_sizes_default_87[0]
        getitem_3211 = split_with_sizes_default_87[1]
        getitem_3212 = split_with_sizes_default_87[2];  split_with_sizes_default_87 = None
        _to_copy_1871 = torch.ops.aten._to_copy.default(getitem_3208, dtype = torch.bfloat16);  getitem_3208 = None
        _to_copy_1872 = torch.ops.aten._to_copy.default(getitem_3205, dtype = torch.bfloat16)
        t_696 = torch.ops.aten.t.default(_to_copy_1871);  _to_copy_1871 = None
        view_3315 = torch.ops.aten.view.default(_to_copy_1872, [262144, 256]);  _to_copy_1872 = None
        mm_649 = torch.ops.aten.mm.default(view_3315, t_696);  view_3315 = t_696 = None
        view_3316 = torch.ops.aten.view.default(mm_649, [1, 512, 512, 512]);  mm_649 = None
        _to_copy_1873 = torch.ops.aten._to_copy.default(getitem_3210, dtype = torch.bfloat16);  getitem_3210 = None
        _to_copy_1874 = torch.ops.aten._to_copy.default(getitem_3205, dtype = torch.bfloat16)
        t_697 = torch.ops.aten.t.default(_to_copy_1873);  _to_copy_1873 = None
        view_3317 = torch.ops.aten.view.default(_to_copy_1874, [262144, 256]);  _to_copy_1874 = None
        mm_650 = torch.ops.aten.mm.default(view_3317, t_697);  view_3317 = t_697 = None
        view_3318 = torch.ops.aten.view.default(mm_650, [1, 512, 512, 512]);  mm_650 = None
        sigmoid_264 = torch.ops.aten.sigmoid.default(view_3318);  view_3318 = None
        mul_437 = torch.ops.aten.mul.Tensor(view_3316, sigmoid_264);  view_3316 = sigmoid_264 = None
        unsqueeze_1070 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_206 = torch.ops.aten.bitwise_not.default(unsqueeze_1070);  unsqueeze_1070 = None
        masked_fill_206 = torch.ops.aten.masked_fill.Scalar(mul_437, bitwise_not_206, 0);  mul_437 = bitwise_not_206 = None
        split_tensor_347 = torch.ops.aten.split.Tensor(masked_fill_206, 256, dim = -1)
        getitem_3215 = split_tensor_347[0]
        unsqueeze_1073 = torch.ops.aten.unsqueeze.default(getitem_3215, 4);  getitem_3215 = None
        permute_1767 = torch.ops.aten.permute.default(unsqueeze_1073, [0, 1, 4, 3, 2]);  unsqueeze_1073 = None
        permute_1768 = torch.ops.aten.permute.default(permute_1767, [3, 1, 4, 0, 2]);  permute_1767 = None
        view_3321 = torch.ops.aten.view.default(permute_1768, [256, 512, 512]);  permute_1768 = None
        split_tensor_348 = torch.ops.aten.split.Tensor(masked_fill_206, 256, dim = -1);  masked_fill_206 = None
        getitem_3218 = split_tensor_348[1];  split_tensor_348 = None
        unsqueeze_1074 = torch.ops.aten.unsqueeze.default(getitem_3218, 4);  getitem_3218 = None
        permute_1769 = torch.ops.aten.permute.default(unsqueeze_1074, [0, 4, 1, 3, 2]);  unsqueeze_1074 = None
        permute_1770 = torch.ops.aten.permute.default(permute_1769, [3, 4, 0, 2, 1]);  permute_1769 = None
        view_3322 = torch.ops.aten.view.default(permute_1770, [256, 512, 512]);  permute_1770 = None
        bmm_263 = torch.ops.aten.bmm.default(view_3321, view_3322);  view_3321 = view_3322 = None
        view_3323 = torch.ops.aten.view.default(bmm_263, [256, 512, 1, 1, 512]);  bmm_263 = None
        permute_1771 = torch.ops.aten.permute.default(view_3323, [3, 1, 4, 0, 2]);  view_3323 = None
        view_3324 = torch.ops.aten.view.default(permute_1771, [1, 512, 512, 256]);  permute_1771 = None
        _to_copy_1875 = torch.ops.aten._to_copy.default(getitem_3209, dtype = torch.bfloat16);  getitem_3209 = None
        _to_copy_1876 = torch.ops.aten._to_copy.default(getitem_3205, dtype = torch.bfloat16)
        t_698 = torch.ops.aten.t.default(_to_copy_1875);  _to_copy_1875 = None
        view_3325 = torch.ops.aten.view.default(_to_copy_1876, [262144, 256]);  _to_copy_1876 = None
        mm_651 = torch.ops.aten.mm.default(view_3325, t_698);  view_3325 = t_698 = None
        view_3326 = torch.ops.aten.view.default(mm_651, [1, 512, 512, 512]);  mm_651 = None
        _to_copy_1877 = torch.ops.aten._to_copy.default(getitem_3211, dtype = torch.bfloat16);  getitem_3211 = None
        _to_copy_1878 = torch.ops.aten._to_copy.default(getitem_3205, dtype = torch.bfloat16)
        t_699 = torch.ops.aten.t.default(_to_copy_1877);  _to_copy_1877 = None
        view_3327 = torch.ops.aten.view.default(_to_copy_1878, [262144, 256]);  _to_copy_1878 = None
        mm_652 = torch.ops.aten.mm.default(view_3327, t_699);  view_3327 = t_699 = None
        view_3328 = torch.ops.aten.view.default(mm_652, [1, 512, 512, 512]);  mm_652 = None
        sigmoid_265 = torch.ops.aten.sigmoid.default(view_3328);  view_3328 = None
        mul_438 = torch.ops.aten.mul.Tensor(view_3326, sigmoid_265);  view_3326 = sigmoid_265 = None
        view_3329 = torch.ops.aten.view.default(mul_438, [262144, 512]);  mul_438 = None
        view_3330 = torch.ops.aten.view.default(view_3329, [1, 512, 512, 512]);  view_3329 = None
        transpose_86 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_1075 = torch.ops.aten.unsqueeze.default(transpose_86, 3);  transpose_86 = None
        clone_280 = torch.ops.aten.clone.default(unsqueeze_1075, memory_format = torch.contiguous_format);  unsqueeze_1075 = None
        bitwise_not_207 = torch.ops.aten.bitwise_not.default(clone_280);  clone_280 = None
        masked_fill_207 = torch.ops.aten.masked_fill.Scalar(view_3330, bitwise_not_207, 0);  view_3330 = bitwise_not_207 = None
        view_3331 = torch.ops.aten.view.default(masked_fill_207, [262144, 512]);  masked_fill_207 = None
        view_3335 = torch.ops.aten.view.default(view_3331, [1, 512, 512, 512])
        split_tensor_349 = torch.ops.aten.split.Tensor(view_3335, 256, dim = -1);  view_3335 = None
        getitem_3221 = split_tensor_349[0]
        unsqueeze_1078 = torch.ops.aten.unsqueeze.default(getitem_3221, 4);  getitem_3221 = None
        permute_1776 = torch.ops.aten.permute.default(unsqueeze_1078, [0, 2, 4, 3, 1]);  unsqueeze_1078 = None
        permute_1777 = torch.ops.aten.permute.default(permute_1776, [3, 1, 4, 0, 2]);  permute_1776 = None
        view_3336 = torch.ops.aten.view.default(permute_1777, [256, 512, 512]);  permute_1777 = None
        view_3337 = torch.ops.aten.view.default(view_3331, [1, 512, 512, 512]);  view_3331 = None
        split_tensor_350 = torch.ops.aten.split.Tensor(view_3337, 256, dim = -1);  view_3337 = None
        getitem_3224 = split_tensor_350[1];  split_tensor_350 = None
        unsqueeze_1079 = torch.ops.aten.unsqueeze.default(getitem_3224, 4);  getitem_3224 = None
        permute_1778 = torch.ops.aten.permute.default(unsqueeze_1079, [0, 4, 2, 3, 1]);  unsqueeze_1079 = None
        permute_1779 = torch.ops.aten.permute.default(permute_1778, [3, 4, 0, 2, 1]);  permute_1778 = None
        view_3338 = torch.ops.aten.view.default(permute_1779, [256, 512, 512]);  permute_1779 = None
        bmm_264 = torch.ops.aten.bmm.default(view_3336, view_3338);  view_3336 = view_3338 = None
        view_3339 = torch.ops.aten.view.default(bmm_264, [256, 512, 1, 1, 512]);  bmm_264 = None
        permute_1780 = torch.ops.aten.permute.default(view_3339, [3, 1, 4, 0, 2]);  view_3339 = None
        view_3340 = torch.ops.aten.view.default(permute_1780, [1, 512, 512, 256]);  permute_1780 = None
        _to_copy_1879 = torch.ops.aten._to_copy.default(view_3324, dtype = torch.float32);  view_3324 = None
        native_layer_norm_default_387 = torch.ops.aten.native_layer_norm.default(_to_copy_1879, [256], None, None, 1e-05);  _to_copy_1879 = None
        getitem_3225 = native_layer_norm_default_387[0]
        _to_copy_1880 = torch.ops.aten._to_copy.default(view_3340, dtype = torch.float32);  view_3340 = None
        native_layer_norm_default_388 = torch.ops.aten.native_layer_norm.default(_to_copy_1880, [256], None, None, 1e-05);  _to_copy_1880 = None
        getitem_3228 = native_layer_norm_default_388[0]
        add_352 = torch.ops.aten.add.Tensor(getitem_3225, getitem_3228);  getitem_3225 = getitem_3228 = None
        _to_copy_1881 = torch.ops.aten._to_copy.default(arg1112_1, dtype = torch.bfloat16);  arg1112_1 = None
        _to_copy_1882 = torch.ops.aten._to_copy.default(add_352, dtype = torch.bfloat16);  add_352 = None
        t_700 = torch.ops.aten.t.default(_to_copy_1881);  _to_copy_1881 = None
        view_3341 = torch.ops.aten.view.default(_to_copy_1882, [262144, 256]);  _to_copy_1882 = None
        mm_653 = torch.ops.aten.mm.default(view_3341, t_700);  view_3341 = t_700 = None
        view_3342 = torch.ops.aten.view.default(mm_653, [1, 512, 512, 256]);  mm_653 = None
        _to_copy_1883 = torch.ops.aten._to_copy.default(getitem_3212, dtype = torch.bfloat16);  getitem_3212 = None
        _to_copy_1884 = torch.ops.aten._to_copy.default(getitem_3205, dtype = torch.bfloat16);  getitem_3205 = None
        t_701 = torch.ops.aten.t.default(_to_copy_1883);  _to_copy_1883 = None
        view_3343 = torch.ops.aten.view.default(_to_copy_1884, [262144, 256]);  _to_copy_1884 = None
        mm_654 = torch.ops.aten.mm.default(view_3343, t_701);  view_3343 = t_701 = None
        view_3344 = torch.ops.aten.view.default(mm_654, [1, 512, 512, 256]);  mm_654 = None
        sigmoid_266 = torch.ops.aten.sigmoid.default(view_3344);  view_3344 = None
        mul_439 = torch.ops.aten.mul.Tensor(view_3342, sigmoid_266);  view_3342 = sigmoid_266 = None
        add_353 = torch.ops.aten.add.Tensor(add_347, mul_439);  mul_439 = None
        _to_copy_1885 = torch.ops.aten._to_copy.default(add_347, dtype = torch.float32)
        native_layer_norm_default_389 = torch.ops.aten.native_layer_norm.default(_to_copy_1885, [256], None, None, 1e-05);  _to_copy_1885 = None
        getitem_3231 = native_layer_norm_default_389[0]
        _to_copy_1886 = torch.ops.aten._to_copy.default(arg1116_1, dtype = torch.bfloat16);  arg1116_1 = None
        _to_copy_1887 = torch.ops.aten._to_copy.default(getitem_3231, dtype = torch.bfloat16)
        t_702 = torch.ops.aten.t.default(_to_copy_1886);  _to_copy_1886 = None
        view_3345 = torch.ops.aten.view.default(_to_copy_1887, [262144, 256]);  _to_copy_1887 = None
        mm_655 = torch.ops.aten.mm.default(view_3345, t_702);  view_3345 = t_702 = None
        view_3346 = torch.ops.aten.view.default(mm_655, [1, 512, 512, 8]);  mm_655 = None
        view_3347 = torch.ops.aten.view.default(view_3346, [1, 512, 512, 2, 4]);  view_3346 = None
        permute_1781 = torch.ops.aten.permute.default(view_3347, [0, 3, 4, 1, 2]);  view_3347 = None
        view_3348 = torch.ops.aten.view.default(permute_1781, [1, 2, 4, 1, 512, 512]);  permute_1781 = None
        view_3349 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_208 = torch.ops.aten.bitwise_not.default(view_3349);  view_3349 = None
        masked_fill_208 = torch.ops.aten.masked_fill.Scalar(view_3348, bitwise_not_208, -10000);  view_3348 = bitwise_not_208 = None
        view_3350 = torch.ops.aten.view.default(masked_fill_208, [1, 2, 4, 512, 512]);  masked_fill_208 = None
        permute_1782 = torch.ops.aten.permute.default(view_3350, [1, 0, 2, 3, 4]);  view_3350 = None
        view_3351 = torch.ops.aten.view.default(permute_1782, [2, 4, 1, 512, 512]);  permute_1782 = None
        _to_copy_1888 = torch.ops.aten._to_copy.default(arg1117_1, dtype = torch.bfloat16);  arg1117_1 = None
        _to_copy_1889 = torch.ops.aten._to_copy.default(getitem_3231, dtype = torch.bfloat16)
        t_703 = torch.ops.aten.t.default(_to_copy_1888);  _to_copy_1888 = None
        view_3352 = torch.ops.aten.view.default(_to_copy_1889, [262144, 256]);  _to_copy_1889 = None
        mm_656 = torch.ops.aten.mm.default(view_3352, t_703);  view_3352 = t_703 = None
        view_3353 = torch.ops.aten.view.default(mm_656, [1, 512, 512, 1024]);  mm_656 = None
        select_87 = torch.ops.aten.select.int(view_3351, 0, 0)
        view_3354 = torch.ops.aten.view.default(view_3353, [1, 512, 512, 4, 4, 64]);  view_3353 = None
        permute_1783 = torch.ops.aten.permute.default(view_3354, [4, 0, 3, 1, 2, 5]);  view_3354 = None
        view_3355 = torch.ops.aten.view.default(permute_1783, [4, 4, 512, 512, 64]);  permute_1783 = None
        unbind_int_151 = torch.ops.aten.unbind.int(view_3355);  view_3355 = None
        getitem_3234 = unbind_int_151[0]
        getitem_3235 = unbind_int_151[1]
        getitem_3236 = unbind_int_151[2]
        getitem_3237 = unbind_int_151[3];  unbind_int_151 = None
        expand_212 = torch.ops.aten.expand.default(select_87, [4, 512, 512, 512]);  select_87 = None
        _scaled_dot_product_efficient_attention_default_123 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3234, getitem_3235, getitem_3236, expand_212, False);  getitem_3234 = getitem_3235 = getitem_3236 = expand_212 = None
        getitem_3238 = _scaled_dot_product_efficient_attention_default_123[0]
        sigmoid_267 = torch.ops.aten.sigmoid.default(getitem_3237);  getitem_3237 = None
        mul_440 = torch.ops.aten.mul.Tensor(getitem_3238, sigmoid_267);  getitem_3238 = sigmoid_267 = None
        view_3356 = torch.ops.aten.view.default(mul_440, [1, 4, 512, 512, 64]);  mul_440 = None
        permute_1784 = torch.ops.aten.permute.default(view_3356, [0, 2, 3, 1, 4]);  view_3356 = None
        clone_281 = torch.ops.aten.clone.default(permute_1784, memory_format = torch.contiguous_format);  permute_1784 = None
        _unsafe_view_236 = torch.ops.aten._unsafe_view.default(clone_281, [1, 512, 512, 256]);  clone_281 = None
        transpose_87 = torch.ops.aten.transpose.int(getitem_3231, 1, 2);  getitem_3231 = None
        _to_copy_1890 = torch.ops.aten._to_copy.default(arg1118_1, dtype = torch.bfloat16);  arg1118_1 = None
        _to_copy_1891 = torch.ops.aten._to_copy.default(transpose_87, dtype = torch.bfloat16);  transpose_87 = None
        t_704 = torch.ops.aten.t.default(_to_copy_1890);  _to_copy_1890 = None
        expand_213 = torch.ops.aten.expand.default(_to_copy_1891, [1, 512, 512, 256]);  _to_copy_1891 = None
        view_3357 = torch.ops.aten.view.default(expand_213, [512, 512, 256]);  expand_213 = None
        expand_214 = torch.ops.aten.expand.default(t_704, [1, 512, 256, 1024]);  t_704 = None
        view_3358 = torch.ops.aten.view.default(expand_214, [512, 256, 1024]);  expand_214 = None
        bmm_265 = torch.ops.aten.bmm.default(view_3357, view_3358);  view_3357 = view_3358 = None
        view_3359 = torch.ops.aten.view.default(bmm_265, [1, 512, 512, 1024]);  bmm_265 = None
        select_88 = torch.ops.aten.select.int(view_3351, 0, 1);  view_3351 = None
        view_3360 = torch.ops.aten.view.default(view_3359, [1, 512, 512, 4, 4, 64]);  view_3359 = None
        permute_1785 = torch.ops.aten.permute.default(view_3360, [4, 0, 3, 1, 2, 5]);  view_3360 = None
        view_3361 = torch.ops.aten.view.default(permute_1785, [4, 4, 512, 512, 64]);  permute_1785 = None
        unbind_int_152 = torch.ops.aten.unbind.int(view_3361);  view_3361 = None
        getitem_3242 = unbind_int_152[0]
        getitem_3243 = unbind_int_152[1]
        getitem_3244 = unbind_int_152[2]
        getitem_3245 = unbind_int_152[3];  unbind_int_152 = None
        expand_215 = torch.ops.aten.expand.default(select_88, [4, 512, 512, 512]);  select_88 = None
        _scaled_dot_product_efficient_attention_default_124 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3242, getitem_3243, getitem_3244, expand_215, False);  getitem_3242 = getitem_3243 = getitem_3244 = expand_215 = None
        getitem_3246 = _scaled_dot_product_efficient_attention_default_124[0]
        sigmoid_268 = torch.ops.aten.sigmoid.default(getitem_3245);  getitem_3245 = None
        mul_441 = torch.ops.aten.mul.Tensor(getitem_3246, sigmoid_268);  getitem_3246 = sigmoid_268 = None
        view_3362 = torch.ops.aten.view.default(mul_441, [1, 4, 512, 512, 64]);  mul_441 = None
        permute_1786 = torch.ops.aten.permute.default(view_3362, [0, 2, 3, 1, 4]);  view_3362 = None
        clone_282 = torch.ops.aten.clone.default(permute_1786, memory_format = torch.contiguous_format);  permute_1786 = None
        _unsafe_view_237 = torch.ops.aten._unsafe_view.default(clone_282, [1, 512, 512, 256]);  clone_282 = None
        cat_49 = torch.ops.aten.cat.default([_unsafe_view_236, _unsafe_view_237], dim = -1);  _unsafe_view_236 = _unsafe_view_237 = None
        slice_242 = torch.ops.aten.slice.Tensor(arg1115_1, dim = 0, start = 0, end = 9223372036854775807);  arg1115_1 = None
        unsqueeze_1080 = torch.ops.aten.unsqueeze.default(slice_242, 1);  slice_242 = None
        mul_442 = torch.ops.aten.mul.Tensor(arg1119_1, unsqueeze_1080);  arg1119_1 = unsqueeze_1080 = None
        _to_copy_1892 = torch.ops.aten._to_copy.default(mul_442, dtype = torch.bfloat16);  mul_442 = None
        t_705 = torch.ops.aten.t.default(_to_copy_1892);  _to_copy_1892 = None
        view_3363 = torch.ops.aten.view.default(cat_49, [262144, 512]);  cat_49 = None
        mm_657 = torch.ops.aten.mm.default(view_3363, t_705);  view_3363 = t_705 = None
        view_3364 = torch.ops.aten.view.default(mm_657, [1, 512, 512, 256]);  mm_657 = None
        add_354 = torch.ops.aten.add.Tensor(add_353, view_3364);  add_353 = view_3364 = None
        split_tensor_351 = torch.ops.aten.split.Tensor(add_347, 512, dim = -2)
        getitem_3250 = split_tensor_351[0];  split_tensor_351 = None
        _to_copy_1893 = torch.ops.aten._to_copy.default(getitem_3250, dtype = torch.float32);  getitem_3250 = None
        native_layer_norm_default_390 = torch.ops.aten.native_layer_norm.default(_to_copy_1893, [256], arg1106_1, arg1107_1, 1e-05);  _to_copy_1893 = arg1106_1 = arg1107_1 = None
        getitem_3251 = native_layer_norm_default_390[0]
        _to_copy_1894 = torch.ops.aten._to_copy.default(arg1108_1, dtype = torch.bfloat16);  arg1108_1 = None
        _to_copy_1895 = torch.ops.aten._to_copy.default(getitem_3251, dtype = torch.bfloat16);  getitem_3251 = None
        t_706 = torch.ops.aten.t.default(_to_copy_1894);  _to_copy_1894 = None
        view_3365 = torch.ops.aten.view.default(_to_copy_1895, [262144, 256]);  _to_copy_1895 = None
        mm_658 = torch.ops.aten.mm.default(view_3365, t_706);  view_3365 = t_706 = None
        view_3366 = torch.ops.aten.view.default(mm_658, [1, 512, 512, 1024]);  mm_658 = None
        split_tensor_352 = torch.ops.aten.split.Tensor(view_3366, 512, dim = -1);  view_3366 = None
        getitem_3254 = split_tensor_352[0]
        getitem_3255 = split_tensor_352[1];  split_tensor_352 = None
        silu_92 = torch.ops.aten.silu.default(getitem_3254);  getitem_3254 = None
        mul_443 = torch.ops.aten.mul.Tensor(silu_92, getitem_3255);  silu_92 = getitem_3255 = None
        _to_copy_1896 = torch.ops.aten._to_copy.default(arg1109_1, dtype = torch.bfloat16);  arg1109_1 = None
        t_707 = torch.ops.aten.t.default(_to_copy_1896);  _to_copy_1896 = None
        view_3368 = torch.ops.aten.view.default(mul_443, [262144, 512]);  mul_443 = None
        mm_659 = torch.ops.aten.mm.default(view_3368, t_707);  view_3368 = t_707 = None
        view_3369 = torch.ops.aten.view.default(mm_659, [1, 512, 512, 256]);  mm_659 = None
        add_355 = torch.ops.aten.add.Tensor(add_354, view_3369);  add_354 = view_3369 = None
        _to_copy_1897 = torch.ops.aten._to_copy.default(add_351, dtype = torch.float32)
        native_layer_norm_default_391 = torch.ops.aten.native_layer_norm.default(_to_copy_1897, [384], arg1124_1, arg1125_1, 1e-05);  _to_copy_1897 = arg1124_1 = arg1125_1 = None
        getitem_3256 = native_layer_norm_default_391[0]
        _to_copy_1898 = torch.ops.aten._to_copy.default(add_347, dtype = torch.float32);  add_347 = None
        native_layer_norm_default_392 = torch.ops.aten.native_layer_norm.default(_to_copy_1898, [256], arg1126_1, arg1127_1, 1e-05);  _to_copy_1898 = arg1126_1 = arg1127_1 = None
        getitem_3259 = native_layer_norm_default_392[0]
        _to_copy_1899 = torch.ops.aten._to_copy.default(arg1128_1, dtype = torch.bfloat16);  arg1128_1 = None
        _to_copy_1900 = torch.ops.aten._to_copy.default(getitem_3259, dtype = torch.bfloat16);  getitem_3259 = None
        t_708 = torch.ops.aten.t.default(_to_copy_1899);  _to_copy_1899 = None
        view_3370 = torch.ops.aten.view.default(_to_copy_1900, [262144, 256]);  _to_copy_1900 = None
        mm_660 = torch.ops.aten.mm.default(view_3370, t_708);  view_3370 = t_708 = None
        view_3371 = torch.ops.aten.view.default(mm_660, [1, 512, 512, 16]);  mm_660 = None
        permute_1787 = torch.ops.aten.permute.default(view_3371, [0, 3, 1, 2]);  view_3371 = None
        view_3372 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_209 = torch.ops.aten.bitwise_not.default(view_3372);  view_3372 = None
        masked_fill_209 = torch.ops.aten.masked_fill.Scalar(permute_1787, bitwise_not_209, -10000);  permute_1787 = bitwise_not_209 = None
        _to_copy_1901 = torch.ops.aten._to_copy.default(getitem_3256, dtype = torch.bfloat16);  getitem_3256 = None
        _to_copy_1902 = torch.ops.aten._to_copy.default(arg1130_1, dtype = torch.bfloat16);  arg1130_1 = None
        unsqueeze_1081 = torch.ops.aten.unsqueeze.default(_to_copy_1901, 3);  _to_copy_1901 = None
        unsqueeze_1082 = torch.ops.aten.unsqueeze.default(unsqueeze_1081, 4);  unsqueeze_1081 = None
        unsqueeze_1083 = torch.ops.aten.unsqueeze.default(unsqueeze_1082, 5);  unsqueeze_1082 = None
        permute_1788 = torch.ops.aten.permute.default(unsqueeze_1083, [3, 0, 4, 1, 5, 2]);  unsqueeze_1083 = None
        unsqueeze_1084 = torch.ops.aten.unsqueeze.default(_to_copy_1902, 4);  _to_copy_1902 = None
        unsqueeze_1085 = torch.ops.aten.unsqueeze.default(unsqueeze_1084, 5);  unsqueeze_1084 = None
        permute_1789 = torch.ops.aten.permute.default(unsqueeze_1085, [1, 4, 2, 5, 3, 0]);  unsqueeze_1085 = None
        permute_1790 = torch.ops.aten.permute.default(permute_1788, [3, 5, 0, 1, 2, 4]);  permute_1788 = None
        view_3373 = torch.ops.aten.view.default(permute_1790, [1, 512, 384]);  permute_1790 = None
        permute_1791 = torch.ops.aten.permute.default(permute_1789, [5, 0, 1, 2, 4, 3]);  permute_1789 = None
        view_3374 = torch.ops.aten.view.default(permute_1791, [1, 384, 1536]);  permute_1791 = None
        bmm_266 = torch.ops.aten.bmm.default(view_3373, view_3374);  view_3373 = view_3374 = None
        view_3375 = torch.ops.aten.view.default(bmm_266, [512, 1, 4, 1, 16, 24]);  bmm_266 = None
        permute_1792 = torch.ops.aten.permute.default(view_3375, [2, 3, 4, 0, 5, 1]);  view_3375 = None
        view_3376 = torch.ops.aten.view.default(permute_1792, [4, 1, 16, 512, 24]);  permute_1792 = None
        unbind_int_153 = torch.ops.aten.unbind.int(view_3376);  view_3376 = None
        getitem_3262 = unbind_int_153[0]
        getitem_3263 = unbind_int_153[1]
        getitem_3264 = unbind_int_153[2]
        getitem_3265 = unbind_int_153[3];  unbind_int_153 = None
        view_3377 = torch.ops.aten.view.default(arg1129_1, [1, 16, 1, 24]);  arg1129_1 = None
        add_356 = torch.ops.aten.add.Tensor(getitem_3262, view_3377);  getitem_3262 = view_3377 = None
        _to_copy_1903 = torch.ops.aten._to_copy.default(add_356, dtype = torch.bfloat16);  add_356 = None
        expand_216 = torch.ops.aten.expand.default(masked_fill_209, [1, 16, 512, 512]);  masked_fill_209 = None
        _scaled_dot_product_efficient_attention_default_125 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1903, getitem_3263, getitem_3264, expand_216, False);  _to_copy_1903 = getitem_3263 = getitem_3264 = expand_216 = None
        getitem_3266 = _scaled_dot_product_efficient_attention_default_125[0]
        add_357 = torch.ops.aten.add.Tensor(getitem_3265, 1);  getitem_3265 = None
        sigmoid_269 = torch.ops.aten.sigmoid.default(add_357);  add_357 = None
        mul_444 = torch.ops.aten.mul.Tensor(getitem_3266, sigmoid_269);  getitem_3266 = sigmoid_269 = None
        _to_copy_1904 = torch.ops.aten._to_copy.default(arg1131_1, dtype = torch.bfloat16);  arg1131_1 = None
        unsqueeze_1086 = torch.ops.aten.unsqueeze.default(mul_444, 4);  mul_444 = None
        permute_1793 = torch.ops.aten.permute.default(unsqueeze_1086, [0, 2, 4, 3, 1]);  unsqueeze_1086 = None
        unsqueeze_1087 = torch.ops.aten.unsqueeze.default(_to_copy_1904, 3);  _to_copy_1904 = None
        unsqueeze_1088 = torch.ops.aten.unsqueeze.default(unsqueeze_1087, 4);  unsqueeze_1087 = None
        permute_1794 = torch.ops.aten.permute.default(unsqueeze_1088, [3, 4, 2, 1, 0]);  unsqueeze_1088 = None
        permute_1795 = torch.ops.aten.permute.default(permute_1793, [1, 3, 4, 0, 2]);  permute_1793 = None
        clone_283 = torch.ops.aten.clone.default(permute_1795, memory_format = torch.contiguous_format);  permute_1795 = None
        _unsafe_view_238 = torch.ops.aten._unsafe_view.default(clone_283, [1, 512, 384]);  clone_283 = None
        permute_1796 = torch.ops.aten.permute.default(permute_1794, [3, 4, 0, 2, 1]);  permute_1794 = None
        clone_284 = torch.ops.aten.clone.default(permute_1796, memory_format = torch.contiguous_format);  permute_1796 = None
        _unsafe_view_239 = torch.ops.aten._unsafe_view.default(clone_284, [1, 384, 384]);  clone_284 = None
        bmm_267 = torch.ops.aten.bmm.default(_unsafe_view_238, _unsafe_view_239);  _unsafe_view_238 = _unsafe_view_239 = None
        view_3378 = torch.ops.aten.view.default(bmm_267, [512, 1, 1, 1, 384]);  bmm_267 = None
        permute_1797 = torch.ops.aten.permute.default(view_3378, [3, 0, 4, 1, 2]);  view_3378 = None
        view_3379 = torch.ops.aten.view.default(permute_1797, [1, 512, 384]);  permute_1797 = None
        unsqueeze_1089 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_445 = torch.ops.aten.mul.Tensor(view_3379, unsqueeze_1089);  view_3379 = unsqueeze_1089 = None
        add_358 = torch.ops.aten.add.Tensor(add_351, mul_445);  mul_445 = None
        split_tensor_353 = torch.ops.aten.split.Tensor(add_351, 512, dim = -2);  add_351 = None
        getitem_3270 = split_tensor_353[0];  split_tensor_353 = None
        _to_copy_1905 = torch.ops.aten._to_copy.default(getitem_3270, dtype = torch.float32);  getitem_3270 = None
        native_layer_norm_default_393 = torch.ops.aten.native_layer_norm.default(_to_copy_1905, [384], arg1120_1, arg1121_1, 1e-05);  _to_copy_1905 = arg1120_1 = arg1121_1 = None
        getitem_3271 = native_layer_norm_default_393[0]
        _to_copy_1906 = torch.ops.aten._to_copy.default(arg1122_1, dtype = torch.bfloat16);  arg1122_1 = None
        _to_copy_1907 = torch.ops.aten._to_copy.default(getitem_3271, dtype = torch.bfloat16);  getitem_3271 = None
        t_709 = torch.ops.aten.t.default(_to_copy_1906);  _to_copy_1906 = None
        view_3380 = torch.ops.aten.view.default(_to_copy_1907, [512, 384]);  _to_copy_1907 = None
        mm_661 = torch.ops.aten.mm.default(view_3380, t_709);  view_3380 = t_709 = None
        view_3381 = torch.ops.aten.view.default(mm_661, [1, 512, 1536]);  mm_661 = None
        split_tensor_354 = torch.ops.aten.split.Tensor(view_3381, 768, dim = -1);  view_3381 = None
        getitem_3274 = split_tensor_354[0]
        getitem_3275 = split_tensor_354[1];  split_tensor_354 = None
        silu_93 = torch.ops.aten.silu.default(getitem_3274);  getitem_3274 = None
        mul_446 = torch.ops.aten.mul.Tensor(silu_93, getitem_3275);  silu_93 = getitem_3275 = None
        _to_copy_1908 = torch.ops.aten._to_copy.default(arg1123_1, dtype = torch.bfloat16);  arg1123_1 = None
        t_710 = torch.ops.aten.t.default(_to_copy_1908);  _to_copy_1908 = None
        view_3383 = torch.ops.aten.view.default(mul_446, [512, 768]);  mul_446 = None
        mm_662 = torch.ops.aten.mm.default(view_3383, t_710);  view_3383 = t_710 = None
        view_3384 = torch.ops.aten.view.default(mm_662, [1, 512, 384]);  mm_662 = None
        add_359 = torch.ops.aten.add.Tensor(add_358, view_3384);  add_358 = view_3384 = None
        _to_copy_1909 = torch.ops.aten._to_copy.default(add_355, dtype = torch.float32)
        native_layer_norm_default_394 = torch.ops.aten.native_layer_norm.default(_to_copy_1909, [256], arg1136_1, arg1137_1, 1e-05);  _to_copy_1909 = arg1136_1 = arg1137_1 = None
        getitem_3276 = native_layer_norm_default_394[0]
        split_with_sizes_default_88 = torch.ops.aten.split_with_sizes.default(arg1139_1, [512, 512]);  arg1139_1 = None
        getitem_3279 = split_with_sizes_default_88[0]
        getitem_3280 = split_with_sizes_default_88[1];  split_with_sizes_default_88 = None
        split_with_sizes_default_89 = torch.ops.aten.split_with_sizes.default(arg1140_1, [512, 512, 256]);  arg1140_1 = None
        getitem_3281 = split_with_sizes_default_89[0]
        getitem_3282 = split_with_sizes_default_89[1]
        getitem_3283 = split_with_sizes_default_89[2];  split_with_sizes_default_89 = None
        _to_copy_1910 = torch.ops.aten._to_copy.default(getitem_3279, dtype = torch.bfloat16);  getitem_3279 = None
        _to_copy_1911 = torch.ops.aten._to_copy.default(getitem_3276, dtype = torch.bfloat16)
        t_711 = torch.ops.aten.t.default(_to_copy_1910);  _to_copy_1910 = None
        view_3385 = torch.ops.aten.view.default(_to_copy_1911, [262144, 256]);  _to_copy_1911 = None
        mm_663 = torch.ops.aten.mm.default(view_3385, t_711);  view_3385 = t_711 = None
        view_3386 = torch.ops.aten.view.default(mm_663, [1, 512, 512, 512]);  mm_663 = None
        _to_copy_1912 = torch.ops.aten._to_copy.default(getitem_3281, dtype = torch.bfloat16);  getitem_3281 = None
        _to_copy_1913 = torch.ops.aten._to_copy.default(getitem_3276, dtype = torch.bfloat16)
        t_712 = torch.ops.aten.t.default(_to_copy_1912);  _to_copy_1912 = None
        view_3387 = torch.ops.aten.view.default(_to_copy_1913, [262144, 256]);  _to_copy_1913 = None
        mm_664 = torch.ops.aten.mm.default(view_3387, t_712);  view_3387 = t_712 = None
        view_3388 = torch.ops.aten.view.default(mm_664, [1, 512, 512, 512]);  mm_664 = None
        sigmoid_270 = torch.ops.aten.sigmoid.default(view_3388);  view_3388 = None
        mul_447 = torch.ops.aten.mul.Tensor(view_3386, sigmoid_270);  view_3386 = sigmoid_270 = None
        unsqueeze_1090 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_210 = torch.ops.aten.bitwise_not.default(unsqueeze_1090);  unsqueeze_1090 = None
        masked_fill_210 = torch.ops.aten.masked_fill.Scalar(mul_447, bitwise_not_210, 0);  mul_447 = bitwise_not_210 = None
        split_tensor_355 = torch.ops.aten.split.Tensor(masked_fill_210, 256, dim = -1)
        getitem_3286 = split_tensor_355[0]
        unsqueeze_1093 = torch.ops.aten.unsqueeze.default(getitem_3286, 4);  getitem_3286 = None
        permute_1802 = torch.ops.aten.permute.default(unsqueeze_1093, [0, 1, 4, 3, 2]);  unsqueeze_1093 = None
        permute_1803 = torch.ops.aten.permute.default(permute_1802, [3, 1, 4, 0, 2]);  permute_1802 = None
        view_3391 = torch.ops.aten.view.default(permute_1803, [256, 512, 512]);  permute_1803 = None
        split_tensor_356 = torch.ops.aten.split.Tensor(masked_fill_210, 256, dim = -1);  masked_fill_210 = None
        getitem_3289 = split_tensor_356[1];  split_tensor_356 = None
        unsqueeze_1094 = torch.ops.aten.unsqueeze.default(getitem_3289, 4);  getitem_3289 = None
        permute_1804 = torch.ops.aten.permute.default(unsqueeze_1094, [0, 4, 1, 3, 2]);  unsqueeze_1094 = None
        permute_1805 = torch.ops.aten.permute.default(permute_1804, [3, 4, 0, 2, 1]);  permute_1804 = None
        view_3392 = torch.ops.aten.view.default(permute_1805, [256, 512, 512]);  permute_1805 = None
        bmm_268 = torch.ops.aten.bmm.default(view_3391, view_3392);  view_3391 = view_3392 = None
        view_3393 = torch.ops.aten.view.default(bmm_268, [256, 512, 1, 1, 512]);  bmm_268 = None
        permute_1806 = torch.ops.aten.permute.default(view_3393, [3, 1, 4, 0, 2]);  view_3393 = None
        view_3394 = torch.ops.aten.view.default(permute_1806, [1, 512, 512, 256]);  permute_1806 = None
        _to_copy_1914 = torch.ops.aten._to_copy.default(getitem_3280, dtype = torch.bfloat16);  getitem_3280 = None
        _to_copy_1915 = torch.ops.aten._to_copy.default(getitem_3276, dtype = torch.bfloat16)
        t_713 = torch.ops.aten.t.default(_to_copy_1914);  _to_copy_1914 = None
        view_3395 = torch.ops.aten.view.default(_to_copy_1915, [262144, 256]);  _to_copy_1915 = None
        mm_665 = torch.ops.aten.mm.default(view_3395, t_713);  view_3395 = t_713 = None
        view_3396 = torch.ops.aten.view.default(mm_665, [1, 512, 512, 512]);  mm_665 = None
        _to_copy_1916 = torch.ops.aten._to_copy.default(getitem_3282, dtype = torch.bfloat16);  getitem_3282 = None
        _to_copy_1917 = torch.ops.aten._to_copy.default(getitem_3276, dtype = torch.bfloat16)
        t_714 = torch.ops.aten.t.default(_to_copy_1916);  _to_copy_1916 = None
        view_3397 = torch.ops.aten.view.default(_to_copy_1917, [262144, 256]);  _to_copy_1917 = None
        mm_666 = torch.ops.aten.mm.default(view_3397, t_714);  view_3397 = t_714 = None
        view_3398 = torch.ops.aten.view.default(mm_666, [1, 512, 512, 512]);  mm_666 = None
        sigmoid_271 = torch.ops.aten.sigmoid.default(view_3398);  view_3398 = None
        mul_448 = torch.ops.aten.mul.Tensor(view_3396, sigmoid_271);  view_3396 = sigmoid_271 = None
        view_3399 = torch.ops.aten.view.default(mul_448, [262144, 512]);  mul_448 = None
        view_3400 = torch.ops.aten.view.default(view_3399, [1, 512, 512, 512]);  view_3399 = None
        transpose_88 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_1095 = torch.ops.aten.unsqueeze.default(transpose_88, 3);  transpose_88 = None
        clone_285 = torch.ops.aten.clone.default(unsqueeze_1095, memory_format = torch.contiguous_format);  unsqueeze_1095 = None
        bitwise_not_211 = torch.ops.aten.bitwise_not.default(clone_285);  clone_285 = None
        masked_fill_211 = torch.ops.aten.masked_fill.Scalar(view_3400, bitwise_not_211, 0);  view_3400 = bitwise_not_211 = None
        view_3401 = torch.ops.aten.view.default(masked_fill_211, [262144, 512]);  masked_fill_211 = None
        view_3405 = torch.ops.aten.view.default(view_3401, [1, 512, 512, 512])
        split_tensor_357 = torch.ops.aten.split.Tensor(view_3405, 256, dim = -1);  view_3405 = None
        getitem_3292 = split_tensor_357[0]
        unsqueeze_1098 = torch.ops.aten.unsqueeze.default(getitem_3292, 4);  getitem_3292 = None
        permute_1811 = torch.ops.aten.permute.default(unsqueeze_1098, [0, 2, 4, 3, 1]);  unsqueeze_1098 = None
        permute_1812 = torch.ops.aten.permute.default(permute_1811, [3, 1, 4, 0, 2]);  permute_1811 = None
        view_3406 = torch.ops.aten.view.default(permute_1812, [256, 512, 512]);  permute_1812 = None
        view_3407 = torch.ops.aten.view.default(view_3401, [1, 512, 512, 512]);  view_3401 = None
        split_tensor_358 = torch.ops.aten.split.Tensor(view_3407, 256, dim = -1);  view_3407 = None
        getitem_3295 = split_tensor_358[1];  split_tensor_358 = None
        unsqueeze_1099 = torch.ops.aten.unsqueeze.default(getitem_3295, 4);  getitem_3295 = None
        permute_1813 = torch.ops.aten.permute.default(unsqueeze_1099, [0, 4, 2, 3, 1]);  unsqueeze_1099 = None
        permute_1814 = torch.ops.aten.permute.default(permute_1813, [3, 4, 0, 2, 1]);  permute_1813 = None
        view_3408 = torch.ops.aten.view.default(permute_1814, [256, 512, 512]);  permute_1814 = None
        bmm_269 = torch.ops.aten.bmm.default(view_3406, view_3408);  view_3406 = view_3408 = None
        view_3409 = torch.ops.aten.view.default(bmm_269, [256, 512, 1, 1, 512]);  bmm_269 = None
        permute_1815 = torch.ops.aten.permute.default(view_3409, [3, 1, 4, 0, 2]);  view_3409 = None
        view_3410 = torch.ops.aten.view.default(permute_1815, [1, 512, 512, 256]);  permute_1815 = None
        _to_copy_1918 = torch.ops.aten._to_copy.default(view_3394, dtype = torch.float32);  view_3394 = None
        native_layer_norm_default_395 = torch.ops.aten.native_layer_norm.default(_to_copy_1918, [256], None, None, 1e-05);  _to_copy_1918 = None
        getitem_3296 = native_layer_norm_default_395[0]
        _to_copy_1919 = torch.ops.aten._to_copy.default(view_3410, dtype = torch.float32);  view_3410 = None
        native_layer_norm_default_396 = torch.ops.aten.native_layer_norm.default(_to_copy_1919, [256], None, None, 1e-05);  _to_copy_1919 = None
        getitem_3299 = native_layer_norm_default_396[0]
        add_360 = torch.ops.aten.add.Tensor(getitem_3296, getitem_3299);  getitem_3296 = getitem_3299 = None
        _to_copy_1920 = torch.ops.aten._to_copy.default(arg1138_1, dtype = torch.bfloat16);  arg1138_1 = None
        _to_copy_1921 = torch.ops.aten._to_copy.default(add_360, dtype = torch.bfloat16);  add_360 = None
        t_715 = torch.ops.aten.t.default(_to_copy_1920);  _to_copy_1920 = None
        view_3411 = torch.ops.aten.view.default(_to_copy_1921, [262144, 256]);  _to_copy_1921 = None
        mm_667 = torch.ops.aten.mm.default(view_3411, t_715);  view_3411 = t_715 = None
        view_3412 = torch.ops.aten.view.default(mm_667, [1, 512, 512, 256]);  mm_667 = None
        _to_copy_1922 = torch.ops.aten._to_copy.default(getitem_3283, dtype = torch.bfloat16);  getitem_3283 = None
        _to_copy_1923 = torch.ops.aten._to_copy.default(getitem_3276, dtype = torch.bfloat16);  getitem_3276 = None
        t_716 = torch.ops.aten.t.default(_to_copy_1922);  _to_copy_1922 = None
        view_3413 = torch.ops.aten.view.default(_to_copy_1923, [262144, 256]);  _to_copy_1923 = None
        mm_668 = torch.ops.aten.mm.default(view_3413, t_716);  view_3413 = t_716 = None
        view_3414 = torch.ops.aten.view.default(mm_668, [1, 512, 512, 256]);  mm_668 = None
        sigmoid_272 = torch.ops.aten.sigmoid.default(view_3414);  view_3414 = None
        mul_449 = torch.ops.aten.mul.Tensor(view_3412, sigmoid_272);  view_3412 = sigmoid_272 = None
        add_361 = torch.ops.aten.add.Tensor(add_355, mul_449);  mul_449 = None
        _to_copy_1924 = torch.ops.aten._to_copy.default(add_355, dtype = torch.float32)
        native_layer_norm_default_397 = torch.ops.aten.native_layer_norm.default(_to_copy_1924, [256], None, None, 1e-05);  _to_copy_1924 = None
        getitem_3302 = native_layer_norm_default_397[0]
        _to_copy_1925 = torch.ops.aten._to_copy.default(arg1142_1, dtype = torch.bfloat16);  arg1142_1 = None
        _to_copy_1926 = torch.ops.aten._to_copy.default(getitem_3302, dtype = torch.bfloat16)
        t_717 = torch.ops.aten.t.default(_to_copy_1925);  _to_copy_1925 = None
        view_3415 = torch.ops.aten.view.default(_to_copy_1926, [262144, 256]);  _to_copy_1926 = None
        mm_669 = torch.ops.aten.mm.default(view_3415, t_717);  view_3415 = t_717 = None
        view_3416 = torch.ops.aten.view.default(mm_669, [1, 512, 512, 8]);  mm_669 = None
        view_3417 = torch.ops.aten.view.default(view_3416, [1, 512, 512, 2, 4]);  view_3416 = None
        permute_1816 = torch.ops.aten.permute.default(view_3417, [0, 3, 4, 1, 2]);  view_3417 = None
        view_3418 = torch.ops.aten.view.default(permute_1816, [1, 2, 4, 1, 512, 512]);  permute_1816 = None
        view_3419 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_212 = torch.ops.aten.bitwise_not.default(view_3419);  view_3419 = None
        masked_fill_212 = torch.ops.aten.masked_fill.Scalar(view_3418, bitwise_not_212, -10000);  view_3418 = bitwise_not_212 = None
        view_3420 = torch.ops.aten.view.default(masked_fill_212, [1, 2, 4, 512, 512]);  masked_fill_212 = None
        permute_1817 = torch.ops.aten.permute.default(view_3420, [1, 0, 2, 3, 4]);  view_3420 = None
        view_3421 = torch.ops.aten.view.default(permute_1817, [2, 4, 1, 512, 512]);  permute_1817 = None
        _to_copy_1927 = torch.ops.aten._to_copy.default(arg1143_1, dtype = torch.bfloat16);  arg1143_1 = None
        _to_copy_1928 = torch.ops.aten._to_copy.default(getitem_3302, dtype = torch.bfloat16)
        t_718 = torch.ops.aten.t.default(_to_copy_1927);  _to_copy_1927 = None
        view_3422 = torch.ops.aten.view.default(_to_copy_1928, [262144, 256]);  _to_copy_1928 = None
        mm_670 = torch.ops.aten.mm.default(view_3422, t_718);  view_3422 = t_718 = None
        view_3423 = torch.ops.aten.view.default(mm_670, [1, 512, 512, 1024]);  mm_670 = None
        select_89 = torch.ops.aten.select.int(view_3421, 0, 0)
        view_3424 = torch.ops.aten.view.default(view_3423, [1, 512, 512, 4, 4, 64]);  view_3423 = None
        permute_1818 = torch.ops.aten.permute.default(view_3424, [4, 0, 3, 1, 2, 5]);  view_3424 = None
        view_3425 = torch.ops.aten.view.default(permute_1818, [4, 4, 512, 512, 64]);  permute_1818 = None
        unbind_int_154 = torch.ops.aten.unbind.int(view_3425);  view_3425 = None
        getitem_3305 = unbind_int_154[0]
        getitem_3306 = unbind_int_154[1]
        getitem_3307 = unbind_int_154[2]
        getitem_3308 = unbind_int_154[3];  unbind_int_154 = None
        expand_217 = torch.ops.aten.expand.default(select_89, [4, 512, 512, 512]);  select_89 = None
        _scaled_dot_product_efficient_attention_default_126 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3305, getitem_3306, getitem_3307, expand_217, False);  getitem_3305 = getitem_3306 = getitem_3307 = expand_217 = None
        getitem_3309 = _scaled_dot_product_efficient_attention_default_126[0]
        sigmoid_273 = torch.ops.aten.sigmoid.default(getitem_3308);  getitem_3308 = None
        mul_450 = torch.ops.aten.mul.Tensor(getitem_3309, sigmoid_273);  getitem_3309 = sigmoid_273 = None
        view_3426 = torch.ops.aten.view.default(mul_450, [1, 4, 512, 512, 64]);  mul_450 = None
        permute_1819 = torch.ops.aten.permute.default(view_3426, [0, 2, 3, 1, 4]);  view_3426 = None
        clone_286 = torch.ops.aten.clone.default(permute_1819, memory_format = torch.contiguous_format);  permute_1819 = None
        _unsafe_view_240 = torch.ops.aten._unsafe_view.default(clone_286, [1, 512, 512, 256]);  clone_286 = None
        transpose_89 = torch.ops.aten.transpose.int(getitem_3302, 1, 2);  getitem_3302 = None
        _to_copy_1929 = torch.ops.aten._to_copy.default(arg1144_1, dtype = torch.bfloat16);  arg1144_1 = None
        _to_copy_1930 = torch.ops.aten._to_copy.default(transpose_89, dtype = torch.bfloat16);  transpose_89 = None
        t_719 = torch.ops.aten.t.default(_to_copy_1929);  _to_copy_1929 = None
        expand_218 = torch.ops.aten.expand.default(_to_copy_1930, [1, 512, 512, 256]);  _to_copy_1930 = None
        view_3427 = torch.ops.aten.view.default(expand_218, [512, 512, 256]);  expand_218 = None
        expand_219 = torch.ops.aten.expand.default(t_719, [1, 512, 256, 1024]);  t_719 = None
        view_3428 = torch.ops.aten.view.default(expand_219, [512, 256, 1024]);  expand_219 = None
        bmm_270 = torch.ops.aten.bmm.default(view_3427, view_3428);  view_3427 = view_3428 = None
        view_3429 = torch.ops.aten.view.default(bmm_270, [1, 512, 512, 1024]);  bmm_270 = None
        select_90 = torch.ops.aten.select.int(view_3421, 0, 1);  view_3421 = None
        view_3430 = torch.ops.aten.view.default(view_3429, [1, 512, 512, 4, 4, 64]);  view_3429 = None
        permute_1820 = torch.ops.aten.permute.default(view_3430, [4, 0, 3, 1, 2, 5]);  view_3430 = None
        view_3431 = torch.ops.aten.view.default(permute_1820, [4, 4, 512, 512, 64]);  permute_1820 = None
        unbind_int_155 = torch.ops.aten.unbind.int(view_3431);  view_3431 = None
        getitem_3313 = unbind_int_155[0]
        getitem_3314 = unbind_int_155[1]
        getitem_3315 = unbind_int_155[2]
        getitem_3316 = unbind_int_155[3];  unbind_int_155 = None
        expand_220 = torch.ops.aten.expand.default(select_90, [4, 512, 512, 512]);  select_90 = None
        _scaled_dot_product_efficient_attention_default_127 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3313, getitem_3314, getitem_3315, expand_220, False);  getitem_3313 = getitem_3314 = getitem_3315 = expand_220 = None
        getitem_3317 = _scaled_dot_product_efficient_attention_default_127[0]
        sigmoid_274 = torch.ops.aten.sigmoid.default(getitem_3316);  getitem_3316 = None
        mul_451 = torch.ops.aten.mul.Tensor(getitem_3317, sigmoid_274);  getitem_3317 = sigmoid_274 = None
        view_3432 = torch.ops.aten.view.default(mul_451, [1, 4, 512, 512, 64]);  mul_451 = None
        permute_1821 = torch.ops.aten.permute.default(view_3432, [0, 2, 3, 1, 4]);  view_3432 = None
        clone_287 = torch.ops.aten.clone.default(permute_1821, memory_format = torch.contiguous_format);  permute_1821 = None
        _unsafe_view_241 = torch.ops.aten._unsafe_view.default(clone_287, [1, 512, 512, 256]);  clone_287 = None
        cat_50 = torch.ops.aten.cat.default([_unsafe_view_240, _unsafe_view_241], dim = -1);  _unsafe_view_240 = _unsafe_view_241 = None
        slice_243 = torch.ops.aten.slice.Tensor(arg1141_1, dim = 0, start = 0, end = 9223372036854775807);  arg1141_1 = None
        unsqueeze_1100 = torch.ops.aten.unsqueeze.default(slice_243, 1);  slice_243 = None
        mul_452 = torch.ops.aten.mul.Tensor(arg1145_1, unsqueeze_1100);  arg1145_1 = unsqueeze_1100 = None
        _to_copy_1931 = torch.ops.aten._to_copy.default(mul_452, dtype = torch.bfloat16);  mul_452 = None
        t_720 = torch.ops.aten.t.default(_to_copy_1931);  _to_copy_1931 = None
        view_3433 = torch.ops.aten.view.default(cat_50, [262144, 512]);  cat_50 = None
        mm_671 = torch.ops.aten.mm.default(view_3433, t_720);  view_3433 = t_720 = None
        view_3434 = torch.ops.aten.view.default(mm_671, [1, 512, 512, 256]);  mm_671 = None
        add_362 = torch.ops.aten.add.Tensor(add_361, view_3434);  add_361 = view_3434 = None
        split_tensor_359 = torch.ops.aten.split.Tensor(add_355, 512, dim = -2)
        getitem_3321 = split_tensor_359[0];  split_tensor_359 = None
        _to_copy_1932 = torch.ops.aten._to_copy.default(getitem_3321, dtype = torch.float32);  getitem_3321 = None
        native_layer_norm_default_398 = torch.ops.aten.native_layer_norm.default(_to_copy_1932, [256], arg1132_1, arg1133_1, 1e-05);  _to_copy_1932 = arg1132_1 = arg1133_1 = None
        getitem_3322 = native_layer_norm_default_398[0]
        _to_copy_1933 = torch.ops.aten._to_copy.default(arg1134_1, dtype = torch.bfloat16);  arg1134_1 = None
        _to_copy_1934 = torch.ops.aten._to_copy.default(getitem_3322, dtype = torch.bfloat16);  getitem_3322 = None
        t_721 = torch.ops.aten.t.default(_to_copy_1933);  _to_copy_1933 = None
        view_3435 = torch.ops.aten.view.default(_to_copy_1934, [262144, 256]);  _to_copy_1934 = None
        mm_672 = torch.ops.aten.mm.default(view_3435, t_721);  view_3435 = t_721 = None
        view_3436 = torch.ops.aten.view.default(mm_672, [1, 512, 512, 1024]);  mm_672 = None
        split_tensor_360 = torch.ops.aten.split.Tensor(view_3436, 512, dim = -1);  view_3436 = None
        getitem_3325 = split_tensor_360[0]
        getitem_3326 = split_tensor_360[1];  split_tensor_360 = None
        silu_94 = torch.ops.aten.silu.default(getitem_3325);  getitem_3325 = None
        mul_453 = torch.ops.aten.mul.Tensor(silu_94, getitem_3326);  silu_94 = getitem_3326 = None
        _to_copy_1935 = torch.ops.aten._to_copy.default(arg1135_1, dtype = torch.bfloat16);  arg1135_1 = None
        t_722 = torch.ops.aten.t.default(_to_copy_1935);  _to_copy_1935 = None
        view_3438 = torch.ops.aten.view.default(mul_453, [262144, 512]);  mul_453 = None
        mm_673 = torch.ops.aten.mm.default(view_3438, t_722);  view_3438 = t_722 = None
        view_3439 = torch.ops.aten.view.default(mm_673, [1, 512, 512, 256]);  mm_673 = None
        add_363 = torch.ops.aten.add.Tensor(add_362, view_3439);  add_362 = view_3439 = None
        _to_copy_1936 = torch.ops.aten._to_copy.default(add_359, dtype = torch.float32)
        native_layer_norm_default_399 = torch.ops.aten.native_layer_norm.default(_to_copy_1936, [384], arg1150_1, arg1151_1, 1e-05);  _to_copy_1936 = arg1150_1 = arg1151_1 = None
        getitem_3327 = native_layer_norm_default_399[0]
        _to_copy_1937 = torch.ops.aten._to_copy.default(add_355, dtype = torch.float32);  add_355 = None
        native_layer_norm_default_400 = torch.ops.aten.native_layer_norm.default(_to_copy_1937, [256], arg1152_1, arg1153_1, 1e-05);  _to_copy_1937 = arg1152_1 = arg1153_1 = None
        getitem_3330 = native_layer_norm_default_400[0]
        _to_copy_1938 = torch.ops.aten._to_copy.default(arg1154_1, dtype = torch.bfloat16);  arg1154_1 = None
        _to_copy_1939 = torch.ops.aten._to_copy.default(getitem_3330, dtype = torch.bfloat16);  getitem_3330 = None
        t_723 = torch.ops.aten.t.default(_to_copy_1938);  _to_copy_1938 = None
        view_3440 = torch.ops.aten.view.default(_to_copy_1939, [262144, 256]);  _to_copy_1939 = None
        mm_674 = torch.ops.aten.mm.default(view_3440, t_723);  view_3440 = t_723 = None
        view_3441 = torch.ops.aten.view.default(mm_674, [1, 512, 512, 16]);  mm_674 = None
        permute_1822 = torch.ops.aten.permute.default(view_3441, [0, 3, 1, 2]);  view_3441 = None
        view_3442 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_213 = torch.ops.aten.bitwise_not.default(view_3442);  view_3442 = None
        masked_fill_213 = torch.ops.aten.masked_fill.Scalar(permute_1822, bitwise_not_213, -10000);  permute_1822 = bitwise_not_213 = None
        _to_copy_1940 = torch.ops.aten._to_copy.default(getitem_3327, dtype = torch.bfloat16);  getitem_3327 = None
        _to_copy_1941 = torch.ops.aten._to_copy.default(arg1156_1, dtype = torch.bfloat16);  arg1156_1 = None
        unsqueeze_1101 = torch.ops.aten.unsqueeze.default(_to_copy_1940, 3);  _to_copy_1940 = None
        unsqueeze_1102 = torch.ops.aten.unsqueeze.default(unsqueeze_1101, 4);  unsqueeze_1101 = None
        unsqueeze_1103 = torch.ops.aten.unsqueeze.default(unsqueeze_1102, 5);  unsqueeze_1102 = None
        permute_1823 = torch.ops.aten.permute.default(unsqueeze_1103, [3, 0, 4, 1, 5, 2]);  unsqueeze_1103 = None
        unsqueeze_1104 = torch.ops.aten.unsqueeze.default(_to_copy_1941, 4);  _to_copy_1941 = None
        unsqueeze_1105 = torch.ops.aten.unsqueeze.default(unsqueeze_1104, 5);  unsqueeze_1104 = None
        permute_1824 = torch.ops.aten.permute.default(unsqueeze_1105, [1, 4, 2, 5, 3, 0]);  unsqueeze_1105 = None
        permute_1825 = torch.ops.aten.permute.default(permute_1823, [3, 5, 0, 1, 2, 4]);  permute_1823 = None
        view_3443 = torch.ops.aten.view.default(permute_1825, [1, 512, 384]);  permute_1825 = None
        permute_1826 = torch.ops.aten.permute.default(permute_1824, [5, 0, 1, 2, 4, 3]);  permute_1824 = None
        view_3444 = torch.ops.aten.view.default(permute_1826, [1, 384, 1536]);  permute_1826 = None
        bmm_271 = torch.ops.aten.bmm.default(view_3443, view_3444);  view_3443 = view_3444 = None
        view_3445 = torch.ops.aten.view.default(bmm_271, [512, 1, 4, 1, 16, 24]);  bmm_271 = None
        permute_1827 = torch.ops.aten.permute.default(view_3445, [2, 3, 4, 0, 5, 1]);  view_3445 = None
        view_3446 = torch.ops.aten.view.default(permute_1827, [4, 1, 16, 512, 24]);  permute_1827 = None
        unbind_int_156 = torch.ops.aten.unbind.int(view_3446);  view_3446 = None
        getitem_3333 = unbind_int_156[0]
        getitem_3334 = unbind_int_156[1]
        getitem_3335 = unbind_int_156[2]
        getitem_3336 = unbind_int_156[3];  unbind_int_156 = None
        view_3447 = torch.ops.aten.view.default(arg1155_1, [1, 16, 1, 24]);  arg1155_1 = None
        add_364 = torch.ops.aten.add.Tensor(getitem_3333, view_3447);  getitem_3333 = view_3447 = None
        _to_copy_1942 = torch.ops.aten._to_copy.default(add_364, dtype = torch.bfloat16);  add_364 = None
        expand_221 = torch.ops.aten.expand.default(masked_fill_213, [1, 16, 512, 512]);  masked_fill_213 = None
        _scaled_dot_product_efficient_attention_default_128 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1942, getitem_3334, getitem_3335, expand_221, False);  _to_copy_1942 = getitem_3334 = getitem_3335 = expand_221 = None
        getitem_3337 = _scaled_dot_product_efficient_attention_default_128[0]
        add_365 = torch.ops.aten.add.Tensor(getitem_3336, 1);  getitem_3336 = None
        sigmoid_275 = torch.ops.aten.sigmoid.default(add_365);  add_365 = None
        mul_454 = torch.ops.aten.mul.Tensor(getitem_3337, sigmoid_275);  getitem_3337 = sigmoid_275 = None
        _to_copy_1943 = torch.ops.aten._to_copy.default(arg1157_1, dtype = torch.bfloat16);  arg1157_1 = None
        unsqueeze_1106 = torch.ops.aten.unsqueeze.default(mul_454, 4);  mul_454 = None
        permute_1828 = torch.ops.aten.permute.default(unsqueeze_1106, [0, 2, 4, 3, 1]);  unsqueeze_1106 = None
        unsqueeze_1107 = torch.ops.aten.unsqueeze.default(_to_copy_1943, 3);  _to_copy_1943 = None
        unsqueeze_1108 = torch.ops.aten.unsqueeze.default(unsqueeze_1107, 4);  unsqueeze_1107 = None
        permute_1829 = torch.ops.aten.permute.default(unsqueeze_1108, [3, 4, 2, 1, 0]);  unsqueeze_1108 = None
        permute_1830 = torch.ops.aten.permute.default(permute_1828, [1, 3, 4, 0, 2]);  permute_1828 = None
        clone_288 = torch.ops.aten.clone.default(permute_1830, memory_format = torch.contiguous_format);  permute_1830 = None
        _unsafe_view_242 = torch.ops.aten._unsafe_view.default(clone_288, [1, 512, 384]);  clone_288 = None
        permute_1831 = torch.ops.aten.permute.default(permute_1829, [3, 4, 0, 2, 1]);  permute_1829 = None
        clone_289 = torch.ops.aten.clone.default(permute_1831, memory_format = torch.contiguous_format);  permute_1831 = None
        _unsafe_view_243 = torch.ops.aten._unsafe_view.default(clone_289, [1, 384, 384]);  clone_289 = None
        bmm_272 = torch.ops.aten.bmm.default(_unsafe_view_242, _unsafe_view_243);  _unsafe_view_242 = _unsafe_view_243 = None
        view_3448 = torch.ops.aten.view.default(bmm_272, [512, 1, 1, 1, 384]);  bmm_272 = None
        permute_1832 = torch.ops.aten.permute.default(view_3448, [3, 0, 4, 1, 2]);  view_3448 = None
        view_3449 = torch.ops.aten.view.default(permute_1832, [1, 512, 384]);  permute_1832 = None
        unsqueeze_1109 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_455 = torch.ops.aten.mul.Tensor(view_3449, unsqueeze_1109);  view_3449 = unsqueeze_1109 = None
        add_366 = torch.ops.aten.add.Tensor(add_359, mul_455);  mul_455 = None
        split_tensor_361 = torch.ops.aten.split.Tensor(add_359, 512, dim = -2);  add_359 = None
        getitem_3341 = split_tensor_361[0];  split_tensor_361 = None
        _to_copy_1944 = torch.ops.aten._to_copy.default(getitem_3341, dtype = torch.float32);  getitem_3341 = None
        native_layer_norm_default_401 = torch.ops.aten.native_layer_norm.default(_to_copy_1944, [384], arg1146_1, arg1147_1, 1e-05);  _to_copy_1944 = arg1146_1 = arg1147_1 = None
        getitem_3342 = native_layer_norm_default_401[0]
        _to_copy_1945 = torch.ops.aten._to_copy.default(arg1148_1, dtype = torch.bfloat16);  arg1148_1 = None
        _to_copy_1946 = torch.ops.aten._to_copy.default(getitem_3342, dtype = torch.bfloat16);  getitem_3342 = None
        t_724 = torch.ops.aten.t.default(_to_copy_1945);  _to_copy_1945 = None
        view_3450 = torch.ops.aten.view.default(_to_copy_1946, [512, 384]);  _to_copy_1946 = None
        mm_675 = torch.ops.aten.mm.default(view_3450, t_724);  view_3450 = t_724 = None
        view_3451 = torch.ops.aten.view.default(mm_675, [1, 512, 1536]);  mm_675 = None
        split_tensor_362 = torch.ops.aten.split.Tensor(view_3451, 768, dim = -1);  view_3451 = None
        getitem_3345 = split_tensor_362[0]
        getitem_3346 = split_tensor_362[1];  split_tensor_362 = None
        silu_95 = torch.ops.aten.silu.default(getitem_3345);  getitem_3345 = None
        mul_456 = torch.ops.aten.mul.Tensor(silu_95, getitem_3346);  silu_95 = getitem_3346 = None
        _to_copy_1947 = torch.ops.aten._to_copy.default(arg1149_1, dtype = torch.bfloat16);  arg1149_1 = None
        t_725 = torch.ops.aten.t.default(_to_copy_1947);  _to_copy_1947 = None
        view_3453 = torch.ops.aten.view.default(mul_456, [512, 768]);  mul_456 = None
        mm_676 = torch.ops.aten.mm.default(view_3453, t_725);  view_3453 = t_725 = None
        view_3454 = torch.ops.aten.view.default(mm_676, [1, 512, 384]);  mm_676 = None
        add_367 = torch.ops.aten.add.Tensor(add_366, view_3454);  add_366 = view_3454 = None
        _to_copy_1948 = torch.ops.aten._to_copy.default(add_363, dtype = torch.float32)
        native_layer_norm_default_402 = torch.ops.aten.native_layer_norm.default(_to_copy_1948, [256], arg1162_1, arg1163_1, 1e-05);  _to_copy_1948 = arg1162_1 = arg1163_1 = None
        getitem_3347 = native_layer_norm_default_402[0]
        split_with_sizes_default_90 = torch.ops.aten.split_with_sizes.default(arg1165_1, [512, 512]);  arg1165_1 = None
        getitem_3350 = split_with_sizes_default_90[0]
        getitem_3351 = split_with_sizes_default_90[1];  split_with_sizes_default_90 = None
        split_with_sizes_default_91 = torch.ops.aten.split_with_sizes.default(arg1166_1, [512, 512, 256]);  arg1166_1 = None
        getitem_3352 = split_with_sizes_default_91[0]
        getitem_3353 = split_with_sizes_default_91[1]
        getitem_3354 = split_with_sizes_default_91[2];  split_with_sizes_default_91 = None
        _to_copy_1949 = torch.ops.aten._to_copy.default(getitem_3350, dtype = torch.bfloat16);  getitem_3350 = None
        _to_copy_1950 = torch.ops.aten._to_copy.default(getitem_3347, dtype = torch.bfloat16)
        t_726 = torch.ops.aten.t.default(_to_copy_1949);  _to_copy_1949 = None
        view_3455 = torch.ops.aten.view.default(_to_copy_1950, [262144, 256]);  _to_copy_1950 = None
        mm_677 = torch.ops.aten.mm.default(view_3455, t_726);  view_3455 = t_726 = None
        view_3456 = torch.ops.aten.view.default(mm_677, [1, 512, 512, 512]);  mm_677 = None
        _to_copy_1951 = torch.ops.aten._to_copy.default(getitem_3352, dtype = torch.bfloat16);  getitem_3352 = None
        _to_copy_1952 = torch.ops.aten._to_copy.default(getitem_3347, dtype = torch.bfloat16)
        t_727 = torch.ops.aten.t.default(_to_copy_1951);  _to_copy_1951 = None
        view_3457 = torch.ops.aten.view.default(_to_copy_1952, [262144, 256]);  _to_copy_1952 = None
        mm_678 = torch.ops.aten.mm.default(view_3457, t_727);  view_3457 = t_727 = None
        view_3458 = torch.ops.aten.view.default(mm_678, [1, 512, 512, 512]);  mm_678 = None
        sigmoid_276 = torch.ops.aten.sigmoid.default(view_3458);  view_3458 = None
        mul_457 = torch.ops.aten.mul.Tensor(view_3456, sigmoid_276);  view_3456 = sigmoid_276 = None
        unsqueeze_1110 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_214 = torch.ops.aten.bitwise_not.default(unsqueeze_1110);  unsqueeze_1110 = None
        masked_fill_214 = torch.ops.aten.masked_fill.Scalar(mul_457, bitwise_not_214, 0);  mul_457 = bitwise_not_214 = None
        split_tensor_363 = torch.ops.aten.split.Tensor(masked_fill_214, 256, dim = -1)
        getitem_3357 = split_tensor_363[0]
        unsqueeze_1113 = torch.ops.aten.unsqueeze.default(getitem_3357, 4);  getitem_3357 = None
        permute_1837 = torch.ops.aten.permute.default(unsqueeze_1113, [0, 1, 4, 3, 2]);  unsqueeze_1113 = None
        permute_1838 = torch.ops.aten.permute.default(permute_1837, [3, 1, 4, 0, 2]);  permute_1837 = None
        view_3461 = torch.ops.aten.view.default(permute_1838, [256, 512, 512]);  permute_1838 = None
        split_tensor_364 = torch.ops.aten.split.Tensor(masked_fill_214, 256, dim = -1);  masked_fill_214 = None
        getitem_3360 = split_tensor_364[1];  split_tensor_364 = None
        unsqueeze_1114 = torch.ops.aten.unsqueeze.default(getitem_3360, 4);  getitem_3360 = None
        permute_1839 = torch.ops.aten.permute.default(unsqueeze_1114, [0, 4, 1, 3, 2]);  unsqueeze_1114 = None
        permute_1840 = torch.ops.aten.permute.default(permute_1839, [3, 4, 0, 2, 1]);  permute_1839 = None
        view_3462 = torch.ops.aten.view.default(permute_1840, [256, 512, 512]);  permute_1840 = None
        bmm_273 = torch.ops.aten.bmm.default(view_3461, view_3462);  view_3461 = view_3462 = None
        view_3463 = torch.ops.aten.view.default(bmm_273, [256, 512, 1, 1, 512]);  bmm_273 = None
        permute_1841 = torch.ops.aten.permute.default(view_3463, [3, 1, 4, 0, 2]);  view_3463 = None
        view_3464 = torch.ops.aten.view.default(permute_1841, [1, 512, 512, 256]);  permute_1841 = None
        _to_copy_1953 = torch.ops.aten._to_copy.default(getitem_3351, dtype = torch.bfloat16);  getitem_3351 = None
        _to_copy_1954 = torch.ops.aten._to_copy.default(getitem_3347, dtype = torch.bfloat16)
        t_728 = torch.ops.aten.t.default(_to_copy_1953);  _to_copy_1953 = None
        view_3465 = torch.ops.aten.view.default(_to_copy_1954, [262144, 256]);  _to_copy_1954 = None
        mm_679 = torch.ops.aten.mm.default(view_3465, t_728);  view_3465 = t_728 = None
        view_3466 = torch.ops.aten.view.default(mm_679, [1, 512, 512, 512]);  mm_679 = None
        _to_copy_1955 = torch.ops.aten._to_copy.default(getitem_3353, dtype = torch.bfloat16);  getitem_3353 = None
        _to_copy_1956 = torch.ops.aten._to_copy.default(getitem_3347, dtype = torch.bfloat16)
        t_729 = torch.ops.aten.t.default(_to_copy_1955);  _to_copy_1955 = None
        view_3467 = torch.ops.aten.view.default(_to_copy_1956, [262144, 256]);  _to_copy_1956 = None
        mm_680 = torch.ops.aten.mm.default(view_3467, t_729);  view_3467 = t_729 = None
        view_3468 = torch.ops.aten.view.default(mm_680, [1, 512, 512, 512]);  mm_680 = None
        sigmoid_277 = torch.ops.aten.sigmoid.default(view_3468);  view_3468 = None
        mul_458 = torch.ops.aten.mul.Tensor(view_3466, sigmoid_277);  view_3466 = sigmoid_277 = None
        view_3469 = torch.ops.aten.view.default(mul_458, [262144, 512]);  mul_458 = None
        view_3470 = torch.ops.aten.view.default(view_3469, [1, 512, 512, 512]);  view_3469 = None
        transpose_90 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_1115 = torch.ops.aten.unsqueeze.default(transpose_90, 3);  transpose_90 = None
        clone_290 = torch.ops.aten.clone.default(unsqueeze_1115, memory_format = torch.contiguous_format);  unsqueeze_1115 = None
        bitwise_not_215 = torch.ops.aten.bitwise_not.default(clone_290);  clone_290 = None
        masked_fill_215 = torch.ops.aten.masked_fill.Scalar(view_3470, bitwise_not_215, 0);  view_3470 = bitwise_not_215 = None
        view_3471 = torch.ops.aten.view.default(masked_fill_215, [262144, 512]);  masked_fill_215 = None
        view_3475 = torch.ops.aten.view.default(view_3471, [1, 512, 512, 512])
        split_tensor_365 = torch.ops.aten.split.Tensor(view_3475, 256, dim = -1);  view_3475 = None
        getitem_3363 = split_tensor_365[0]
        unsqueeze_1118 = torch.ops.aten.unsqueeze.default(getitem_3363, 4);  getitem_3363 = None
        permute_1846 = torch.ops.aten.permute.default(unsqueeze_1118, [0, 2, 4, 3, 1]);  unsqueeze_1118 = None
        permute_1847 = torch.ops.aten.permute.default(permute_1846, [3, 1, 4, 0, 2]);  permute_1846 = None
        view_3476 = torch.ops.aten.view.default(permute_1847, [256, 512, 512]);  permute_1847 = None
        view_3477 = torch.ops.aten.view.default(view_3471, [1, 512, 512, 512]);  view_3471 = None
        split_tensor_366 = torch.ops.aten.split.Tensor(view_3477, 256, dim = -1);  view_3477 = None
        getitem_3366 = split_tensor_366[1];  split_tensor_366 = None
        unsqueeze_1119 = torch.ops.aten.unsqueeze.default(getitem_3366, 4);  getitem_3366 = None
        permute_1848 = torch.ops.aten.permute.default(unsqueeze_1119, [0, 4, 2, 3, 1]);  unsqueeze_1119 = None
        permute_1849 = torch.ops.aten.permute.default(permute_1848, [3, 4, 0, 2, 1]);  permute_1848 = None
        view_3478 = torch.ops.aten.view.default(permute_1849, [256, 512, 512]);  permute_1849 = None
        bmm_274 = torch.ops.aten.bmm.default(view_3476, view_3478);  view_3476 = view_3478 = None
        view_3479 = torch.ops.aten.view.default(bmm_274, [256, 512, 1, 1, 512]);  bmm_274 = None
        permute_1850 = torch.ops.aten.permute.default(view_3479, [3, 1, 4, 0, 2]);  view_3479 = None
        view_3480 = torch.ops.aten.view.default(permute_1850, [1, 512, 512, 256]);  permute_1850 = None
        _to_copy_1957 = torch.ops.aten._to_copy.default(view_3464, dtype = torch.float32);  view_3464 = None
        native_layer_norm_default_403 = torch.ops.aten.native_layer_norm.default(_to_copy_1957, [256], None, None, 1e-05);  _to_copy_1957 = None
        getitem_3367 = native_layer_norm_default_403[0]
        _to_copy_1958 = torch.ops.aten._to_copy.default(view_3480, dtype = torch.float32);  view_3480 = None
        native_layer_norm_default_404 = torch.ops.aten.native_layer_norm.default(_to_copy_1958, [256], None, None, 1e-05);  _to_copy_1958 = None
        getitem_3370 = native_layer_norm_default_404[0]
        add_368 = torch.ops.aten.add.Tensor(getitem_3367, getitem_3370);  getitem_3367 = getitem_3370 = None
        _to_copy_1959 = torch.ops.aten._to_copy.default(arg1164_1, dtype = torch.bfloat16);  arg1164_1 = None
        _to_copy_1960 = torch.ops.aten._to_copy.default(add_368, dtype = torch.bfloat16);  add_368 = None
        t_730 = torch.ops.aten.t.default(_to_copy_1959);  _to_copy_1959 = None
        view_3481 = torch.ops.aten.view.default(_to_copy_1960, [262144, 256]);  _to_copy_1960 = None
        mm_681 = torch.ops.aten.mm.default(view_3481, t_730);  view_3481 = t_730 = None
        view_3482 = torch.ops.aten.view.default(mm_681, [1, 512, 512, 256]);  mm_681 = None
        _to_copy_1961 = torch.ops.aten._to_copy.default(getitem_3354, dtype = torch.bfloat16);  getitem_3354 = None
        _to_copy_1962 = torch.ops.aten._to_copy.default(getitem_3347, dtype = torch.bfloat16);  getitem_3347 = None
        t_731 = torch.ops.aten.t.default(_to_copy_1961);  _to_copy_1961 = None
        view_3483 = torch.ops.aten.view.default(_to_copy_1962, [262144, 256]);  _to_copy_1962 = None
        mm_682 = torch.ops.aten.mm.default(view_3483, t_731);  view_3483 = t_731 = None
        view_3484 = torch.ops.aten.view.default(mm_682, [1, 512, 512, 256]);  mm_682 = None
        sigmoid_278 = torch.ops.aten.sigmoid.default(view_3484);  view_3484 = None
        mul_459 = torch.ops.aten.mul.Tensor(view_3482, sigmoid_278);  view_3482 = sigmoid_278 = None
        add_369 = torch.ops.aten.add.Tensor(add_363, mul_459);  mul_459 = None
        _to_copy_1963 = torch.ops.aten._to_copy.default(add_363, dtype = torch.float32)
        native_layer_norm_default_405 = torch.ops.aten.native_layer_norm.default(_to_copy_1963, [256], None, None, 1e-05);  _to_copy_1963 = None
        getitem_3373 = native_layer_norm_default_405[0]
        _to_copy_1964 = torch.ops.aten._to_copy.default(arg1168_1, dtype = torch.bfloat16);  arg1168_1 = None
        _to_copy_1965 = torch.ops.aten._to_copy.default(getitem_3373, dtype = torch.bfloat16)
        t_732 = torch.ops.aten.t.default(_to_copy_1964);  _to_copy_1964 = None
        view_3485 = torch.ops.aten.view.default(_to_copy_1965, [262144, 256]);  _to_copy_1965 = None
        mm_683 = torch.ops.aten.mm.default(view_3485, t_732);  view_3485 = t_732 = None
        view_3486 = torch.ops.aten.view.default(mm_683, [1, 512, 512, 8]);  mm_683 = None
        view_3487 = torch.ops.aten.view.default(view_3486, [1, 512, 512, 2, 4]);  view_3486 = None
        permute_1851 = torch.ops.aten.permute.default(view_3487, [0, 3, 4, 1, 2]);  view_3487 = None
        view_3488 = torch.ops.aten.view.default(permute_1851, [1, 2, 4, 1, 512, 512]);  permute_1851 = None
        view_3489 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_216 = torch.ops.aten.bitwise_not.default(view_3489);  view_3489 = None
        masked_fill_216 = torch.ops.aten.masked_fill.Scalar(view_3488, bitwise_not_216, -10000);  view_3488 = bitwise_not_216 = None
        view_3490 = torch.ops.aten.view.default(masked_fill_216, [1, 2, 4, 512, 512]);  masked_fill_216 = None
        permute_1852 = torch.ops.aten.permute.default(view_3490, [1, 0, 2, 3, 4]);  view_3490 = None
        view_3491 = torch.ops.aten.view.default(permute_1852, [2, 4, 1, 512, 512]);  permute_1852 = None
        _to_copy_1966 = torch.ops.aten._to_copy.default(arg1169_1, dtype = torch.bfloat16);  arg1169_1 = None
        _to_copy_1967 = torch.ops.aten._to_copy.default(getitem_3373, dtype = torch.bfloat16)
        t_733 = torch.ops.aten.t.default(_to_copy_1966);  _to_copy_1966 = None
        view_3492 = torch.ops.aten.view.default(_to_copy_1967, [262144, 256]);  _to_copy_1967 = None
        mm_684 = torch.ops.aten.mm.default(view_3492, t_733);  view_3492 = t_733 = None
        view_3493 = torch.ops.aten.view.default(mm_684, [1, 512, 512, 1024]);  mm_684 = None
        select_91 = torch.ops.aten.select.int(view_3491, 0, 0)
        view_3494 = torch.ops.aten.view.default(view_3493, [1, 512, 512, 4, 4, 64]);  view_3493 = None
        permute_1853 = torch.ops.aten.permute.default(view_3494, [4, 0, 3, 1, 2, 5]);  view_3494 = None
        view_3495 = torch.ops.aten.view.default(permute_1853, [4, 4, 512, 512, 64]);  permute_1853 = None
        unbind_int_157 = torch.ops.aten.unbind.int(view_3495);  view_3495 = None
        getitem_3376 = unbind_int_157[0]
        getitem_3377 = unbind_int_157[1]
        getitem_3378 = unbind_int_157[2]
        getitem_3379 = unbind_int_157[3];  unbind_int_157 = None
        expand_222 = torch.ops.aten.expand.default(select_91, [4, 512, 512, 512]);  select_91 = None
        _scaled_dot_product_efficient_attention_default_129 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3376, getitem_3377, getitem_3378, expand_222, False);  getitem_3376 = getitem_3377 = getitem_3378 = expand_222 = None
        getitem_3380 = _scaled_dot_product_efficient_attention_default_129[0]
        sigmoid_279 = torch.ops.aten.sigmoid.default(getitem_3379);  getitem_3379 = None
        mul_460 = torch.ops.aten.mul.Tensor(getitem_3380, sigmoid_279);  getitem_3380 = sigmoid_279 = None
        view_3496 = torch.ops.aten.view.default(mul_460, [1, 4, 512, 512, 64]);  mul_460 = None
        permute_1854 = torch.ops.aten.permute.default(view_3496, [0, 2, 3, 1, 4]);  view_3496 = None
        clone_291 = torch.ops.aten.clone.default(permute_1854, memory_format = torch.contiguous_format);  permute_1854 = None
        _unsafe_view_244 = torch.ops.aten._unsafe_view.default(clone_291, [1, 512, 512, 256]);  clone_291 = None
        transpose_91 = torch.ops.aten.transpose.int(getitem_3373, 1, 2);  getitem_3373 = None
        _to_copy_1968 = torch.ops.aten._to_copy.default(arg1170_1, dtype = torch.bfloat16);  arg1170_1 = None
        _to_copy_1969 = torch.ops.aten._to_copy.default(transpose_91, dtype = torch.bfloat16);  transpose_91 = None
        t_734 = torch.ops.aten.t.default(_to_copy_1968);  _to_copy_1968 = None
        expand_223 = torch.ops.aten.expand.default(_to_copy_1969, [1, 512, 512, 256]);  _to_copy_1969 = None
        view_3497 = torch.ops.aten.view.default(expand_223, [512, 512, 256]);  expand_223 = None
        expand_224 = torch.ops.aten.expand.default(t_734, [1, 512, 256, 1024]);  t_734 = None
        view_3498 = torch.ops.aten.view.default(expand_224, [512, 256, 1024]);  expand_224 = None
        bmm_275 = torch.ops.aten.bmm.default(view_3497, view_3498);  view_3497 = view_3498 = None
        view_3499 = torch.ops.aten.view.default(bmm_275, [1, 512, 512, 1024]);  bmm_275 = None
        select_92 = torch.ops.aten.select.int(view_3491, 0, 1);  view_3491 = None
        view_3500 = torch.ops.aten.view.default(view_3499, [1, 512, 512, 4, 4, 64]);  view_3499 = None
        permute_1855 = torch.ops.aten.permute.default(view_3500, [4, 0, 3, 1, 2, 5]);  view_3500 = None
        view_3501 = torch.ops.aten.view.default(permute_1855, [4, 4, 512, 512, 64]);  permute_1855 = None
        unbind_int_158 = torch.ops.aten.unbind.int(view_3501);  view_3501 = None
        getitem_3384 = unbind_int_158[0]
        getitem_3385 = unbind_int_158[1]
        getitem_3386 = unbind_int_158[2]
        getitem_3387 = unbind_int_158[3];  unbind_int_158 = None
        expand_225 = torch.ops.aten.expand.default(select_92, [4, 512, 512, 512]);  select_92 = None
        _scaled_dot_product_efficient_attention_default_130 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3384, getitem_3385, getitem_3386, expand_225, False);  getitem_3384 = getitem_3385 = getitem_3386 = expand_225 = None
        getitem_3388 = _scaled_dot_product_efficient_attention_default_130[0]
        sigmoid_280 = torch.ops.aten.sigmoid.default(getitem_3387);  getitem_3387 = None
        mul_461 = torch.ops.aten.mul.Tensor(getitem_3388, sigmoid_280);  getitem_3388 = sigmoid_280 = None
        view_3502 = torch.ops.aten.view.default(mul_461, [1, 4, 512, 512, 64]);  mul_461 = None
        permute_1856 = torch.ops.aten.permute.default(view_3502, [0, 2, 3, 1, 4]);  view_3502 = None
        clone_292 = torch.ops.aten.clone.default(permute_1856, memory_format = torch.contiguous_format);  permute_1856 = None
        _unsafe_view_245 = torch.ops.aten._unsafe_view.default(clone_292, [1, 512, 512, 256]);  clone_292 = None
        cat_51 = torch.ops.aten.cat.default([_unsafe_view_244, _unsafe_view_245], dim = -1);  _unsafe_view_244 = _unsafe_view_245 = None
        slice_244 = torch.ops.aten.slice.Tensor(arg1167_1, dim = 0, start = 0, end = 9223372036854775807);  arg1167_1 = None
        unsqueeze_1120 = torch.ops.aten.unsqueeze.default(slice_244, 1);  slice_244 = None
        mul_462 = torch.ops.aten.mul.Tensor(arg1171_1, unsqueeze_1120);  arg1171_1 = unsqueeze_1120 = None
        _to_copy_1970 = torch.ops.aten._to_copy.default(mul_462, dtype = torch.bfloat16);  mul_462 = None
        t_735 = torch.ops.aten.t.default(_to_copy_1970);  _to_copy_1970 = None
        view_3503 = torch.ops.aten.view.default(cat_51, [262144, 512]);  cat_51 = None
        mm_685 = torch.ops.aten.mm.default(view_3503, t_735);  view_3503 = t_735 = None
        view_3504 = torch.ops.aten.view.default(mm_685, [1, 512, 512, 256]);  mm_685 = None
        add_370 = torch.ops.aten.add.Tensor(add_369, view_3504);  add_369 = view_3504 = None
        split_tensor_367 = torch.ops.aten.split.Tensor(add_363, 512, dim = -2)
        getitem_3392 = split_tensor_367[0];  split_tensor_367 = None
        _to_copy_1971 = torch.ops.aten._to_copy.default(getitem_3392, dtype = torch.float32);  getitem_3392 = None
        native_layer_norm_default_406 = torch.ops.aten.native_layer_norm.default(_to_copy_1971, [256], arg1158_1, arg1159_1, 1e-05);  _to_copy_1971 = arg1158_1 = arg1159_1 = None
        getitem_3393 = native_layer_norm_default_406[0]
        _to_copy_1972 = torch.ops.aten._to_copy.default(arg1160_1, dtype = torch.bfloat16);  arg1160_1 = None
        _to_copy_1973 = torch.ops.aten._to_copy.default(getitem_3393, dtype = torch.bfloat16);  getitem_3393 = None
        t_736 = torch.ops.aten.t.default(_to_copy_1972);  _to_copy_1972 = None
        view_3505 = torch.ops.aten.view.default(_to_copy_1973, [262144, 256]);  _to_copy_1973 = None
        mm_686 = torch.ops.aten.mm.default(view_3505, t_736);  view_3505 = t_736 = None
        view_3506 = torch.ops.aten.view.default(mm_686, [1, 512, 512, 1024]);  mm_686 = None
        split_tensor_368 = torch.ops.aten.split.Tensor(view_3506, 512, dim = -1);  view_3506 = None
        getitem_3396 = split_tensor_368[0]
        getitem_3397 = split_tensor_368[1];  split_tensor_368 = None
        silu_96 = torch.ops.aten.silu.default(getitem_3396);  getitem_3396 = None
        mul_463 = torch.ops.aten.mul.Tensor(silu_96, getitem_3397);  silu_96 = getitem_3397 = None
        _to_copy_1974 = torch.ops.aten._to_copy.default(arg1161_1, dtype = torch.bfloat16);  arg1161_1 = None
        t_737 = torch.ops.aten.t.default(_to_copy_1974);  _to_copy_1974 = None
        view_3508 = torch.ops.aten.view.default(mul_463, [262144, 512]);  mul_463 = None
        mm_687 = torch.ops.aten.mm.default(view_3508, t_737);  view_3508 = t_737 = None
        view_3509 = torch.ops.aten.view.default(mm_687, [1, 512, 512, 256]);  mm_687 = None
        add_371 = torch.ops.aten.add.Tensor(add_370, view_3509);  add_370 = view_3509 = None
        _to_copy_1975 = torch.ops.aten._to_copy.default(add_367, dtype = torch.float32)
        native_layer_norm_default_407 = torch.ops.aten.native_layer_norm.default(_to_copy_1975, [384], arg1176_1, arg1177_1, 1e-05);  _to_copy_1975 = arg1176_1 = arg1177_1 = None
        getitem_3398 = native_layer_norm_default_407[0]
        _to_copy_1976 = torch.ops.aten._to_copy.default(add_363, dtype = torch.float32);  add_363 = None
        native_layer_norm_default_408 = torch.ops.aten.native_layer_norm.default(_to_copy_1976, [256], arg1178_1, arg1179_1, 1e-05);  _to_copy_1976 = arg1178_1 = arg1179_1 = None
        getitem_3401 = native_layer_norm_default_408[0]
        _to_copy_1977 = torch.ops.aten._to_copy.default(arg1180_1, dtype = torch.bfloat16);  arg1180_1 = None
        _to_copy_1978 = torch.ops.aten._to_copy.default(getitem_3401, dtype = torch.bfloat16);  getitem_3401 = None
        t_738 = torch.ops.aten.t.default(_to_copy_1977);  _to_copy_1977 = None
        view_3510 = torch.ops.aten.view.default(_to_copy_1978, [262144, 256]);  _to_copy_1978 = None
        mm_688 = torch.ops.aten.mm.default(view_3510, t_738);  view_3510 = t_738 = None
        view_3511 = torch.ops.aten.view.default(mm_688, [1, 512, 512, 16]);  mm_688 = None
        permute_1857 = torch.ops.aten.permute.default(view_3511, [0, 3, 1, 2]);  view_3511 = None
        view_3512 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_217 = torch.ops.aten.bitwise_not.default(view_3512);  view_3512 = None
        masked_fill_217 = torch.ops.aten.masked_fill.Scalar(permute_1857, bitwise_not_217, -10000);  permute_1857 = bitwise_not_217 = None
        _to_copy_1979 = torch.ops.aten._to_copy.default(getitem_3398, dtype = torch.bfloat16);  getitem_3398 = None
        _to_copy_1980 = torch.ops.aten._to_copy.default(arg1182_1, dtype = torch.bfloat16);  arg1182_1 = None
        unsqueeze_1121 = torch.ops.aten.unsqueeze.default(_to_copy_1979, 3);  _to_copy_1979 = None
        unsqueeze_1122 = torch.ops.aten.unsqueeze.default(unsqueeze_1121, 4);  unsqueeze_1121 = None
        unsqueeze_1123 = torch.ops.aten.unsqueeze.default(unsqueeze_1122, 5);  unsqueeze_1122 = None
        permute_1858 = torch.ops.aten.permute.default(unsqueeze_1123, [3, 0, 4, 1, 5, 2]);  unsqueeze_1123 = None
        unsqueeze_1124 = torch.ops.aten.unsqueeze.default(_to_copy_1980, 4);  _to_copy_1980 = None
        unsqueeze_1125 = torch.ops.aten.unsqueeze.default(unsqueeze_1124, 5);  unsqueeze_1124 = None
        permute_1859 = torch.ops.aten.permute.default(unsqueeze_1125, [1, 4, 2, 5, 3, 0]);  unsqueeze_1125 = None
        permute_1860 = torch.ops.aten.permute.default(permute_1858, [3, 5, 0, 1, 2, 4]);  permute_1858 = None
        view_3513 = torch.ops.aten.view.default(permute_1860, [1, 512, 384]);  permute_1860 = None
        permute_1861 = torch.ops.aten.permute.default(permute_1859, [5, 0, 1, 2, 4, 3]);  permute_1859 = None
        view_3514 = torch.ops.aten.view.default(permute_1861, [1, 384, 1536]);  permute_1861 = None
        bmm_276 = torch.ops.aten.bmm.default(view_3513, view_3514);  view_3513 = view_3514 = None
        view_3515 = torch.ops.aten.view.default(bmm_276, [512, 1, 4, 1, 16, 24]);  bmm_276 = None
        permute_1862 = torch.ops.aten.permute.default(view_3515, [2, 3, 4, 0, 5, 1]);  view_3515 = None
        view_3516 = torch.ops.aten.view.default(permute_1862, [4, 1, 16, 512, 24]);  permute_1862 = None
        unbind_int_159 = torch.ops.aten.unbind.int(view_3516);  view_3516 = None
        getitem_3404 = unbind_int_159[0]
        getitem_3405 = unbind_int_159[1]
        getitem_3406 = unbind_int_159[2]
        getitem_3407 = unbind_int_159[3];  unbind_int_159 = None
        view_3517 = torch.ops.aten.view.default(arg1181_1, [1, 16, 1, 24]);  arg1181_1 = None
        add_372 = torch.ops.aten.add.Tensor(getitem_3404, view_3517);  getitem_3404 = view_3517 = None
        _to_copy_1981 = torch.ops.aten._to_copy.default(add_372, dtype = torch.bfloat16);  add_372 = None
        expand_226 = torch.ops.aten.expand.default(masked_fill_217, [1, 16, 512, 512]);  masked_fill_217 = None
        _scaled_dot_product_efficient_attention_default_131 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_1981, getitem_3405, getitem_3406, expand_226, False);  _to_copy_1981 = getitem_3405 = getitem_3406 = expand_226 = None
        getitem_3408 = _scaled_dot_product_efficient_attention_default_131[0]
        add_373 = torch.ops.aten.add.Tensor(getitem_3407, 1);  getitem_3407 = None
        sigmoid_281 = torch.ops.aten.sigmoid.default(add_373);  add_373 = None
        mul_464 = torch.ops.aten.mul.Tensor(getitem_3408, sigmoid_281);  getitem_3408 = sigmoid_281 = None
        _to_copy_1982 = torch.ops.aten._to_copy.default(arg1183_1, dtype = torch.bfloat16);  arg1183_1 = None
        unsqueeze_1126 = torch.ops.aten.unsqueeze.default(mul_464, 4);  mul_464 = None
        permute_1863 = torch.ops.aten.permute.default(unsqueeze_1126, [0, 2, 4, 3, 1]);  unsqueeze_1126 = None
        unsqueeze_1127 = torch.ops.aten.unsqueeze.default(_to_copy_1982, 3);  _to_copy_1982 = None
        unsqueeze_1128 = torch.ops.aten.unsqueeze.default(unsqueeze_1127, 4);  unsqueeze_1127 = None
        permute_1864 = torch.ops.aten.permute.default(unsqueeze_1128, [3, 4, 2, 1, 0]);  unsqueeze_1128 = None
        permute_1865 = torch.ops.aten.permute.default(permute_1863, [1, 3, 4, 0, 2]);  permute_1863 = None
        clone_293 = torch.ops.aten.clone.default(permute_1865, memory_format = torch.contiguous_format);  permute_1865 = None
        _unsafe_view_246 = torch.ops.aten._unsafe_view.default(clone_293, [1, 512, 384]);  clone_293 = None
        permute_1866 = torch.ops.aten.permute.default(permute_1864, [3, 4, 0, 2, 1]);  permute_1864 = None
        clone_294 = torch.ops.aten.clone.default(permute_1866, memory_format = torch.contiguous_format);  permute_1866 = None
        _unsafe_view_247 = torch.ops.aten._unsafe_view.default(clone_294, [1, 384, 384]);  clone_294 = None
        bmm_277 = torch.ops.aten.bmm.default(_unsafe_view_246, _unsafe_view_247);  _unsafe_view_246 = _unsafe_view_247 = None
        view_3518 = torch.ops.aten.view.default(bmm_277, [512, 1, 1, 1, 384]);  bmm_277 = None
        permute_1867 = torch.ops.aten.permute.default(view_3518, [3, 0, 4, 1, 2]);  view_3518 = None
        view_3519 = torch.ops.aten.view.default(permute_1867, [1, 512, 384]);  permute_1867 = None
        unsqueeze_1129 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_465 = torch.ops.aten.mul.Tensor(view_3519, unsqueeze_1129);  view_3519 = unsqueeze_1129 = None
        add_374 = torch.ops.aten.add.Tensor(add_367, mul_465);  mul_465 = None
        split_tensor_369 = torch.ops.aten.split.Tensor(add_367, 512, dim = -2);  add_367 = None
        getitem_3412 = split_tensor_369[0];  split_tensor_369 = None
        _to_copy_1983 = torch.ops.aten._to_copy.default(getitem_3412, dtype = torch.float32);  getitem_3412 = None
        native_layer_norm_default_409 = torch.ops.aten.native_layer_norm.default(_to_copy_1983, [384], arg1172_1, arg1173_1, 1e-05);  _to_copy_1983 = arg1172_1 = arg1173_1 = None
        getitem_3413 = native_layer_norm_default_409[0]
        _to_copy_1984 = torch.ops.aten._to_copy.default(arg1174_1, dtype = torch.bfloat16);  arg1174_1 = None
        _to_copy_1985 = torch.ops.aten._to_copy.default(getitem_3413, dtype = torch.bfloat16);  getitem_3413 = None
        t_739 = torch.ops.aten.t.default(_to_copy_1984);  _to_copy_1984 = None
        view_3520 = torch.ops.aten.view.default(_to_copy_1985, [512, 384]);  _to_copy_1985 = None
        mm_689 = torch.ops.aten.mm.default(view_3520, t_739);  view_3520 = t_739 = None
        view_3521 = torch.ops.aten.view.default(mm_689, [1, 512, 1536]);  mm_689 = None
        split_tensor_370 = torch.ops.aten.split.Tensor(view_3521, 768, dim = -1);  view_3521 = None
        getitem_3416 = split_tensor_370[0]
        getitem_3417 = split_tensor_370[1];  split_tensor_370 = None
        silu_97 = torch.ops.aten.silu.default(getitem_3416);  getitem_3416 = None
        mul_466 = torch.ops.aten.mul.Tensor(silu_97, getitem_3417);  silu_97 = getitem_3417 = None
        _to_copy_1986 = torch.ops.aten._to_copy.default(arg1175_1, dtype = torch.bfloat16);  arg1175_1 = None
        t_740 = torch.ops.aten.t.default(_to_copy_1986);  _to_copy_1986 = None
        view_3523 = torch.ops.aten.view.default(mul_466, [512, 768]);  mul_466 = None
        mm_690 = torch.ops.aten.mm.default(view_3523, t_740);  view_3523 = t_740 = None
        view_3524 = torch.ops.aten.view.default(mm_690, [1, 512, 384]);  mm_690 = None
        add_375 = torch.ops.aten.add.Tensor(add_374, view_3524);  add_374 = view_3524 = None
        _to_copy_1987 = torch.ops.aten._to_copy.default(add_371, dtype = torch.float32)
        native_layer_norm_default_410 = torch.ops.aten.native_layer_norm.default(_to_copy_1987, [256], arg1188_1, arg1189_1, 1e-05);  _to_copy_1987 = arg1188_1 = arg1189_1 = None
        getitem_3418 = native_layer_norm_default_410[0]
        split_with_sizes_default_92 = torch.ops.aten.split_with_sizes.default(arg1191_1, [512, 512]);  arg1191_1 = None
        getitem_3421 = split_with_sizes_default_92[0]
        getitem_3422 = split_with_sizes_default_92[1];  split_with_sizes_default_92 = None
        split_with_sizes_default_93 = torch.ops.aten.split_with_sizes.default(arg1192_1, [512, 512, 256]);  arg1192_1 = None
        getitem_3423 = split_with_sizes_default_93[0]
        getitem_3424 = split_with_sizes_default_93[1]
        getitem_3425 = split_with_sizes_default_93[2];  split_with_sizes_default_93 = None
        _to_copy_1988 = torch.ops.aten._to_copy.default(getitem_3421, dtype = torch.bfloat16);  getitem_3421 = None
        _to_copy_1989 = torch.ops.aten._to_copy.default(getitem_3418, dtype = torch.bfloat16)
        t_741 = torch.ops.aten.t.default(_to_copy_1988);  _to_copy_1988 = None
        view_3525 = torch.ops.aten.view.default(_to_copy_1989, [262144, 256]);  _to_copy_1989 = None
        mm_691 = torch.ops.aten.mm.default(view_3525, t_741);  view_3525 = t_741 = None
        view_3526 = torch.ops.aten.view.default(mm_691, [1, 512, 512, 512]);  mm_691 = None
        _to_copy_1990 = torch.ops.aten._to_copy.default(getitem_3423, dtype = torch.bfloat16);  getitem_3423 = None
        _to_copy_1991 = torch.ops.aten._to_copy.default(getitem_3418, dtype = torch.bfloat16)
        t_742 = torch.ops.aten.t.default(_to_copy_1990);  _to_copy_1990 = None
        view_3527 = torch.ops.aten.view.default(_to_copy_1991, [262144, 256]);  _to_copy_1991 = None
        mm_692 = torch.ops.aten.mm.default(view_3527, t_742);  view_3527 = t_742 = None
        view_3528 = torch.ops.aten.view.default(mm_692, [1, 512, 512, 512]);  mm_692 = None
        sigmoid_282 = torch.ops.aten.sigmoid.default(view_3528);  view_3528 = None
        mul_467 = torch.ops.aten.mul.Tensor(view_3526, sigmoid_282);  view_3526 = sigmoid_282 = None
        unsqueeze_1130 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_218 = torch.ops.aten.bitwise_not.default(unsqueeze_1130);  unsqueeze_1130 = None
        masked_fill_218 = torch.ops.aten.masked_fill.Scalar(mul_467, bitwise_not_218, 0);  mul_467 = bitwise_not_218 = None
        split_tensor_371 = torch.ops.aten.split.Tensor(masked_fill_218, 256, dim = -1)
        getitem_3428 = split_tensor_371[0]
        unsqueeze_1133 = torch.ops.aten.unsqueeze.default(getitem_3428, 4);  getitem_3428 = None
        permute_1872 = torch.ops.aten.permute.default(unsqueeze_1133, [0, 1, 4, 3, 2]);  unsqueeze_1133 = None
        permute_1873 = torch.ops.aten.permute.default(permute_1872, [3, 1, 4, 0, 2]);  permute_1872 = None
        view_3531 = torch.ops.aten.view.default(permute_1873, [256, 512, 512]);  permute_1873 = None
        split_tensor_372 = torch.ops.aten.split.Tensor(masked_fill_218, 256, dim = -1);  masked_fill_218 = None
        getitem_3431 = split_tensor_372[1];  split_tensor_372 = None
        unsqueeze_1134 = torch.ops.aten.unsqueeze.default(getitem_3431, 4);  getitem_3431 = None
        permute_1874 = torch.ops.aten.permute.default(unsqueeze_1134, [0, 4, 1, 3, 2]);  unsqueeze_1134 = None
        permute_1875 = torch.ops.aten.permute.default(permute_1874, [3, 4, 0, 2, 1]);  permute_1874 = None
        view_3532 = torch.ops.aten.view.default(permute_1875, [256, 512, 512]);  permute_1875 = None
        bmm_278 = torch.ops.aten.bmm.default(view_3531, view_3532);  view_3531 = view_3532 = None
        view_3533 = torch.ops.aten.view.default(bmm_278, [256, 512, 1, 1, 512]);  bmm_278 = None
        permute_1876 = torch.ops.aten.permute.default(view_3533, [3, 1, 4, 0, 2]);  view_3533 = None
        view_3534 = torch.ops.aten.view.default(permute_1876, [1, 512, 512, 256]);  permute_1876 = None
        _to_copy_1992 = torch.ops.aten._to_copy.default(getitem_3422, dtype = torch.bfloat16);  getitem_3422 = None
        _to_copy_1993 = torch.ops.aten._to_copy.default(getitem_3418, dtype = torch.bfloat16)
        t_743 = torch.ops.aten.t.default(_to_copy_1992);  _to_copy_1992 = None
        view_3535 = torch.ops.aten.view.default(_to_copy_1993, [262144, 256]);  _to_copy_1993 = None
        mm_693 = torch.ops.aten.mm.default(view_3535, t_743);  view_3535 = t_743 = None
        view_3536 = torch.ops.aten.view.default(mm_693, [1, 512, 512, 512]);  mm_693 = None
        _to_copy_1994 = torch.ops.aten._to_copy.default(getitem_3424, dtype = torch.bfloat16);  getitem_3424 = None
        _to_copy_1995 = torch.ops.aten._to_copy.default(getitem_3418, dtype = torch.bfloat16)
        t_744 = torch.ops.aten.t.default(_to_copy_1994);  _to_copy_1994 = None
        view_3537 = torch.ops.aten.view.default(_to_copy_1995, [262144, 256]);  _to_copy_1995 = None
        mm_694 = torch.ops.aten.mm.default(view_3537, t_744);  view_3537 = t_744 = None
        view_3538 = torch.ops.aten.view.default(mm_694, [1, 512, 512, 512]);  mm_694 = None
        sigmoid_283 = torch.ops.aten.sigmoid.default(view_3538);  view_3538 = None
        mul_468 = torch.ops.aten.mul.Tensor(view_3536, sigmoid_283);  view_3536 = sigmoid_283 = None
        view_3539 = torch.ops.aten.view.default(mul_468, [262144, 512]);  mul_468 = None
        view_3540 = torch.ops.aten.view.default(view_3539, [1, 512, 512, 512]);  view_3539 = None
        transpose_92 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_1135 = torch.ops.aten.unsqueeze.default(transpose_92, 3);  transpose_92 = None
        clone_295 = torch.ops.aten.clone.default(unsqueeze_1135, memory_format = torch.contiguous_format);  unsqueeze_1135 = None
        bitwise_not_219 = torch.ops.aten.bitwise_not.default(clone_295);  clone_295 = None
        masked_fill_219 = torch.ops.aten.masked_fill.Scalar(view_3540, bitwise_not_219, 0);  view_3540 = bitwise_not_219 = None
        view_3541 = torch.ops.aten.view.default(masked_fill_219, [262144, 512]);  masked_fill_219 = None
        view_3545 = torch.ops.aten.view.default(view_3541, [1, 512, 512, 512])
        split_tensor_373 = torch.ops.aten.split.Tensor(view_3545, 256, dim = -1);  view_3545 = None
        getitem_3434 = split_tensor_373[0]
        unsqueeze_1138 = torch.ops.aten.unsqueeze.default(getitem_3434, 4);  getitem_3434 = None
        permute_1881 = torch.ops.aten.permute.default(unsqueeze_1138, [0, 2, 4, 3, 1]);  unsqueeze_1138 = None
        permute_1882 = torch.ops.aten.permute.default(permute_1881, [3, 1, 4, 0, 2]);  permute_1881 = None
        view_3546 = torch.ops.aten.view.default(permute_1882, [256, 512, 512]);  permute_1882 = None
        view_3547 = torch.ops.aten.view.default(view_3541, [1, 512, 512, 512]);  view_3541 = None
        split_tensor_374 = torch.ops.aten.split.Tensor(view_3547, 256, dim = -1);  view_3547 = None
        getitem_3437 = split_tensor_374[1];  split_tensor_374 = None
        unsqueeze_1139 = torch.ops.aten.unsqueeze.default(getitem_3437, 4);  getitem_3437 = None
        permute_1883 = torch.ops.aten.permute.default(unsqueeze_1139, [0, 4, 2, 3, 1]);  unsqueeze_1139 = None
        permute_1884 = torch.ops.aten.permute.default(permute_1883, [3, 4, 0, 2, 1]);  permute_1883 = None
        view_3548 = torch.ops.aten.view.default(permute_1884, [256, 512, 512]);  permute_1884 = None
        bmm_279 = torch.ops.aten.bmm.default(view_3546, view_3548);  view_3546 = view_3548 = None
        view_3549 = torch.ops.aten.view.default(bmm_279, [256, 512, 1, 1, 512]);  bmm_279 = None
        permute_1885 = torch.ops.aten.permute.default(view_3549, [3, 1, 4, 0, 2]);  view_3549 = None
        view_3550 = torch.ops.aten.view.default(permute_1885, [1, 512, 512, 256]);  permute_1885 = None
        _to_copy_1996 = torch.ops.aten._to_copy.default(view_3534, dtype = torch.float32);  view_3534 = None
        native_layer_norm_default_411 = torch.ops.aten.native_layer_norm.default(_to_copy_1996, [256], None, None, 1e-05);  _to_copy_1996 = None
        getitem_3438 = native_layer_norm_default_411[0]
        _to_copy_1997 = torch.ops.aten._to_copy.default(view_3550, dtype = torch.float32);  view_3550 = None
        native_layer_norm_default_412 = torch.ops.aten.native_layer_norm.default(_to_copy_1997, [256], None, None, 1e-05);  _to_copy_1997 = None
        getitem_3441 = native_layer_norm_default_412[0]
        add_376 = torch.ops.aten.add.Tensor(getitem_3438, getitem_3441);  getitem_3438 = getitem_3441 = None
        _to_copy_1998 = torch.ops.aten._to_copy.default(arg1190_1, dtype = torch.bfloat16);  arg1190_1 = None
        _to_copy_1999 = torch.ops.aten._to_copy.default(add_376, dtype = torch.bfloat16);  add_376 = None
        t_745 = torch.ops.aten.t.default(_to_copy_1998);  _to_copy_1998 = None
        view_3551 = torch.ops.aten.view.default(_to_copy_1999, [262144, 256]);  _to_copy_1999 = None
        mm_695 = torch.ops.aten.mm.default(view_3551, t_745);  view_3551 = t_745 = None
        view_3552 = torch.ops.aten.view.default(mm_695, [1, 512, 512, 256]);  mm_695 = None
        _to_copy_2000 = torch.ops.aten._to_copy.default(getitem_3425, dtype = torch.bfloat16);  getitem_3425 = None
        _to_copy_2001 = torch.ops.aten._to_copy.default(getitem_3418, dtype = torch.bfloat16);  getitem_3418 = None
        t_746 = torch.ops.aten.t.default(_to_copy_2000);  _to_copy_2000 = None
        view_3553 = torch.ops.aten.view.default(_to_copy_2001, [262144, 256]);  _to_copy_2001 = None
        mm_696 = torch.ops.aten.mm.default(view_3553, t_746);  view_3553 = t_746 = None
        view_3554 = torch.ops.aten.view.default(mm_696, [1, 512, 512, 256]);  mm_696 = None
        sigmoid_284 = torch.ops.aten.sigmoid.default(view_3554);  view_3554 = None
        mul_469 = torch.ops.aten.mul.Tensor(view_3552, sigmoid_284);  view_3552 = sigmoid_284 = None
        add_377 = torch.ops.aten.add.Tensor(add_371, mul_469);  mul_469 = None
        _to_copy_2002 = torch.ops.aten._to_copy.default(add_371, dtype = torch.float32)
        native_layer_norm_default_413 = torch.ops.aten.native_layer_norm.default(_to_copy_2002, [256], None, None, 1e-05);  _to_copy_2002 = None
        getitem_3444 = native_layer_norm_default_413[0]
        _to_copy_2003 = torch.ops.aten._to_copy.default(arg1194_1, dtype = torch.bfloat16);  arg1194_1 = None
        _to_copy_2004 = torch.ops.aten._to_copy.default(getitem_3444, dtype = torch.bfloat16)
        t_747 = torch.ops.aten.t.default(_to_copy_2003);  _to_copy_2003 = None
        view_3555 = torch.ops.aten.view.default(_to_copy_2004, [262144, 256]);  _to_copy_2004 = None
        mm_697 = torch.ops.aten.mm.default(view_3555, t_747);  view_3555 = t_747 = None
        view_3556 = torch.ops.aten.view.default(mm_697, [1, 512, 512, 8]);  mm_697 = None
        view_3557 = torch.ops.aten.view.default(view_3556, [1, 512, 512, 2, 4]);  view_3556 = None
        permute_1886 = torch.ops.aten.permute.default(view_3557, [0, 3, 4, 1, 2]);  view_3557 = None
        view_3558 = torch.ops.aten.view.default(permute_1886, [1, 2, 4, 1, 512, 512]);  permute_1886 = None
        view_3559 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_220 = torch.ops.aten.bitwise_not.default(view_3559);  view_3559 = None
        masked_fill_220 = torch.ops.aten.masked_fill.Scalar(view_3558, bitwise_not_220, -10000);  view_3558 = bitwise_not_220 = None
        view_3560 = torch.ops.aten.view.default(masked_fill_220, [1, 2, 4, 512, 512]);  masked_fill_220 = None
        permute_1887 = torch.ops.aten.permute.default(view_3560, [1, 0, 2, 3, 4]);  view_3560 = None
        view_3561 = torch.ops.aten.view.default(permute_1887, [2, 4, 1, 512, 512]);  permute_1887 = None
        _to_copy_2005 = torch.ops.aten._to_copy.default(arg1195_1, dtype = torch.bfloat16);  arg1195_1 = None
        _to_copy_2006 = torch.ops.aten._to_copy.default(getitem_3444, dtype = torch.bfloat16)
        t_748 = torch.ops.aten.t.default(_to_copy_2005);  _to_copy_2005 = None
        view_3562 = torch.ops.aten.view.default(_to_copy_2006, [262144, 256]);  _to_copy_2006 = None
        mm_698 = torch.ops.aten.mm.default(view_3562, t_748);  view_3562 = t_748 = None
        view_3563 = torch.ops.aten.view.default(mm_698, [1, 512, 512, 1024]);  mm_698 = None
        select_93 = torch.ops.aten.select.int(view_3561, 0, 0)
        view_3564 = torch.ops.aten.view.default(view_3563, [1, 512, 512, 4, 4, 64]);  view_3563 = None
        permute_1888 = torch.ops.aten.permute.default(view_3564, [4, 0, 3, 1, 2, 5]);  view_3564 = None
        view_3565 = torch.ops.aten.view.default(permute_1888, [4, 4, 512, 512, 64]);  permute_1888 = None
        unbind_int_160 = torch.ops.aten.unbind.int(view_3565);  view_3565 = None
        getitem_3447 = unbind_int_160[0]
        getitem_3448 = unbind_int_160[1]
        getitem_3449 = unbind_int_160[2]
        getitem_3450 = unbind_int_160[3];  unbind_int_160 = None
        expand_227 = torch.ops.aten.expand.default(select_93, [4, 512, 512, 512]);  select_93 = None
        _scaled_dot_product_efficient_attention_default_132 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3447, getitem_3448, getitem_3449, expand_227, False);  getitem_3447 = getitem_3448 = getitem_3449 = expand_227 = None
        getitem_3451 = _scaled_dot_product_efficient_attention_default_132[0]
        sigmoid_285 = torch.ops.aten.sigmoid.default(getitem_3450);  getitem_3450 = None
        mul_470 = torch.ops.aten.mul.Tensor(getitem_3451, sigmoid_285);  getitem_3451 = sigmoid_285 = None
        view_3566 = torch.ops.aten.view.default(mul_470, [1, 4, 512, 512, 64]);  mul_470 = None
        permute_1889 = torch.ops.aten.permute.default(view_3566, [0, 2, 3, 1, 4]);  view_3566 = None
        clone_296 = torch.ops.aten.clone.default(permute_1889, memory_format = torch.contiguous_format);  permute_1889 = None
        _unsafe_view_248 = torch.ops.aten._unsafe_view.default(clone_296, [1, 512, 512, 256]);  clone_296 = None
        transpose_93 = torch.ops.aten.transpose.int(getitem_3444, 1, 2);  getitem_3444 = None
        _to_copy_2007 = torch.ops.aten._to_copy.default(arg1196_1, dtype = torch.bfloat16);  arg1196_1 = None
        _to_copy_2008 = torch.ops.aten._to_copy.default(transpose_93, dtype = torch.bfloat16);  transpose_93 = None
        t_749 = torch.ops.aten.t.default(_to_copy_2007);  _to_copy_2007 = None
        expand_228 = torch.ops.aten.expand.default(_to_copy_2008, [1, 512, 512, 256]);  _to_copy_2008 = None
        view_3567 = torch.ops.aten.view.default(expand_228, [512, 512, 256]);  expand_228 = None
        expand_229 = torch.ops.aten.expand.default(t_749, [1, 512, 256, 1024]);  t_749 = None
        view_3568 = torch.ops.aten.view.default(expand_229, [512, 256, 1024]);  expand_229 = None
        bmm_280 = torch.ops.aten.bmm.default(view_3567, view_3568);  view_3567 = view_3568 = None
        view_3569 = torch.ops.aten.view.default(bmm_280, [1, 512, 512, 1024]);  bmm_280 = None
        select_94 = torch.ops.aten.select.int(view_3561, 0, 1);  view_3561 = None
        view_3570 = torch.ops.aten.view.default(view_3569, [1, 512, 512, 4, 4, 64]);  view_3569 = None
        permute_1890 = torch.ops.aten.permute.default(view_3570, [4, 0, 3, 1, 2, 5]);  view_3570 = None
        view_3571 = torch.ops.aten.view.default(permute_1890, [4, 4, 512, 512, 64]);  permute_1890 = None
        unbind_int_161 = torch.ops.aten.unbind.int(view_3571);  view_3571 = None
        getitem_3455 = unbind_int_161[0]
        getitem_3456 = unbind_int_161[1]
        getitem_3457 = unbind_int_161[2]
        getitem_3458 = unbind_int_161[3];  unbind_int_161 = None
        expand_230 = torch.ops.aten.expand.default(select_94, [4, 512, 512, 512]);  select_94 = None
        _scaled_dot_product_efficient_attention_default_133 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3455, getitem_3456, getitem_3457, expand_230, False);  getitem_3455 = getitem_3456 = getitem_3457 = expand_230 = None
        getitem_3459 = _scaled_dot_product_efficient_attention_default_133[0]
        sigmoid_286 = torch.ops.aten.sigmoid.default(getitem_3458);  getitem_3458 = None
        mul_471 = torch.ops.aten.mul.Tensor(getitem_3459, sigmoid_286);  getitem_3459 = sigmoid_286 = None
        view_3572 = torch.ops.aten.view.default(mul_471, [1, 4, 512, 512, 64]);  mul_471 = None
        permute_1891 = torch.ops.aten.permute.default(view_3572, [0, 2, 3, 1, 4]);  view_3572 = None
        clone_297 = torch.ops.aten.clone.default(permute_1891, memory_format = torch.contiguous_format);  permute_1891 = None
        _unsafe_view_249 = torch.ops.aten._unsafe_view.default(clone_297, [1, 512, 512, 256]);  clone_297 = None
        cat_52 = torch.ops.aten.cat.default([_unsafe_view_248, _unsafe_view_249], dim = -1);  _unsafe_view_248 = _unsafe_view_249 = None
        slice_245 = torch.ops.aten.slice.Tensor(arg1193_1, dim = 0, start = 0, end = 9223372036854775807);  arg1193_1 = None
        unsqueeze_1140 = torch.ops.aten.unsqueeze.default(slice_245, 1);  slice_245 = None
        mul_472 = torch.ops.aten.mul.Tensor(arg1197_1, unsqueeze_1140);  arg1197_1 = unsqueeze_1140 = None
        _to_copy_2009 = torch.ops.aten._to_copy.default(mul_472, dtype = torch.bfloat16);  mul_472 = None
        t_750 = torch.ops.aten.t.default(_to_copy_2009);  _to_copy_2009 = None
        view_3573 = torch.ops.aten.view.default(cat_52, [262144, 512]);  cat_52 = None
        mm_699 = torch.ops.aten.mm.default(view_3573, t_750);  view_3573 = t_750 = None
        view_3574 = torch.ops.aten.view.default(mm_699, [1, 512, 512, 256]);  mm_699 = None
        add_378 = torch.ops.aten.add.Tensor(add_377, view_3574);  add_377 = view_3574 = None
        split_tensor_375 = torch.ops.aten.split.Tensor(add_371, 512, dim = -2)
        getitem_3463 = split_tensor_375[0];  split_tensor_375 = None
        _to_copy_2010 = torch.ops.aten._to_copy.default(getitem_3463, dtype = torch.float32);  getitem_3463 = None
        native_layer_norm_default_414 = torch.ops.aten.native_layer_norm.default(_to_copy_2010, [256], arg1184_1, arg1185_1, 1e-05);  _to_copy_2010 = arg1184_1 = arg1185_1 = None
        getitem_3464 = native_layer_norm_default_414[0]
        _to_copy_2011 = torch.ops.aten._to_copy.default(arg1186_1, dtype = torch.bfloat16);  arg1186_1 = None
        _to_copy_2012 = torch.ops.aten._to_copy.default(getitem_3464, dtype = torch.bfloat16);  getitem_3464 = None
        t_751 = torch.ops.aten.t.default(_to_copy_2011);  _to_copy_2011 = None
        view_3575 = torch.ops.aten.view.default(_to_copy_2012, [262144, 256]);  _to_copy_2012 = None
        mm_700 = torch.ops.aten.mm.default(view_3575, t_751);  view_3575 = t_751 = None
        view_3576 = torch.ops.aten.view.default(mm_700, [1, 512, 512, 1024]);  mm_700 = None
        split_tensor_376 = torch.ops.aten.split.Tensor(view_3576, 512, dim = -1);  view_3576 = None
        getitem_3467 = split_tensor_376[0]
        getitem_3468 = split_tensor_376[1];  split_tensor_376 = None
        silu_98 = torch.ops.aten.silu.default(getitem_3467);  getitem_3467 = None
        mul_473 = torch.ops.aten.mul.Tensor(silu_98, getitem_3468);  silu_98 = getitem_3468 = None
        _to_copy_2013 = torch.ops.aten._to_copy.default(arg1187_1, dtype = torch.bfloat16);  arg1187_1 = None
        t_752 = torch.ops.aten.t.default(_to_copy_2013);  _to_copy_2013 = None
        view_3578 = torch.ops.aten.view.default(mul_473, [262144, 512]);  mul_473 = None
        mm_701 = torch.ops.aten.mm.default(view_3578, t_752);  view_3578 = t_752 = None
        view_3579 = torch.ops.aten.view.default(mm_701, [1, 512, 512, 256]);  mm_701 = None
        add_379 = torch.ops.aten.add.Tensor(add_378, view_3579);  add_378 = view_3579 = None
        _to_copy_2014 = torch.ops.aten._to_copy.default(add_375, dtype = torch.float32)
        native_layer_norm_default_415 = torch.ops.aten.native_layer_norm.default(_to_copy_2014, [384], arg1202_1, arg1203_1, 1e-05);  _to_copy_2014 = arg1202_1 = arg1203_1 = None
        getitem_3469 = native_layer_norm_default_415[0]
        _to_copy_2015 = torch.ops.aten._to_copy.default(add_371, dtype = torch.float32);  add_371 = None
        native_layer_norm_default_416 = torch.ops.aten.native_layer_norm.default(_to_copy_2015, [256], arg1204_1, arg1205_1, 1e-05);  _to_copy_2015 = arg1204_1 = arg1205_1 = None
        getitem_3472 = native_layer_norm_default_416[0]
        _to_copy_2016 = torch.ops.aten._to_copy.default(arg1206_1, dtype = torch.bfloat16);  arg1206_1 = None
        _to_copy_2017 = torch.ops.aten._to_copy.default(getitem_3472, dtype = torch.bfloat16);  getitem_3472 = None
        t_753 = torch.ops.aten.t.default(_to_copy_2016);  _to_copy_2016 = None
        view_3580 = torch.ops.aten.view.default(_to_copy_2017, [262144, 256]);  _to_copy_2017 = None
        mm_702 = torch.ops.aten.mm.default(view_3580, t_753);  view_3580 = t_753 = None
        view_3581 = torch.ops.aten.view.default(mm_702, [1, 512, 512, 16]);  mm_702 = None
        permute_1892 = torch.ops.aten.permute.default(view_3581, [0, 3, 1, 2]);  view_3581 = None
        view_3582 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_221 = torch.ops.aten.bitwise_not.default(view_3582);  view_3582 = None
        masked_fill_221 = torch.ops.aten.masked_fill.Scalar(permute_1892, bitwise_not_221, -10000);  permute_1892 = bitwise_not_221 = None
        _to_copy_2018 = torch.ops.aten._to_copy.default(getitem_3469, dtype = torch.bfloat16);  getitem_3469 = None
        _to_copy_2019 = torch.ops.aten._to_copy.default(arg1208_1, dtype = torch.bfloat16);  arg1208_1 = None
        unsqueeze_1141 = torch.ops.aten.unsqueeze.default(_to_copy_2018, 3);  _to_copy_2018 = None
        unsqueeze_1142 = torch.ops.aten.unsqueeze.default(unsqueeze_1141, 4);  unsqueeze_1141 = None
        unsqueeze_1143 = torch.ops.aten.unsqueeze.default(unsqueeze_1142, 5);  unsqueeze_1142 = None
        permute_1893 = torch.ops.aten.permute.default(unsqueeze_1143, [3, 0, 4, 1, 5, 2]);  unsqueeze_1143 = None
        unsqueeze_1144 = torch.ops.aten.unsqueeze.default(_to_copy_2019, 4);  _to_copy_2019 = None
        unsqueeze_1145 = torch.ops.aten.unsqueeze.default(unsqueeze_1144, 5);  unsqueeze_1144 = None
        permute_1894 = torch.ops.aten.permute.default(unsqueeze_1145, [1, 4, 2, 5, 3, 0]);  unsqueeze_1145 = None
        permute_1895 = torch.ops.aten.permute.default(permute_1893, [3, 5, 0, 1, 2, 4]);  permute_1893 = None
        view_3583 = torch.ops.aten.view.default(permute_1895, [1, 512, 384]);  permute_1895 = None
        permute_1896 = torch.ops.aten.permute.default(permute_1894, [5, 0, 1, 2, 4, 3]);  permute_1894 = None
        view_3584 = torch.ops.aten.view.default(permute_1896, [1, 384, 1536]);  permute_1896 = None
        bmm_281 = torch.ops.aten.bmm.default(view_3583, view_3584);  view_3583 = view_3584 = None
        view_3585 = torch.ops.aten.view.default(bmm_281, [512, 1, 4, 1, 16, 24]);  bmm_281 = None
        permute_1897 = torch.ops.aten.permute.default(view_3585, [2, 3, 4, 0, 5, 1]);  view_3585 = None
        view_3586 = torch.ops.aten.view.default(permute_1897, [4, 1, 16, 512, 24]);  permute_1897 = None
        unbind_int_162 = torch.ops.aten.unbind.int(view_3586);  view_3586 = None
        getitem_3475 = unbind_int_162[0]
        getitem_3476 = unbind_int_162[1]
        getitem_3477 = unbind_int_162[2]
        getitem_3478 = unbind_int_162[3];  unbind_int_162 = None
        view_3587 = torch.ops.aten.view.default(arg1207_1, [1, 16, 1, 24]);  arg1207_1 = None
        add_380 = torch.ops.aten.add.Tensor(getitem_3475, view_3587);  getitem_3475 = view_3587 = None
        _to_copy_2020 = torch.ops.aten._to_copy.default(add_380, dtype = torch.bfloat16);  add_380 = None
        expand_231 = torch.ops.aten.expand.default(masked_fill_221, [1, 16, 512, 512]);  masked_fill_221 = None
        _scaled_dot_product_efficient_attention_default_134 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_2020, getitem_3476, getitem_3477, expand_231, False);  _to_copy_2020 = getitem_3476 = getitem_3477 = expand_231 = None
        getitem_3479 = _scaled_dot_product_efficient_attention_default_134[0]
        add_381 = torch.ops.aten.add.Tensor(getitem_3478, 1);  getitem_3478 = None
        sigmoid_287 = torch.ops.aten.sigmoid.default(add_381);  add_381 = None
        mul_474 = torch.ops.aten.mul.Tensor(getitem_3479, sigmoid_287);  getitem_3479 = sigmoid_287 = None
        _to_copy_2021 = torch.ops.aten._to_copy.default(arg1209_1, dtype = torch.bfloat16);  arg1209_1 = None
        unsqueeze_1146 = torch.ops.aten.unsqueeze.default(mul_474, 4);  mul_474 = None
        permute_1898 = torch.ops.aten.permute.default(unsqueeze_1146, [0, 2, 4, 3, 1]);  unsqueeze_1146 = None
        unsqueeze_1147 = torch.ops.aten.unsqueeze.default(_to_copy_2021, 3);  _to_copy_2021 = None
        unsqueeze_1148 = torch.ops.aten.unsqueeze.default(unsqueeze_1147, 4);  unsqueeze_1147 = None
        permute_1899 = torch.ops.aten.permute.default(unsqueeze_1148, [3, 4, 2, 1, 0]);  unsqueeze_1148 = None
        permute_1900 = torch.ops.aten.permute.default(permute_1898, [1, 3, 4, 0, 2]);  permute_1898 = None
        clone_298 = torch.ops.aten.clone.default(permute_1900, memory_format = torch.contiguous_format);  permute_1900 = None
        _unsafe_view_250 = torch.ops.aten._unsafe_view.default(clone_298, [1, 512, 384]);  clone_298 = None
        permute_1901 = torch.ops.aten.permute.default(permute_1899, [3, 4, 0, 2, 1]);  permute_1899 = None
        clone_299 = torch.ops.aten.clone.default(permute_1901, memory_format = torch.contiguous_format);  permute_1901 = None
        _unsafe_view_251 = torch.ops.aten._unsafe_view.default(clone_299, [1, 384, 384]);  clone_299 = None
        bmm_282 = torch.ops.aten.bmm.default(_unsafe_view_250, _unsafe_view_251);  _unsafe_view_250 = _unsafe_view_251 = None
        view_3588 = torch.ops.aten.view.default(bmm_282, [512, 1, 1, 1, 384]);  bmm_282 = None
        permute_1902 = torch.ops.aten.permute.default(view_3588, [3, 0, 4, 1, 2]);  view_3588 = None
        view_3589 = torch.ops.aten.view.default(permute_1902, [1, 512, 384]);  permute_1902 = None
        unsqueeze_1149 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_475 = torch.ops.aten.mul.Tensor(view_3589, unsqueeze_1149);  view_3589 = unsqueeze_1149 = None
        add_382 = torch.ops.aten.add.Tensor(add_375, mul_475);  mul_475 = None
        split_tensor_377 = torch.ops.aten.split.Tensor(add_375, 512, dim = -2);  add_375 = None
        getitem_3483 = split_tensor_377[0];  split_tensor_377 = None
        _to_copy_2022 = torch.ops.aten._to_copy.default(getitem_3483, dtype = torch.float32);  getitem_3483 = None
        native_layer_norm_default_417 = torch.ops.aten.native_layer_norm.default(_to_copy_2022, [384], arg1198_1, arg1199_1, 1e-05);  _to_copy_2022 = arg1198_1 = arg1199_1 = None
        getitem_3484 = native_layer_norm_default_417[0]
        _to_copy_2023 = torch.ops.aten._to_copy.default(arg1200_1, dtype = torch.bfloat16);  arg1200_1 = None
        _to_copy_2024 = torch.ops.aten._to_copy.default(getitem_3484, dtype = torch.bfloat16);  getitem_3484 = None
        t_754 = torch.ops.aten.t.default(_to_copy_2023);  _to_copy_2023 = None
        view_3590 = torch.ops.aten.view.default(_to_copy_2024, [512, 384]);  _to_copy_2024 = None
        mm_703 = torch.ops.aten.mm.default(view_3590, t_754);  view_3590 = t_754 = None
        view_3591 = torch.ops.aten.view.default(mm_703, [1, 512, 1536]);  mm_703 = None
        split_tensor_378 = torch.ops.aten.split.Tensor(view_3591, 768, dim = -1);  view_3591 = None
        getitem_3487 = split_tensor_378[0]
        getitem_3488 = split_tensor_378[1];  split_tensor_378 = None
        silu_99 = torch.ops.aten.silu.default(getitem_3487);  getitem_3487 = None
        mul_476 = torch.ops.aten.mul.Tensor(silu_99, getitem_3488);  silu_99 = getitem_3488 = None
        _to_copy_2025 = torch.ops.aten._to_copy.default(arg1201_1, dtype = torch.bfloat16);  arg1201_1 = None
        t_755 = torch.ops.aten.t.default(_to_copy_2025);  _to_copy_2025 = None
        view_3593 = torch.ops.aten.view.default(mul_476, [512, 768]);  mul_476 = None
        mm_704 = torch.ops.aten.mm.default(view_3593, t_755);  view_3593 = t_755 = None
        view_3594 = torch.ops.aten.view.default(mm_704, [1, 512, 384]);  mm_704 = None
        add_383 = torch.ops.aten.add.Tensor(add_382, view_3594);  add_382 = view_3594 = None
        _to_copy_2026 = torch.ops.aten._to_copy.default(add_379, dtype = torch.float32)
        native_layer_norm_default_418 = torch.ops.aten.native_layer_norm.default(_to_copy_2026, [256], arg1214_1, arg1215_1, 1e-05);  _to_copy_2026 = arg1214_1 = arg1215_1 = None
        getitem_3489 = native_layer_norm_default_418[0]
        split_with_sizes_default_94 = torch.ops.aten.split_with_sizes.default(arg1217_1, [512, 512]);  arg1217_1 = None
        getitem_3492 = split_with_sizes_default_94[0]
        getitem_3493 = split_with_sizes_default_94[1];  split_with_sizes_default_94 = None
        split_with_sizes_default_95 = torch.ops.aten.split_with_sizes.default(arg1218_1, [512, 512, 256]);  arg1218_1 = None
        getitem_3494 = split_with_sizes_default_95[0]
        getitem_3495 = split_with_sizes_default_95[1]
        getitem_3496 = split_with_sizes_default_95[2];  split_with_sizes_default_95 = None
        _to_copy_2027 = torch.ops.aten._to_copy.default(getitem_3492, dtype = torch.bfloat16);  getitem_3492 = None
        _to_copy_2028 = torch.ops.aten._to_copy.default(getitem_3489, dtype = torch.bfloat16)
        t_756 = torch.ops.aten.t.default(_to_copy_2027);  _to_copy_2027 = None
        view_3595 = torch.ops.aten.view.default(_to_copy_2028, [262144, 256]);  _to_copy_2028 = None
        mm_705 = torch.ops.aten.mm.default(view_3595, t_756);  view_3595 = t_756 = None
        view_3596 = torch.ops.aten.view.default(mm_705, [1, 512, 512, 512]);  mm_705 = None
        _to_copy_2029 = torch.ops.aten._to_copy.default(getitem_3494, dtype = torch.bfloat16);  getitem_3494 = None
        _to_copy_2030 = torch.ops.aten._to_copy.default(getitem_3489, dtype = torch.bfloat16)
        t_757 = torch.ops.aten.t.default(_to_copy_2029);  _to_copy_2029 = None
        view_3597 = torch.ops.aten.view.default(_to_copy_2030, [262144, 256]);  _to_copy_2030 = None
        mm_706 = torch.ops.aten.mm.default(view_3597, t_757);  view_3597 = t_757 = None
        view_3598 = torch.ops.aten.view.default(mm_706, [1, 512, 512, 512]);  mm_706 = None
        sigmoid_288 = torch.ops.aten.sigmoid.default(view_3598);  view_3598 = None
        mul_477 = torch.ops.aten.mul.Tensor(view_3596, sigmoid_288);  view_3596 = sigmoid_288 = None
        unsqueeze_1150 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_222 = torch.ops.aten.bitwise_not.default(unsqueeze_1150);  unsqueeze_1150 = None
        masked_fill_222 = torch.ops.aten.masked_fill.Scalar(mul_477, bitwise_not_222, 0);  mul_477 = bitwise_not_222 = None
        split_tensor_379 = torch.ops.aten.split.Tensor(masked_fill_222, 256, dim = -1)
        getitem_3499 = split_tensor_379[0]
        unsqueeze_1153 = torch.ops.aten.unsqueeze.default(getitem_3499, 4);  getitem_3499 = None
        permute_1907 = torch.ops.aten.permute.default(unsqueeze_1153, [0, 1, 4, 3, 2]);  unsqueeze_1153 = None
        permute_1908 = torch.ops.aten.permute.default(permute_1907, [3, 1, 4, 0, 2]);  permute_1907 = None
        view_3601 = torch.ops.aten.view.default(permute_1908, [256, 512, 512]);  permute_1908 = None
        split_tensor_380 = torch.ops.aten.split.Tensor(masked_fill_222, 256, dim = -1);  masked_fill_222 = None
        getitem_3502 = split_tensor_380[1];  split_tensor_380 = None
        unsqueeze_1154 = torch.ops.aten.unsqueeze.default(getitem_3502, 4);  getitem_3502 = None
        permute_1909 = torch.ops.aten.permute.default(unsqueeze_1154, [0, 4, 1, 3, 2]);  unsqueeze_1154 = None
        permute_1910 = torch.ops.aten.permute.default(permute_1909, [3, 4, 0, 2, 1]);  permute_1909 = None
        view_3602 = torch.ops.aten.view.default(permute_1910, [256, 512, 512]);  permute_1910 = None
        bmm_283 = torch.ops.aten.bmm.default(view_3601, view_3602);  view_3601 = view_3602 = None
        view_3603 = torch.ops.aten.view.default(bmm_283, [256, 512, 1, 1, 512]);  bmm_283 = None
        permute_1911 = torch.ops.aten.permute.default(view_3603, [3, 1, 4, 0, 2]);  view_3603 = None
        view_3604 = torch.ops.aten.view.default(permute_1911, [1, 512, 512, 256]);  permute_1911 = None
        _to_copy_2031 = torch.ops.aten._to_copy.default(getitem_3493, dtype = torch.bfloat16);  getitem_3493 = None
        _to_copy_2032 = torch.ops.aten._to_copy.default(getitem_3489, dtype = torch.bfloat16)
        t_758 = torch.ops.aten.t.default(_to_copy_2031);  _to_copy_2031 = None
        view_3605 = torch.ops.aten.view.default(_to_copy_2032, [262144, 256]);  _to_copy_2032 = None
        mm_707 = torch.ops.aten.mm.default(view_3605, t_758);  view_3605 = t_758 = None
        view_3606 = torch.ops.aten.view.default(mm_707, [1, 512, 512, 512]);  mm_707 = None
        _to_copy_2033 = torch.ops.aten._to_copy.default(getitem_3495, dtype = torch.bfloat16);  getitem_3495 = None
        _to_copy_2034 = torch.ops.aten._to_copy.default(getitem_3489, dtype = torch.bfloat16)
        t_759 = torch.ops.aten.t.default(_to_copy_2033);  _to_copy_2033 = None
        view_3607 = torch.ops.aten.view.default(_to_copy_2034, [262144, 256]);  _to_copy_2034 = None
        mm_708 = torch.ops.aten.mm.default(view_3607, t_759);  view_3607 = t_759 = None
        view_3608 = torch.ops.aten.view.default(mm_708, [1, 512, 512, 512]);  mm_708 = None
        sigmoid_289 = torch.ops.aten.sigmoid.default(view_3608);  view_3608 = None
        mul_478 = torch.ops.aten.mul.Tensor(view_3606, sigmoid_289);  view_3606 = sigmoid_289 = None
        view_3609 = torch.ops.aten.view.default(mul_478, [262144, 512]);  mul_478 = None
        view_3610 = torch.ops.aten.view.default(view_3609, [1, 512, 512, 512]);  view_3609 = None
        transpose_94 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_1155 = torch.ops.aten.unsqueeze.default(transpose_94, 3);  transpose_94 = None
        clone_300 = torch.ops.aten.clone.default(unsqueeze_1155, memory_format = torch.contiguous_format);  unsqueeze_1155 = None
        bitwise_not_223 = torch.ops.aten.bitwise_not.default(clone_300);  clone_300 = None
        masked_fill_223 = torch.ops.aten.masked_fill.Scalar(view_3610, bitwise_not_223, 0);  view_3610 = bitwise_not_223 = None
        view_3611 = torch.ops.aten.view.default(masked_fill_223, [262144, 512]);  masked_fill_223 = None
        view_3615 = torch.ops.aten.view.default(view_3611, [1, 512, 512, 512])
        split_tensor_381 = torch.ops.aten.split.Tensor(view_3615, 256, dim = -1);  view_3615 = None
        getitem_3505 = split_tensor_381[0]
        unsqueeze_1158 = torch.ops.aten.unsqueeze.default(getitem_3505, 4);  getitem_3505 = None
        permute_1916 = torch.ops.aten.permute.default(unsqueeze_1158, [0, 2, 4, 3, 1]);  unsqueeze_1158 = None
        permute_1917 = torch.ops.aten.permute.default(permute_1916, [3, 1, 4, 0, 2]);  permute_1916 = None
        view_3616 = torch.ops.aten.view.default(permute_1917, [256, 512, 512]);  permute_1917 = None
        view_3617 = torch.ops.aten.view.default(view_3611, [1, 512, 512, 512]);  view_3611 = None
        split_tensor_382 = torch.ops.aten.split.Tensor(view_3617, 256, dim = -1);  view_3617 = None
        getitem_3508 = split_tensor_382[1];  split_tensor_382 = None
        unsqueeze_1159 = torch.ops.aten.unsqueeze.default(getitem_3508, 4);  getitem_3508 = None
        permute_1918 = torch.ops.aten.permute.default(unsqueeze_1159, [0, 4, 2, 3, 1]);  unsqueeze_1159 = None
        permute_1919 = torch.ops.aten.permute.default(permute_1918, [3, 4, 0, 2, 1]);  permute_1918 = None
        view_3618 = torch.ops.aten.view.default(permute_1919, [256, 512, 512]);  permute_1919 = None
        bmm_284 = torch.ops.aten.bmm.default(view_3616, view_3618);  view_3616 = view_3618 = None
        view_3619 = torch.ops.aten.view.default(bmm_284, [256, 512, 1, 1, 512]);  bmm_284 = None
        permute_1920 = torch.ops.aten.permute.default(view_3619, [3, 1, 4, 0, 2]);  view_3619 = None
        view_3620 = torch.ops.aten.view.default(permute_1920, [1, 512, 512, 256]);  permute_1920 = None
        _to_copy_2035 = torch.ops.aten._to_copy.default(view_3604, dtype = torch.float32);  view_3604 = None
        native_layer_norm_default_419 = torch.ops.aten.native_layer_norm.default(_to_copy_2035, [256], None, None, 1e-05);  _to_copy_2035 = None
        getitem_3509 = native_layer_norm_default_419[0]
        _to_copy_2036 = torch.ops.aten._to_copy.default(view_3620, dtype = torch.float32);  view_3620 = None
        native_layer_norm_default_420 = torch.ops.aten.native_layer_norm.default(_to_copy_2036, [256], None, None, 1e-05);  _to_copy_2036 = None
        getitem_3512 = native_layer_norm_default_420[0]
        add_384 = torch.ops.aten.add.Tensor(getitem_3509, getitem_3512);  getitem_3509 = getitem_3512 = None
        _to_copy_2037 = torch.ops.aten._to_copy.default(arg1216_1, dtype = torch.bfloat16);  arg1216_1 = None
        _to_copy_2038 = torch.ops.aten._to_copy.default(add_384, dtype = torch.bfloat16);  add_384 = None
        t_760 = torch.ops.aten.t.default(_to_copy_2037);  _to_copy_2037 = None
        view_3621 = torch.ops.aten.view.default(_to_copy_2038, [262144, 256]);  _to_copy_2038 = None
        mm_709 = torch.ops.aten.mm.default(view_3621, t_760);  view_3621 = t_760 = None
        view_3622 = torch.ops.aten.view.default(mm_709, [1, 512, 512, 256]);  mm_709 = None
        _to_copy_2039 = torch.ops.aten._to_copy.default(getitem_3496, dtype = torch.bfloat16);  getitem_3496 = None
        _to_copy_2040 = torch.ops.aten._to_copy.default(getitem_3489, dtype = torch.bfloat16);  getitem_3489 = None
        t_761 = torch.ops.aten.t.default(_to_copy_2039);  _to_copy_2039 = None
        view_3623 = torch.ops.aten.view.default(_to_copy_2040, [262144, 256]);  _to_copy_2040 = None
        mm_710 = torch.ops.aten.mm.default(view_3623, t_761);  view_3623 = t_761 = None
        view_3624 = torch.ops.aten.view.default(mm_710, [1, 512, 512, 256]);  mm_710 = None
        sigmoid_290 = torch.ops.aten.sigmoid.default(view_3624);  view_3624 = None
        mul_479 = torch.ops.aten.mul.Tensor(view_3622, sigmoid_290);  view_3622 = sigmoid_290 = None
        add_385 = torch.ops.aten.add.Tensor(add_379, mul_479);  mul_479 = None
        _to_copy_2041 = torch.ops.aten._to_copy.default(add_379, dtype = torch.float32)
        native_layer_norm_default_421 = torch.ops.aten.native_layer_norm.default(_to_copy_2041, [256], None, None, 1e-05);  _to_copy_2041 = None
        getitem_3515 = native_layer_norm_default_421[0]
        _to_copy_2042 = torch.ops.aten._to_copy.default(arg1220_1, dtype = torch.bfloat16);  arg1220_1 = None
        _to_copy_2043 = torch.ops.aten._to_copy.default(getitem_3515, dtype = torch.bfloat16)
        t_762 = torch.ops.aten.t.default(_to_copy_2042);  _to_copy_2042 = None
        view_3625 = torch.ops.aten.view.default(_to_copy_2043, [262144, 256]);  _to_copy_2043 = None
        mm_711 = torch.ops.aten.mm.default(view_3625, t_762);  view_3625 = t_762 = None
        view_3626 = torch.ops.aten.view.default(mm_711, [1, 512, 512, 8]);  mm_711 = None
        view_3627 = torch.ops.aten.view.default(view_3626, [1, 512, 512, 2, 4]);  view_3626 = None
        permute_1921 = torch.ops.aten.permute.default(view_3627, [0, 3, 4, 1, 2]);  view_3627 = None
        view_3628 = torch.ops.aten.view.default(permute_1921, [1, 2, 4, 1, 512, 512]);  permute_1921 = None
        view_3629 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_224 = torch.ops.aten.bitwise_not.default(view_3629);  view_3629 = None
        masked_fill_224 = torch.ops.aten.masked_fill.Scalar(view_3628, bitwise_not_224, -10000);  view_3628 = bitwise_not_224 = None
        view_3630 = torch.ops.aten.view.default(masked_fill_224, [1, 2, 4, 512, 512]);  masked_fill_224 = None
        permute_1922 = torch.ops.aten.permute.default(view_3630, [1, 0, 2, 3, 4]);  view_3630 = None
        view_3631 = torch.ops.aten.view.default(permute_1922, [2, 4, 1, 512, 512]);  permute_1922 = None
        _to_copy_2044 = torch.ops.aten._to_copy.default(arg1221_1, dtype = torch.bfloat16);  arg1221_1 = None
        _to_copy_2045 = torch.ops.aten._to_copy.default(getitem_3515, dtype = torch.bfloat16)
        t_763 = torch.ops.aten.t.default(_to_copy_2044);  _to_copy_2044 = None
        view_3632 = torch.ops.aten.view.default(_to_copy_2045, [262144, 256]);  _to_copy_2045 = None
        mm_712 = torch.ops.aten.mm.default(view_3632, t_763);  view_3632 = t_763 = None
        view_3633 = torch.ops.aten.view.default(mm_712, [1, 512, 512, 1024]);  mm_712 = None
        select_95 = torch.ops.aten.select.int(view_3631, 0, 0)
        view_3634 = torch.ops.aten.view.default(view_3633, [1, 512, 512, 4, 4, 64]);  view_3633 = None
        permute_1923 = torch.ops.aten.permute.default(view_3634, [4, 0, 3, 1, 2, 5]);  view_3634 = None
        view_3635 = torch.ops.aten.view.default(permute_1923, [4, 4, 512, 512, 64]);  permute_1923 = None
        unbind_int_163 = torch.ops.aten.unbind.int(view_3635);  view_3635 = None
        getitem_3518 = unbind_int_163[0]
        getitem_3519 = unbind_int_163[1]
        getitem_3520 = unbind_int_163[2]
        getitem_3521 = unbind_int_163[3];  unbind_int_163 = None
        expand_232 = torch.ops.aten.expand.default(select_95, [4, 512, 512, 512]);  select_95 = None
        _scaled_dot_product_efficient_attention_default_135 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3518, getitem_3519, getitem_3520, expand_232, False);  getitem_3518 = getitem_3519 = getitem_3520 = expand_232 = None
        getitem_3522 = _scaled_dot_product_efficient_attention_default_135[0]
        sigmoid_291 = torch.ops.aten.sigmoid.default(getitem_3521);  getitem_3521 = None
        mul_480 = torch.ops.aten.mul.Tensor(getitem_3522, sigmoid_291);  getitem_3522 = sigmoid_291 = None
        view_3636 = torch.ops.aten.view.default(mul_480, [1, 4, 512, 512, 64]);  mul_480 = None
        permute_1924 = torch.ops.aten.permute.default(view_3636, [0, 2, 3, 1, 4]);  view_3636 = None
        clone_301 = torch.ops.aten.clone.default(permute_1924, memory_format = torch.contiguous_format);  permute_1924 = None
        _unsafe_view_252 = torch.ops.aten._unsafe_view.default(clone_301, [1, 512, 512, 256]);  clone_301 = None
        transpose_95 = torch.ops.aten.transpose.int(getitem_3515, 1, 2);  getitem_3515 = None
        _to_copy_2046 = torch.ops.aten._to_copy.default(arg1222_1, dtype = torch.bfloat16);  arg1222_1 = None
        _to_copy_2047 = torch.ops.aten._to_copy.default(transpose_95, dtype = torch.bfloat16);  transpose_95 = None
        t_764 = torch.ops.aten.t.default(_to_copy_2046);  _to_copy_2046 = None
        expand_233 = torch.ops.aten.expand.default(_to_copy_2047, [1, 512, 512, 256]);  _to_copy_2047 = None
        view_3637 = torch.ops.aten.view.default(expand_233, [512, 512, 256]);  expand_233 = None
        expand_234 = torch.ops.aten.expand.default(t_764, [1, 512, 256, 1024]);  t_764 = None
        view_3638 = torch.ops.aten.view.default(expand_234, [512, 256, 1024]);  expand_234 = None
        bmm_285 = torch.ops.aten.bmm.default(view_3637, view_3638);  view_3637 = view_3638 = None
        view_3639 = torch.ops.aten.view.default(bmm_285, [1, 512, 512, 1024]);  bmm_285 = None
        select_96 = torch.ops.aten.select.int(view_3631, 0, 1);  view_3631 = None
        view_3640 = torch.ops.aten.view.default(view_3639, [1, 512, 512, 4, 4, 64]);  view_3639 = None
        permute_1925 = torch.ops.aten.permute.default(view_3640, [4, 0, 3, 1, 2, 5]);  view_3640 = None
        view_3641 = torch.ops.aten.view.default(permute_1925, [4, 4, 512, 512, 64]);  permute_1925 = None
        unbind_int_164 = torch.ops.aten.unbind.int(view_3641);  view_3641 = None
        getitem_3526 = unbind_int_164[0]
        getitem_3527 = unbind_int_164[1]
        getitem_3528 = unbind_int_164[2]
        getitem_3529 = unbind_int_164[3];  unbind_int_164 = None
        expand_235 = torch.ops.aten.expand.default(select_96, [4, 512, 512, 512]);  select_96 = None
        _scaled_dot_product_efficient_attention_default_136 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3526, getitem_3527, getitem_3528, expand_235, False);  getitem_3526 = getitem_3527 = getitem_3528 = expand_235 = None
        getitem_3530 = _scaled_dot_product_efficient_attention_default_136[0]
        sigmoid_292 = torch.ops.aten.sigmoid.default(getitem_3529);  getitem_3529 = None
        mul_481 = torch.ops.aten.mul.Tensor(getitem_3530, sigmoid_292);  getitem_3530 = sigmoid_292 = None
        view_3642 = torch.ops.aten.view.default(mul_481, [1, 4, 512, 512, 64]);  mul_481 = None
        permute_1926 = torch.ops.aten.permute.default(view_3642, [0, 2, 3, 1, 4]);  view_3642 = None
        clone_302 = torch.ops.aten.clone.default(permute_1926, memory_format = torch.contiguous_format);  permute_1926 = None
        _unsafe_view_253 = torch.ops.aten._unsafe_view.default(clone_302, [1, 512, 512, 256]);  clone_302 = None
        cat_53 = torch.ops.aten.cat.default([_unsafe_view_252, _unsafe_view_253], dim = -1);  _unsafe_view_252 = _unsafe_view_253 = None
        slice_246 = torch.ops.aten.slice.Tensor(arg1219_1, dim = 0, start = 0, end = 9223372036854775807);  arg1219_1 = None
        unsqueeze_1160 = torch.ops.aten.unsqueeze.default(slice_246, 1);  slice_246 = None
        mul_482 = torch.ops.aten.mul.Tensor(arg1223_1, unsqueeze_1160);  arg1223_1 = unsqueeze_1160 = None
        _to_copy_2048 = torch.ops.aten._to_copy.default(mul_482, dtype = torch.bfloat16);  mul_482 = None
        t_765 = torch.ops.aten.t.default(_to_copy_2048);  _to_copy_2048 = None
        view_3643 = torch.ops.aten.view.default(cat_53, [262144, 512]);  cat_53 = None
        mm_713 = torch.ops.aten.mm.default(view_3643, t_765);  view_3643 = t_765 = None
        view_3644 = torch.ops.aten.view.default(mm_713, [1, 512, 512, 256]);  mm_713 = None
        add_386 = torch.ops.aten.add.Tensor(add_385, view_3644);  add_385 = view_3644 = None
        split_tensor_383 = torch.ops.aten.split.Tensor(add_379, 512, dim = -2)
        getitem_3534 = split_tensor_383[0];  split_tensor_383 = None
        _to_copy_2049 = torch.ops.aten._to_copy.default(getitem_3534, dtype = torch.float32);  getitem_3534 = None
        native_layer_norm_default_422 = torch.ops.aten.native_layer_norm.default(_to_copy_2049, [256], arg1210_1, arg1211_1, 1e-05);  _to_copy_2049 = arg1210_1 = arg1211_1 = None
        getitem_3535 = native_layer_norm_default_422[0]
        _to_copy_2050 = torch.ops.aten._to_copy.default(arg1212_1, dtype = torch.bfloat16);  arg1212_1 = None
        _to_copy_2051 = torch.ops.aten._to_copy.default(getitem_3535, dtype = torch.bfloat16);  getitem_3535 = None
        t_766 = torch.ops.aten.t.default(_to_copy_2050);  _to_copy_2050 = None
        view_3645 = torch.ops.aten.view.default(_to_copy_2051, [262144, 256]);  _to_copy_2051 = None
        mm_714 = torch.ops.aten.mm.default(view_3645, t_766);  view_3645 = t_766 = None
        view_3646 = torch.ops.aten.view.default(mm_714, [1, 512, 512, 1024]);  mm_714 = None
        split_tensor_384 = torch.ops.aten.split.Tensor(view_3646, 512, dim = -1);  view_3646 = None
        getitem_3538 = split_tensor_384[0]
        getitem_3539 = split_tensor_384[1];  split_tensor_384 = None
        silu_100 = torch.ops.aten.silu.default(getitem_3538);  getitem_3538 = None
        mul_483 = torch.ops.aten.mul.Tensor(silu_100, getitem_3539);  silu_100 = getitem_3539 = None
        _to_copy_2052 = torch.ops.aten._to_copy.default(arg1213_1, dtype = torch.bfloat16);  arg1213_1 = None
        t_767 = torch.ops.aten.t.default(_to_copy_2052);  _to_copy_2052 = None
        view_3648 = torch.ops.aten.view.default(mul_483, [262144, 512]);  mul_483 = None
        mm_715 = torch.ops.aten.mm.default(view_3648, t_767);  view_3648 = t_767 = None
        view_3649 = torch.ops.aten.view.default(mm_715, [1, 512, 512, 256]);  mm_715 = None
        add_387 = torch.ops.aten.add.Tensor(add_386, view_3649);  add_386 = view_3649 = None
        _to_copy_2053 = torch.ops.aten._to_copy.default(add_383, dtype = torch.float32)
        native_layer_norm_default_423 = torch.ops.aten.native_layer_norm.default(_to_copy_2053, [384], arg1228_1, arg1229_1, 1e-05);  _to_copy_2053 = arg1228_1 = arg1229_1 = None
        getitem_3540 = native_layer_norm_default_423[0]
        _to_copy_2054 = torch.ops.aten._to_copy.default(add_379, dtype = torch.float32);  add_379 = None
        native_layer_norm_default_424 = torch.ops.aten.native_layer_norm.default(_to_copy_2054, [256], arg1230_1, arg1231_1, 1e-05);  _to_copy_2054 = arg1230_1 = arg1231_1 = None
        getitem_3543 = native_layer_norm_default_424[0]
        _to_copy_2055 = torch.ops.aten._to_copy.default(arg1232_1, dtype = torch.bfloat16);  arg1232_1 = None
        _to_copy_2056 = torch.ops.aten._to_copy.default(getitem_3543, dtype = torch.bfloat16);  getitem_3543 = None
        t_768 = torch.ops.aten.t.default(_to_copy_2055);  _to_copy_2055 = None
        view_3650 = torch.ops.aten.view.default(_to_copy_2056, [262144, 256]);  _to_copy_2056 = None
        mm_716 = torch.ops.aten.mm.default(view_3650, t_768);  view_3650 = t_768 = None
        view_3651 = torch.ops.aten.view.default(mm_716, [1, 512, 512, 16]);  mm_716 = None
        permute_1927 = torch.ops.aten.permute.default(view_3651, [0, 3, 1, 2]);  view_3651 = None
        view_3652 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_225 = torch.ops.aten.bitwise_not.default(view_3652);  view_3652 = None
        masked_fill_225 = torch.ops.aten.masked_fill.Scalar(permute_1927, bitwise_not_225, -10000);  permute_1927 = bitwise_not_225 = None
        _to_copy_2057 = torch.ops.aten._to_copy.default(getitem_3540, dtype = torch.bfloat16);  getitem_3540 = None
        _to_copy_2058 = torch.ops.aten._to_copy.default(arg1234_1, dtype = torch.bfloat16);  arg1234_1 = None
        unsqueeze_1161 = torch.ops.aten.unsqueeze.default(_to_copy_2057, 3);  _to_copy_2057 = None
        unsqueeze_1162 = torch.ops.aten.unsqueeze.default(unsqueeze_1161, 4);  unsqueeze_1161 = None
        unsqueeze_1163 = torch.ops.aten.unsqueeze.default(unsqueeze_1162, 5);  unsqueeze_1162 = None
        permute_1928 = torch.ops.aten.permute.default(unsqueeze_1163, [3, 0, 4, 1, 5, 2]);  unsqueeze_1163 = None
        unsqueeze_1164 = torch.ops.aten.unsqueeze.default(_to_copy_2058, 4);  _to_copy_2058 = None
        unsqueeze_1165 = torch.ops.aten.unsqueeze.default(unsqueeze_1164, 5);  unsqueeze_1164 = None
        permute_1929 = torch.ops.aten.permute.default(unsqueeze_1165, [1, 4, 2, 5, 3, 0]);  unsqueeze_1165 = None
        permute_1930 = torch.ops.aten.permute.default(permute_1928, [3, 5, 0, 1, 2, 4]);  permute_1928 = None
        view_3653 = torch.ops.aten.view.default(permute_1930, [1, 512, 384]);  permute_1930 = None
        permute_1931 = torch.ops.aten.permute.default(permute_1929, [5, 0, 1, 2, 4, 3]);  permute_1929 = None
        view_3654 = torch.ops.aten.view.default(permute_1931, [1, 384, 1536]);  permute_1931 = None
        bmm_286 = torch.ops.aten.bmm.default(view_3653, view_3654);  view_3653 = view_3654 = None
        view_3655 = torch.ops.aten.view.default(bmm_286, [512, 1, 4, 1, 16, 24]);  bmm_286 = None
        permute_1932 = torch.ops.aten.permute.default(view_3655, [2, 3, 4, 0, 5, 1]);  view_3655 = None
        view_3656 = torch.ops.aten.view.default(permute_1932, [4, 1, 16, 512, 24]);  permute_1932 = None
        unbind_int_165 = torch.ops.aten.unbind.int(view_3656);  view_3656 = None
        getitem_3546 = unbind_int_165[0]
        getitem_3547 = unbind_int_165[1]
        getitem_3548 = unbind_int_165[2]
        getitem_3549 = unbind_int_165[3];  unbind_int_165 = None
        view_3657 = torch.ops.aten.view.default(arg1233_1, [1, 16, 1, 24]);  arg1233_1 = None
        add_388 = torch.ops.aten.add.Tensor(getitem_3546, view_3657);  getitem_3546 = view_3657 = None
        _to_copy_2059 = torch.ops.aten._to_copy.default(add_388, dtype = torch.bfloat16);  add_388 = None
        expand_236 = torch.ops.aten.expand.default(masked_fill_225, [1, 16, 512, 512]);  masked_fill_225 = None
        _scaled_dot_product_efficient_attention_default_137 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_2059, getitem_3547, getitem_3548, expand_236, False);  _to_copy_2059 = getitem_3547 = getitem_3548 = expand_236 = None
        getitem_3550 = _scaled_dot_product_efficient_attention_default_137[0]
        add_389 = torch.ops.aten.add.Tensor(getitem_3549, 1);  getitem_3549 = None
        sigmoid_293 = torch.ops.aten.sigmoid.default(add_389);  add_389 = None
        mul_484 = torch.ops.aten.mul.Tensor(getitem_3550, sigmoid_293);  getitem_3550 = sigmoid_293 = None
        _to_copy_2060 = torch.ops.aten._to_copy.default(arg1235_1, dtype = torch.bfloat16);  arg1235_1 = None
        unsqueeze_1166 = torch.ops.aten.unsqueeze.default(mul_484, 4);  mul_484 = None
        permute_1933 = torch.ops.aten.permute.default(unsqueeze_1166, [0, 2, 4, 3, 1]);  unsqueeze_1166 = None
        unsqueeze_1167 = torch.ops.aten.unsqueeze.default(_to_copy_2060, 3);  _to_copy_2060 = None
        unsqueeze_1168 = torch.ops.aten.unsqueeze.default(unsqueeze_1167, 4);  unsqueeze_1167 = None
        permute_1934 = torch.ops.aten.permute.default(unsqueeze_1168, [3, 4, 2, 1, 0]);  unsqueeze_1168 = None
        permute_1935 = torch.ops.aten.permute.default(permute_1933, [1, 3, 4, 0, 2]);  permute_1933 = None
        clone_303 = torch.ops.aten.clone.default(permute_1935, memory_format = torch.contiguous_format);  permute_1935 = None
        _unsafe_view_254 = torch.ops.aten._unsafe_view.default(clone_303, [1, 512, 384]);  clone_303 = None
        permute_1936 = torch.ops.aten.permute.default(permute_1934, [3, 4, 0, 2, 1]);  permute_1934 = None
        clone_304 = torch.ops.aten.clone.default(permute_1936, memory_format = torch.contiguous_format);  permute_1936 = None
        _unsafe_view_255 = torch.ops.aten._unsafe_view.default(clone_304, [1, 384, 384]);  clone_304 = None
        bmm_287 = torch.ops.aten.bmm.default(_unsafe_view_254, _unsafe_view_255);  _unsafe_view_254 = _unsafe_view_255 = None
        view_3658 = torch.ops.aten.view.default(bmm_287, [512, 1, 1, 1, 384]);  bmm_287 = None
        permute_1937 = torch.ops.aten.permute.default(view_3658, [3, 0, 4, 1, 2]);  view_3658 = None
        view_3659 = torch.ops.aten.view.default(permute_1937, [1, 512, 384]);  permute_1937 = None
        unsqueeze_1169 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_485 = torch.ops.aten.mul.Tensor(view_3659, unsqueeze_1169);  view_3659 = unsqueeze_1169 = None
        add_390 = torch.ops.aten.add.Tensor(add_383, mul_485);  mul_485 = None
        split_tensor_385 = torch.ops.aten.split.Tensor(add_383, 512, dim = -2);  add_383 = None
        getitem_3554 = split_tensor_385[0];  split_tensor_385 = None
        _to_copy_2061 = torch.ops.aten._to_copy.default(getitem_3554, dtype = torch.float32);  getitem_3554 = None
        native_layer_norm_default_425 = torch.ops.aten.native_layer_norm.default(_to_copy_2061, [384], arg1224_1, arg1225_1, 1e-05);  _to_copy_2061 = arg1224_1 = arg1225_1 = None
        getitem_3555 = native_layer_norm_default_425[0]
        _to_copy_2062 = torch.ops.aten._to_copy.default(arg1226_1, dtype = torch.bfloat16);  arg1226_1 = None
        _to_copy_2063 = torch.ops.aten._to_copy.default(getitem_3555, dtype = torch.bfloat16);  getitem_3555 = None
        t_769 = torch.ops.aten.t.default(_to_copy_2062);  _to_copy_2062 = None
        view_3660 = torch.ops.aten.view.default(_to_copy_2063, [512, 384]);  _to_copy_2063 = None
        mm_717 = torch.ops.aten.mm.default(view_3660, t_769);  view_3660 = t_769 = None
        view_3661 = torch.ops.aten.view.default(mm_717, [1, 512, 1536]);  mm_717 = None
        split_tensor_386 = torch.ops.aten.split.Tensor(view_3661, 768, dim = -1);  view_3661 = None
        getitem_3558 = split_tensor_386[0]
        getitem_3559 = split_tensor_386[1];  split_tensor_386 = None
        silu_101 = torch.ops.aten.silu.default(getitem_3558);  getitem_3558 = None
        mul_486 = torch.ops.aten.mul.Tensor(silu_101, getitem_3559);  silu_101 = getitem_3559 = None
        _to_copy_2064 = torch.ops.aten._to_copy.default(arg1227_1, dtype = torch.bfloat16);  arg1227_1 = None
        t_770 = torch.ops.aten.t.default(_to_copy_2064);  _to_copy_2064 = None
        view_3663 = torch.ops.aten.view.default(mul_486, [512, 768]);  mul_486 = None
        mm_718 = torch.ops.aten.mm.default(view_3663, t_770);  view_3663 = t_770 = None
        view_3664 = torch.ops.aten.view.default(mm_718, [1, 512, 384]);  mm_718 = None
        add_391 = torch.ops.aten.add.Tensor(add_390, view_3664);  add_390 = view_3664 = None
        _to_copy_2065 = torch.ops.aten._to_copy.default(add_387, dtype = torch.float32)
        native_layer_norm_default_426 = torch.ops.aten.native_layer_norm.default(_to_copy_2065, [256], arg1240_1, arg1241_1, 1e-05);  _to_copy_2065 = arg1240_1 = arg1241_1 = None
        getitem_3560 = native_layer_norm_default_426[0]
        split_with_sizes_default_96 = torch.ops.aten.split_with_sizes.default(arg1243_1, [512, 512]);  arg1243_1 = None
        getitem_3563 = split_with_sizes_default_96[0]
        getitem_3564 = split_with_sizes_default_96[1];  split_with_sizes_default_96 = None
        split_with_sizes_default_97 = torch.ops.aten.split_with_sizes.default(arg1244_1, [512, 512, 256]);  arg1244_1 = None
        getitem_3565 = split_with_sizes_default_97[0]
        getitem_3566 = split_with_sizes_default_97[1]
        getitem_3567 = split_with_sizes_default_97[2];  split_with_sizes_default_97 = None
        _to_copy_2066 = torch.ops.aten._to_copy.default(getitem_3563, dtype = torch.bfloat16);  getitem_3563 = None
        _to_copy_2067 = torch.ops.aten._to_copy.default(getitem_3560, dtype = torch.bfloat16)
        t_771 = torch.ops.aten.t.default(_to_copy_2066);  _to_copy_2066 = None
        view_3665 = torch.ops.aten.view.default(_to_copy_2067, [262144, 256]);  _to_copy_2067 = None
        mm_719 = torch.ops.aten.mm.default(view_3665, t_771);  view_3665 = t_771 = None
        view_3666 = torch.ops.aten.view.default(mm_719, [1, 512, 512, 512]);  mm_719 = None
        _to_copy_2068 = torch.ops.aten._to_copy.default(getitem_3565, dtype = torch.bfloat16);  getitem_3565 = None
        _to_copy_2069 = torch.ops.aten._to_copy.default(getitem_3560, dtype = torch.bfloat16)
        t_772 = torch.ops.aten.t.default(_to_copy_2068);  _to_copy_2068 = None
        view_3667 = torch.ops.aten.view.default(_to_copy_2069, [262144, 256]);  _to_copy_2069 = None
        mm_720 = torch.ops.aten.mm.default(view_3667, t_772);  view_3667 = t_772 = None
        view_3668 = torch.ops.aten.view.default(mm_720, [1, 512, 512, 512]);  mm_720 = None
        sigmoid_294 = torch.ops.aten.sigmoid.default(view_3668);  view_3668 = None
        mul_487 = torch.ops.aten.mul.Tensor(view_3666, sigmoid_294);  view_3666 = sigmoid_294 = None
        unsqueeze_1170 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_226 = torch.ops.aten.bitwise_not.default(unsqueeze_1170);  unsqueeze_1170 = None
        masked_fill_226 = torch.ops.aten.masked_fill.Scalar(mul_487, bitwise_not_226, 0);  mul_487 = bitwise_not_226 = None
        split_tensor_387 = torch.ops.aten.split.Tensor(masked_fill_226, 256, dim = -1)
        getitem_3570 = split_tensor_387[0]
        unsqueeze_1173 = torch.ops.aten.unsqueeze.default(getitem_3570, 4);  getitem_3570 = None
        permute_1942 = torch.ops.aten.permute.default(unsqueeze_1173, [0, 1, 4, 3, 2]);  unsqueeze_1173 = None
        permute_1943 = torch.ops.aten.permute.default(permute_1942, [3, 1, 4, 0, 2]);  permute_1942 = None
        view_3671 = torch.ops.aten.view.default(permute_1943, [256, 512, 512]);  permute_1943 = None
        split_tensor_388 = torch.ops.aten.split.Tensor(masked_fill_226, 256, dim = -1);  masked_fill_226 = None
        getitem_3573 = split_tensor_388[1];  split_tensor_388 = None
        unsqueeze_1174 = torch.ops.aten.unsqueeze.default(getitem_3573, 4);  getitem_3573 = None
        permute_1944 = torch.ops.aten.permute.default(unsqueeze_1174, [0, 4, 1, 3, 2]);  unsqueeze_1174 = None
        permute_1945 = torch.ops.aten.permute.default(permute_1944, [3, 4, 0, 2, 1]);  permute_1944 = None
        view_3672 = torch.ops.aten.view.default(permute_1945, [256, 512, 512]);  permute_1945 = None
        bmm_288 = torch.ops.aten.bmm.default(view_3671, view_3672);  view_3671 = view_3672 = None
        view_3673 = torch.ops.aten.view.default(bmm_288, [256, 512, 1, 1, 512]);  bmm_288 = None
        permute_1946 = torch.ops.aten.permute.default(view_3673, [3, 1, 4, 0, 2]);  view_3673 = None
        view_3674 = torch.ops.aten.view.default(permute_1946, [1, 512, 512, 256]);  permute_1946 = None
        _to_copy_2070 = torch.ops.aten._to_copy.default(getitem_3564, dtype = torch.bfloat16);  getitem_3564 = None
        _to_copy_2071 = torch.ops.aten._to_copy.default(getitem_3560, dtype = torch.bfloat16)
        t_773 = torch.ops.aten.t.default(_to_copy_2070);  _to_copy_2070 = None
        view_3675 = torch.ops.aten.view.default(_to_copy_2071, [262144, 256]);  _to_copy_2071 = None
        mm_721 = torch.ops.aten.mm.default(view_3675, t_773);  view_3675 = t_773 = None
        view_3676 = torch.ops.aten.view.default(mm_721, [1, 512, 512, 512]);  mm_721 = None
        _to_copy_2072 = torch.ops.aten._to_copy.default(getitem_3566, dtype = torch.bfloat16);  getitem_3566 = None
        _to_copy_2073 = torch.ops.aten._to_copy.default(getitem_3560, dtype = torch.bfloat16)
        t_774 = torch.ops.aten.t.default(_to_copy_2072);  _to_copy_2072 = None
        view_3677 = torch.ops.aten.view.default(_to_copy_2073, [262144, 256]);  _to_copy_2073 = None
        mm_722 = torch.ops.aten.mm.default(view_3677, t_774);  view_3677 = t_774 = None
        view_3678 = torch.ops.aten.view.default(mm_722, [1, 512, 512, 512]);  mm_722 = None
        sigmoid_295 = torch.ops.aten.sigmoid.default(view_3678);  view_3678 = None
        mul_488 = torch.ops.aten.mul.Tensor(view_3676, sigmoid_295);  view_3676 = sigmoid_295 = None
        view_3679 = torch.ops.aten.view.default(mul_488, [262144, 512]);  mul_488 = None
        view_3680 = torch.ops.aten.view.default(view_3679, [1, 512, 512, 512]);  view_3679 = None
        transpose_96 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_1175 = torch.ops.aten.unsqueeze.default(transpose_96, 3);  transpose_96 = None
        clone_305 = torch.ops.aten.clone.default(unsqueeze_1175, memory_format = torch.contiguous_format);  unsqueeze_1175 = None
        bitwise_not_227 = torch.ops.aten.bitwise_not.default(clone_305);  clone_305 = None
        masked_fill_227 = torch.ops.aten.masked_fill.Scalar(view_3680, bitwise_not_227, 0);  view_3680 = bitwise_not_227 = None
        view_3681 = torch.ops.aten.view.default(masked_fill_227, [262144, 512]);  masked_fill_227 = None
        view_3685 = torch.ops.aten.view.default(view_3681, [1, 512, 512, 512])
        split_tensor_389 = torch.ops.aten.split.Tensor(view_3685, 256, dim = -1);  view_3685 = None
        getitem_3576 = split_tensor_389[0]
        unsqueeze_1178 = torch.ops.aten.unsqueeze.default(getitem_3576, 4);  getitem_3576 = None
        permute_1951 = torch.ops.aten.permute.default(unsqueeze_1178, [0, 2, 4, 3, 1]);  unsqueeze_1178 = None
        permute_1952 = torch.ops.aten.permute.default(permute_1951, [3, 1, 4, 0, 2]);  permute_1951 = None
        view_3686 = torch.ops.aten.view.default(permute_1952, [256, 512, 512]);  permute_1952 = None
        view_3687 = torch.ops.aten.view.default(view_3681, [1, 512, 512, 512]);  view_3681 = None
        split_tensor_390 = torch.ops.aten.split.Tensor(view_3687, 256, dim = -1);  view_3687 = None
        getitem_3579 = split_tensor_390[1];  split_tensor_390 = None
        unsqueeze_1179 = torch.ops.aten.unsqueeze.default(getitem_3579, 4);  getitem_3579 = None
        permute_1953 = torch.ops.aten.permute.default(unsqueeze_1179, [0, 4, 2, 3, 1]);  unsqueeze_1179 = None
        permute_1954 = torch.ops.aten.permute.default(permute_1953, [3, 4, 0, 2, 1]);  permute_1953 = None
        view_3688 = torch.ops.aten.view.default(permute_1954, [256, 512, 512]);  permute_1954 = None
        bmm_289 = torch.ops.aten.bmm.default(view_3686, view_3688);  view_3686 = view_3688 = None
        view_3689 = torch.ops.aten.view.default(bmm_289, [256, 512, 1, 1, 512]);  bmm_289 = None
        permute_1955 = torch.ops.aten.permute.default(view_3689, [3, 1, 4, 0, 2]);  view_3689 = None
        view_3690 = torch.ops.aten.view.default(permute_1955, [1, 512, 512, 256]);  permute_1955 = None
        _to_copy_2074 = torch.ops.aten._to_copy.default(view_3674, dtype = torch.float32);  view_3674 = None
        native_layer_norm_default_427 = torch.ops.aten.native_layer_norm.default(_to_copy_2074, [256], None, None, 1e-05);  _to_copy_2074 = None
        getitem_3580 = native_layer_norm_default_427[0]
        _to_copy_2075 = torch.ops.aten._to_copy.default(view_3690, dtype = torch.float32);  view_3690 = None
        native_layer_norm_default_428 = torch.ops.aten.native_layer_norm.default(_to_copy_2075, [256], None, None, 1e-05);  _to_copy_2075 = None
        getitem_3583 = native_layer_norm_default_428[0]
        add_392 = torch.ops.aten.add.Tensor(getitem_3580, getitem_3583);  getitem_3580 = getitem_3583 = None
        _to_copy_2076 = torch.ops.aten._to_copy.default(arg1242_1, dtype = torch.bfloat16);  arg1242_1 = None
        _to_copy_2077 = torch.ops.aten._to_copy.default(add_392, dtype = torch.bfloat16);  add_392 = None
        t_775 = torch.ops.aten.t.default(_to_copy_2076);  _to_copy_2076 = None
        view_3691 = torch.ops.aten.view.default(_to_copy_2077, [262144, 256]);  _to_copy_2077 = None
        mm_723 = torch.ops.aten.mm.default(view_3691, t_775);  view_3691 = t_775 = None
        view_3692 = torch.ops.aten.view.default(mm_723, [1, 512, 512, 256]);  mm_723 = None
        _to_copy_2078 = torch.ops.aten._to_copy.default(getitem_3567, dtype = torch.bfloat16);  getitem_3567 = None
        _to_copy_2079 = torch.ops.aten._to_copy.default(getitem_3560, dtype = torch.bfloat16);  getitem_3560 = None
        t_776 = torch.ops.aten.t.default(_to_copy_2078);  _to_copy_2078 = None
        view_3693 = torch.ops.aten.view.default(_to_copy_2079, [262144, 256]);  _to_copy_2079 = None
        mm_724 = torch.ops.aten.mm.default(view_3693, t_776);  view_3693 = t_776 = None
        view_3694 = torch.ops.aten.view.default(mm_724, [1, 512, 512, 256]);  mm_724 = None
        sigmoid_296 = torch.ops.aten.sigmoid.default(view_3694);  view_3694 = None
        mul_489 = torch.ops.aten.mul.Tensor(view_3692, sigmoid_296);  view_3692 = sigmoid_296 = None
        add_393 = torch.ops.aten.add.Tensor(add_387, mul_489);  mul_489 = None
        _to_copy_2080 = torch.ops.aten._to_copy.default(add_387, dtype = torch.float32)
        native_layer_norm_default_429 = torch.ops.aten.native_layer_norm.default(_to_copy_2080, [256], None, None, 1e-05);  _to_copy_2080 = None
        getitem_3586 = native_layer_norm_default_429[0]
        _to_copy_2081 = torch.ops.aten._to_copy.default(arg1246_1, dtype = torch.bfloat16);  arg1246_1 = None
        _to_copy_2082 = torch.ops.aten._to_copy.default(getitem_3586, dtype = torch.bfloat16)
        t_777 = torch.ops.aten.t.default(_to_copy_2081);  _to_copy_2081 = None
        view_3695 = torch.ops.aten.view.default(_to_copy_2082, [262144, 256]);  _to_copy_2082 = None
        mm_725 = torch.ops.aten.mm.default(view_3695, t_777);  view_3695 = t_777 = None
        view_3696 = torch.ops.aten.view.default(mm_725, [1, 512, 512, 8]);  mm_725 = None
        view_3697 = torch.ops.aten.view.default(view_3696, [1, 512, 512, 2, 4]);  view_3696 = None
        permute_1956 = torch.ops.aten.permute.default(view_3697, [0, 3, 4, 1, 2]);  view_3697 = None
        view_3698 = torch.ops.aten.view.default(permute_1956, [1, 2, 4, 1, 512, 512]);  permute_1956 = None
        view_3699 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_228 = torch.ops.aten.bitwise_not.default(view_3699);  view_3699 = None
        masked_fill_228 = torch.ops.aten.masked_fill.Scalar(view_3698, bitwise_not_228, -10000);  view_3698 = bitwise_not_228 = None
        view_3700 = torch.ops.aten.view.default(masked_fill_228, [1, 2, 4, 512, 512]);  masked_fill_228 = None
        permute_1957 = torch.ops.aten.permute.default(view_3700, [1, 0, 2, 3, 4]);  view_3700 = None
        view_3701 = torch.ops.aten.view.default(permute_1957, [2, 4, 1, 512, 512]);  permute_1957 = None
        _to_copy_2083 = torch.ops.aten._to_copy.default(arg1247_1, dtype = torch.bfloat16);  arg1247_1 = None
        _to_copy_2084 = torch.ops.aten._to_copy.default(getitem_3586, dtype = torch.bfloat16)
        t_778 = torch.ops.aten.t.default(_to_copy_2083);  _to_copy_2083 = None
        view_3702 = torch.ops.aten.view.default(_to_copy_2084, [262144, 256]);  _to_copy_2084 = None
        mm_726 = torch.ops.aten.mm.default(view_3702, t_778);  view_3702 = t_778 = None
        view_3703 = torch.ops.aten.view.default(mm_726, [1, 512, 512, 1024]);  mm_726 = None
        select_97 = torch.ops.aten.select.int(view_3701, 0, 0)
        view_3704 = torch.ops.aten.view.default(view_3703, [1, 512, 512, 4, 4, 64]);  view_3703 = None
        permute_1958 = torch.ops.aten.permute.default(view_3704, [4, 0, 3, 1, 2, 5]);  view_3704 = None
        view_3705 = torch.ops.aten.view.default(permute_1958, [4, 4, 512, 512, 64]);  permute_1958 = None
        unbind_int_166 = torch.ops.aten.unbind.int(view_3705);  view_3705 = None
        getitem_3589 = unbind_int_166[0]
        getitem_3590 = unbind_int_166[1]
        getitem_3591 = unbind_int_166[2]
        getitem_3592 = unbind_int_166[3];  unbind_int_166 = None
        expand_237 = torch.ops.aten.expand.default(select_97, [4, 512, 512, 512]);  select_97 = None
        _scaled_dot_product_efficient_attention_default_138 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3589, getitem_3590, getitem_3591, expand_237, False);  getitem_3589 = getitem_3590 = getitem_3591 = expand_237 = None
        getitem_3593 = _scaled_dot_product_efficient_attention_default_138[0]
        sigmoid_297 = torch.ops.aten.sigmoid.default(getitem_3592);  getitem_3592 = None
        mul_490 = torch.ops.aten.mul.Tensor(getitem_3593, sigmoid_297);  getitem_3593 = sigmoid_297 = None
        view_3706 = torch.ops.aten.view.default(mul_490, [1, 4, 512, 512, 64]);  mul_490 = None
        permute_1959 = torch.ops.aten.permute.default(view_3706, [0, 2, 3, 1, 4]);  view_3706 = None
        clone_306 = torch.ops.aten.clone.default(permute_1959, memory_format = torch.contiguous_format);  permute_1959 = None
        _unsafe_view_256 = torch.ops.aten._unsafe_view.default(clone_306, [1, 512, 512, 256]);  clone_306 = None
        transpose_97 = torch.ops.aten.transpose.int(getitem_3586, 1, 2);  getitem_3586 = None
        _to_copy_2085 = torch.ops.aten._to_copy.default(arg1248_1, dtype = torch.bfloat16);  arg1248_1 = None
        _to_copy_2086 = torch.ops.aten._to_copy.default(transpose_97, dtype = torch.bfloat16);  transpose_97 = None
        t_779 = torch.ops.aten.t.default(_to_copy_2085);  _to_copy_2085 = None
        expand_238 = torch.ops.aten.expand.default(_to_copy_2086, [1, 512, 512, 256]);  _to_copy_2086 = None
        view_3707 = torch.ops.aten.view.default(expand_238, [512, 512, 256]);  expand_238 = None
        expand_239 = torch.ops.aten.expand.default(t_779, [1, 512, 256, 1024]);  t_779 = None
        view_3708 = torch.ops.aten.view.default(expand_239, [512, 256, 1024]);  expand_239 = None
        bmm_290 = torch.ops.aten.bmm.default(view_3707, view_3708);  view_3707 = view_3708 = None
        view_3709 = torch.ops.aten.view.default(bmm_290, [1, 512, 512, 1024]);  bmm_290 = None
        select_98 = torch.ops.aten.select.int(view_3701, 0, 1);  view_3701 = None
        view_3710 = torch.ops.aten.view.default(view_3709, [1, 512, 512, 4, 4, 64]);  view_3709 = None
        permute_1960 = torch.ops.aten.permute.default(view_3710, [4, 0, 3, 1, 2, 5]);  view_3710 = None
        view_3711 = torch.ops.aten.view.default(permute_1960, [4, 4, 512, 512, 64]);  permute_1960 = None
        unbind_int_167 = torch.ops.aten.unbind.int(view_3711);  view_3711 = None
        getitem_3597 = unbind_int_167[0]
        getitem_3598 = unbind_int_167[1]
        getitem_3599 = unbind_int_167[2]
        getitem_3600 = unbind_int_167[3];  unbind_int_167 = None
        expand_240 = torch.ops.aten.expand.default(select_98, [4, 512, 512, 512]);  select_98 = None
        _scaled_dot_product_efficient_attention_default_139 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3597, getitem_3598, getitem_3599, expand_240, False);  getitem_3597 = getitem_3598 = getitem_3599 = expand_240 = None
        getitem_3601 = _scaled_dot_product_efficient_attention_default_139[0]
        sigmoid_298 = torch.ops.aten.sigmoid.default(getitem_3600);  getitem_3600 = None
        mul_491 = torch.ops.aten.mul.Tensor(getitem_3601, sigmoid_298);  getitem_3601 = sigmoid_298 = None
        view_3712 = torch.ops.aten.view.default(mul_491, [1, 4, 512, 512, 64]);  mul_491 = None
        permute_1961 = torch.ops.aten.permute.default(view_3712, [0, 2, 3, 1, 4]);  view_3712 = None
        clone_307 = torch.ops.aten.clone.default(permute_1961, memory_format = torch.contiguous_format);  permute_1961 = None
        _unsafe_view_257 = torch.ops.aten._unsafe_view.default(clone_307, [1, 512, 512, 256]);  clone_307 = None
        cat_54 = torch.ops.aten.cat.default([_unsafe_view_256, _unsafe_view_257], dim = -1);  _unsafe_view_256 = _unsafe_view_257 = None
        slice_247 = torch.ops.aten.slice.Tensor(arg1245_1, dim = 0, start = 0, end = 9223372036854775807);  arg1245_1 = None
        unsqueeze_1180 = torch.ops.aten.unsqueeze.default(slice_247, 1);  slice_247 = None
        mul_492 = torch.ops.aten.mul.Tensor(arg1249_1, unsqueeze_1180);  arg1249_1 = unsqueeze_1180 = None
        _to_copy_2087 = torch.ops.aten._to_copy.default(mul_492, dtype = torch.bfloat16);  mul_492 = None
        t_780 = torch.ops.aten.t.default(_to_copy_2087);  _to_copy_2087 = None
        view_3713 = torch.ops.aten.view.default(cat_54, [262144, 512]);  cat_54 = None
        mm_727 = torch.ops.aten.mm.default(view_3713, t_780);  view_3713 = t_780 = None
        view_3714 = torch.ops.aten.view.default(mm_727, [1, 512, 512, 256]);  mm_727 = None
        add_394 = torch.ops.aten.add.Tensor(add_393, view_3714);  add_393 = view_3714 = None
        split_tensor_391 = torch.ops.aten.split.Tensor(add_387, 512, dim = -2)
        getitem_3605 = split_tensor_391[0];  split_tensor_391 = None
        _to_copy_2088 = torch.ops.aten._to_copy.default(getitem_3605, dtype = torch.float32);  getitem_3605 = None
        native_layer_norm_default_430 = torch.ops.aten.native_layer_norm.default(_to_copy_2088, [256], arg1236_1, arg1237_1, 1e-05);  _to_copy_2088 = arg1236_1 = arg1237_1 = None
        getitem_3606 = native_layer_norm_default_430[0]
        _to_copy_2089 = torch.ops.aten._to_copy.default(arg1238_1, dtype = torch.bfloat16);  arg1238_1 = None
        _to_copy_2090 = torch.ops.aten._to_copy.default(getitem_3606, dtype = torch.bfloat16);  getitem_3606 = None
        t_781 = torch.ops.aten.t.default(_to_copy_2089);  _to_copy_2089 = None
        view_3715 = torch.ops.aten.view.default(_to_copy_2090, [262144, 256]);  _to_copy_2090 = None
        mm_728 = torch.ops.aten.mm.default(view_3715, t_781);  view_3715 = t_781 = None
        view_3716 = torch.ops.aten.view.default(mm_728, [1, 512, 512, 1024]);  mm_728 = None
        split_tensor_392 = torch.ops.aten.split.Tensor(view_3716, 512, dim = -1);  view_3716 = None
        getitem_3609 = split_tensor_392[0]
        getitem_3610 = split_tensor_392[1];  split_tensor_392 = None
        silu_102 = torch.ops.aten.silu.default(getitem_3609);  getitem_3609 = None
        mul_493 = torch.ops.aten.mul.Tensor(silu_102, getitem_3610);  silu_102 = getitem_3610 = None
        _to_copy_2091 = torch.ops.aten._to_copy.default(arg1239_1, dtype = torch.bfloat16);  arg1239_1 = None
        t_782 = torch.ops.aten.t.default(_to_copy_2091);  _to_copy_2091 = None
        view_3718 = torch.ops.aten.view.default(mul_493, [262144, 512]);  mul_493 = None
        mm_729 = torch.ops.aten.mm.default(view_3718, t_782);  view_3718 = t_782 = None
        view_3719 = torch.ops.aten.view.default(mm_729, [1, 512, 512, 256]);  mm_729 = None
        add_395 = torch.ops.aten.add.Tensor(add_394, view_3719);  add_394 = view_3719 = None
        _to_copy_2092 = torch.ops.aten._to_copy.default(add_391, dtype = torch.float32)
        native_layer_norm_default_431 = torch.ops.aten.native_layer_norm.default(_to_copy_2092, [384], arg1254_1, arg1255_1, 1e-05);  _to_copy_2092 = arg1254_1 = arg1255_1 = None
        getitem_3611 = native_layer_norm_default_431[0]
        _to_copy_2093 = torch.ops.aten._to_copy.default(add_387, dtype = torch.float32);  add_387 = None
        native_layer_norm_default_432 = torch.ops.aten.native_layer_norm.default(_to_copy_2093, [256], arg1256_1, arg1257_1, 1e-05);  _to_copy_2093 = arg1256_1 = arg1257_1 = None
        getitem_3614 = native_layer_norm_default_432[0]
        _to_copy_2094 = torch.ops.aten._to_copy.default(arg1258_1, dtype = torch.bfloat16);  arg1258_1 = None
        _to_copy_2095 = torch.ops.aten._to_copy.default(getitem_3614, dtype = torch.bfloat16);  getitem_3614 = None
        t_783 = torch.ops.aten.t.default(_to_copy_2094);  _to_copy_2094 = None
        view_3720 = torch.ops.aten.view.default(_to_copy_2095, [262144, 256]);  _to_copy_2095 = None
        mm_730 = torch.ops.aten.mm.default(view_3720, t_783);  view_3720 = t_783 = None
        view_3721 = torch.ops.aten.view.default(mm_730, [1, 512, 512, 16]);  mm_730 = None
        permute_1962 = torch.ops.aten.permute.default(view_3721, [0, 3, 1, 2]);  view_3721 = None
        view_3722 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_229 = torch.ops.aten.bitwise_not.default(view_3722);  view_3722 = None
        masked_fill_229 = torch.ops.aten.masked_fill.Scalar(permute_1962, bitwise_not_229, -10000);  permute_1962 = bitwise_not_229 = None
        _to_copy_2096 = torch.ops.aten._to_copy.default(getitem_3611, dtype = torch.bfloat16);  getitem_3611 = None
        _to_copy_2097 = torch.ops.aten._to_copy.default(arg1260_1, dtype = torch.bfloat16);  arg1260_1 = None
        unsqueeze_1181 = torch.ops.aten.unsqueeze.default(_to_copy_2096, 3);  _to_copy_2096 = None
        unsqueeze_1182 = torch.ops.aten.unsqueeze.default(unsqueeze_1181, 4);  unsqueeze_1181 = None
        unsqueeze_1183 = torch.ops.aten.unsqueeze.default(unsqueeze_1182, 5);  unsqueeze_1182 = None
        permute_1963 = torch.ops.aten.permute.default(unsqueeze_1183, [3, 0, 4, 1, 5, 2]);  unsqueeze_1183 = None
        unsqueeze_1184 = torch.ops.aten.unsqueeze.default(_to_copy_2097, 4);  _to_copy_2097 = None
        unsqueeze_1185 = torch.ops.aten.unsqueeze.default(unsqueeze_1184, 5);  unsqueeze_1184 = None
        permute_1964 = torch.ops.aten.permute.default(unsqueeze_1185, [1, 4, 2, 5, 3, 0]);  unsqueeze_1185 = None
        permute_1965 = torch.ops.aten.permute.default(permute_1963, [3, 5, 0, 1, 2, 4]);  permute_1963 = None
        view_3723 = torch.ops.aten.view.default(permute_1965, [1, 512, 384]);  permute_1965 = None
        permute_1966 = torch.ops.aten.permute.default(permute_1964, [5, 0, 1, 2, 4, 3]);  permute_1964 = None
        view_3724 = torch.ops.aten.view.default(permute_1966, [1, 384, 1536]);  permute_1966 = None
        bmm_291 = torch.ops.aten.bmm.default(view_3723, view_3724);  view_3723 = view_3724 = None
        view_3725 = torch.ops.aten.view.default(bmm_291, [512, 1, 4, 1, 16, 24]);  bmm_291 = None
        permute_1967 = torch.ops.aten.permute.default(view_3725, [2, 3, 4, 0, 5, 1]);  view_3725 = None
        view_3726 = torch.ops.aten.view.default(permute_1967, [4, 1, 16, 512, 24]);  permute_1967 = None
        unbind_int_168 = torch.ops.aten.unbind.int(view_3726);  view_3726 = None
        getitem_3617 = unbind_int_168[0]
        getitem_3618 = unbind_int_168[1]
        getitem_3619 = unbind_int_168[2]
        getitem_3620 = unbind_int_168[3];  unbind_int_168 = None
        view_3727 = torch.ops.aten.view.default(arg1259_1, [1, 16, 1, 24]);  arg1259_1 = None
        add_396 = torch.ops.aten.add.Tensor(getitem_3617, view_3727);  getitem_3617 = view_3727 = None
        _to_copy_2098 = torch.ops.aten._to_copy.default(add_396, dtype = torch.bfloat16);  add_396 = None
        expand_241 = torch.ops.aten.expand.default(masked_fill_229, [1, 16, 512, 512]);  masked_fill_229 = None
        _scaled_dot_product_efficient_attention_default_140 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_2098, getitem_3618, getitem_3619, expand_241, False);  _to_copy_2098 = getitem_3618 = getitem_3619 = expand_241 = None
        getitem_3621 = _scaled_dot_product_efficient_attention_default_140[0]
        add_397 = torch.ops.aten.add.Tensor(getitem_3620, 1);  getitem_3620 = None
        sigmoid_299 = torch.ops.aten.sigmoid.default(add_397);  add_397 = None
        mul_494 = torch.ops.aten.mul.Tensor(getitem_3621, sigmoid_299);  getitem_3621 = sigmoid_299 = None
        _to_copy_2099 = torch.ops.aten._to_copy.default(arg1261_1, dtype = torch.bfloat16);  arg1261_1 = None
        unsqueeze_1186 = torch.ops.aten.unsqueeze.default(mul_494, 4);  mul_494 = None
        permute_1968 = torch.ops.aten.permute.default(unsqueeze_1186, [0, 2, 4, 3, 1]);  unsqueeze_1186 = None
        unsqueeze_1187 = torch.ops.aten.unsqueeze.default(_to_copy_2099, 3);  _to_copy_2099 = None
        unsqueeze_1188 = torch.ops.aten.unsqueeze.default(unsqueeze_1187, 4);  unsqueeze_1187 = None
        permute_1969 = torch.ops.aten.permute.default(unsqueeze_1188, [3, 4, 2, 1, 0]);  unsqueeze_1188 = None
        permute_1970 = torch.ops.aten.permute.default(permute_1968, [1, 3, 4, 0, 2]);  permute_1968 = None
        clone_308 = torch.ops.aten.clone.default(permute_1970, memory_format = torch.contiguous_format);  permute_1970 = None
        _unsafe_view_258 = torch.ops.aten._unsafe_view.default(clone_308, [1, 512, 384]);  clone_308 = None
        permute_1971 = torch.ops.aten.permute.default(permute_1969, [3, 4, 0, 2, 1]);  permute_1969 = None
        clone_309 = torch.ops.aten.clone.default(permute_1971, memory_format = torch.contiguous_format);  permute_1971 = None
        _unsafe_view_259 = torch.ops.aten._unsafe_view.default(clone_309, [1, 384, 384]);  clone_309 = None
        bmm_292 = torch.ops.aten.bmm.default(_unsafe_view_258, _unsafe_view_259);  _unsafe_view_258 = _unsafe_view_259 = None
        view_3728 = torch.ops.aten.view.default(bmm_292, [512, 1, 1, 1, 384]);  bmm_292 = None
        permute_1972 = torch.ops.aten.permute.default(view_3728, [3, 0, 4, 1, 2]);  view_3728 = None
        view_3729 = torch.ops.aten.view.default(permute_1972, [1, 512, 384]);  permute_1972 = None
        unsqueeze_1189 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_495 = torch.ops.aten.mul.Tensor(view_3729, unsqueeze_1189);  view_3729 = unsqueeze_1189 = None
        add_398 = torch.ops.aten.add.Tensor(add_391, mul_495);  mul_495 = None
        split_tensor_393 = torch.ops.aten.split.Tensor(add_391, 512, dim = -2);  add_391 = None
        getitem_3625 = split_tensor_393[0];  split_tensor_393 = None
        _to_copy_2100 = torch.ops.aten._to_copy.default(getitem_3625, dtype = torch.float32);  getitem_3625 = None
        native_layer_norm_default_433 = torch.ops.aten.native_layer_norm.default(_to_copy_2100, [384], arg1250_1, arg1251_1, 1e-05);  _to_copy_2100 = arg1250_1 = arg1251_1 = None
        getitem_3626 = native_layer_norm_default_433[0]
        _to_copy_2101 = torch.ops.aten._to_copy.default(arg1252_1, dtype = torch.bfloat16);  arg1252_1 = None
        _to_copy_2102 = torch.ops.aten._to_copy.default(getitem_3626, dtype = torch.bfloat16);  getitem_3626 = None
        t_784 = torch.ops.aten.t.default(_to_copy_2101);  _to_copy_2101 = None
        view_3730 = torch.ops.aten.view.default(_to_copy_2102, [512, 384]);  _to_copy_2102 = None
        mm_731 = torch.ops.aten.mm.default(view_3730, t_784);  view_3730 = t_784 = None
        view_3731 = torch.ops.aten.view.default(mm_731, [1, 512, 1536]);  mm_731 = None
        split_tensor_394 = torch.ops.aten.split.Tensor(view_3731, 768, dim = -1);  view_3731 = None
        getitem_3629 = split_tensor_394[0]
        getitem_3630 = split_tensor_394[1];  split_tensor_394 = None
        silu_103 = torch.ops.aten.silu.default(getitem_3629);  getitem_3629 = None
        mul_496 = torch.ops.aten.mul.Tensor(silu_103, getitem_3630);  silu_103 = getitem_3630 = None
        _to_copy_2103 = torch.ops.aten._to_copy.default(arg1253_1, dtype = torch.bfloat16);  arg1253_1 = None
        t_785 = torch.ops.aten.t.default(_to_copy_2103);  _to_copy_2103 = None
        view_3733 = torch.ops.aten.view.default(mul_496, [512, 768]);  mul_496 = None
        mm_732 = torch.ops.aten.mm.default(view_3733, t_785);  view_3733 = t_785 = None
        view_3734 = torch.ops.aten.view.default(mm_732, [1, 512, 384]);  mm_732 = None
        add_399 = torch.ops.aten.add.Tensor(add_398, view_3734);  add_398 = view_3734 = None
        _to_copy_2104 = torch.ops.aten._to_copy.default(add_395, dtype = torch.float32)
        native_layer_norm_default_434 = torch.ops.aten.native_layer_norm.default(_to_copy_2104, [256], arg1266_1, arg1267_1, 1e-05);  _to_copy_2104 = arg1266_1 = arg1267_1 = None
        getitem_3631 = native_layer_norm_default_434[0]
        split_with_sizes_default_98 = torch.ops.aten.split_with_sizes.default(arg1269_1, [512, 512]);  arg1269_1 = None
        getitem_3634 = split_with_sizes_default_98[0]
        getitem_3635 = split_with_sizes_default_98[1];  split_with_sizes_default_98 = None
        split_with_sizes_default_99 = torch.ops.aten.split_with_sizes.default(arg1270_1, [512, 512, 256]);  arg1270_1 = None
        getitem_3636 = split_with_sizes_default_99[0]
        getitem_3637 = split_with_sizes_default_99[1]
        getitem_3638 = split_with_sizes_default_99[2];  split_with_sizes_default_99 = None
        _to_copy_2105 = torch.ops.aten._to_copy.default(getitem_3634, dtype = torch.bfloat16);  getitem_3634 = None
        _to_copy_2106 = torch.ops.aten._to_copy.default(getitem_3631, dtype = torch.bfloat16)
        t_786 = torch.ops.aten.t.default(_to_copy_2105);  _to_copy_2105 = None
        view_3735 = torch.ops.aten.view.default(_to_copy_2106, [262144, 256]);  _to_copy_2106 = None
        mm_733 = torch.ops.aten.mm.default(view_3735, t_786);  view_3735 = t_786 = None
        view_3736 = torch.ops.aten.view.default(mm_733, [1, 512, 512, 512]);  mm_733 = None
        _to_copy_2107 = torch.ops.aten._to_copy.default(getitem_3636, dtype = torch.bfloat16);  getitem_3636 = None
        _to_copy_2108 = torch.ops.aten._to_copy.default(getitem_3631, dtype = torch.bfloat16)
        t_787 = torch.ops.aten.t.default(_to_copy_2107);  _to_copy_2107 = None
        view_3737 = torch.ops.aten.view.default(_to_copy_2108, [262144, 256]);  _to_copy_2108 = None
        mm_734 = torch.ops.aten.mm.default(view_3737, t_787);  view_3737 = t_787 = None
        view_3738 = torch.ops.aten.view.default(mm_734, [1, 512, 512, 512]);  mm_734 = None
        sigmoid_300 = torch.ops.aten.sigmoid.default(view_3738);  view_3738 = None
        mul_497 = torch.ops.aten.mul.Tensor(view_3736, sigmoid_300);  view_3736 = sigmoid_300 = None
        unsqueeze_1190 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_230 = torch.ops.aten.bitwise_not.default(unsqueeze_1190);  unsqueeze_1190 = None
        masked_fill_230 = torch.ops.aten.masked_fill.Scalar(mul_497, bitwise_not_230, 0);  mul_497 = bitwise_not_230 = None
        split_tensor_395 = torch.ops.aten.split.Tensor(masked_fill_230, 256, dim = -1)
        getitem_3641 = split_tensor_395[0]
        unsqueeze_1193 = torch.ops.aten.unsqueeze.default(getitem_3641, 4);  getitem_3641 = None
        permute_1977 = torch.ops.aten.permute.default(unsqueeze_1193, [0, 1, 4, 3, 2]);  unsqueeze_1193 = None
        permute_1978 = torch.ops.aten.permute.default(permute_1977, [3, 1, 4, 0, 2]);  permute_1977 = None
        view_3741 = torch.ops.aten.view.default(permute_1978, [256, 512, 512]);  permute_1978 = None
        split_tensor_396 = torch.ops.aten.split.Tensor(masked_fill_230, 256, dim = -1);  masked_fill_230 = None
        getitem_3644 = split_tensor_396[1];  split_tensor_396 = None
        unsqueeze_1194 = torch.ops.aten.unsqueeze.default(getitem_3644, 4);  getitem_3644 = None
        permute_1979 = torch.ops.aten.permute.default(unsqueeze_1194, [0, 4, 1, 3, 2]);  unsqueeze_1194 = None
        permute_1980 = torch.ops.aten.permute.default(permute_1979, [3, 4, 0, 2, 1]);  permute_1979 = None
        view_3742 = torch.ops.aten.view.default(permute_1980, [256, 512, 512]);  permute_1980 = None
        bmm_293 = torch.ops.aten.bmm.default(view_3741, view_3742);  view_3741 = view_3742 = None
        view_3743 = torch.ops.aten.view.default(bmm_293, [256, 512, 1, 1, 512]);  bmm_293 = None
        permute_1981 = torch.ops.aten.permute.default(view_3743, [3, 1, 4, 0, 2]);  view_3743 = None
        view_3744 = torch.ops.aten.view.default(permute_1981, [1, 512, 512, 256]);  permute_1981 = None
        _to_copy_2109 = torch.ops.aten._to_copy.default(getitem_3635, dtype = torch.bfloat16);  getitem_3635 = None
        _to_copy_2110 = torch.ops.aten._to_copy.default(getitem_3631, dtype = torch.bfloat16)
        t_788 = torch.ops.aten.t.default(_to_copy_2109);  _to_copy_2109 = None
        view_3745 = torch.ops.aten.view.default(_to_copy_2110, [262144, 256]);  _to_copy_2110 = None
        mm_735 = torch.ops.aten.mm.default(view_3745, t_788);  view_3745 = t_788 = None
        view_3746 = torch.ops.aten.view.default(mm_735, [1, 512, 512, 512]);  mm_735 = None
        _to_copy_2111 = torch.ops.aten._to_copy.default(getitem_3637, dtype = torch.bfloat16);  getitem_3637 = None
        _to_copy_2112 = torch.ops.aten._to_copy.default(getitem_3631, dtype = torch.bfloat16)
        t_789 = torch.ops.aten.t.default(_to_copy_2111);  _to_copy_2111 = None
        view_3747 = torch.ops.aten.view.default(_to_copy_2112, [262144, 256]);  _to_copy_2112 = None
        mm_736 = torch.ops.aten.mm.default(view_3747, t_789);  view_3747 = t_789 = None
        view_3748 = torch.ops.aten.view.default(mm_736, [1, 512, 512, 512]);  mm_736 = None
        sigmoid_301 = torch.ops.aten.sigmoid.default(view_3748);  view_3748 = None
        mul_498 = torch.ops.aten.mul.Tensor(view_3746, sigmoid_301);  view_3746 = sigmoid_301 = None
        view_3749 = torch.ops.aten.view.default(mul_498, [262144, 512]);  mul_498 = None
        view_3750 = torch.ops.aten.view.default(view_3749, [1, 512, 512, 512]);  view_3749 = None
        transpose_98 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_1195 = torch.ops.aten.unsqueeze.default(transpose_98, 3);  transpose_98 = None
        clone_310 = torch.ops.aten.clone.default(unsqueeze_1195, memory_format = torch.contiguous_format);  unsqueeze_1195 = None
        bitwise_not_231 = torch.ops.aten.bitwise_not.default(clone_310);  clone_310 = None
        masked_fill_231 = torch.ops.aten.masked_fill.Scalar(view_3750, bitwise_not_231, 0);  view_3750 = bitwise_not_231 = None
        view_3751 = torch.ops.aten.view.default(masked_fill_231, [262144, 512]);  masked_fill_231 = None
        view_3755 = torch.ops.aten.view.default(view_3751, [1, 512, 512, 512])
        split_tensor_397 = torch.ops.aten.split.Tensor(view_3755, 256, dim = -1);  view_3755 = None
        getitem_3647 = split_tensor_397[0]
        unsqueeze_1198 = torch.ops.aten.unsqueeze.default(getitem_3647, 4);  getitem_3647 = None
        permute_1986 = torch.ops.aten.permute.default(unsqueeze_1198, [0, 2, 4, 3, 1]);  unsqueeze_1198 = None
        permute_1987 = torch.ops.aten.permute.default(permute_1986, [3, 1, 4, 0, 2]);  permute_1986 = None
        view_3756 = torch.ops.aten.view.default(permute_1987, [256, 512, 512]);  permute_1987 = None
        view_3757 = torch.ops.aten.view.default(view_3751, [1, 512, 512, 512]);  view_3751 = None
        split_tensor_398 = torch.ops.aten.split.Tensor(view_3757, 256, dim = -1);  view_3757 = None
        getitem_3650 = split_tensor_398[1];  split_tensor_398 = None
        unsqueeze_1199 = torch.ops.aten.unsqueeze.default(getitem_3650, 4);  getitem_3650 = None
        permute_1988 = torch.ops.aten.permute.default(unsqueeze_1199, [0, 4, 2, 3, 1]);  unsqueeze_1199 = None
        permute_1989 = torch.ops.aten.permute.default(permute_1988, [3, 4, 0, 2, 1]);  permute_1988 = None
        view_3758 = torch.ops.aten.view.default(permute_1989, [256, 512, 512]);  permute_1989 = None
        bmm_294 = torch.ops.aten.bmm.default(view_3756, view_3758);  view_3756 = view_3758 = None
        view_3759 = torch.ops.aten.view.default(bmm_294, [256, 512, 1, 1, 512]);  bmm_294 = None
        permute_1990 = torch.ops.aten.permute.default(view_3759, [3, 1, 4, 0, 2]);  view_3759 = None
        view_3760 = torch.ops.aten.view.default(permute_1990, [1, 512, 512, 256]);  permute_1990 = None
        _to_copy_2113 = torch.ops.aten._to_copy.default(view_3744, dtype = torch.float32);  view_3744 = None
        native_layer_norm_default_435 = torch.ops.aten.native_layer_norm.default(_to_copy_2113, [256], None, None, 1e-05);  _to_copy_2113 = None
        getitem_3651 = native_layer_norm_default_435[0]
        _to_copy_2114 = torch.ops.aten._to_copy.default(view_3760, dtype = torch.float32);  view_3760 = None
        native_layer_norm_default_436 = torch.ops.aten.native_layer_norm.default(_to_copy_2114, [256], None, None, 1e-05);  _to_copy_2114 = None
        getitem_3654 = native_layer_norm_default_436[0]
        add_400 = torch.ops.aten.add.Tensor(getitem_3651, getitem_3654);  getitem_3651 = getitem_3654 = None
        _to_copy_2115 = torch.ops.aten._to_copy.default(arg1268_1, dtype = torch.bfloat16);  arg1268_1 = None
        _to_copy_2116 = torch.ops.aten._to_copy.default(add_400, dtype = torch.bfloat16);  add_400 = None
        t_790 = torch.ops.aten.t.default(_to_copy_2115);  _to_copy_2115 = None
        view_3761 = torch.ops.aten.view.default(_to_copy_2116, [262144, 256]);  _to_copy_2116 = None
        mm_737 = torch.ops.aten.mm.default(view_3761, t_790);  view_3761 = t_790 = None
        view_3762 = torch.ops.aten.view.default(mm_737, [1, 512, 512, 256]);  mm_737 = None
        _to_copy_2117 = torch.ops.aten._to_copy.default(getitem_3638, dtype = torch.bfloat16);  getitem_3638 = None
        _to_copy_2118 = torch.ops.aten._to_copy.default(getitem_3631, dtype = torch.bfloat16);  getitem_3631 = None
        t_791 = torch.ops.aten.t.default(_to_copy_2117);  _to_copy_2117 = None
        view_3763 = torch.ops.aten.view.default(_to_copy_2118, [262144, 256]);  _to_copy_2118 = None
        mm_738 = torch.ops.aten.mm.default(view_3763, t_791);  view_3763 = t_791 = None
        view_3764 = torch.ops.aten.view.default(mm_738, [1, 512, 512, 256]);  mm_738 = None
        sigmoid_302 = torch.ops.aten.sigmoid.default(view_3764);  view_3764 = None
        mul_499 = torch.ops.aten.mul.Tensor(view_3762, sigmoid_302);  view_3762 = sigmoid_302 = None
        add_401 = torch.ops.aten.add.Tensor(add_395, mul_499);  mul_499 = None
        _to_copy_2119 = torch.ops.aten._to_copy.default(add_395, dtype = torch.float32)
        native_layer_norm_default_437 = torch.ops.aten.native_layer_norm.default(_to_copy_2119, [256], None, None, 1e-05);  _to_copy_2119 = None
        getitem_3657 = native_layer_norm_default_437[0]
        _to_copy_2120 = torch.ops.aten._to_copy.default(arg1272_1, dtype = torch.bfloat16);  arg1272_1 = None
        _to_copy_2121 = torch.ops.aten._to_copy.default(getitem_3657, dtype = torch.bfloat16)
        t_792 = torch.ops.aten.t.default(_to_copy_2120);  _to_copy_2120 = None
        view_3765 = torch.ops.aten.view.default(_to_copy_2121, [262144, 256]);  _to_copy_2121 = None
        mm_739 = torch.ops.aten.mm.default(view_3765, t_792);  view_3765 = t_792 = None
        view_3766 = torch.ops.aten.view.default(mm_739, [1, 512, 512, 8]);  mm_739 = None
        view_3767 = torch.ops.aten.view.default(view_3766, [1, 512, 512, 2, 4]);  view_3766 = None
        permute_1991 = torch.ops.aten.permute.default(view_3767, [0, 3, 4, 1, 2]);  view_3767 = None
        view_3768 = torch.ops.aten.view.default(permute_1991, [1, 2, 4, 1, 512, 512]);  permute_1991 = None
        view_3769 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_232 = torch.ops.aten.bitwise_not.default(view_3769);  view_3769 = None
        masked_fill_232 = torch.ops.aten.masked_fill.Scalar(view_3768, bitwise_not_232, -10000);  view_3768 = bitwise_not_232 = None
        view_3770 = torch.ops.aten.view.default(masked_fill_232, [1, 2, 4, 512, 512]);  masked_fill_232 = None
        permute_1992 = torch.ops.aten.permute.default(view_3770, [1, 0, 2, 3, 4]);  view_3770 = None
        view_3771 = torch.ops.aten.view.default(permute_1992, [2, 4, 1, 512, 512]);  permute_1992 = None
        _to_copy_2122 = torch.ops.aten._to_copy.default(arg1273_1, dtype = torch.bfloat16);  arg1273_1 = None
        _to_copy_2123 = torch.ops.aten._to_copy.default(getitem_3657, dtype = torch.bfloat16)
        t_793 = torch.ops.aten.t.default(_to_copy_2122);  _to_copy_2122 = None
        view_3772 = torch.ops.aten.view.default(_to_copy_2123, [262144, 256]);  _to_copy_2123 = None
        mm_740 = torch.ops.aten.mm.default(view_3772, t_793);  view_3772 = t_793 = None
        view_3773 = torch.ops.aten.view.default(mm_740, [1, 512, 512, 1024]);  mm_740 = None
        select_99 = torch.ops.aten.select.int(view_3771, 0, 0)
        view_3774 = torch.ops.aten.view.default(view_3773, [1, 512, 512, 4, 4, 64]);  view_3773 = None
        permute_1993 = torch.ops.aten.permute.default(view_3774, [4, 0, 3, 1, 2, 5]);  view_3774 = None
        view_3775 = torch.ops.aten.view.default(permute_1993, [4, 4, 512, 512, 64]);  permute_1993 = None
        unbind_int_169 = torch.ops.aten.unbind.int(view_3775);  view_3775 = None
        getitem_3660 = unbind_int_169[0]
        getitem_3661 = unbind_int_169[1]
        getitem_3662 = unbind_int_169[2]
        getitem_3663 = unbind_int_169[3];  unbind_int_169 = None
        expand_242 = torch.ops.aten.expand.default(select_99, [4, 512, 512, 512]);  select_99 = None
        _scaled_dot_product_efficient_attention_default_141 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3660, getitem_3661, getitem_3662, expand_242, False);  getitem_3660 = getitem_3661 = getitem_3662 = expand_242 = None
        getitem_3664 = _scaled_dot_product_efficient_attention_default_141[0]
        sigmoid_303 = torch.ops.aten.sigmoid.default(getitem_3663);  getitem_3663 = None
        mul_500 = torch.ops.aten.mul.Tensor(getitem_3664, sigmoid_303);  getitem_3664 = sigmoid_303 = None
        view_3776 = torch.ops.aten.view.default(mul_500, [1, 4, 512, 512, 64]);  mul_500 = None
        permute_1994 = torch.ops.aten.permute.default(view_3776, [0, 2, 3, 1, 4]);  view_3776 = None
        clone_311 = torch.ops.aten.clone.default(permute_1994, memory_format = torch.contiguous_format);  permute_1994 = None
        _unsafe_view_260 = torch.ops.aten._unsafe_view.default(clone_311, [1, 512, 512, 256]);  clone_311 = None
        transpose_99 = torch.ops.aten.transpose.int(getitem_3657, 1, 2);  getitem_3657 = None
        _to_copy_2124 = torch.ops.aten._to_copy.default(arg1274_1, dtype = torch.bfloat16);  arg1274_1 = None
        _to_copy_2125 = torch.ops.aten._to_copy.default(transpose_99, dtype = torch.bfloat16);  transpose_99 = None
        t_794 = torch.ops.aten.t.default(_to_copy_2124);  _to_copy_2124 = None
        expand_243 = torch.ops.aten.expand.default(_to_copy_2125, [1, 512, 512, 256]);  _to_copy_2125 = None
        view_3777 = torch.ops.aten.view.default(expand_243, [512, 512, 256]);  expand_243 = None
        expand_244 = torch.ops.aten.expand.default(t_794, [1, 512, 256, 1024]);  t_794 = None
        view_3778 = torch.ops.aten.view.default(expand_244, [512, 256, 1024]);  expand_244 = None
        bmm_295 = torch.ops.aten.bmm.default(view_3777, view_3778);  view_3777 = view_3778 = None
        view_3779 = torch.ops.aten.view.default(bmm_295, [1, 512, 512, 1024]);  bmm_295 = None
        select_100 = torch.ops.aten.select.int(view_3771, 0, 1);  view_3771 = None
        view_3780 = torch.ops.aten.view.default(view_3779, [1, 512, 512, 4, 4, 64]);  view_3779 = None
        permute_1995 = torch.ops.aten.permute.default(view_3780, [4, 0, 3, 1, 2, 5]);  view_3780 = None
        view_3781 = torch.ops.aten.view.default(permute_1995, [4, 4, 512, 512, 64]);  permute_1995 = None
        unbind_int_170 = torch.ops.aten.unbind.int(view_3781);  view_3781 = None
        getitem_3668 = unbind_int_170[0]
        getitem_3669 = unbind_int_170[1]
        getitem_3670 = unbind_int_170[2]
        getitem_3671 = unbind_int_170[3];  unbind_int_170 = None
        expand_245 = torch.ops.aten.expand.default(select_100, [4, 512, 512, 512]);  select_100 = None
        _scaled_dot_product_efficient_attention_default_142 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3668, getitem_3669, getitem_3670, expand_245, False);  getitem_3668 = getitem_3669 = getitem_3670 = expand_245 = None
        getitem_3672 = _scaled_dot_product_efficient_attention_default_142[0]
        sigmoid_304 = torch.ops.aten.sigmoid.default(getitem_3671);  getitem_3671 = None
        mul_501 = torch.ops.aten.mul.Tensor(getitem_3672, sigmoid_304);  getitem_3672 = sigmoid_304 = None
        view_3782 = torch.ops.aten.view.default(mul_501, [1, 4, 512, 512, 64]);  mul_501 = None
        permute_1996 = torch.ops.aten.permute.default(view_3782, [0, 2, 3, 1, 4]);  view_3782 = None
        clone_312 = torch.ops.aten.clone.default(permute_1996, memory_format = torch.contiguous_format);  permute_1996 = None
        _unsafe_view_261 = torch.ops.aten._unsafe_view.default(clone_312, [1, 512, 512, 256]);  clone_312 = None
        cat_55 = torch.ops.aten.cat.default([_unsafe_view_260, _unsafe_view_261], dim = -1);  _unsafe_view_260 = _unsafe_view_261 = None
        slice_248 = torch.ops.aten.slice.Tensor(arg1271_1, dim = 0, start = 0, end = 9223372036854775807);  arg1271_1 = None
        unsqueeze_1200 = torch.ops.aten.unsqueeze.default(slice_248, 1);  slice_248 = None
        mul_502 = torch.ops.aten.mul.Tensor(arg1275_1, unsqueeze_1200);  arg1275_1 = unsqueeze_1200 = None
        _to_copy_2126 = torch.ops.aten._to_copy.default(mul_502, dtype = torch.bfloat16);  mul_502 = None
        t_795 = torch.ops.aten.t.default(_to_copy_2126);  _to_copy_2126 = None
        view_3783 = torch.ops.aten.view.default(cat_55, [262144, 512]);  cat_55 = None
        mm_741 = torch.ops.aten.mm.default(view_3783, t_795);  view_3783 = t_795 = None
        view_3784 = torch.ops.aten.view.default(mm_741, [1, 512, 512, 256]);  mm_741 = None
        add_402 = torch.ops.aten.add.Tensor(add_401, view_3784);  add_401 = view_3784 = None
        split_tensor_399 = torch.ops.aten.split.Tensor(add_395, 512, dim = -2)
        getitem_3676 = split_tensor_399[0];  split_tensor_399 = None
        _to_copy_2127 = torch.ops.aten._to_copy.default(getitem_3676, dtype = torch.float32);  getitem_3676 = None
        native_layer_norm_default_438 = torch.ops.aten.native_layer_norm.default(_to_copy_2127, [256], arg1262_1, arg1263_1, 1e-05);  _to_copy_2127 = arg1262_1 = arg1263_1 = None
        getitem_3677 = native_layer_norm_default_438[0]
        _to_copy_2128 = torch.ops.aten._to_copy.default(arg1264_1, dtype = torch.bfloat16);  arg1264_1 = None
        _to_copy_2129 = torch.ops.aten._to_copy.default(getitem_3677, dtype = torch.bfloat16);  getitem_3677 = None
        t_796 = torch.ops.aten.t.default(_to_copy_2128);  _to_copy_2128 = None
        view_3785 = torch.ops.aten.view.default(_to_copy_2129, [262144, 256]);  _to_copy_2129 = None
        mm_742 = torch.ops.aten.mm.default(view_3785, t_796);  view_3785 = t_796 = None
        view_3786 = torch.ops.aten.view.default(mm_742, [1, 512, 512, 1024]);  mm_742 = None
        split_tensor_400 = torch.ops.aten.split.Tensor(view_3786, 512, dim = -1);  view_3786 = None
        getitem_3680 = split_tensor_400[0]
        getitem_3681 = split_tensor_400[1];  split_tensor_400 = None
        silu_104 = torch.ops.aten.silu.default(getitem_3680);  getitem_3680 = None
        mul_503 = torch.ops.aten.mul.Tensor(silu_104, getitem_3681);  silu_104 = getitem_3681 = None
        _to_copy_2130 = torch.ops.aten._to_copy.default(arg1265_1, dtype = torch.bfloat16);  arg1265_1 = None
        t_797 = torch.ops.aten.t.default(_to_copy_2130);  _to_copy_2130 = None
        view_3788 = torch.ops.aten.view.default(mul_503, [262144, 512]);  mul_503 = None
        mm_743 = torch.ops.aten.mm.default(view_3788, t_797);  view_3788 = t_797 = None
        view_3789 = torch.ops.aten.view.default(mm_743, [1, 512, 512, 256]);  mm_743 = None
        add_403 = torch.ops.aten.add.Tensor(add_402, view_3789);  add_402 = view_3789 = None
        _to_copy_2131 = torch.ops.aten._to_copy.default(add_399, dtype = torch.float32)
        native_layer_norm_default_439 = torch.ops.aten.native_layer_norm.default(_to_copy_2131, [384], arg1280_1, arg1281_1, 1e-05);  _to_copy_2131 = arg1280_1 = arg1281_1 = None
        getitem_3682 = native_layer_norm_default_439[0]
        _to_copy_2132 = torch.ops.aten._to_copy.default(add_395, dtype = torch.float32);  add_395 = None
        native_layer_norm_default_440 = torch.ops.aten.native_layer_norm.default(_to_copy_2132, [256], arg1282_1, arg1283_1, 1e-05);  _to_copy_2132 = arg1282_1 = arg1283_1 = None
        getitem_3685 = native_layer_norm_default_440[0]
        _to_copy_2133 = torch.ops.aten._to_copy.default(arg1284_1, dtype = torch.bfloat16);  arg1284_1 = None
        _to_copy_2134 = torch.ops.aten._to_copy.default(getitem_3685, dtype = torch.bfloat16);  getitem_3685 = None
        t_798 = torch.ops.aten.t.default(_to_copy_2133);  _to_copy_2133 = None
        view_3790 = torch.ops.aten.view.default(_to_copy_2134, [262144, 256]);  _to_copy_2134 = None
        mm_744 = torch.ops.aten.mm.default(view_3790, t_798);  view_3790 = t_798 = None
        view_3791 = torch.ops.aten.view.default(mm_744, [1, 512, 512, 16]);  mm_744 = None
        permute_1997 = torch.ops.aten.permute.default(view_3791, [0, 3, 1, 2]);  view_3791 = None
        view_3792 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_233 = torch.ops.aten.bitwise_not.default(view_3792);  view_3792 = None
        masked_fill_233 = torch.ops.aten.masked_fill.Scalar(permute_1997, bitwise_not_233, -10000);  permute_1997 = bitwise_not_233 = None
        _to_copy_2135 = torch.ops.aten._to_copy.default(getitem_3682, dtype = torch.bfloat16);  getitem_3682 = None
        _to_copy_2136 = torch.ops.aten._to_copy.default(arg1286_1, dtype = torch.bfloat16);  arg1286_1 = None
        unsqueeze_1201 = torch.ops.aten.unsqueeze.default(_to_copy_2135, 3);  _to_copy_2135 = None
        unsqueeze_1202 = torch.ops.aten.unsqueeze.default(unsqueeze_1201, 4);  unsqueeze_1201 = None
        unsqueeze_1203 = torch.ops.aten.unsqueeze.default(unsqueeze_1202, 5);  unsqueeze_1202 = None
        permute_1998 = torch.ops.aten.permute.default(unsqueeze_1203, [3, 0, 4, 1, 5, 2]);  unsqueeze_1203 = None
        unsqueeze_1204 = torch.ops.aten.unsqueeze.default(_to_copy_2136, 4);  _to_copy_2136 = None
        unsqueeze_1205 = torch.ops.aten.unsqueeze.default(unsqueeze_1204, 5);  unsqueeze_1204 = None
        permute_1999 = torch.ops.aten.permute.default(unsqueeze_1205, [1, 4, 2, 5, 3, 0]);  unsqueeze_1205 = None
        permute_2000 = torch.ops.aten.permute.default(permute_1998, [3, 5, 0, 1, 2, 4]);  permute_1998 = None
        view_3793 = torch.ops.aten.view.default(permute_2000, [1, 512, 384]);  permute_2000 = None
        permute_2001 = torch.ops.aten.permute.default(permute_1999, [5, 0, 1, 2, 4, 3]);  permute_1999 = None
        view_3794 = torch.ops.aten.view.default(permute_2001, [1, 384, 1536]);  permute_2001 = None
        bmm_296 = torch.ops.aten.bmm.default(view_3793, view_3794);  view_3793 = view_3794 = None
        view_3795 = torch.ops.aten.view.default(bmm_296, [512, 1, 4, 1, 16, 24]);  bmm_296 = None
        permute_2002 = torch.ops.aten.permute.default(view_3795, [2, 3, 4, 0, 5, 1]);  view_3795 = None
        view_3796 = torch.ops.aten.view.default(permute_2002, [4, 1, 16, 512, 24]);  permute_2002 = None
        unbind_int_171 = torch.ops.aten.unbind.int(view_3796);  view_3796 = None
        getitem_3688 = unbind_int_171[0]
        getitem_3689 = unbind_int_171[1]
        getitem_3690 = unbind_int_171[2]
        getitem_3691 = unbind_int_171[3];  unbind_int_171 = None
        view_3797 = torch.ops.aten.view.default(arg1285_1, [1, 16, 1, 24]);  arg1285_1 = None
        add_404 = torch.ops.aten.add.Tensor(getitem_3688, view_3797);  getitem_3688 = view_3797 = None
        _to_copy_2137 = torch.ops.aten._to_copy.default(add_404, dtype = torch.bfloat16);  add_404 = None
        expand_246 = torch.ops.aten.expand.default(masked_fill_233, [1, 16, 512, 512]);  masked_fill_233 = None
        _scaled_dot_product_efficient_attention_default_143 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_2137, getitem_3689, getitem_3690, expand_246, False);  _to_copy_2137 = getitem_3689 = getitem_3690 = expand_246 = None
        getitem_3692 = _scaled_dot_product_efficient_attention_default_143[0]
        add_405 = torch.ops.aten.add.Tensor(getitem_3691, 1);  getitem_3691 = None
        sigmoid_305 = torch.ops.aten.sigmoid.default(add_405);  add_405 = None
        mul_504 = torch.ops.aten.mul.Tensor(getitem_3692, sigmoid_305);  getitem_3692 = sigmoid_305 = None
        _to_copy_2138 = torch.ops.aten._to_copy.default(arg1287_1, dtype = torch.bfloat16);  arg1287_1 = None
        unsqueeze_1206 = torch.ops.aten.unsqueeze.default(mul_504, 4);  mul_504 = None
        permute_2003 = torch.ops.aten.permute.default(unsqueeze_1206, [0, 2, 4, 3, 1]);  unsqueeze_1206 = None
        unsqueeze_1207 = torch.ops.aten.unsqueeze.default(_to_copy_2138, 3);  _to_copy_2138 = None
        unsqueeze_1208 = torch.ops.aten.unsqueeze.default(unsqueeze_1207, 4);  unsqueeze_1207 = None
        permute_2004 = torch.ops.aten.permute.default(unsqueeze_1208, [3, 4, 2, 1, 0]);  unsqueeze_1208 = None
        permute_2005 = torch.ops.aten.permute.default(permute_2003, [1, 3, 4, 0, 2]);  permute_2003 = None
        clone_313 = torch.ops.aten.clone.default(permute_2005, memory_format = torch.contiguous_format);  permute_2005 = None
        _unsafe_view_262 = torch.ops.aten._unsafe_view.default(clone_313, [1, 512, 384]);  clone_313 = None
        permute_2006 = torch.ops.aten.permute.default(permute_2004, [3, 4, 0, 2, 1]);  permute_2004 = None
        clone_314 = torch.ops.aten.clone.default(permute_2006, memory_format = torch.contiguous_format);  permute_2006 = None
        _unsafe_view_263 = torch.ops.aten._unsafe_view.default(clone_314, [1, 384, 384]);  clone_314 = None
        bmm_297 = torch.ops.aten.bmm.default(_unsafe_view_262, _unsafe_view_263);  _unsafe_view_262 = _unsafe_view_263 = None
        view_3798 = torch.ops.aten.view.default(bmm_297, [512, 1, 1, 1, 384]);  bmm_297 = None
        permute_2007 = torch.ops.aten.permute.default(view_3798, [3, 0, 4, 1, 2]);  view_3798 = None
        view_3799 = torch.ops.aten.view.default(permute_2007, [1, 512, 384]);  permute_2007 = None
        unsqueeze_1209 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_505 = torch.ops.aten.mul.Tensor(view_3799, unsqueeze_1209);  view_3799 = unsqueeze_1209 = None
        add_406 = torch.ops.aten.add.Tensor(add_399, mul_505);  mul_505 = None
        split_tensor_401 = torch.ops.aten.split.Tensor(add_399, 512, dim = -2);  add_399 = None
        getitem_3696 = split_tensor_401[0];  split_tensor_401 = None
        _to_copy_2139 = torch.ops.aten._to_copy.default(getitem_3696, dtype = torch.float32);  getitem_3696 = None
        native_layer_norm_default_441 = torch.ops.aten.native_layer_norm.default(_to_copy_2139, [384], arg1276_1, arg1277_1, 1e-05);  _to_copy_2139 = arg1276_1 = arg1277_1 = None
        getitem_3697 = native_layer_norm_default_441[0]
        _to_copy_2140 = torch.ops.aten._to_copy.default(arg1278_1, dtype = torch.bfloat16);  arg1278_1 = None
        _to_copy_2141 = torch.ops.aten._to_copy.default(getitem_3697, dtype = torch.bfloat16);  getitem_3697 = None
        t_799 = torch.ops.aten.t.default(_to_copy_2140);  _to_copy_2140 = None
        view_3800 = torch.ops.aten.view.default(_to_copy_2141, [512, 384]);  _to_copy_2141 = None
        mm_745 = torch.ops.aten.mm.default(view_3800, t_799);  view_3800 = t_799 = None
        view_3801 = torch.ops.aten.view.default(mm_745, [1, 512, 1536]);  mm_745 = None
        split_tensor_402 = torch.ops.aten.split.Tensor(view_3801, 768, dim = -1);  view_3801 = None
        getitem_3700 = split_tensor_402[0]
        getitem_3701 = split_tensor_402[1];  split_tensor_402 = None
        silu_105 = torch.ops.aten.silu.default(getitem_3700);  getitem_3700 = None
        mul_506 = torch.ops.aten.mul.Tensor(silu_105, getitem_3701);  silu_105 = getitem_3701 = None
        _to_copy_2142 = torch.ops.aten._to_copy.default(arg1279_1, dtype = torch.bfloat16);  arg1279_1 = None
        t_800 = torch.ops.aten.t.default(_to_copy_2142);  _to_copy_2142 = None
        view_3803 = torch.ops.aten.view.default(mul_506, [512, 768]);  mul_506 = None
        mm_746 = torch.ops.aten.mm.default(view_3803, t_800);  view_3803 = t_800 = None
        view_3804 = torch.ops.aten.view.default(mm_746, [1, 512, 384]);  mm_746 = None
        add_407 = torch.ops.aten.add.Tensor(add_406, view_3804);  add_406 = view_3804 = None
        _to_copy_2143 = torch.ops.aten._to_copy.default(add_403, dtype = torch.float32)
        native_layer_norm_default_442 = torch.ops.aten.native_layer_norm.default(_to_copy_2143, [256], arg1292_1, arg1293_1, 1e-05);  _to_copy_2143 = arg1292_1 = arg1293_1 = None
        getitem_3702 = native_layer_norm_default_442[0]
        split_with_sizes_default_100 = torch.ops.aten.split_with_sizes.default(arg1295_1, [512, 512]);  arg1295_1 = None
        getitem_3705 = split_with_sizes_default_100[0]
        getitem_3706 = split_with_sizes_default_100[1];  split_with_sizes_default_100 = None
        split_with_sizes_default_101 = torch.ops.aten.split_with_sizes.default(arg1296_1, [512, 512, 256]);  arg1296_1 = None
        getitem_3707 = split_with_sizes_default_101[0]
        getitem_3708 = split_with_sizes_default_101[1]
        getitem_3709 = split_with_sizes_default_101[2];  split_with_sizes_default_101 = None
        _to_copy_2144 = torch.ops.aten._to_copy.default(getitem_3705, dtype = torch.bfloat16);  getitem_3705 = None
        _to_copy_2145 = torch.ops.aten._to_copy.default(getitem_3702, dtype = torch.bfloat16)
        t_801 = torch.ops.aten.t.default(_to_copy_2144);  _to_copy_2144 = None
        view_3805 = torch.ops.aten.view.default(_to_copy_2145, [262144, 256]);  _to_copy_2145 = None
        mm_747 = torch.ops.aten.mm.default(view_3805, t_801);  view_3805 = t_801 = None
        view_3806 = torch.ops.aten.view.default(mm_747, [1, 512, 512, 512]);  mm_747 = None
        _to_copy_2146 = torch.ops.aten._to_copy.default(getitem_3707, dtype = torch.bfloat16);  getitem_3707 = None
        _to_copy_2147 = torch.ops.aten._to_copy.default(getitem_3702, dtype = torch.bfloat16)
        t_802 = torch.ops.aten.t.default(_to_copy_2146);  _to_copy_2146 = None
        view_3807 = torch.ops.aten.view.default(_to_copy_2147, [262144, 256]);  _to_copy_2147 = None
        mm_748 = torch.ops.aten.mm.default(view_3807, t_802);  view_3807 = t_802 = None
        view_3808 = torch.ops.aten.view.default(mm_748, [1, 512, 512, 512]);  mm_748 = None
        sigmoid_306 = torch.ops.aten.sigmoid.default(view_3808);  view_3808 = None
        mul_507 = torch.ops.aten.mul.Tensor(view_3806, sigmoid_306);  view_3806 = sigmoid_306 = None
        unsqueeze_1210 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_234 = torch.ops.aten.bitwise_not.default(unsqueeze_1210);  unsqueeze_1210 = None
        masked_fill_234 = torch.ops.aten.masked_fill.Scalar(mul_507, bitwise_not_234, 0);  mul_507 = bitwise_not_234 = None
        split_tensor_403 = torch.ops.aten.split.Tensor(masked_fill_234, 256, dim = -1)
        getitem_3712 = split_tensor_403[0]
        unsqueeze_1213 = torch.ops.aten.unsqueeze.default(getitem_3712, 4);  getitem_3712 = None
        permute_2012 = torch.ops.aten.permute.default(unsqueeze_1213, [0, 1, 4, 3, 2]);  unsqueeze_1213 = None
        permute_2013 = torch.ops.aten.permute.default(permute_2012, [3, 1, 4, 0, 2]);  permute_2012 = None
        view_3811 = torch.ops.aten.view.default(permute_2013, [256, 512, 512]);  permute_2013 = None
        split_tensor_404 = torch.ops.aten.split.Tensor(masked_fill_234, 256, dim = -1);  masked_fill_234 = None
        getitem_3715 = split_tensor_404[1];  split_tensor_404 = None
        unsqueeze_1214 = torch.ops.aten.unsqueeze.default(getitem_3715, 4);  getitem_3715 = None
        permute_2014 = torch.ops.aten.permute.default(unsqueeze_1214, [0, 4, 1, 3, 2]);  unsqueeze_1214 = None
        permute_2015 = torch.ops.aten.permute.default(permute_2014, [3, 4, 0, 2, 1]);  permute_2014 = None
        view_3812 = torch.ops.aten.view.default(permute_2015, [256, 512, 512]);  permute_2015 = None
        bmm_298 = torch.ops.aten.bmm.default(view_3811, view_3812);  view_3811 = view_3812 = None
        view_3813 = torch.ops.aten.view.default(bmm_298, [256, 512, 1, 1, 512]);  bmm_298 = None
        permute_2016 = torch.ops.aten.permute.default(view_3813, [3, 1, 4, 0, 2]);  view_3813 = None
        view_3814 = torch.ops.aten.view.default(permute_2016, [1, 512, 512, 256]);  permute_2016 = None
        _to_copy_2148 = torch.ops.aten._to_copy.default(getitem_3706, dtype = torch.bfloat16);  getitem_3706 = None
        _to_copy_2149 = torch.ops.aten._to_copy.default(getitem_3702, dtype = torch.bfloat16)
        t_803 = torch.ops.aten.t.default(_to_copy_2148);  _to_copy_2148 = None
        view_3815 = torch.ops.aten.view.default(_to_copy_2149, [262144, 256]);  _to_copy_2149 = None
        mm_749 = torch.ops.aten.mm.default(view_3815, t_803);  view_3815 = t_803 = None
        view_3816 = torch.ops.aten.view.default(mm_749, [1, 512, 512, 512]);  mm_749 = None
        _to_copy_2150 = torch.ops.aten._to_copy.default(getitem_3708, dtype = torch.bfloat16);  getitem_3708 = None
        _to_copy_2151 = torch.ops.aten._to_copy.default(getitem_3702, dtype = torch.bfloat16)
        t_804 = torch.ops.aten.t.default(_to_copy_2150);  _to_copy_2150 = None
        view_3817 = torch.ops.aten.view.default(_to_copy_2151, [262144, 256]);  _to_copy_2151 = None
        mm_750 = torch.ops.aten.mm.default(view_3817, t_804);  view_3817 = t_804 = None
        view_3818 = torch.ops.aten.view.default(mm_750, [1, 512, 512, 512]);  mm_750 = None
        sigmoid_307 = torch.ops.aten.sigmoid.default(view_3818);  view_3818 = None
        mul_508 = torch.ops.aten.mul.Tensor(view_3816, sigmoid_307);  view_3816 = sigmoid_307 = None
        view_3819 = torch.ops.aten.view.default(mul_508, [262144, 512]);  mul_508 = None
        view_3820 = torch.ops.aten.view.default(view_3819, [1, 512, 512, 512]);  view_3819 = None
        transpose_100 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_1215 = torch.ops.aten.unsqueeze.default(transpose_100, 3);  transpose_100 = None
        clone_315 = torch.ops.aten.clone.default(unsqueeze_1215, memory_format = torch.contiguous_format);  unsqueeze_1215 = None
        bitwise_not_235 = torch.ops.aten.bitwise_not.default(clone_315);  clone_315 = None
        masked_fill_235 = torch.ops.aten.masked_fill.Scalar(view_3820, bitwise_not_235, 0);  view_3820 = bitwise_not_235 = None
        view_3821 = torch.ops.aten.view.default(masked_fill_235, [262144, 512]);  masked_fill_235 = None
        view_3825 = torch.ops.aten.view.default(view_3821, [1, 512, 512, 512])
        split_tensor_405 = torch.ops.aten.split.Tensor(view_3825, 256, dim = -1);  view_3825 = None
        getitem_3718 = split_tensor_405[0]
        unsqueeze_1218 = torch.ops.aten.unsqueeze.default(getitem_3718, 4);  getitem_3718 = None
        permute_2021 = torch.ops.aten.permute.default(unsqueeze_1218, [0, 2, 4, 3, 1]);  unsqueeze_1218 = None
        permute_2022 = torch.ops.aten.permute.default(permute_2021, [3, 1, 4, 0, 2]);  permute_2021 = None
        view_3826 = torch.ops.aten.view.default(permute_2022, [256, 512, 512]);  permute_2022 = None
        view_3827 = torch.ops.aten.view.default(view_3821, [1, 512, 512, 512]);  view_3821 = None
        split_tensor_406 = torch.ops.aten.split.Tensor(view_3827, 256, dim = -1);  view_3827 = None
        getitem_3721 = split_tensor_406[1];  split_tensor_406 = None
        unsqueeze_1219 = torch.ops.aten.unsqueeze.default(getitem_3721, 4);  getitem_3721 = None
        permute_2023 = torch.ops.aten.permute.default(unsqueeze_1219, [0, 4, 2, 3, 1]);  unsqueeze_1219 = None
        permute_2024 = torch.ops.aten.permute.default(permute_2023, [3, 4, 0, 2, 1]);  permute_2023 = None
        view_3828 = torch.ops.aten.view.default(permute_2024, [256, 512, 512]);  permute_2024 = None
        bmm_299 = torch.ops.aten.bmm.default(view_3826, view_3828);  view_3826 = view_3828 = None
        view_3829 = torch.ops.aten.view.default(bmm_299, [256, 512, 1, 1, 512]);  bmm_299 = None
        permute_2025 = torch.ops.aten.permute.default(view_3829, [3, 1, 4, 0, 2]);  view_3829 = None
        view_3830 = torch.ops.aten.view.default(permute_2025, [1, 512, 512, 256]);  permute_2025 = None
        _to_copy_2152 = torch.ops.aten._to_copy.default(view_3814, dtype = torch.float32);  view_3814 = None
        native_layer_norm_default_443 = torch.ops.aten.native_layer_norm.default(_to_copy_2152, [256], None, None, 1e-05);  _to_copy_2152 = None
        getitem_3722 = native_layer_norm_default_443[0]
        _to_copy_2153 = torch.ops.aten._to_copy.default(view_3830, dtype = torch.float32);  view_3830 = None
        native_layer_norm_default_444 = torch.ops.aten.native_layer_norm.default(_to_copy_2153, [256], None, None, 1e-05);  _to_copy_2153 = None
        getitem_3725 = native_layer_norm_default_444[0]
        add_408 = torch.ops.aten.add.Tensor(getitem_3722, getitem_3725);  getitem_3722 = getitem_3725 = None
        _to_copy_2154 = torch.ops.aten._to_copy.default(arg1294_1, dtype = torch.bfloat16);  arg1294_1 = None
        _to_copy_2155 = torch.ops.aten._to_copy.default(add_408, dtype = torch.bfloat16);  add_408 = None
        t_805 = torch.ops.aten.t.default(_to_copy_2154);  _to_copy_2154 = None
        view_3831 = torch.ops.aten.view.default(_to_copy_2155, [262144, 256]);  _to_copy_2155 = None
        mm_751 = torch.ops.aten.mm.default(view_3831, t_805);  view_3831 = t_805 = None
        view_3832 = torch.ops.aten.view.default(mm_751, [1, 512, 512, 256]);  mm_751 = None
        _to_copy_2156 = torch.ops.aten._to_copy.default(getitem_3709, dtype = torch.bfloat16);  getitem_3709 = None
        _to_copy_2157 = torch.ops.aten._to_copy.default(getitem_3702, dtype = torch.bfloat16);  getitem_3702 = None
        t_806 = torch.ops.aten.t.default(_to_copy_2156);  _to_copy_2156 = None
        view_3833 = torch.ops.aten.view.default(_to_copy_2157, [262144, 256]);  _to_copy_2157 = None
        mm_752 = torch.ops.aten.mm.default(view_3833, t_806);  view_3833 = t_806 = None
        view_3834 = torch.ops.aten.view.default(mm_752, [1, 512, 512, 256]);  mm_752 = None
        sigmoid_308 = torch.ops.aten.sigmoid.default(view_3834);  view_3834 = None
        mul_509 = torch.ops.aten.mul.Tensor(view_3832, sigmoid_308);  view_3832 = sigmoid_308 = None
        add_409 = torch.ops.aten.add.Tensor(add_403, mul_509);  mul_509 = None
        _to_copy_2158 = torch.ops.aten._to_copy.default(add_403, dtype = torch.float32)
        native_layer_norm_default_445 = torch.ops.aten.native_layer_norm.default(_to_copy_2158, [256], None, None, 1e-05);  _to_copy_2158 = None
        getitem_3728 = native_layer_norm_default_445[0]
        _to_copy_2159 = torch.ops.aten._to_copy.default(arg1298_1, dtype = torch.bfloat16);  arg1298_1 = None
        _to_copy_2160 = torch.ops.aten._to_copy.default(getitem_3728, dtype = torch.bfloat16)
        t_807 = torch.ops.aten.t.default(_to_copy_2159);  _to_copy_2159 = None
        view_3835 = torch.ops.aten.view.default(_to_copy_2160, [262144, 256]);  _to_copy_2160 = None
        mm_753 = torch.ops.aten.mm.default(view_3835, t_807);  view_3835 = t_807 = None
        view_3836 = torch.ops.aten.view.default(mm_753, [1, 512, 512, 8]);  mm_753 = None
        view_3837 = torch.ops.aten.view.default(view_3836, [1, 512, 512, 2, 4]);  view_3836 = None
        permute_2026 = torch.ops.aten.permute.default(view_3837, [0, 3, 4, 1, 2]);  view_3837 = None
        view_3838 = torch.ops.aten.view.default(permute_2026, [1, 2, 4, 1, 512, 512]);  permute_2026 = None
        view_3839 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_236 = torch.ops.aten.bitwise_not.default(view_3839);  view_3839 = None
        masked_fill_236 = torch.ops.aten.masked_fill.Scalar(view_3838, bitwise_not_236, -10000);  view_3838 = bitwise_not_236 = None
        view_3840 = torch.ops.aten.view.default(masked_fill_236, [1, 2, 4, 512, 512]);  masked_fill_236 = None
        permute_2027 = torch.ops.aten.permute.default(view_3840, [1, 0, 2, 3, 4]);  view_3840 = None
        view_3841 = torch.ops.aten.view.default(permute_2027, [2, 4, 1, 512, 512]);  permute_2027 = None
        _to_copy_2161 = torch.ops.aten._to_copy.default(arg1299_1, dtype = torch.bfloat16);  arg1299_1 = None
        _to_copy_2162 = torch.ops.aten._to_copy.default(getitem_3728, dtype = torch.bfloat16)
        t_808 = torch.ops.aten.t.default(_to_copy_2161);  _to_copy_2161 = None
        view_3842 = torch.ops.aten.view.default(_to_copy_2162, [262144, 256]);  _to_copy_2162 = None
        mm_754 = torch.ops.aten.mm.default(view_3842, t_808);  view_3842 = t_808 = None
        view_3843 = torch.ops.aten.view.default(mm_754, [1, 512, 512, 1024]);  mm_754 = None
        select_101 = torch.ops.aten.select.int(view_3841, 0, 0)
        view_3844 = torch.ops.aten.view.default(view_3843, [1, 512, 512, 4, 4, 64]);  view_3843 = None
        permute_2028 = torch.ops.aten.permute.default(view_3844, [4, 0, 3, 1, 2, 5]);  view_3844 = None
        view_3845 = torch.ops.aten.view.default(permute_2028, [4, 4, 512, 512, 64]);  permute_2028 = None
        unbind_int_172 = torch.ops.aten.unbind.int(view_3845);  view_3845 = None
        getitem_3731 = unbind_int_172[0]
        getitem_3732 = unbind_int_172[1]
        getitem_3733 = unbind_int_172[2]
        getitem_3734 = unbind_int_172[3];  unbind_int_172 = None
        expand_247 = torch.ops.aten.expand.default(select_101, [4, 512, 512, 512]);  select_101 = None
        _scaled_dot_product_efficient_attention_default_144 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3731, getitem_3732, getitem_3733, expand_247, False);  getitem_3731 = getitem_3732 = getitem_3733 = expand_247 = None
        getitem_3735 = _scaled_dot_product_efficient_attention_default_144[0]
        sigmoid_309 = torch.ops.aten.sigmoid.default(getitem_3734);  getitem_3734 = None
        mul_510 = torch.ops.aten.mul.Tensor(getitem_3735, sigmoid_309);  getitem_3735 = sigmoid_309 = None
        view_3846 = torch.ops.aten.view.default(mul_510, [1, 4, 512, 512, 64]);  mul_510 = None
        permute_2029 = torch.ops.aten.permute.default(view_3846, [0, 2, 3, 1, 4]);  view_3846 = None
        clone_316 = torch.ops.aten.clone.default(permute_2029, memory_format = torch.contiguous_format);  permute_2029 = None
        _unsafe_view_264 = torch.ops.aten._unsafe_view.default(clone_316, [1, 512, 512, 256]);  clone_316 = None
        transpose_101 = torch.ops.aten.transpose.int(getitem_3728, 1, 2);  getitem_3728 = None
        _to_copy_2163 = torch.ops.aten._to_copy.default(arg1300_1, dtype = torch.bfloat16);  arg1300_1 = None
        _to_copy_2164 = torch.ops.aten._to_copy.default(transpose_101, dtype = torch.bfloat16);  transpose_101 = None
        t_809 = torch.ops.aten.t.default(_to_copy_2163);  _to_copy_2163 = None
        expand_248 = torch.ops.aten.expand.default(_to_copy_2164, [1, 512, 512, 256]);  _to_copy_2164 = None
        view_3847 = torch.ops.aten.view.default(expand_248, [512, 512, 256]);  expand_248 = None
        expand_249 = torch.ops.aten.expand.default(t_809, [1, 512, 256, 1024]);  t_809 = None
        view_3848 = torch.ops.aten.view.default(expand_249, [512, 256, 1024]);  expand_249 = None
        bmm_300 = torch.ops.aten.bmm.default(view_3847, view_3848);  view_3847 = view_3848 = None
        view_3849 = torch.ops.aten.view.default(bmm_300, [1, 512, 512, 1024]);  bmm_300 = None
        select_102 = torch.ops.aten.select.int(view_3841, 0, 1);  view_3841 = None
        view_3850 = torch.ops.aten.view.default(view_3849, [1, 512, 512, 4, 4, 64]);  view_3849 = None
        permute_2030 = torch.ops.aten.permute.default(view_3850, [4, 0, 3, 1, 2, 5]);  view_3850 = None
        view_3851 = torch.ops.aten.view.default(permute_2030, [4, 4, 512, 512, 64]);  permute_2030 = None
        unbind_int_173 = torch.ops.aten.unbind.int(view_3851);  view_3851 = None
        getitem_3739 = unbind_int_173[0]
        getitem_3740 = unbind_int_173[1]
        getitem_3741 = unbind_int_173[2]
        getitem_3742 = unbind_int_173[3];  unbind_int_173 = None
        expand_250 = torch.ops.aten.expand.default(select_102, [4, 512, 512, 512]);  select_102 = None
        _scaled_dot_product_efficient_attention_default_145 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3739, getitem_3740, getitem_3741, expand_250, False);  getitem_3739 = getitem_3740 = getitem_3741 = expand_250 = None
        getitem_3743 = _scaled_dot_product_efficient_attention_default_145[0]
        sigmoid_310 = torch.ops.aten.sigmoid.default(getitem_3742);  getitem_3742 = None
        mul_511 = torch.ops.aten.mul.Tensor(getitem_3743, sigmoid_310);  getitem_3743 = sigmoid_310 = None
        view_3852 = torch.ops.aten.view.default(mul_511, [1, 4, 512, 512, 64]);  mul_511 = None
        permute_2031 = torch.ops.aten.permute.default(view_3852, [0, 2, 3, 1, 4]);  view_3852 = None
        clone_317 = torch.ops.aten.clone.default(permute_2031, memory_format = torch.contiguous_format);  permute_2031 = None
        _unsafe_view_265 = torch.ops.aten._unsafe_view.default(clone_317, [1, 512, 512, 256]);  clone_317 = None
        cat_56 = torch.ops.aten.cat.default([_unsafe_view_264, _unsafe_view_265], dim = -1);  _unsafe_view_264 = _unsafe_view_265 = None
        slice_249 = torch.ops.aten.slice.Tensor(arg1297_1, dim = 0, start = 0, end = 9223372036854775807);  arg1297_1 = None
        unsqueeze_1220 = torch.ops.aten.unsqueeze.default(slice_249, 1);  slice_249 = None
        mul_512 = torch.ops.aten.mul.Tensor(arg1301_1, unsqueeze_1220);  arg1301_1 = unsqueeze_1220 = None
        _to_copy_2165 = torch.ops.aten._to_copy.default(mul_512, dtype = torch.bfloat16);  mul_512 = None
        t_810 = torch.ops.aten.t.default(_to_copy_2165);  _to_copy_2165 = None
        view_3853 = torch.ops.aten.view.default(cat_56, [262144, 512]);  cat_56 = None
        mm_755 = torch.ops.aten.mm.default(view_3853, t_810);  view_3853 = t_810 = None
        view_3854 = torch.ops.aten.view.default(mm_755, [1, 512, 512, 256]);  mm_755 = None
        add_410 = torch.ops.aten.add.Tensor(add_409, view_3854);  add_409 = view_3854 = None
        split_tensor_407 = torch.ops.aten.split.Tensor(add_403, 512, dim = -2)
        getitem_3747 = split_tensor_407[0];  split_tensor_407 = None
        _to_copy_2166 = torch.ops.aten._to_copy.default(getitem_3747, dtype = torch.float32);  getitem_3747 = None
        native_layer_norm_default_446 = torch.ops.aten.native_layer_norm.default(_to_copy_2166, [256], arg1288_1, arg1289_1, 1e-05);  _to_copy_2166 = arg1288_1 = arg1289_1 = None
        getitem_3748 = native_layer_norm_default_446[0]
        _to_copy_2167 = torch.ops.aten._to_copy.default(arg1290_1, dtype = torch.bfloat16);  arg1290_1 = None
        _to_copy_2168 = torch.ops.aten._to_copy.default(getitem_3748, dtype = torch.bfloat16);  getitem_3748 = None
        t_811 = torch.ops.aten.t.default(_to_copy_2167);  _to_copy_2167 = None
        view_3855 = torch.ops.aten.view.default(_to_copy_2168, [262144, 256]);  _to_copy_2168 = None
        mm_756 = torch.ops.aten.mm.default(view_3855, t_811);  view_3855 = t_811 = None
        view_3856 = torch.ops.aten.view.default(mm_756, [1, 512, 512, 1024]);  mm_756 = None
        split_tensor_408 = torch.ops.aten.split.Tensor(view_3856, 512, dim = -1);  view_3856 = None
        getitem_3751 = split_tensor_408[0]
        getitem_3752 = split_tensor_408[1];  split_tensor_408 = None
        silu_106 = torch.ops.aten.silu.default(getitem_3751);  getitem_3751 = None
        mul_513 = torch.ops.aten.mul.Tensor(silu_106, getitem_3752);  silu_106 = getitem_3752 = None
        _to_copy_2169 = torch.ops.aten._to_copy.default(arg1291_1, dtype = torch.bfloat16);  arg1291_1 = None
        t_812 = torch.ops.aten.t.default(_to_copy_2169);  _to_copy_2169 = None
        view_3858 = torch.ops.aten.view.default(mul_513, [262144, 512]);  mul_513 = None
        mm_757 = torch.ops.aten.mm.default(view_3858, t_812);  view_3858 = t_812 = None
        view_3859 = torch.ops.aten.view.default(mm_757, [1, 512, 512, 256]);  mm_757 = None
        add_411 = torch.ops.aten.add.Tensor(add_410, view_3859);  add_410 = view_3859 = None
        _to_copy_2170 = torch.ops.aten._to_copy.default(add_407, dtype = torch.float32)
        native_layer_norm_default_447 = torch.ops.aten.native_layer_norm.default(_to_copy_2170, [384], arg1306_1, arg1307_1, 1e-05);  _to_copy_2170 = arg1306_1 = arg1307_1 = None
        getitem_3753 = native_layer_norm_default_447[0]
        _to_copy_2171 = torch.ops.aten._to_copy.default(add_403, dtype = torch.float32);  add_403 = None
        native_layer_norm_default_448 = torch.ops.aten.native_layer_norm.default(_to_copy_2171, [256], arg1308_1, arg1309_1, 1e-05);  _to_copy_2171 = arg1308_1 = arg1309_1 = None
        getitem_3756 = native_layer_norm_default_448[0]
        _to_copy_2172 = torch.ops.aten._to_copy.default(arg1310_1, dtype = torch.bfloat16);  arg1310_1 = None
        _to_copy_2173 = torch.ops.aten._to_copy.default(getitem_3756, dtype = torch.bfloat16);  getitem_3756 = None
        t_813 = torch.ops.aten.t.default(_to_copy_2172);  _to_copy_2172 = None
        view_3860 = torch.ops.aten.view.default(_to_copy_2173, [262144, 256]);  _to_copy_2173 = None
        mm_758 = torch.ops.aten.mm.default(view_3860, t_813);  view_3860 = t_813 = None
        view_3861 = torch.ops.aten.view.default(mm_758, [1, 512, 512, 16]);  mm_758 = None
        permute_2032 = torch.ops.aten.permute.default(view_3861, [0, 3, 1, 2]);  view_3861 = None
        view_3862 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_237 = torch.ops.aten.bitwise_not.default(view_3862);  view_3862 = None
        masked_fill_237 = torch.ops.aten.masked_fill.Scalar(permute_2032, bitwise_not_237, -10000);  permute_2032 = bitwise_not_237 = None
        _to_copy_2174 = torch.ops.aten._to_copy.default(getitem_3753, dtype = torch.bfloat16);  getitem_3753 = None
        _to_copy_2175 = torch.ops.aten._to_copy.default(arg1312_1, dtype = torch.bfloat16);  arg1312_1 = None
        unsqueeze_1221 = torch.ops.aten.unsqueeze.default(_to_copy_2174, 3);  _to_copy_2174 = None
        unsqueeze_1222 = torch.ops.aten.unsqueeze.default(unsqueeze_1221, 4);  unsqueeze_1221 = None
        unsqueeze_1223 = torch.ops.aten.unsqueeze.default(unsqueeze_1222, 5);  unsqueeze_1222 = None
        permute_2033 = torch.ops.aten.permute.default(unsqueeze_1223, [3, 0, 4, 1, 5, 2]);  unsqueeze_1223 = None
        unsqueeze_1224 = torch.ops.aten.unsqueeze.default(_to_copy_2175, 4);  _to_copy_2175 = None
        unsqueeze_1225 = torch.ops.aten.unsqueeze.default(unsqueeze_1224, 5);  unsqueeze_1224 = None
        permute_2034 = torch.ops.aten.permute.default(unsqueeze_1225, [1, 4, 2, 5, 3, 0]);  unsqueeze_1225 = None
        permute_2035 = torch.ops.aten.permute.default(permute_2033, [3, 5, 0, 1, 2, 4]);  permute_2033 = None
        view_3863 = torch.ops.aten.view.default(permute_2035, [1, 512, 384]);  permute_2035 = None
        permute_2036 = torch.ops.aten.permute.default(permute_2034, [5, 0, 1, 2, 4, 3]);  permute_2034 = None
        view_3864 = torch.ops.aten.view.default(permute_2036, [1, 384, 1536]);  permute_2036 = None
        bmm_301 = torch.ops.aten.bmm.default(view_3863, view_3864);  view_3863 = view_3864 = None
        view_3865 = torch.ops.aten.view.default(bmm_301, [512, 1, 4, 1, 16, 24]);  bmm_301 = None
        permute_2037 = torch.ops.aten.permute.default(view_3865, [2, 3, 4, 0, 5, 1]);  view_3865 = None
        view_3866 = torch.ops.aten.view.default(permute_2037, [4, 1, 16, 512, 24]);  permute_2037 = None
        unbind_int_174 = torch.ops.aten.unbind.int(view_3866);  view_3866 = None
        getitem_3759 = unbind_int_174[0]
        getitem_3760 = unbind_int_174[1]
        getitem_3761 = unbind_int_174[2]
        getitem_3762 = unbind_int_174[3];  unbind_int_174 = None
        view_3867 = torch.ops.aten.view.default(arg1311_1, [1, 16, 1, 24]);  arg1311_1 = None
        add_412 = torch.ops.aten.add.Tensor(getitem_3759, view_3867);  getitem_3759 = view_3867 = None
        _to_copy_2176 = torch.ops.aten._to_copy.default(add_412, dtype = torch.bfloat16);  add_412 = None
        expand_251 = torch.ops.aten.expand.default(masked_fill_237, [1, 16, 512, 512]);  masked_fill_237 = None
        _scaled_dot_product_efficient_attention_default_146 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_2176, getitem_3760, getitem_3761, expand_251, False);  _to_copy_2176 = getitem_3760 = getitem_3761 = expand_251 = None
        getitem_3763 = _scaled_dot_product_efficient_attention_default_146[0]
        add_413 = torch.ops.aten.add.Tensor(getitem_3762, 1);  getitem_3762 = None
        sigmoid_311 = torch.ops.aten.sigmoid.default(add_413);  add_413 = None
        mul_514 = torch.ops.aten.mul.Tensor(getitem_3763, sigmoid_311);  getitem_3763 = sigmoid_311 = None
        _to_copy_2177 = torch.ops.aten._to_copy.default(arg1313_1, dtype = torch.bfloat16);  arg1313_1 = None
        unsqueeze_1226 = torch.ops.aten.unsqueeze.default(mul_514, 4);  mul_514 = None
        permute_2038 = torch.ops.aten.permute.default(unsqueeze_1226, [0, 2, 4, 3, 1]);  unsqueeze_1226 = None
        unsqueeze_1227 = torch.ops.aten.unsqueeze.default(_to_copy_2177, 3);  _to_copy_2177 = None
        unsqueeze_1228 = torch.ops.aten.unsqueeze.default(unsqueeze_1227, 4);  unsqueeze_1227 = None
        permute_2039 = torch.ops.aten.permute.default(unsqueeze_1228, [3, 4, 2, 1, 0]);  unsqueeze_1228 = None
        permute_2040 = torch.ops.aten.permute.default(permute_2038, [1, 3, 4, 0, 2]);  permute_2038 = None
        clone_318 = torch.ops.aten.clone.default(permute_2040, memory_format = torch.contiguous_format);  permute_2040 = None
        _unsafe_view_266 = torch.ops.aten._unsafe_view.default(clone_318, [1, 512, 384]);  clone_318 = None
        permute_2041 = torch.ops.aten.permute.default(permute_2039, [3, 4, 0, 2, 1]);  permute_2039 = None
        clone_319 = torch.ops.aten.clone.default(permute_2041, memory_format = torch.contiguous_format);  permute_2041 = None
        _unsafe_view_267 = torch.ops.aten._unsafe_view.default(clone_319, [1, 384, 384]);  clone_319 = None
        bmm_302 = torch.ops.aten.bmm.default(_unsafe_view_266, _unsafe_view_267);  _unsafe_view_266 = _unsafe_view_267 = None
        view_3868 = torch.ops.aten.view.default(bmm_302, [512, 1, 1, 1, 384]);  bmm_302 = None
        permute_2042 = torch.ops.aten.permute.default(view_3868, [3, 0, 4, 1, 2]);  view_3868 = None
        view_3869 = torch.ops.aten.view.default(permute_2042, [1, 512, 384]);  permute_2042 = None
        unsqueeze_1229 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_515 = torch.ops.aten.mul.Tensor(view_3869, unsqueeze_1229);  view_3869 = unsqueeze_1229 = None
        add_414 = torch.ops.aten.add.Tensor(add_407, mul_515);  mul_515 = None
        split_tensor_409 = torch.ops.aten.split.Tensor(add_407, 512, dim = -2);  add_407 = None
        getitem_3767 = split_tensor_409[0];  split_tensor_409 = None
        _to_copy_2178 = torch.ops.aten._to_copy.default(getitem_3767, dtype = torch.float32);  getitem_3767 = None
        native_layer_norm_default_449 = torch.ops.aten.native_layer_norm.default(_to_copy_2178, [384], arg1302_1, arg1303_1, 1e-05);  _to_copy_2178 = arg1302_1 = arg1303_1 = None
        getitem_3768 = native_layer_norm_default_449[0]
        _to_copy_2179 = torch.ops.aten._to_copy.default(arg1304_1, dtype = torch.bfloat16);  arg1304_1 = None
        _to_copy_2180 = torch.ops.aten._to_copy.default(getitem_3768, dtype = torch.bfloat16);  getitem_3768 = None
        t_814 = torch.ops.aten.t.default(_to_copy_2179);  _to_copy_2179 = None
        view_3870 = torch.ops.aten.view.default(_to_copy_2180, [512, 384]);  _to_copy_2180 = None
        mm_759 = torch.ops.aten.mm.default(view_3870, t_814);  view_3870 = t_814 = None
        view_3871 = torch.ops.aten.view.default(mm_759, [1, 512, 1536]);  mm_759 = None
        split_tensor_410 = torch.ops.aten.split.Tensor(view_3871, 768, dim = -1);  view_3871 = None
        getitem_3771 = split_tensor_410[0]
        getitem_3772 = split_tensor_410[1];  split_tensor_410 = None
        silu_107 = torch.ops.aten.silu.default(getitem_3771);  getitem_3771 = None
        mul_516 = torch.ops.aten.mul.Tensor(silu_107, getitem_3772);  silu_107 = getitem_3772 = None
        _to_copy_2181 = torch.ops.aten._to_copy.default(arg1305_1, dtype = torch.bfloat16);  arg1305_1 = None
        t_815 = torch.ops.aten.t.default(_to_copy_2181);  _to_copy_2181 = None
        view_3873 = torch.ops.aten.view.default(mul_516, [512, 768]);  mul_516 = None
        mm_760 = torch.ops.aten.mm.default(view_3873, t_815);  view_3873 = t_815 = None
        view_3874 = torch.ops.aten.view.default(mm_760, [1, 512, 384]);  mm_760 = None
        add_415 = torch.ops.aten.add.Tensor(add_414, view_3874);  add_414 = view_3874 = None
        _to_copy_2182 = torch.ops.aten._to_copy.default(add_411, dtype = torch.float32)
        native_layer_norm_default_450 = torch.ops.aten.native_layer_norm.default(_to_copy_2182, [256], arg1318_1, arg1319_1, 1e-05);  _to_copy_2182 = arg1318_1 = arg1319_1 = None
        getitem_3773 = native_layer_norm_default_450[0]
        split_with_sizes_default_102 = torch.ops.aten.split_with_sizes.default(arg1321_1, [512, 512]);  arg1321_1 = None
        getitem_3776 = split_with_sizes_default_102[0]
        getitem_3777 = split_with_sizes_default_102[1];  split_with_sizes_default_102 = None
        split_with_sizes_default_103 = torch.ops.aten.split_with_sizes.default(arg1322_1, [512, 512, 256]);  arg1322_1 = None
        getitem_3778 = split_with_sizes_default_103[0]
        getitem_3779 = split_with_sizes_default_103[1]
        getitem_3780 = split_with_sizes_default_103[2];  split_with_sizes_default_103 = None
        _to_copy_2183 = torch.ops.aten._to_copy.default(getitem_3776, dtype = torch.bfloat16);  getitem_3776 = None
        _to_copy_2184 = torch.ops.aten._to_copy.default(getitem_3773, dtype = torch.bfloat16)
        t_816 = torch.ops.aten.t.default(_to_copy_2183);  _to_copy_2183 = None
        view_3875 = torch.ops.aten.view.default(_to_copy_2184, [262144, 256]);  _to_copy_2184 = None
        mm_761 = torch.ops.aten.mm.default(view_3875, t_816);  view_3875 = t_816 = None
        view_3876 = torch.ops.aten.view.default(mm_761, [1, 512, 512, 512]);  mm_761 = None
        _to_copy_2185 = torch.ops.aten._to_copy.default(getitem_3778, dtype = torch.bfloat16);  getitem_3778 = None
        _to_copy_2186 = torch.ops.aten._to_copy.default(getitem_3773, dtype = torch.bfloat16)
        t_817 = torch.ops.aten.t.default(_to_copy_2185);  _to_copy_2185 = None
        view_3877 = torch.ops.aten.view.default(_to_copy_2186, [262144, 256]);  _to_copy_2186 = None
        mm_762 = torch.ops.aten.mm.default(view_3877, t_817);  view_3877 = t_817 = None
        view_3878 = torch.ops.aten.view.default(mm_762, [1, 512, 512, 512]);  mm_762 = None
        sigmoid_312 = torch.ops.aten.sigmoid.default(view_3878);  view_3878 = None
        mul_517 = torch.ops.aten.mul.Tensor(view_3876, sigmoid_312);  view_3876 = sigmoid_312 = None
        unsqueeze_1230 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_238 = torch.ops.aten.bitwise_not.default(unsqueeze_1230);  unsqueeze_1230 = None
        masked_fill_238 = torch.ops.aten.masked_fill.Scalar(mul_517, bitwise_not_238, 0);  mul_517 = bitwise_not_238 = None
        split_tensor_411 = torch.ops.aten.split.Tensor(masked_fill_238, 256, dim = -1)
        getitem_3783 = split_tensor_411[0]
        unsqueeze_1233 = torch.ops.aten.unsqueeze.default(getitem_3783, 4);  getitem_3783 = None
        permute_2047 = torch.ops.aten.permute.default(unsqueeze_1233, [0, 1, 4, 3, 2]);  unsqueeze_1233 = None
        permute_2048 = torch.ops.aten.permute.default(permute_2047, [3, 1, 4, 0, 2]);  permute_2047 = None
        view_3881 = torch.ops.aten.view.default(permute_2048, [256, 512, 512]);  permute_2048 = None
        split_tensor_412 = torch.ops.aten.split.Tensor(masked_fill_238, 256, dim = -1);  masked_fill_238 = None
        getitem_3786 = split_tensor_412[1];  split_tensor_412 = None
        unsqueeze_1234 = torch.ops.aten.unsqueeze.default(getitem_3786, 4);  getitem_3786 = None
        permute_2049 = torch.ops.aten.permute.default(unsqueeze_1234, [0, 4, 1, 3, 2]);  unsqueeze_1234 = None
        permute_2050 = torch.ops.aten.permute.default(permute_2049, [3, 4, 0, 2, 1]);  permute_2049 = None
        view_3882 = torch.ops.aten.view.default(permute_2050, [256, 512, 512]);  permute_2050 = None
        bmm_303 = torch.ops.aten.bmm.default(view_3881, view_3882);  view_3881 = view_3882 = None
        view_3883 = torch.ops.aten.view.default(bmm_303, [256, 512, 1, 1, 512]);  bmm_303 = None
        permute_2051 = torch.ops.aten.permute.default(view_3883, [3, 1, 4, 0, 2]);  view_3883 = None
        view_3884 = torch.ops.aten.view.default(permute_2051, [1, 512, 512, 256]);  permute_2051 = None
        _to_copy_2187 = torch.ops.aten._to_copy.default(getitem_3777, dtype = torch.bfloat16);  getitem_3777 = None
        _to_copy_2188 = torch.ops.aten._to_copy.default(getitem_3773, dtype = torch.bfloat16)
        t_818 = torch.ops.aten.t.default(_to_copy_2187);  _to_copy_2187 = None
        view_3885 = torch.ops.aten.view.default(_to_copy_2188, [262144, 256]);  _to_copy_2188 = None
        mm_763 = torch.ops.aten.mm.default(view_3885, t_818);  view_3885 = t_818 = None
        view_3886 = torch.ops.aten.view.default(mm_763, [1, 512, 512, 512]);  mm_763 = None
        _to_copy_2189 = torch.ops.aten._to_copy.default(getitem_3779, dtype = torch.bfloat16);  getitem_3779 = None
        _to_copy_2190 = torch.ops.aten._to_copy.default(getitem_3773, dtype = torch.bfloat16)
        t_819 = torch.ops.aten.t.default(_to_copy_2189);  _to_copy_2189 = None
        view_3887 = torch.ops.aten.view.default(_to_copy_2190, [262144, 256]);  _to_copy_2190 = None
        mm_764 = torch.ops.aten.mm.default(view_3887, t_819);  view_3887 = t_819 = None
        view_3888 = torch.ops.aten.view.default(mm_764, [1, 512, 512, 512]);  mm_764 = None
        sigmoid_313 = torch.ops.aten.sigmoid.default(view_3888);  view_3888 = None
        mul_518 = torch.ops.aten.mul.Tensor(view_3886, sigmoid_313);  view_3886 = sigmoid_313 = None
        view_3889 = torch.ops.aten.view.default(mul_518, [262144, 512]);  mul_518 = None
        view_3890 = torch.ops.aten.view.default(view_3889, [1, 512, 512, 512]);  view_3889 = None
        transpose_102 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_1235 = torch.ops.aten.unsqueeze.default(transpose_102, 3);  transpose_102 = None
        clone_320 = torch.ops.aten.clone.default(unsqueeze_1235, memory_format = torch.contiguous_format);  unsqueeze_1235 = None
        bitwise_not_239 = torch.ops.aten.bitwise_not.default(clone_320);  clone_320 = None
        masked_fill_239 = torch.ops.aten.masked_fill.Scalar(view_3890, bitwise_not_239, 0);  view_3890 = bitwise_not_239 = None
        view_3891 = torch.ops.aten.view.default(masked_fill_239, [262144, 512]);  masked_fill_239 = None
        view_3895 = torch.ops.aten.view.default(view_3891, [1, 512, 512, 512])
        split_tensor_413 = torch.ops.aten.split.Tensor(view_3895, 256, dim = -1);  view_3895 = None
        getitem_3789 = split_tensor_413[0]
        unsqueeze_1238 = torch.ops.aten.unsqueeze.default(getitem_3789, 4);  getitem_3789 = None
        permute_2056 = torch.ops.aten.permute.default(unsqueeze_1238, [0, 2, 4, 3, 1]);  unsqueeze_1238 = None
        permute_2057 = torch.ops.aten.permute.default(permute_2056, [3, 1, 4, 0, 2]);  permute_2056 = None
        view_3896 = torch.ops.aten.view.default(permute_2057, [256, 512, 512]);  permute_2057 = None
        view_3897 = torch.ops.aten.view.default(view_3891, [1, 512, 512, 512]);  view_3891 = None
        split_tensor_414 = torch.ops.aten.split.Tensor(view_3897, 256, dim = -1);  view_3897 = None
        getitem_3792 = split_tensor_414[1];  split_tensor_414 = None
        unsqueeze_1239 = torch.ops.aten.unsqueeze.default(getitem_3792, 4);  getitem_3792 = None
        permute_2058 = torch.ops.aten.permute.default(unsqueeze_1239, [0, 4, 2, 3, 1]);  unsqueeze_1239 = None
        permute_2059 = torch.ops.aten.permute.default(permute_2058, [3, 4, 0, 2, 1]);  permute_2058 = None
        view_3898 = torch.ops.aten.view.default(permute_2059, [256, 512, 512]);  permute_2059 = None
        bmm_304 = torch.ops.aten.bmm.default(view_3896, view_3898);  view_3896 = view_3898 = None
        view_3899 = torch.ops.aten.view.default(bmm_304, [256, 512, 1, 1, 512]);  bmm_304 = None
        permute_2060 = torch.ops.aten.permute.default(view_3899, [3, 1, 4, 0, 2]);  view_3899 = None
        view_3900 = torch.ops.aten.view.default(permute_2060, [1, 512, 512, 256]);  permute_2060 = None
        _to_copy_2191 = torch.ops.aten._to_copy.default(view_3884, dtype = torch.float32);  view_3884 = None
        native_layer_norm_default_451 = torch.ops.aten.native_layer_norm.default(_to_copy_2191, [256], None, None, 1e-05);  _to_copy_2191 = None
        getitem_3793 = native_layer_norm_default_451[0]
        _to_copy_2192 = torch.ops.aten._to_copy.default(view_3900, dtype = torch.float32);  view_3900 = None
        native_layer_norm_default_452 = torch.ops.aten.native_layer_norm.default(_to_copy_2192, [256], None, None, 1e-05);  _to_copy_2192 = None
        getitem_3796 = native_layer_norm_default_452[0]
        add_416 = torch.ops.aten.add.Tensor(getitem_3793, getitem_3796);  getitem_3793 = getitem_3796 = None
        _to_copy_2193 = torch.ops.aten._to_copy.default(arg1320_1, dtype = torch.bfloat16);  arg1320_1 = None
        _to_copy_2194 = torch.ops.aten._to_copy.default(add_416, dtype = torch.bfloat16);  add_416 = None
        t_820 = torch.ops.aten.t.default(_to_copy_2193);  _to_copy_2193 = None
        view_3901 = torch.ops.aten.view.default(_to_copy_2194, [262144, 256]);  _to_copy_2194 = None
        mm_765 = torch.ops.aten.mm.default(view_3901, t_820);  view_3901 = t_820 = None
        view_3902 = torch.ops.aten.view.default(mm_765, [1, 512, 512, 256]);  mm_765 = None
        _to_copy_2195 = torch.ops.aten._to_copy.default(getitem_3780, dtype = torch.bfloat16);  getitem_3780 = None
        _to_copy_2196 = torch.ops.aten._to_copy.default(getitem_3773, dtype = torch.bfloat16);  getitem_3773 = None
        t_821 = torch.ops.aten.t.default(_to_copy_2195);  _to_copy_2195 = None
        view_3903 = torch.ops.aten.view.default(_to_copy_2196, [262144, 256]);  _to_copy_2196 = None
        mm_766 = torch.ops.aten.mm.default(view_3903, t_821);  view_3903 = t_821 = None
        view_3904 = torch.ops.aten.view.default(mm_766, [1, 512, 512, 256]);  mm_766 = None
        sigmoid_314 = torch.ops.aten.sigmoid.default(view_3904);  view_3904 = None
        mul_519 = torch.ops.aten.mul.Tensor(view_3902, sigmoid_314);  view_3902 = sigmoid_314 = None
        add_417 = torch.ops.aten.add.Tensor(add_411, mul_519);  mul_519 = None
        _to_copy_2197 = torch.ops.aten._to_copy.default(add_411, dtype = torch.float32)
        native_layer_norm_default_453 = torch.ops.aten.native_layer_norm.default(_to_copy_2197, [256], None, None, 1e-05);  _to_copy_2197 = None
        getitem_3799 = native_layer_norm_default_453[0]
        _to_copy_2198 = torch.ops.aten._to_copy.default(arg1324_1, dtype = torch.bfloat16);  arg1324_1 = None
        _to_copy_2199 = torch.ops.aten._to_copy.default(getitem_3799, dtype = torch.bfloat16)
        t_822 = torch.ops.aten.t.default(_to_copy_2198);  _to_copy_2198 = None
        view_3905 = torch.ops.aten.view.default(_to_copy_2199, [262144, 256]);  _to_copy_2199 = None
        mm_767 = torch.ops.aten.mm.default(view_3905, t_822);  view_3905 = t_822 = None
        view_3906 = torch.ops.aten.view.default(mm_767, [1, 512, 512, 8]);  mm_767 = None
        view_3907 = torch.ops.aten.view.default(view_3906, [1, 512, 512, 2, 4]);  view_3906 = None
        permute_2061 = torch.ops.aten.permute.default(view_3907, [0, 3, 4, 1, 2]);  view_3907 = None
        view_3908 = torch.ops.aten.view.default(permute_2061, [1, 2, 4, 1, 512, 512]);  permute_2061 = None
        view_3909 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_240 = torch.ops.aten.bitwise_not.default(view_3909);  view_3909 = None
        masked_fill_240 = torch.ops.aten.masked_fill.Scalar(view_3908, bitwise_not_240, -10000);  view_3908 = bitwise_not_240 = None
        view_3910 = torch.ops.aten.view.default(masked_fill_240, [1, 2, 4, 512, 512]);  masked_fill_240 = None
        permute_2062 = torch.ops.aten.permute.default(view_3910, [1, 0, 2, 3, 4]);  view_3910 = None
        view_3911 = torch.ops.aten.view.default(permute_2062, [2, 4, 1, 512, 512]);  permute_2062 = None
        _to_copy_2200 = torch.ops.aten._to_copy.default(arg1325_1, dtype = torch.bfloat16);  arg1325_1 = None
        _to_copy_2201 = torch.ops.aten._to_copy.default(getitem_3799, dtype = torch.bfloat16)
        t_823 = torch.ops.aten.t.default(_to_copy_2200);  _to_copy_2200 = None
        view_3912 = torch.ops.aten.view.default(_to_copy_2201, [262144, 256]);  _to_copy_2201 = None
        mm_768 = torch.ops.aten.mm.default(view_3912, t_823);  view_3912 = t_823 = None
        view_3913 = torch.ops.aten.view.default(mm_768, [1, 512, 512, 1024]);  mm_768 = None
        select_103 = torch.ops.aten.select.int(view_3911, 0, 0)
        view_3914 = torch.ops.aten.view.default(view_3913, [1, 512, 512, 4, 4, 64]);  view_3913 = None
        permute_2063 = torch.ops.aten.permute.default(view_3914, [4, 0, 3, 1, 2, 5]);  view_3914 = None
        view_3915 = torch.ops.aten.view.default(permute_2063, [4, 4, 512, 512, 64]);  permute_2063 = None
        unbind_int_175 = torch.ops.aten.unbind.int(view_3915);  view_3915 = None
        getitem_3802 = unbind_int_175[0]
        getitem_3803 = unbind_int_175[1]
        getitem_3804 = unbind_int_175[2]
        getitem_3805 = unbind_int_175[3];  unbind_int_175 = None
        expand_252 = torch.ops.aten.expand.default(select_103, [4, 512, 512, 512]);  select_103 = None
        _scaled_dot_product_efficient_attention_default_147 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3802, getitem_3803, getitem_3804, expand_252, False);  getitem_3802 = getitem_3803 = getitem_3804 = expand_252 = None
        getitem_3806 = _scaled_dot_product_efficient_attention_default_147[0]
        sigmoid_315 = torch.ops.aten.sigmoid.default(getitem_3805);  getitem_3805 = None
        mul_520 = torch.ops.aten.mul.Tensor(getitem_3806, sigmoid_315);  getitem_3806 = sigmoid_315 = None
        view_3916 = torch.ops.aten.view.default(mul_520, [1, 4, 512, 512, 64]);  mul_520 = None
        permute_2064 = torch.ops.aten.permute.default(view_3916, [0, 2, 3, 1, 4]);  view_3916 = None
        clone_321 = torch.ops.aten.clone.default(permute_2064, memory_format = torch.contiguous_format);  permute_2064 = None
        _unsafe_view_268 = torch.ops.aten._unsafe_view.default(clone_321, [1, 512, 512, 256]);  clone_321 = None
        transpose_103 = torch.ops.aten.transpose.int(getitem_3799, 1, 2);  getitem_3799 = None
        _to_copy_2202 = torch.ops.aten._to_copy.default(arg1326_1, dtype = torch.bfloat16);  arg1326_1 = None
        _to_copy_2203 = torch.ops.aten._to_copy.default(transpose_103, dtype = torch.bfloat16);  transpose_103 = None
        t_824 = torch.ops.aten.t.default(_to_copy_2202);  _to_copy_2202 = None
        expand_253 = torch.ops.aten.expand.default(_to_copy_2203, [1, 512, 512, 256]);  _to_copy_2203 = None
        view_3917 = torch.ops.aten.view.default(expand_253, [512, 512, 256]);  expand_253 = None
        expand_254 = torch.ops.aten.expand.default(t_824, [1, 512, 256, 1024]);  t_824 = None
        view_3918 = torch.ops.aten.view.default(expand_254, [512, 256, 1024]);  expand_254 = None
        bmm_305 = torch.ops.aten.bmm.default(view_3917, view_3918);  view_3917 = view_3918 = None
        view_3919 = torch.ops.aten.view.default(bmm_305, [1, 512, 512, 1024]);  bmm_305 = None
        select_104 = torch.ops.aten.select.int(view_3911, 0, 1);  view_3911 = None
        view_3920 = torch.ops.aten.view.default(view_3919, [1, 512, 512, 4, 4, 64]);  view_3919 = None
        permute_2065 = torch.ops.aten.permute.default(view_3920, [4, 0, 3, 1, 2, 5]);  view_3920 = None
        view_3921 = torch.ops.aten.view.default(permute_2065, [4, 4, 512, 512, 64]);  permute_2065 = None
        unbind_int_176 = torch.ops.aten.unbind.int(view_3921);  view_3921 = None
        getitem_3810 = unbind_int_176[0]
        getitem_3811 = unbind_int_176[1]
        getitem_3812 = unbind_int_176[2]
        getitem_3813 = unbind_int_176[3];  unbind_int_176 = None
        expand_255 = torch.ops.aten.expand.default(select_104, [4, 512, 512, 512]);  select_104 = None
        _scaled_dot_product_efficient_attention_default_148 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3810, getitem_3811, getitem_3812, expand_255, False);  getitem_3810 = getitem_3811 = getitem_3812 = expand_255 = None
        getitem_3814 = _scaled_dot_product_efficient_attention_default_148[0]
        sigmoid_316 = torch.ops.aten.sigmoid.default(getitem_3813);  getitem_3813 = None
        mul_521 = torch.ops.aten.mul.Tensor(getitem_3814, sigmoid_316);  getitem_3814 = sigmoid_316 = None
        view_3922 = torch.ops.aten.view.default(mul_521, [1, 4, 512, 512, 64]);  mul_521 = None
        permute_2066 = torch.ops.aten.permute.default(view_3922, [0, 2, 3, 1, 4]);  view_3922 = None
        clone_322 = torch.ops.aten.clone.default(permute_2066, memory_format = torch.contiguous_format);  permute_2066 = None
        _unsafe_view_269 = torch.ops.aten._unsafe_view.default(clone_322, [1, 512, 512, 256]);  clone_322 = None
        cat_57 = torch.ops.aten.cat.default([_unsafe_view_268, _unsafe_view_269], dim = -1);  _unsafe_view_268 = _unsafe_view_269 = None
        slice_250 = torch.ops.aten.slice.Tensor(arg1323_1, dim = 0, start = 0, end = 9223372036854775807);  arg1323_1 = None
        unsqueeze_1240 = torch.ops.aten.unsqueeze.default(slice_250, 1);  slice_250 = None
        mul_522 = torch.ops.aten.mul.Tensor(arg1327_1, unsqueeze_1240);  arg1327_1 = unsqueeze_1240 = None
        _to_copy_2204 = torch.ops.aten._to_copy.default(mul_522, dtype = torch.bfloat16);  mul_522 = None
        t_825 = torch.ops.aten.t.default(_to_copy_2204);  _to_copy_2204 = None
        view_3923 = torch.ops.aten.view.default(cat_57, [262144, 512]);  cat_57 = None
        mm_769 = torch.ops.aten.mm.default(view_3923, t_825);  view_3923 = t_825 = None
        view_3924 = torch.ops.aten.view.default(mm_769, [1, 512, 512, 256]);  mm_769 = None
        add_418 = torch.ops.aten.add.Tensor(add_417, view_3924);  add_417 = view_3924 = None
        split_tensor_415 = torch.ops.aten.split.Tensor(add_411, 512, dim = -2)
        getitem_3818 = split_tensor_415[0];  split_tensor_415 = None
        _to_copy_2205 = torch.ops.aten._to_copy.default(getitem_3818, dtype = torch.float32);  getitem_3818 = None
        native_layer_norm_default_454 = torch.ops.aten.native_layer_norm.default(_to_copy_2205, [256], arg1314_1, arg1315_1, 1e-05);  _to_copy_2205 = arg1314_1 = arg1315_1 = None
        getitem_3819 = native_layer_norm_default_454[0]
        _to_copy_2206 = torch.ops.aten._to_copy.default(arg1316_1, dtype = torch.bfloat16);  arg1316_1 = None
        _to_copy_2207 = torch.ops.aten._to_copy.default(getitem_3819, dtype = torch.bfloat16);  getitem_3819 = None
        t_826 = torch.ops.aten.t.default(_to_copy_2206);  _to_copy_2206 = None
        view_3925 = torch.ops.aten.view.default(_to_copy_2207, [262144, 256]);  _to_copy_2207 = None
        mm_770 = torch.ops.aten.mm.default(view_3925, t_826);  view_3925 = t_826 = None
        view_3926 = torch.ops.aten.view.default(mm_770, [1, 512, 512, 1024]);  mm_770 = None
        split_tensor_416 = torch.ops.aten.split.Tensor(view_3926, 512, dim = -1);  view_3926 = None
        getitem_3822 = split_tensor_416[0]
        getitem_3823 = split_tensor_416[1];  split_tensor_416 = None
        silu_108 = torch.ops.aten.silu.default(getitem_3822);  getitem_3822 = None
        mul_523 = torch.ops.aten.mul.Tensor(silu_108, getitem_3823);  silu_108 = getitem_3823 = None
        _to_copy_2208 = torch.ops.aten._to_copy.default(arg1317_1, dtype = torch.bfloat16);  arg1317_1 = None
        t_827 = torch.ops.aten.t.default(_to_copy_2208);  _to_copy_2208 = None
        view_3928 = torch.ops.aten.view.default(mul_523, [262144, 512]);  mul_523 = None
        mm_771 = torch.ops.aten.mm.default(view_3928, t_827);  view_3928 = t_827 = None
        view_3929 = torch.ops.aten.view.default(mm_771, [1, 512, 512, 256]);  mm_771 = None
        add_419 = torch.ops.aten.add.Tensor(add_418, view_3929);  add_418 = view_3929 = None
        _to_copy_2209 = torch.ops.aten._to_copy.default(add_415, dtype = torch.float32)
        native_layer_norm_default_455 = torch.ops.aten.native_layer_norm.default(_to_copy_2209, [384], arg1332_1, arg1333_1, 1e-05);  _to_copy_2209 = arg1332_1 = arg1333_1 = None
        getitem_3824 = native_layer_norm_default_455[0]
        _to_copy_2210 = torch.ops.aten._to_copy.default(add_411, dtype = torch.float32);  add_411 = None
        native_layer_norm_default_456 = torch.ops.aten.native_layer_norm.default(_to_copy_2210, [256], arg1334_1, arg1335_1, 1e-05);  _to_copy_2210 = arg1334_1 = arg1335_1 = None
        getitem_3827 = native_layer_norm_default_456[0]
        _to_copy_2211 = torch.ops.aten._to_copy.default(arg1336_1, dtype = torch.bfloat16);  arg1336_1 = None
        _to_copy_2212 = torch.ops.aten._to_copy.default(getitem_3827, dtype = torch.bfloat16);  getitem_3827 = None
        t_828 = torch.ops.aten.t.default(_to_copy_2211);  _to_copy_2211 = None
        view_3930 = torch.ops.aten.view.default(_to_copy_2212, [262144, 256]);  _to_copy_2212 = None
        mm_772 = torch.ops.aten.mm.default(view_3930, t_828);  view_3930 = t_828 = None
        view_3931 = torch.ops.aten.view.default(mm_772, [1, 512, 512, 16]);  mm_772 = None
        permute_2067 = torch.ops.aten.permute.default(view_3931, [0, 3, 1, 2]);  view_3931 = None
        view_3932 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_241 = torch.ops.aten.bitwise_not.default(view_3932);  view_3932 = None
        masked_fill_241 = torch.ops.aten.masked_fill.Scalar(permute_2067, bitwise_not_241, -10000);  permute_2067 = bitwise_not_241 = None
        _to_copy_2213 = torch.ops.aten._to_copy.default(getitem_3824, dtype = torch.bfloat16);  getitem_3824 = None
        _to_copy_2214 = torch.ops.aten._to_copy.default(arg1338_1, dtype = torch.bfloat16);  arg1338_1 = None
        unsqueeze_1241 = torch.ops.aten.unsqueeze.default(_to_copy_2213, 3);  _to_copy_2213 = None
        unsqueeze_1242 = torch.ops.aten.unsqueeze.default(unsqueeze_1241, 4);  unsqueeze_1241 = None
        unsqueeze_1243 = torch.ops.aten.unsqueeze.default(unsqueeze_1242, 5);  unsqueeze_1242 = None
        permute_2068 = torch.ops.aten.permute.default(unsqueeze_1243, [3, 0, 4, 1, 5, 2]);  unsqueeze_1243 = None
        unsqueeze_1244 = torch.ops.aten.unsqueeze.default(_to_copy_2214, 4);  _to_copy_2214 = None
        unsqueeze_1245 = torch.ops.aten.unsqueeze.default(unsqueeze_1244, 5);  unsqueeze_1244 = None
        permute_2069 = torch.ops.aten.permute.default(unsqueeze_1245, [1, 4, 2, 5, 3, 0]);  unsqueeze_1245 = None
        permute_2070 = torch.ops.aten.permute.default(permute_2068, [3, 5, 0, 1, 2, 4]);  permute_2068 = None
        view_3933 = torch.ops.aten.view.default(permute_2070, [1, 512, 384]);  permute_2070 = None
        permute_2071 = torch.ops.aten.permute.default(permute_2069, [5, 0, 1, 2, 4, 3]);  permute_2069 = None
        view_3934 = torch.ops.aten.view.default(permute_2071, [1, 384, 1536]);  permute_2071 = None
        bmm_306 = torch.ops.aten.bmm.default(view_3933, view_3934);  view_3933 = view_3934 = None
        view_3935 = torch.ops.aten.view.default(bmm_306, [512, 1, 4, 1, 16, 24]);  bmm_306 = None
        permute_2072 = torch.ops.aten.permute.default(view_3935, [2, 3, 4, 0, 5, 1]);  view_3935 = None
        view_3936 = torch.ops.aten.view.default(permute_2072, [4, 1, 16, 512, 24]);  permute_2072 = None
        unbind_int_177 = torch.ops.aten.unbind.int(view_3936);  view_3936 = None
        getitem_3830 = unbind_int_177[0]
        getitem_3831 = unbind_int_177[1]
        getitem_3832 = unbind_int_177[2]
        getitem_3833 = unbind_int_177[3];  unbind_int_177 = None
        view_3937 = torch.ops.aten.view.default(arg1337_1, [1, 16, 1, 24]);  arg1337_1 = None
        add_420 = torch.ops.aten.add.Tensor(getitem_3830, view_3937);  getitem_3830 = view_3937 = None
        _to_copy_2215 = torch.ops.aten._to_copy.default(add_420, dtype = torch.bfloat16);  add_420 = None
        expand_256 = torch.ops.aten.expand.default(masked_fill_241, [1, 16, 512, 512]);  masked_fill_241 = None
        _scaled_dot_product_efficient_attention_default_149 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_2215, getitem_3831, getitem_3832, expand_256, False);  _to_copy_2215 = getitem_3831 = getitem_3832 = expand_256 = None
        getitem_3834 = _scaled_dot_product_efficient_attention_default_149[0]
        add_421 = torch.ops.aten.add.Tensor(getitem_3833, 1);  getitem_3833 = None
        sigmoid_317 = torch.ops.aten.sigmoid.default(add_421);  add_421 = None
        mul_524 = torch.ops.aten.mul.Tensor(getitem_3834, sigmoid_317);  getitem_3834 = sigmoid_317 = None
        _to_copy_2216 = torch.ops.aten._to_copy.default(arg1339_1, dtype = torch.bfloat16);  arg1339_1 = None
        unsqueeze_1246 = torch.ops.aten.unsqueeze.default(mul_524, 4);  mul_524 = None
        permute_2073 = torch.ops.aten.permute.default(unsqueeze_1246, [0, 2, 4, 3, 1]);  unsqueeze_1246 = None
        unsqueeze_1247 = torch.ops.aten.unsqueeze.default(_to_copy_2216, 3);  _to_copy_2216 = None
        unsqueeze_1248 = torch.ops.aten.unsqueeze.default(unsqueeze_1247, 4);  unsqueeze_1247 = None
        permute_2074 = torch.ops.aten.permute.default(unsqueeze_1248, [3, 4, 2, 1, 0]);  unsqueeze_1248 = None
        permute_2075 = torch.ops.aten.permute.default(permute_2073, [1, 3, 4, 0, 2]);  permute_2073 = None
        clone_323 = torch.ops.aten.clone.default(permute_2075, memory_format = torch.contiguous_format);  permute_2075 = None
        _unsafe_view_270 = torch.ops.aten._unsafe_view.default(clone_323, [1, 512, 384]);  clone_323 = None
        permute_2076 = torch.ops.aten.permute.default(permute_2074, [3, 4, 0, 2, 1]);  permute_2074 = None
        clone_324 = torch.ops.aten.clone.default(permute_2076, memory_format = torch.contiguous_format);  permute_2076 = None
        _unsafe_view_271 = torch.ops.aten._unsafe_view.default(clone_324, [1, 384, 384]);  clone_324 = None
        bmm_307 = torch.ops.aten.bmm.default(_unsafe_view_270, _unsafe_view_271);  _unsafe_view_270 = _unsafe_view_271 = None
        view_3938 = torch.ops.aten.view.default(bmm_307, [512, 1, 1, 1, 384]);  bmm_307 = None
        permute_2077 = torch.ops.aten.permute.default(view_3938, [3, 0, 4, 1, 2]);  view_3938 = None
        view_3939 = torch.ops.aten.view.default(permute_2077, [1, 512, 384]);  permute_2077 = None
        unsqueeze_1249 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_525 = torch.ops.aten.mul.Tensor(view_3939, unsqueeze_1249);  view_3939 = unsqueeze_1249 = None
        add_422 = torch.ops.aten.add.Tensor(add_415, mul_525);  mul_525 = None
        split_tensor_417 = torch.ops.aten.split.Tensor(add_415, 512, dim = -2);  add_415 = None
        getitem_3838 = split_tensor_417[0];  split_tensor_417 = None
        _to_copy_2217 = torch.ops.aten._to_copy.default(getitem_3838, dtype = torch.float32);  getitem_3838 = None
        native_layer_norm_default_457 = torch.ops.aten.native_layer_norm.default(_to_copy_2217, [384], arg1328_1, arg1329_1, 1e-05);  _to_copy_2217 = arg1328_1 = arg1329_1 = None
        getitem_3839 = native_layer_norm_default_457[0]
        _to_copy_2218 = torch.ops.aten._to_copy.default(arg1330_1, dtype = torch.bfloat16);  arg1330_1 = None
        _to_copy_2219 = torch.ops.aten._to_copy.default(getitem_3839, dtype = torch.bfloat16);  getitem_3839 = None
        t_829 = torch.ops.aten.t.default(_to_copy_2218);  _to_copy_2218 = None
        view_3940 = torch.ops.aten.view.default(_to_copy_2219, [512, 384]);  _to_copy_2219 = None
        mm_773 = torch.ops.aten.mm.default(view_3940, t_829);  view_3940 = t_829 = None
        view_3941 = torch.ops.aten.view.default(mm_773, [1, 512, 1536]);  mm_773 = None
        split_tensor_418 = torch.ops.aten.split.Tensor(view_3941, 768, dim = -1);  view_3941 = None
        getitem_3842 = split_tensor_418[0]
        getitem_3843 = split_tensor_418[1];  split_tensor_418 = None
        silu_109 = torch.ops.aten.silu.default(getitem_3842);  getitem_3842 = None
        mul_526 = torch.ops.aten.mul.Tensor(silu_109, getitem_3843);  silu_109 = getitem_3843 = None
        _to_copy_2220 = torch.ops.aten._to_copy.default(arg1331_1, dtype = torch.bfloat16);  arg1331_1 = None
        t_830 = torch.ops.aten.t.default(_to_copy_2220);  _to_copy_2220 = None
        view_3943 = torch.ops.aten.view.default(mul_526, [512, 768]);  mul_526 = None
        mm_774 = torch.ops.aten.mm.default(view_3943, t_830);  view_3943 = t_830 = None
        view_3944 = torch.ops.aten.view.default(mm_774, [1, 512, 384]);  mm_774 = None
        add_423 = torch.ops.aten.add.Tensor(add_422, view_3944);  add_422 = view_3944 = None
        _to_copy_2221 = torch.ops.aten._to_copy.default(add_419, dtype = torch.float32)
        native_layer_norm_default_458 = torch.ops.aten.native_layer_norm.default(_to_copy_2221, [256], arg1344_1, arg1345_1, 1e-05);  _to_copy_2221 = arg1344_1 = arg1345_1 = None
        getitem_3844 = native_layer_norm_default_458[0]
        split_with_sizes_default_104 = torch.ops.aten.split_with_sizes.default(arg1347_1, [512, 512]);  arg1347_1 = None
        getitem_3847 = split_with_sizes_default_104[0]
        getitem_3848 = split_with_sizes_default_104[1];  split_with_sizes_default_104 = None
        split_with_sizes_default_105 = torch.ops.aten.split_with_sizes.default(arg1348_1, [512, 512, 256]);  arg1348_1 = None
        getitem_3849 = split_with_sizes_default_105[0]
        getitem_3850 = split_with_sizes_default_105[1]
        getitem_3851 = split_with_sizes_default_105[2];  split_with_sizes_default_105 = None
        _to_copy_2222 = torch.ops.aten._to_copy.default(getitem_3847, dtype = torch.bfloat16);  getitem_3847 = None
        _to_copy_2223 = torch.ops.aten._to_copy.default(getitem_3844, dtype = torch.bfloat16)
        t_831 = torch.ops.aten.t.default(_to_copy_2222);  _to_copy_2222 = None
        view_3945 = torch.ops.aten.view.default(_to_copy_2223, [262144, 256]);  _to_copy_2223 = None
        mm_775 = torch.ops.aten.mm.default(view_3945, t_831);  view_3945 = t_831 = None
        view_3946 = torch.ops.aten.view.default(mm_775, [1, 512, 512, 512]);  mm_775 = None
        _to_copy_2224 = torch.ops.aten._to_copy.default(getitem_3849, dtype = torch.bfloat16);  getitem_3849 = None
        _to_copy_2225 = torch.ops.aten._to_copy.default(getitem_3844, dtype = torch.bfloat16)
        t_832 = torch.ops.aten.t.default(_to_copy_2224);  _to_copy_2224 = None
        view_3947 = torch.ops.aten.view.default(_to_copy_2225, [262144, 256]);  _to_copy_2225 = None
        mm_776 = torch.ops.aten.mm.default(view_3947, t_832);  view_3947 = t_832 = None
        view_3948 = torch.ops.aten.view.default(mm_776, [1, 512, 512, 512]);  mm_776 = None
        sigmoid_318 = torch.ops.aten.sigmoid.default(view_3948);  view_3948 = None
        mul_527 = torch.ops.aten.mul.Tensor(view_3946, sigmoid_318);  view_3946 = sigmoid_318 = None
        unsqueeze_1250 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_242 = torch.ops.aten.bitwise_not.default(unsqueeze_1250);  unsqueeze_1250 = None
        masked_fill_242 = torch.ops.aten.masked_fill.Scalar(mul_527, bitwise_not_242, 0);  mul_527 = bitwise_not_242 = None
        split_tensor_419 = torch.ops.aten.split.Tensor(masked_fill_242, 256, dim = -1)
        getitem_3854 = split_tensor_419[0]
        unsqueeze_1253 = torch.ops.aten.unsqueeze.default(getitem_3854, 4);  getitem_3854 = None
        permute_2082 = torch.ops.aten.permute.default(unsqueeze_1253, [0, 1, 4, 3, 2]);  unsqueeze_1253 = None
        permute_2083 = torch.ops.aten.permute.default(permute_2082, [3, 1, 4, 0, 2]);  permute_2082 = None
        view_3951 = torch.ops.aten.view.default(permute_2083, [256, 512, 512]);  permute_2083 = None
        split_tensor_420 = torch.ops.aten.split.Tensor(masked_fill_242, 256, dim = -1);  masked_fill_242 = None
        getitem_3857 = split_tensor_420[1];  split_tensor_420 = None
        unsqueeze_1254 = torch.ops.aten.unsqueeze.default(getitem_3857, 4);  getitem_3857 = None
        permute_2084 = torch.ops.aten.permute.default(unsqueeze_1254, [0, 4, 1, 3, 2]);  unsqueeze_1254 = None
        permute_2085 = torch.ops.aten.permute.default(permute_2084, [3, 4, 0, 2, 1]);  permute_2084 = None
        view_3952 = torch.ops.aten.view.default(permute_2085, [256, 512, 512]);  permute_2085 = None
        bmm_308 = torch.ops.aten.bmm.default(view_3951, view_3952);  view_3951 = view_3952 = None
        view_3953 = torch.ops.aten.view.default(bmm_308, [256, 512, 1, 1, 512]);  bmm_308 = None
        permute_2086 = torch.ops.aten.permute.default(view_3953, [3, 1, 4, 0, 2]);  view_3953 = None
        view_3954 = torch.ops.aten.view.default(permute_2086, [1, 512, 512, 256]);  permute_2086 = None
        _to_copy_2226 = torch.ops.aten._to_copy.default(getitem_3848, dtype = torch.bfloat16);  getitem_3848 = None
        _to_copy_2227 = torch.ops.aten._to_copy.default(getitem_3844, dtype = torch.bfloat16)
        t_833 = torch.ops.aten.t.default(_to_copy_2226);  _to_copy_2226 = None
        view_3955 = torch.ops.aten.view.default(_to_copy_2227, [262144, 256]);  _to_copy_2227 = None
        mm_777 = torch.ops.aten.mm.default(view_3955, t_833);  view_3955 = t_833 = None
        view_3956 = torch.ops.aten.view.default(mm_777, [1, 512, 512, 512]);  mm_777 = None
        _to_copy_2228 = torch.ops.aten._to_copy.default(getitem_3850, dtype = torch.bfloat16);  getitem_3850 = None
        _to_copy_2229 = torch.ops.aten._to_copy.default(getitem_3844, dtype = torch.bfloat16)
        t_834 = torch.ops.aten.t.default(_to_copy_2228);  _to_copy_2228 = None
        view_3957 = torch.ops.aten.view.default(_to_copy_2229, [262144, 256]);  _to_copy_2229 = None
        mm_778 = torch.ops.aten.mm.default(view_3957, t_834);  view_3957 = t_834 = None
        view_3958 = torch.ops.aten.view.default(mm_778, [1, 512, 512, 512]);  mm_778 = None
        sigmoid_319 = torch.ops.aten.sigmoid.default(view_3958);  view_3958 = None
        mul_528 = torch.ops.aten.mul.Tensor(view_3956, sigmoid_319);  view_3956 = sigmoid_319 = None
        view_3959 = torch.ops.aten.view.default(mul_528, [262144, 512]);  mul_528 = None
        view_3960 = torch.ops.aten.view.default(view_3959, [1, 512, 512, 512]);  view_3959 = None
        transpose_104 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_1255 = torch.ops.aten.unsqueeze.default(transpose_104, 3);  transpose_104 = None
        clone_325 = torch.ops.aten.clone.default(unsqueeze_1255, memory_format = torch.contiguous_format);  unsqueeze_1255 = None
        bitwise_not_243 = torch.ops.aten.bitwise_not.default(clone_325);  clone_325 = None
        masked_fill_243 = torch.ops.aten.masked_fill.Scalar(view_3960, bitwise_not_243, 0);  view_3960 = bitwise_not_243 = None
        view_3961 = torch.ops.aten.view.default(masked_fill_243, [262144, 512]);  masked_fill_243 = None
        view_3965 = torch.ops.aten.view.default(view_3961, [1, 512, 512, 512])
        split_tensor_421 = torch.ops.aten.split.Tensor(view_3965, 256, dim = -1);  view_3965 = None
        getitem_3860 = split_tensor_421[0]
        unsqueeze_1258 = torch.ops.aten.unsqueeze.default(getitem_3860, 4);  getitem_3860 = None
        permute_2091 = torch.ops.aten.permute.default(unsqueeze_1258, [0, 2, 4, 3, 1]);  unsqueeze_1258 = None
        permute_2092 = torch.ops.aten.permute.default(permute_2091, [3, 1, 4, 0, 2]);  permute_2091 = None
        view_3966 = torch.ops.aten.view.default(permute_2092, [256, 512, 512]);  permute_2092 = None
        view_3967 = torch.ops.aten.view.default(view_3961, [1, 512, 512, 512]);  view_3961 = None
        split_tensor_422 = torch.ops.aten.split.Tensor(view_3967, 256, dim = -1);  view_3967 = None
        getitem_3863 = split_tensor_422[1];  split_tensor_422 = None
        unsqueeze_1259 = torch.ops.aten.unsqueeze.default(getitem_3863, 4);  getitem_3863 = None
        permute_2093 = torch.ops.aten.permute.default(unsqueeze_1259, [0, 4, 2, 3, 1]);  unsqueeze_1259 = None
        permute_2094 = torch.ops.aten.permute.default(permute_2093, [3, 4, 0, 2, 1]);  permute_2093 = None
        view_3968 = torch.ops.aten.view.default(permute_2094, [256, 512, 512]);  permute_2094 = None
        bmm_309 = torch.ops.aten.bmm.default(view_3966, view_3968);  view_3966 = view_3968 = None
        view_3969 = torch.ops.aten.view.default(bmm_309, [256, 512, 1, 1, 512]);  bmm_309 = None
        permute_2095 = torch.ops.aten.permute.default(view_3969, [3, 1, 4, 0, 2]);  view_3969 = None
        view_3970 = torch.ops.aten.view.default(permute_2095, [1, 512, 512, 256]);  permute_2095 = None
        _to_copy_2230 = torch.ops.aten._to_copy.default(view_3954, dtype = torch.float32);  view_3954 = None
        native_layer_norm_default_459 = torch.ops.aten.native_layer_norm.default(_to_copy_2230, [256], None, None, 1e-05);  _to_copy_2230 = None
        getitem_3864 = native_layer_norm_default_459[0]
        _to_copy_2231 = torch.ops.aten._to_copy.default(view_3970, dtype = torch.float32);  view_3970 = None
        native_layer_norm_default_460 = torch.ops.aten.native_layer_norm.default(_to_copy_2231, [256], None, None, 1e-05);  _to_copy_2231 = None
        getitem_3867 = native_layer_norm_default_460[0]
        add_424 = torch.ops.aten.add.Tensor(getitem_3864, getitem_3867);  getitem_3864 = getitem_3867 = None
        _to_copy_2232 = torch.ops.aten._to_copy.default(arg1346_1, dtype = torch.bfloat16);  arg1346_1 = None
        _to_copy_2233 = torch.ops.aten._to_copy.default(add_424, dtype = torch.bfloat16);  add_424 = None
        t_835 = torch.ops.aten.t.default(_to_copy_2232);  _to_copy_2232 = None
        view_3971 = torch.ops.aten.view.default(_to_copy_2233, [262144, 256]);  _to_copy_2233 = None
        mm_779 = torch.ops.aten.mm.default(view_3971, t_835);  view_3971 = t_835 = None
        view_3972 = torch.ops.aten.view.default(mm_779, [1, 512, 512, 256]);  mm_779 = None
        _to_copy_2234 = torch.ops.aten._to_copy.default(getitem_3851, dtype = torch.bfloat16);  getitem_3851 = None
        _to_copy_2235 = torch.ops.aten._to_copy.default(getitem_3844, dtype = torch.bfloat16);  getitem_3844 = None
        t_836 = torch.ops.aten.t.default(_to_copy_2234);  _to_copy_2234 = None
        view_3973 = torch.ops.aten.view.default(_to_copy_2235, [262144, 256]);  _to_copy_2235 = None
        mm_780 = torch.ops.aten.mm.default(view_3973, t_836);  view_3973 = t_836 = None
        view_3974 = torch.ops.aten.view.default(mm_780, [1, 512, 512, 256]);  mm_780 = None
        sigmoid_320 = torch.ops.aten.sigmoid.default(view_3974);  view_3974 = None
        mul_529 = torch.ops.aten.mul.Tensor(view_3972, sigmoid_320);  view_3972 = sigmoid_320 = None
        add_425 = torch.ops.aten.add.Tensor(add_419, mul_529);  mul_529 = None
        _to_copy_2236 = torch.ops.aten._to_copy.default(add_419, dtype = torch.float32)
        native_layer_norm_default_461 = torch.ops.aten.native_layer_norm.default(_to_copy_2236, [256], None, None, 1e-05);  _to_copy_2236 = None
        getitem_3870 = native_layer_norm_default_461[0]
        _to_copy_2237 = torch.ops.aten._to_copy.default(arg1350_1, dtype = torch.bfloat16);  arg1350_1 = None
        _to_copy_2238 = torch.ops.aten._to_copy.default(getitem_3870, dtype = torch.bfloat16)
        t_837 = torch.ops.aten.t.default(_to_copy_2237);  _to_copy_2237 = None
        view_3975 = torch.ops.aten.view.default(_to_copy_2238, [262144, 256]);  _to_copy_2238 = None
        mm_781 = torch.ops.aten.mm.default(view_3975, t_837);  view_3975 = t_837 = None
        view_3976 = torch.ops.aten.view.default(mm_781, [1, 512, 512, 8]);  mm_781 = None
        view_3977 = torch.ops.aten.view.default(view_3976, [1, 512, 512, 2, 4]);  view_3976 = None
        permute_2096 = torch.ops.aten.permute.default(view_3977, [0, 3, 4, 1, 2]);  view_3977 = None
        view_3978 = torch.ops.aten.view.default(permute_2096, [1, 2, 4, 1, 512, 512]);  permute_2096 = None
        view_3979 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_244 = torch.ops.aten.bitwise_not.default(view_3979);  view_3979 = None
        masked_fill_244 = torch.ops.aten.masked_fill.Scalar(view_3978, bitwise_not_244, -10000);  view_3978 = bitwise_not_244 = None
        view_3980 = torch.ops.aten.view.default(masked_fill_244, [1, 2, 4, 512, 512]);  masked_fill_244 = None
        permute_2097 = torch.ops.aten.permute.default(view_3980, [1, 0, 2, 3, 4]);  view_3980 = None
        view_3981 = torch.ops.aten.view.default(permute_2097, [2, 4, 1, 512, 512]);  permute_2097 = None
        _to_copy_2239 = torch.ops.aten._to_copy.default(arg1351_1, dtype = torch.bfloat16);  arg1351_1 = None
        _to_copy_2240 = torch.ops.aten._to_copy.default(getitem_3870, dtype = torch.bfloat16)
        t_838 = torch.ops.aten.t.default(_to_copy_2239);  _to_copy_2239 = None
        view_3982 = torch.ops.aten.view.default(_to_copy_2240, [262144, 256]);  _to_copy_2240 = None
        mm_782 = torch.ops.aten.mm.default(view_3982, t_838);  view_3982 = t_838 = None
        view_3983 = torch.ops.aten.view.default(mm_782, [1, 512, 512, 1024]);  mm_782 = None
        select_105 = torch.ops.aten.select.int(view_3981, 0, 0)
        view_3984 = torch.ops.aten.view.default(view_3983, [1, 512, 512, 4, 4, 64]);  view_3983 = None
        permute_2098 = torch.ops.aten.permute.default(view_3984, [4, 0, 3, 1, 2, 5]);  view_3984 = None
        view_3985 = torch.ops.aten.view.default(permute_2098, [4, 4, 512, 512, 64]);  permute_2098 = None
        unbind_int_178 = torch.ops.aten.unbind.int(view_3985);  view_3985 = None
        getitem_3873 = unbind_int_178[0]
        getitem_3874 = unbind_int_178[1]
        getitem_3875 = unbind_int_178[2]
        getitem_3876 = unbind_int_178[3];  unbind_int_178 = None
        expand_257 = torch.ops.aten.expand.default(select_105, [4, 512, 512, 512]);  select_105 = None
        _scaled_dot_product_efficient_attention_default_150 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3873, getitem_3874, getitem_3875, expand_257, False);  getitem_3873 = getitem_3874 = getitem_3875 = expand_257 = None
        getitem_3877 = _scaled_dot_product_efficient_attention_default_150[0]
        sigmoid_321 = torch.ops.aten.sigmoid.default(getitem_3876);  getitem_3876 = None
        mul_530 = torch.ops.aten.mul.Tensor(getitem_3877, sigmoid_321);  getitem_3877 = sigmoid_321 = None
        view_3986 = torch.ops.aten.view.default(mul_530, [1, 4, 512, 512, 64]);  mul_530 = None
        permute_2099 = torch.ops.aten.permute.default(view_3986, [0, 2, 3, 1, 4]);  view_3986 = None
        clone_326 = torch.ops.aten.clone.default(permute_2099, memory_format = torch.contiguous_format);  permute_2099 = None
        _unsafe_view_272 = torch.ops.aten._unsafe_view.default(clone_326, [1, 512, 512, 256]);  clone_326 = None
        transpose_105 = torch.ops.aten.transpose.int(getitem_3870, 1, 2);  getitem_3870 = None
        _to_copy_2241 = torch.ops.aten._to_copy.default(arg1352_1, dtype = torch.bfloat16);  arg1352_1 = None
        _to_copy_2242 = torch.ops.aten._to_copy.default(transpose_105, dtype = torch.bfloat16);  transpose_105 = None
        t_839 = torch.ops.aten.t.default(_to_copy_2241);  _to_copy_2241 = None
        expand_258 = torch.ops.aten.expand.default(_to_copy_2242, [1, 512, 512, 256]);  _to_copy_2242 = None
        view_3987 = torch.ops.aten.view.default(expand_258, [512, 512, 256]);  expand_258 = None
        expand_259 = torch.ops.aten.expand.default(t_839, [1, 512, 256, 1024]);  t_839 = None
        view_3988 = torch.ops.aten.view.default(expand_259, [512, 256, 1024]);  expand_259 = None
        bmm_310 = torch.ops.aten.bmm.default(view_3987, view_3988);  view_3987 = view_3988 = None
        view_3989 = torch.ops.aten.view.default(bmm_310, [1, 512, 512, 1024]);  bmm_310 = None
        select_106 = torch.ops.aten.select.int(view_3981, 0, 1);  view_3981 = None
        view_3990 = torch.ops.aten.view.default(view_3989, [1, 512, 512, 4, 4, 64]);  view_3989 = None
        permute_2100 = torch.ops.aten.permute.default(view_3990, [4, 0, 3, 1, 2, 5]);  view_3990 = None
        view_3991 = torch.ops.aten.view.default(permute_2100, [4, 4, 512, 512, 64]);  permute_2100 = None
        unbind_int_179 = torch.ops.aten.unbind.int(view_3991);  view_3991 = None
        getitem_3881 = unbind_int_179[0]
        getitem_3882 = unbind_int_179[1]
        getitem_3883 = unbind_int_179[2]
        getitem_3884 = unbind_int_179[3];  unbind_int_179 = None
        expand_260 = torch.ops.aten.expand.default(select_106, [4, 512, 512, 512]);  select_106 = None
        _scaled_dot_product_efficient_attention_default_151 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3881, getitem_3882, getitem_3883, expand_260, False);  getitem_3881 = getitem_3882 = getitem_3883 = expand_260 = None
        getitem_3885 = _scaled_dot_product_efficient_attention_default_151[0]
        sigmoid_322 = torch.ops.aten.sigmoid.default(getitem_3884);  getitem_3884 = None
        mul_531 = torch.ops.aten.mul.Tensor(getitem_3885, sigmoid_322);  getitem_3885 = sigmoid_322 = None
        view_3992 = torch.ops.aten.view.default(mul_531, [1, 4, 512, 512, 64]);  mul_531 = None
        permute_2101 = torch.ops.aten.permute.default(view_3992, [0, 2, 3, 1, 4]);  view_3992 = None
        clone_327 = torch.ops.aten.clone.default(permute_2101, memory_format = torch.contiguous_format);  permute_2101 = None
        _unsafe_view_273 = torch.ops.aten._unsafe_view.default(clone_327, [1, 512, 512, 256]);  clone_327 = None
        cat_58 = torch.ops.aten.cat.default([_unsafe_view_272, _unsafe_view_273], dim = -1);  _unsafe_view_272 = _unsafe_view_273 = None
        slice_251 = torch.ops.aten.slice.Tensor(arg1349_1, dim = 0, start = 0, end = 9223372036854775807);  arg1349_1 = None
        unsqueeze_1260 = torch.ops.aten.unsqueeze.default(slice_251, 1);  slice_251 = None
        mul_532 = torch.ops.aten.mul.Tensor(arg1353_1, unsqueeze_1260);  arg1353_1 = unsqueeze_1260 = None
        _to_copy_2243 = torch.ops.aten._to_copy.default(mul_532, dtype = torch.bfloat16);  mul_532 = None
        t_840 = torch.ops.aten.t.default(_to_copy_2243);  _to_copy_2243 = None
        view_3993 = torch.ops.aten.view.default(cat_58, [262144, 512]);  cat_58 = None
        mm_783 = torch.ops.aten.mm.default(view_3993, t_840);  view_3993 = t_840 = None
        view_3994 = torch.ops.aten.view.default(mm_783, [1, 512, 512, 256]);  mm_783 = None
        add_426 = torch.ops.aten.add.Tensor(add_425, view_3994);  add_425 = view_3994 = None
        split_tensor_423 = torch.ops.aten.split.Tensor(add_419, 512, dim = -2)
        getitem_3889 = split_tensor_423[0];  split_tensor_423 = None
        _to_copy_2244 = torch.ops.aten._to_copy.default(getitem_3889, dtype = torch.float32);  getitem_3889 = None
        native_layer_norm_default_462 = torch.ops.aten.native_layer_norm.default(_to_copy_2244, [256], arg1340_1, arg1341_1, 1e-05);  _to_copy_2244 = arg1340_1 = arg1341_1 = None
        getitem_3890 = native_layer_norm_default_462[0]
        _to_copy_2245 = torch.ops.aten._to_copy.default(arg1342_1, dtype = torch.bfloat16);  arg1342_1 = None
        _to_copy_2246 = torch.ops.aten._to_copy.default(getitem_3890, dtype = torch.bfloat16);  getitem_3890 = None
        t_841 = torch.ops.aten.t.default(_to_copy_2245);  _to_copy_2245 = None
        view_3995 = torch.ops.aten.view.default(_to_copy_2246, [262144, 256]);  _to_copy_2246 = None
        mm_784 = torch.ops.aten.mm.default(view_3995, t_841);  view_3995 = t_841 = None
        view_3996 = torch.ops.aten.view.default(mm_784, [1, 512, 512, 1024]);  mm_784 = None
        split_tensor_424 = torch.ops.aten.split.Tensor(view_3996, 512, dim = -1);  view_3996 = None
        getitem_3893 = split_tensor_424[0]
        getitem_3894 = split_tensor_424[1];  split_tensor_424 = None
        silu_110 = torch.ops.aten.silu.default(getitem_3893);  getitem_3893 = None
        mul_533 = torch.ops.aten.mul.Tensor(silu_110, getitem_3894);  silu_110 = getitem_3894 = None
        _to_copy_2247 = torch.ops.aten._to_copy.default(arg1343_1, dtype = torch.bfloat16);  arg1343_1 = None
        t_842 = torch.ops.aten.t.default(_to_copy_2247);  _to_copy_2247 = None
        view_3998 = torch.ops.aten.view.default(mul_533, [262144, 512]);  mul_533 = None
        mm_785 = torch.ops.aten.mm.default(view_3998, t_842);  view_3998 = t_842 = None
        view_3999 = torch.ops.aten.view.default(mm_785, [1, 512, 512, 256]);  mm_785 = None
        add_427 = torch.ops.aten.add.Tensor(add_426, view_3999);  add_426 = view_3999 = None
        _to_copy_2248 = torch.ops.aten._to_copy.default(add_423, dtype = torch.float32)
        native_layer_norm_default_463 = torch.ops.aten.native_layer_norm.default(_to_copy_2248, [384], arg1358_1, arg1359_1, 1e-05);  _to_copy_2248 = arg1358_1 = arg1359_1 = None
        getitem_3895 = native_layer_norm_default_463[0]
        _to_copy_2249 = torch.ops.aten._to_copy.default(add_419, dtype = torch.float32);  add_419 = None
        native_layer_norm_default_464 = torch.ops.aten.native_layer_norm.default(_to_copy_2249, [256], arg1360_1, arg1361_1, 1e-05);  _to_copy_2249 = arg1360_1 = arg1361_1 = None
        getitem_3898 = native_layer_norm_default_464[0]
        _to_copy_2250 = torch.ops.aten._to_copy.default(arg1362_1, dtype = torch.bfloat16);  arg1362_1 = None
        _to_copy_2251 = torch.ops.aten._to_copy.default(getitem_3898, dtype = torch.bfloat16);  getitem_3898 = None
        t_843 = torch.ops.aten.t.default(_to_copy_2250);  _to_copy_2250 = None
        view_4000 = torch.ops.aten.view.default(_to_copy_2251, [262144, 256]);  _to_copy_2251 = None
        mm_786 = torch.ops.aten.mm.default(view_4000, t_843);  view_4000 = t_843 = None
        view_4001 = torch.ops.aten.view.default(mm_786, [1, 512, 512, 16]);  mm_786 = None
        permute_2102 = torch.ops.aten.permute.default(view_4001, [0, 3, 1, 2]);  view_4001 = None
        view_4002 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512])
        bitwise_not_245 = torch.ops.aten.bitwise_not.default(view_4002);  view_4002 = None
        masked_fill_245 = torch.ops.aten.masked_fill.Scalar(permute_2102, bitwise_not_245, -10000);  permute_2102 = bitwise_not_245 = None
        _to_copy_2252 = torch.ops.aten._to_copy.default(getitem_3895, dtype = torch.bfloat16);  getitem_3895 = None
        _to_copy_2253 = torch.ops.aten._to_copy.default(arg1364_1, dtype = torch.bfloat16);  arg1364_1 = None
        unsqueeze_1261 = torch.ops.aten.unsqueeze.default(_to_copy_2252, 3);  _to_copy_2252 = None
        unsqueeze_1262 = torch.ops.aten.unsqueeze.default(unsqueeze_1261, 4);  unsqueeze_1261 = None
        unsqueeze_1263 = torch.ops.aten.unsqueeze.default(unsqueeze_1262, 5);  unsqueeze_1262 = None
        permute_2103 = torch.ops.aten.permute.default(unsqueeze_1263, [3, 0, 4, 1, 5, 2]);  unsqueeze_1263 = None
        unsqueeze_1264 = torch.ops.aten.unsqueeze.default(_to_copy_2253, 4);  _to_copy_2253 = None
        unsqueeze_1265 = torch.ops.aten.unsqueeze.default(unsqueeze_1264, 5);  unsqueeze_1264 = None
        permute_2104 = torch.ops.aten.permute.default(unsqueeze_1265, [1, 4, 2, 5, 3, 0]);  unsqueeze_1265 = None
        permute_2105 = torch.ops.aten.permute.default(permute_2103, [3, 5, 0, 1, 2, 4]);  permute_2103 = None
        view_4003 = torch.ops.aten.view.default(permute_2105, [1, 512, 384]);  permute_2105 = None
        permute_2106 = torch.ops.aten.permute.default(permute_2104, [5, 0, 1, 2, 4, 3]);  permute_2104 = None
        view_4004 = torch.ops.aten.view.default(permute_2106, [1, 384, 1536]);  permute_2106 = None
        bmm_311 = torch.ops.aten.bmm.default(view_4003, view_4004);  view_4003 = view_4004 = None
        view_4005 = torch.ops.aten.view.default(bmm_311, [512, 1, 4, 1, 16, 24]);  bmm_311 = None
        permute_2107 = torch.ops.aten.permute.default(view_4005, [2, 3, 4, 0, 5, 1]);  view_4005 = None
        view_4006 = torch.ops.aten.view.default(permute_2107, [4, 1, 16, 512, 24]);  permute_2107 = None
        unbind_int_180 = torch.ops.aten.unbind.int(view_4006);  view_4006 = None
        getitem_3901 = unbind_int_180[0]
        getitem_3902 = unbind_int_180[1]
        getitem_3903 = unbind_int_180[2]
        getitem_3904 = unbind_int_180[3];  unbind_int_180 = None
        view_4007 = torch.ops.aten.view.default(arg1363_1, [1, 16, 1, 24]);  arg1363_1 = None
        add_428 = torch.ops.aten.add.Tensor(getitem_3901, view_4007);  getitem_3901 = view_4007 = None
        _to_copy_2254 = torch.ops.aten._to_copy.default(add_428, dtype = torch.bfloat16);  add_428 = None
        expand_261 = torch.ops.aten.expand.default(masked_fill_245, [1, 16, 512, 512]);  masked_fill_245 = None
        _scaled_dot_product_efficient_attention_default_152 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_2254, getitem_3902, getitem_3903, expand_261, False);  _to_copy_2254 = getitem_3902 = getitem_3903 = expand_261 = None
        getitem_3905 = _scaled_dot_product_efficient_attention_default_152[0]
        add_429 = torch.ops.aten.add.Tensor(getitem_3904, 1);  getitem_3904 = None
        sigmoid_323 = torch.ops.aten.sigmoid.default(add_429);  add_429 = None
        mul_534 = torch.ops.aten.mul.Tensor(getitem_3905, sigmoid_323);  getitem_3905 = sigmoid_323 = None
        _to_copy_2255 = torch.ops.aten._to_copy.default(arg1365_1, dtype = torch.bfloat16);  arg1365_1 = None
        unsqueeze_1266 = torch.ops.aten.unsqueeze.default(mul_534, 4);  mul_534 = None
        permute_2108 = torch.ops.aten.permute.default(unsqueeze_1266, [0, 2, 4, 3, 1]);  unsqueeze_1266 = None
        unsqueeze_1267 = torch.ops.aten.unsqueeze.default(_to_copy_2255, 3);  _to_copy_2255 = None
        unsqueeze_1268 = torch.ops.aten.unsqueeze.default(unsqueeze_1267, 4);  unsqueeze_1267 = None
        permute_2109 = torch.ops.aten.permute.default(unsqueeze_1268, [3, 4, 2, 1, 0]);  unsqueeze_1268 = None
        permute_2110 = torch.ops.aten.permute.default(permute_2108, [1, 3, 4, 0, 2]);  permute_2108 = None
        clone_328 = torch.ops.aten.clone.default(permute_2110, memory_format = torch.contiguous_format);  permute_2110 = None
        _unsafe_view_274 = torch.ops.aten._unsafe_view.default(clone_328, [1, 512, 384]);  clone_328 = None
        permute_2111 = torch.ops.aten.permute.default(permute_2109, [3, 4, 0, 2, 1]);  permute_2109 = None
        clone_329 = torch.ops.aten.clone.default(permute_2111, memory_format = torch.contiguous_format);  permute_2111 = None
        _unsafe_view_275 = torch.ops.aten._unsafe_view.default(clone_329, [1, 384, 384]);  clone_329 = None
        bmm_312 = torch.ops.aten.bmm.default(_unsafe_view_274, _unsafe_view_275);  _unsafe_view_274 = _unsafe_view_275 = None
        view_4008 = torch.ops.aten.view.default(bmm_312, [512, 1, 1, 1, 384]);  bmm_312 = None
        permute_2112 = torch.ops.aten.permute.default(view_4008, [3, 0, 4, 1, 2]);  view_4008 = None
        view_4009 = torch.ops.aten.view.default(permute_2112, [1, 512, 384]);  permute_2112 = None
        unsqueeze_1269 = torch.ops.aten.unsqueeze.default(arg1406_1, -1)
        mul_535 = torch.ops.aten.mul.Tensor(view_4009, unsqueeze_1269);  view_4009 = unsqueeze_1269 = None
        add_430 = torch.ops.aten.add.Tensor(add_423, mul_535);  mul_535 = None
        split_tensor_425 = torch.ops.aten.split.Tensor(add_423, 512, dim = -2);  add_423 = None
        getitem_3909 = split_tensor_425[0];  split_tensor_425 = None
        _to_copy_2256 = torch.ops.aten._to_copy.default(getitem_3909, dtype = torch.float32);  getitem_3909 = None
        native_layer_norm_default_465 = torch.ops.aten.native_layer_norm.default(_to_copy_2256, [384], arg1354_1, arg1355_1, 1e-05);  _to_copy_2256 = arg1354_1 = arg1355_1 = None
        getitem_3910 = native_layer_norm_default_465[0]
        _to_copy_2257 = torch.ops.aten._to_copy.default(arg1356_1, dtype = torch.bfloat16);  arg1356_1 = None
        _to_copy_2258 = torch.ops.aten._to_copy.default(getitem_3910, dtype = torch.bfloat16);  getitem_3910 = None
        t_844 = torch.ops.aten.t.default(_to_copy_2257);  _to_copy_2257 = None
        view_4010 = torch.ops.aten.view.default(_to_copy_2258, [512, 384]);  _to_copy_2258 = None
        mm_787 = torch.ops.aten.mm.default(view_4010, t_844);  view_4010 = t_844 = None
        view_4011 = torch.ops.aten.view.default(mm_787, [1, 512, 1536]);  mm_787 = None
        split_tensor_426 = torch.ops.aten.split.Tensor(view_4011, 768, dim = -1);  view_4011 = None
        getitem_3913 = split_tensor_426[0]
        getitem_3914 = split_tensor_426[1];  split_tensor_426 = None
        silu_111 = torch.ops.aten.silu.default(getitem_3913);  getitem_3913 = None
        mul_536 = torch.ops.aten.mul.Tensor(silu_111, getitem_3914);  silu_111 = getitem_3914 = None
        _to_copy_2259 = torch.ops.aten._to_copy.default(arg1357_1, dtype = torch.bfloat16);  arg1357_1 = None
        t_845 = torch.ops.aten.t.default(_to_copy_2259);  _to_copy_2259 = None
        view_4013 = torch.ops.aten.view.default(mul_536, [512, 768]);  mul_536 = None
        mm_788 = torch.ops.aten.mm.default(view_4013, t_845);  view_4013 = t_845 = None
        view_4014 = torch.ops.aten.view.default(mm_788, [1, 512, 384]);  mm_788 = None
        add_431 = torch.ops.aten.add.Tensor(add_430, view_4014);  add_430 = view_4014 = None
        _to_copy_2260 = torch.ops.aten._to_copy.default(add_427, dtype = torch.float32)
        native_layer_norm_default_466 = torch.ops.aten.native_layer_norm.default(_to_copy_2260, [256], arg1370_1, arg1371_1, 1e-05);  _to_copy_2260 = arg1370_1 = arg1371_1 = None
        getitem_3915 = native_layer_norm_default_466[0]
        split_with_sizes_default_106 = torch.ops.aten.split_with_sizes.default(arg1373_1, [512, 512]);  arg1373_1 = None
        getitem_3918 = split_with_sizes_default_106[0]
        getitem_3919 = split_with_sizes_default_106[1];  split_with_sizes_default_106 = None
        split_with_sizes_default_107 = torch.ops.aten.split_with_sizes.default(arg1374_1, [512, 512, 256]);  arg1374_1 = None
        getitem_3920 = split_with_sizes_default_107[0]
        getitem_3921 = split_with_sizes_default_107[1]
        getitem_3922 = split_with_sizes_default_107[2];  split_with_sizes_default_107 = None
        _to_copy_2261 = torch.ops.aten._to_copy.default(getitem_3918, dtype = torch.bfloat16);  getitem_3918 = None
        _to_copy_2262 = torch.ops.aten._to_copy.default(getitem_3915, dtype = torch.bfloat16)
        t_846 = torch.ops.aten.t.default(_to_copy_2261);  _to_copy_2261 = None
        view_4015 = torch.ops.aten.view.default(_to_copy_2262, [262144, 256]);  _to_copy_2262 = None
        mm_789 = torch.ops.aten.mm.default(view_4015, t_846);  view_4015 = t_846 = None
        view_4016 = torch.ops.aten.view.default(mm_789, [1, 512, 512, 512]);  mm_789 = None
        _to_copy_2263 = torch.ops.aten._to_copy.default(getitem_3920, dtype = torch.bfloat16);  getitem_3920 = None
        _to_copy_2264 = torch.ops.aten._to_copy.default(getitem_3915, dtype = torch.bfloat16)
        t_847 = torch.ops.aten.t.default(_to_copy_2263);  _to_copy_2263 = None
        view_4017 = torch.ops.aten.view.default(_to_copy_2264, [262144, 256]);  _to_copy_2264 = None
        mm_790 = torch.ops.aten.mm.default(view_4017, t_847);  view_4017 = t_847 = None
        view_4018 = torch.ops.aten.view.default(mm_790, [1, 512, 512, 512]);  mm_790 = None
        sigmoid_324 = torch.ops.aten.sigmoid.default(view_4018);  view_4018 = None
        mul_537 = torch.ops.aten.mul.Tensor(view_4016, sigmoid_324);  view_4016 = sigmoid_324 = None
        unsqueeze_1270 = torch.ops.aten.unsqueeze.default(arg1407_1, 3)
        bitwise_not_246 = torch.ops.aten.bitwise_not.default(unsqueeze_1270);  unsqueeze_1270 = None
        masked_fill_246 = torch.ops.aten.masked_fill.Scalar(mul_537, bitwise_not_246, 0);  mul_537 = bitwise_not_246 = None
        split_tensor_427 = torch.ops.aten.split.Tensor(masked_fill_246, 256, dim = -1)
        getitem_3925 = split_tensor_427[0]
        unsqueeze_1273 = torch.ops.aten.unsqueeze.default(getitem_3925, 4);  getitem_3925 = None
        permute_2117 = torch.ops.aten.permute.default(unsqueeze_1273, [0, 1, 4, 3, 2]);  unsqueeze_1273 = None
        permute_2118 = torch.ops.aten.permute.default(permute_2117, [3, 1, 4, 0, 2]);  permute_2117 = None
        view_4021 = torch.ops.aten.view.default(permute_2118, [256, 512, 512]);  permute_2118 = None
        split_tensor_428 = torch.ops.aten.split.Tensor(masked_fill_246, 256, dim = -1);  masked_fill_246 = None
        getitem_3928 = split_tensor_428[1];  split_tensor_428 = None
        unsqueeze_1274 = torch.ops.aten.unsqueeze.default(getitem_3928, 4);  getitem_3928 = None
        permute_2119 = torch.ops.aten.permute.default(unsqueeze_1274, [0, 4, 1, 3, 2]);  unsqueeze_1274 = None
        permute_2120 = torch.ops.aten.permute.default(permute_2119, [3, 4, 0, 2, 1]);  permute_2119 = None
        view_4022 = torch.ops.aten.view.default(permute_2120, [256, 512, 512]);  permute_2120 = None
        bmm_313 = torch.ops.aten.bmm.default(view_4021, view_4022);  view_4021 = view_4022 = None
        view_4023 = torch.ops.aten.view.default(bmm_313, [256, 512, 1, 1, 512]);  bmm_313 = None
        permute_2121 = torch.ops.aten.permute.default(view_4023, [3, 1, 4, 0, 2]);  view_4023 = None
        view_4024 = torch.ops.aten.view.default(permute_2121, [1, 512, 512, 256]);  permute_2121 = None
        _to_copy_2265 = torch.ops.aten._to_copy.default(getitem_3919, dtype = torch.bfloat16);  getitem_3919 = None
        _to_copy_2266 = torch.ops.aten._to_copy.default(getitem_3915, dtype = torch.bfloat16)
        t_848 = torch.ops.aten.t.default(_to_copy_2265);  _to_copy_2265 = None
        view_4025 = torch.ops.aten.view.default(_to_copy_2266, [262144, 256]);  _to_copy_2266 = None
        mm_791 = torch.ops.aten.mm.default(view_4025, t_848);  view_4025 = t_848 = None
        view_4026 = torch.ops.aten.view.default(mm_791, [1, 512, 512, 512]);  mm_791 = None
        _to_copy_2267 = torch.ops.aten._to_copy.default(getitem_3921, dtype = torch.bfloat16);  getitem_3921 = None
        _to_copy_2268 = torch.ops.aten._to_copy.default(getitem_3915, dtype = torch.bfloat16)
        t_849 = torch.ops.aten.t.default(_to_copy_2267);  _to_copy_2267 = None
        view_4027 = torch.ops.aten.view.default(_to_copy_2268, [262144, 256]);  _to_copy_2268 = None
        mm_792 = torch.ops.aten.mm.default(view_4027, t_849);  view_4027 = t_849 = None
        view_4028 = torch.ops.aten.view.default(mm_792, [1, 512, 512, 512]);  mm_792 = None
        sigmoid_325 = torch.ops.aten.sigmoid.default(view_4028);  view_4028 = None
        mul_538 = torch.ops.aten.mul.Tensor(view_4026, sigmoid_325);  view_4026 = sigmoid_325 = None
        view_4029 = torch.ops.aten.view.default(mul_538, [262144, 512]);  mul_538 = None
        view_4030 = torch.ops.aten.view.default(view_4029, [1, 512, 512, 512]);  view_4029 = None
        transpose_106 = torch.ops.aten.transpose.int(arg1407_1, 1, 2)
        unsqueeze_1275 = torch.ops.aten.unsqueeze.default(transpose_106, 3);  transpose_106 = None
        clone_330 = torch.ops.aten.clone.default(unsqueeze_1275, memory_format = torch.contiguous_format);  unsqueeze_1275 = None
        bitwise_not_247 = torch.ops.aten.bitwise_not.default(clone_330);  clone_330 = None
        masked_fill_247 = torch.ops.aten.masked_fill.Scalar(view_4030, bitwise_not_247, 0);  view_4030 = bitwise_not_247 = None
        view_4031 = torch.ops.aten.view.default(masked_fill_247, [262144, 512]);  masked_fill_247 = None
        view_4035 = torch.ops.aten.view.default(view_4031, [1, 512, 512, 512])
        split_tensor_429 = torch.ops.aten.split.Tensor(view_4035, 256, dim = -1);  view_4035 = None
        getitem_3931 = split_tensor_429[0]
        unsqueeze_1278 = torch.ops.aten.unsqueeze.default(getitem_3931, 4);  getitem_3931 = None
        permute_2126 = torch.ops.aten.permute.default(unsqueeze_1278, [0, 2, 4, 3, 1]);  unsqueeze_1278 = None
        permute_2127 = torch.ops.aten.permute.default(permute_2126, [3, 1, 4, 0, 2]);  permute_2126 = None
        view_4036 = torch.ops.aten.view.default(permute_2127, [256, 512, 512]);  permute_2127 = None
        view_4037 = torch.ops.aten.view.default(view_4031, [1, 512, 512, 512]);  view_4031 = None
        split_tensor_430 = torch.ops.aten.split.Tensor(view_4037, 256, dim = -1);  view_4037 = None
        getitem_3934 = split_tensor_430[1];  split_tensor_430 = None
        unsqueeze_1279 = torch.ops.aten.unsqueeze.default(getitem_3934, 4);  getitem_3934 = None
        permute_2128 = torch.ops.aten.permute.default(unsqueeze_1279, [0, 4, 2, 3, 1]);  unsqueeze_1279 = None
        permute_2129 = torch.ops.aten.permute.default(permute_2128, [3, 4, 0, 2, 1]);  permute_2128 = None
        view_4038 = torch.ops.aten.view.default(permute_2129, [256, 512, 512]);  permute_2129 = None
        bmm_314 = torch.ops.aten.bmm.default(view_4036, view_4038);  view_4036 = view_4038 = None
        view_4039 = torch.ops.aten.view.default(bmm_314, [256, 512, 1, 1, 512]);  bmm_314 = None
        permute_2130 = torch.ops.aten.permute.default(view_4039, [3, 1, 4, 0, 2]);  view_4039 = None
        view_4040 = torch.ops.aten.view.default(permute_2130, [1, 512, 512, 256]);  permute_2130 = None
        _to_copy_2269 = torch.ops.aten._to_copy.default(view_4024, dtype = torch.float32);  view_4024 = None
        native_layer_norm_default_467 = torch.ops.aten.native_layer_norm.default(_to_copy_2269, [256], None, None, 1e-05);  _to_copy_2269 = None
        getitem_3935 = native_layer_norm_default_467[0]
        _to_copy_2270 = torch.ops.aten._to_copy.default(view_4040, dtype = torch.float32);  view_4040 = None
        native_layer_norm_default_468 = torch.ops.aten.native_layer_norm.default(_to_copy_2270, [256], None, None, 1e-05);  _to_copy_2270 = None
        getitem_3938 = native_layer_norm_default_468[0]
        add_432 = torch.ops.aten.add.Tensor(getitem_3935, getitem_3938);  getitem_3935 = getitem_3938 = None
        _to_copy_2271 = torch.ops.aten._to_copy.default(arg1372_1, dtype = torch.bfloat16);  arg1372_1 = None
        _to_copy_2272 = torch.ops.aten._to_copy.default(add_432, dtype = torch.bfloat16);  add_432 = None
        t_850 = torch.ops.aten.t.default(_to_copy_2271);  _to_copy_2271 = None
        view_4041 = torch.ops.aten.view.default(_to_copy_2272, [262144, 256]);  _to_copy_2272 = None
        mm_793 = torch.ops.aten.mm.default(view_4041, t_850);  view_4041 = t_850 = None
        view_4042 = torch.ops.aten.view.default(mm_793, [1, 512, 512, 256]);  mm_793 = None
        _to_copy_2273 = torch.ops.aten._to_copy.default(getitem_3922, dtype = torch.bfloat16);  getitem_3922 = None
        _to_copy_2274 = torch.ops.aten._to_copy.default(getitem_3915, dtype = torch.bfloat16);  getitem_3915 = None
        t_851 = torch.ops.aten.t.default(_to_copy_2273);  _to_copy_2273 = None
        view_4043 = torch.ops.aten.view.default(_to_copy_2274, [262144, 256]);  _to_copy_2274 = None
        mm_794 = torch.ops.aten.mm.default(view_4043, t_851);  view_4043 = t_851 = None
        view_4044 = torch.ops.aten.view.default(mm_794, [1, 512, 512, 256]);  mm_794 = None
        sigmoid_326 = torch.ops.aten.sigmoid.default(view_4044);  view_4044 = None
        mul_539 = torch.ops.aten.mul.Tensor(view_4042, sigmoid_326);  view_4042 = sigmoid_326 = None
        add_433 = torch.ops.aten.add.Tensor(add_427, mul_539);  mul_539 = None
        _to_copy_2275 = torch.ops.aten._to_copy.default(add_427, dtype = torch.float32)
        native_layer_norm_default_469 = torch.ops.aten.native_layer_norm.default(_to_copy_2275, [256], None, None, 1e-05);  _to_copy_2275 = None
        getitem_3941 = native_layer_norm_default_469[0]
        _to_copy_2276 = torch.ops.aten._to_copy.default(arg1376_1, dtype = torch.bfloat16);  arg1376_1 = None
        _to_copy_2277 = torch.ops.aten._to_copy.default(getitem_3941, dtype = torch.bfloat16)
        t_852 = torch.ops.aten.t.default(_to_copy_2276);  _to_copy_2276 = None
        view_4045 = torch.ops.aten.view.default(_to_copy_2277, [262144, 256]);  _to_copy_2277 = None
        mm_795 = torch.ops.aten.mm.default(view_4045, t_852);  view_4045 = t_852 = None
        view_4046 = torch.ops.aten.view.default(mm_795, [1, 512, 512, 8]);  mm_795 = None
        view_4047 = torch.ops.aten.view.default(view_4046, [1, 512, 512, 2, 4]);  view_4046 = None
        permute_2131 = torch.ops.aten.permute.default(view_4047, [0, 3, 4, 1, 2]);  view_4047 = None
        view_4048 = torch.ops.aten.view.default(permute_2131, [1, 2, 4, 1, 512, 512]);  permute_2131 = None
        view_4049 = torch.ops.aten.view.default(arg1407_1, [1, 1, 1, 1, 512, 512])
        bitwise_not_248 = torch.ops.aten.bitwise_not.default(view_4049);  view_4049 = None
        masked_fill_248 = torch.ops.aten.masked_fill.Scalar(view_4048, bitwise_not_248, -10000);  view_4048 = bitwise_not_248 = None
        view_4050 = torch.ops.aten.view.default(masked_fill_248, [1, 2, 4, 512, 512]);  masked_fill_248 = None
        permute_2132 = torch.ops.aten.permute.default(view_4050, [1, 0, 2, 3, 4]);  view_4050 = None
        view_4051 = torch.ops.aten.view.default(permute_2132, [2, 4, 1, 512, 512]);  permute_2132 = None
        _to_copy_2278 = torch.ops.aten._to_copy.default(arg1377_1, dtype = torch.bfloat16);  arg1377_1 = None
        _to_copy_2279 = torch.ops.aten._to_copy.default(getitem_3941, dtype = torch.bfloat16)
        t_853 = torch.ops.aten.t.default(_to_copy_2278);  _to_copy_2278 = None
        view_4052 = torch.ops.aten.view.default(_to_copy_2279, [262144, 256]);  _to_copy_2279 = None
        mm_796 = torch.ops.aten.mm.default(view_4052, t_853);  view_4052 = t_853 = None
        view_4053 = torch.ops.aten.view.default(mm_796, [1, 512, 512, 1024]);  mm_796 = None
        select_107 = torch.ops.aten.select.int(view_4051, 0, 0)
        view_4054 = torch.ops.aten.view.default(view_4053, [1, 512, 512, 4, 4, 64]);  view_4053 = None
        permute_2133 = torch.ops.aten.permute.default(view_4054, [4, 0, 3, 1, 2, 5]);  view_4054 = None
        view_4055 = torch.ops.aten.view.default(permute_2133, [4, 4, 512, 512, 64]);  permute_2133 = None
        unbind_int_181 = torch.ops.aten.unbind.int(view_4055);  view_4055 = None
        getitem_3944 = unbind_int_181[0]
        getitem_3945 = unbind_int_181[1]
        getitem_3946 = unbind_int_181[2]
        getitem_3947 = unbind_int_181[3];  unbind_int_181 = None
        expand_262 = torch.ops.aten.expand.default(select_107, [4, 512, 512, 512]);  select_107 = None
        _scaled_dot_product_efficient_attention_default_153 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3944, getitem_3945, getitem_3946, expand_262, False);  getitem_3944 = getitem_3945 = getitem_3946 = expand_262 = None
        getitem_3948 = _scaled_dot_product_efficient_attention_default_153[0]
        sigmoid_327 = torch.ops.aten.sigmoid.default(getitem_3947);  getitem_3947 = None
        mul_540 = torch.ops.aten.mul.Tensor(getitem_3948, sigmoid_327);  getitem_3948 = sigmoid_327 = None
        view_4056 = torch.ops.aten.view.default(mul_540, [1, 4, 512, 512, 64]);  mul_540 = None
        permute_2134 = torch.ops.aten.permute.default(view_4056, [0, 2, 3, 1, 4]);  view_4056 = None
        clone_331 = torch.ops.aten.clone.default(permute_2134, memory_format = torch.contiguous_format);  permute_2134 = None
        _unsafe_view_276 = torch.ops.aten._unsafe_view.default(clone_331, [1, 512, 512, 256]);  clone_331 = None
        transpose_107 = torch.ops.aten.transpose.int(getitem_3941, 1, 2);  getitem_3941 = None
        _to_copy_2280 = torch.ops.aten._to_copy.default(arg1378_1, dtype = torch.bfloat16);  arg1378_1 = None
        _to_copy_2281 = torch.ops.aten._to_copy.default(transpose_107, dtype = torch.bfloat16);  transpose_107 = None
        t_854 = torch.ops.aten.t.default(_to_copy_2280);  _to_copy_2280 = None
        expand_263 = torch.ops.aten.expand.default(_to_copy_2281, [1, 512, 512, 256]);  _to_copy_2281 = None
        view_4057 = torch.ops.aten.view.default(expand_263, [512, 512, 256]);  expand_263 = None
        expand_264 = torch.ops.aten.expand.default(t_854, [1, 512, 256, 1024]);  t_854 = None
        view_4058 = torch.ops.aten.view.default(expand_264, [512, 256, 1024]);  expand_264 = None
        bmm_315 = torch.ops.aten.bmm.default(view_4057, view_4058);  view_4057 = view_4058 = None
        view_4059 = torch.ops.aten.view.default(bmm_315, [1, 512, 512, 1024]);  bmm_315 = None
        select_108 = torch.ops.aten.select.int(view_4051, 0, 1);  view_4051 = None
        view_4060 = torch.ops.aten.view.default(view_4059, [1, 512, 512, 4, 4, 64]);  view_4059 = None
        permute_2135 = torch.ops.aten.permute.default(view_4060, [4, 0, 3, 1, 2, 5]);  view_4060 = None
        view_4061 = torch.ops.aten.view.default(permute_2135, [4, 4, 512, 512, 64]);  permute_2135 = None
        unbind_int_182 = torch.ops.aten.unbind.int(view_4061);  view_4061 = None
        getitem_3952 = unbind_int_182[0]
        getitem_3953 = unbind_int_182[1]
        getitem_3954 = unbind_int_182[2]
        getitem_3955 = unbind_int_182[3];  unbind_int_182 = None
        expand_265 = torch.ops.aten.expand.default(select_108, [4, 512, 512, 512]);  select_108 = None
        _scaled_dot_product_efficient_attention_default_154 = torch.ops.aten._scaled_dot_product_efficient_attention.default(getitem_3952, getitem_3953, getitem_3954, expand_265, False);  getitem_3952 = getitem_3953 = getitem_3954 = expand_265 = None
        getitem_3956 = _scaled_dot_product_efficient_attention_default_154[0]
        sigmoid_328 = torch.ops.aten.sigmoid.default(getitem_3955);  getitem_3955 = None
        mul_541 = torch.ops.aten.mul.Tensor(getitem_3956, sigmoid_328);  getitem_3956 = sigmoid_328 = None
        view_4062 = torch.ops.aten.view.default(mul_541, [1, 4, 512, 512, 64]);  mul_541 = None
        permute_2136 = torch.ops.aten.permute.default(view_4062, [0, 2, 3, 1, 4]);  view_4062 = None
        clone_332 = torch.ops.aten.clone.default(permute_2136, memory_format = torch.contiguous_format);  permute_2136 = None
        _unsafe_view_277 = torch.ops.aten._unsafe_view.default(clone_332, [1, 512, 512, 256]);  clone_332 = None
        cat_59 = torch.ops.aten.cat.default([_unsafe_view_276, _unsafe_view_277], dim = -1);  _unsafe_view_276 = _unsafe_view_277 = None
        slice_252 = torch.ops.aten.slice.Tensor(arg1375_1, dim = 0, start = 0, end = 9223372036854775807);  arg1375_1 = None
        unsqueeze_1280 = torch.ops.aten.unsqueeze.default(slice_252, 1);  slice_252 = None
        mul_542 = torch.ops.aten.mul.Tensor(arg1379_1, unsqueeze_1280);  arg1379_1 = unsqueeze_1280 = None
        _to_copy_2282 = torch.ops.aten._to_copy.default(mul_542, dtype = torch.bfloat16);  mul_542 = None
        t_855 = torch.ops.aten.t.default(_to_copy_2282);  _to_copy_2282 = None
        view_4063 = torch.ops.aten.view.default(cat_59, [262144, 512]);  cat_59 = None
        mm_797 = torch.ops.aten.mm.default(view_4063, t_855);  view_4063 = t_855 = None
        view_4064 = torch.ops.aten.view.default(mm_797, [1, 512, 512, 256]);  mm_797 = None
        add_434 = torch.ops.aten.add.Tensor(add_433, view_4064);  add_433 = view_4064 = None
        split_tensor_431 = torch.ops.aten.split.Tensor(add_427, 512, dim = -2)
        getitem_3960 = split_tensor_431[0];  split_tensor_431 = None
        _to_copy_2283 = torch.ops.aten._to_copy.default(getitem_3960, dtype = torch.float32);  getitem_3960 = None
        native_layer_norm_default_470 = torch.ops.aten.native_layer_norm.default(_to_copy_2283, [256], arg1366_1, arg1367_1, 1e-05);  _to_copy_2283 = arg1366_1 = arg1367_1 = None
        getitem_3961 = native_layer_norm_default_470[0]
        _to_copy_2284 = torch.ops.aten._to_copy.default(arg1368_1, dtype = torch.bfloat16);  arg1368_1 = None
        _to_copy_2285 = torch.ops.aten._to_copy.default(getitem_3961, dtype = torch.bfloat16);  getitem_3961 = None
        t_856 = torch.ops.aten.t.default(_to_copy_2284);  _to_copy_2284 = None
        view_4065 = torch.ops.aten.view.default(_to_copy_2285, [262144, 256]);  _to_copy_2285 = None
        mm_798 = torch.ops.aten.mm.default(view_4065, t_856);  view_4065 = t_856 = None
        view_4066 = torch.ops.aten.view.default(mm_798, [1, 512, 512, 1024]);  mm_798 = None
        split_tensor_432 = torch.ops.aten.split.Tensor(view_4066, 512, dim = -1);  view_4066 = None
        getitem_3964 = split_tensor_432[0]
        getitem_3965 = split_tensor_432[1];  split_tensor_432 = None
        silu_112 = torch.ops.aten.silu.default(getitem_3964);  getitem_3964 = None
        mul_543 = torch.ops.aten.mul.Tensor(silu_112, getitem_3965);  silu_112 = getitem_3965 = None
        _to_copy_2286 = torch.ops.aten._to_copy.default(arg1369_1, dtype = torch.bfloat16);  arg1369_1 = None
        t_857 = torch.ops.aten.t.default(_to_copy_2286);  _to_copy_2286 = None
        view_4068 = torch.ops.aten.view.default(mul_543, [262144, 512]);  mul_543 = None
        mm_799 = torch.ops.aten.mm.default(view_4068, t_857);  view_4068 = t_857 = None
        view_4069 = torch.ops.aten.view.default(mm_799, [1, 512, 512, 256]);  mm_799 = None
        add_435 = torch.ops.aten.add.Tensor(add_434, view_4069);  add_434 = view_4069 = None
        _to_copy_2287 = torch.ops.aten._to_copy.default(add_431, dtype = torch.float32)
        native_layer_norm_default_471 = torch.ops.aten.native_layer_norm.default(_to_copy_2287, [384], arg1384_1, arg1385_1, 1e-05);  _to_copy_2287 = arg1384_1 = arg1385_1 = None
        getitem_3966 = native_layer_norm_default_471[0]
        _to_copy_2288 = torch.ops.aten._to_copy.default(add_427, dtype = torch.float32);  add_427 = None
        native_layer_norm_default_472 = torch.ops.aten.native_layer_norm.default(_to_copy_2288, [256], arg1386_1, arg1387_1, 1e-05);  _to_copy_2288 = arg1386_1 = arg1387_1 = None
        getitem_3969 = native_layer_norm_default_472[0]
        _to_copy_2289 = torch.ops.aten._to_copy.default(arg1388_1, dtype = torch.bfloat16);  arg1388_1 = None
        _to_copy_2290 = torch.ops.aten._to_copy.default(getitem_3969, dtype = torch.bfloat16);  getitem_3969 = None
        t_858 = torch.ops.aten.t.default(_to_copy_2289);  _to_copy_2289 = None
        view_4070 = torch.ops.aten.view.default(_to_copy_2290, [262144, 256]);  _to_copy_2290 = None
        mm_800 = torch.ops.aten.mm.default(view_4070, t_858);  view_4070 = t_858 = None
        view_4071 = torch.ops.aten.view.default(mm_800, [1, 512, 512, 16]);  mm_800 = None
        permute_2137 = torch.ops.aten.permute.default(view_4071, [0, 3, 1, 2]);  view_4071 = None
        view_4072 = torch.ops.aten.view.default(arg1407_1, [1, 1, 512, 512]);  arg1407_1 = None
        bitwise_not_249 = torch.ops.aten.bitwise_not.default(view_4072);  view_4072 = None
        masked_fill_249 = torch.ops.aten.masked_fill.Scalar(permute_2137, bitwise_not_249, -10000);  permute_2137 = bitwise_not_249 = None
        _to_copy_2291 = torch.ops.aten._to_copy.default(getitem_3966, dtype = torch.bfloat16);  getitem_3966 = None
        _to_copy_2292 = torch.ops.aten._to_copy.default(arg1390_1, dtype = torch.bfloat16);  arg1390_1 = None
        unsqueeze_1281 = torch.ops.aten.unsqueeze.default(_to_copy_2291, 3);  _to_copy_2291 = None
        unsqueeze_1282 = torch.ops.aten.unsqueeze.default(unsqueeze_1281, 4);  unsqueeze_1281 = None
        unsqueeze_1283 = torch.ops.aten.unsqueeze.default(unsqueeze_1282, 5);  unsqueeze_1282 = None
        permute_2138 = torch.ops.aten.permute.default(unsqueeze_1283, [3, 0, 4, 1, 5, 2]);  unsqueeze_1283 = None
        unsqueeze_1284 = torch.ops.aten.unsqueeze.default(_to_copy_2292, 4);  _to_copy_2292 = None
        unsqueeze_1285 = torch.ops.aten.unsqueeze.default(unsqueeze_1284, 5);  unsqueeze_1284 = None
        permute_2139 = torch.ops.aten.permute.default(unsqueeze_1285, [1, 4, 2, 5, 3, 0]);  unsqueeze_1285 = None
        permute_2140 = torch.ops.aten.permute.default(permute_2138, [3, 5, 0, 1, 2, 4]);  permute_2138 = None
        view_4073 = torch.ops.aten.view.default(permute_2140, [1, 512, 384]);  permute_2140 = None
        permute_2141 = torch.ops.aten.permute.default(permute_2139, [5, 0, 1, 2, 4, 3]);  permute_2139 = None
        view_4074 = torch.ops.aten.view.default(permute_2141, [1, 384, 1536]);  permute_2141 = None
        bmm_316 = torch.ops.aten.bmm.default(view_4073, view_4074);  view_4073 = view_4074 = None
        view_4075 = torch.ops.aten.view.default(bmm_316, [512, 1, 4, 1, 16, 24]);  bmm_316 = None
        permute_2142 = torch.ops.aten.permute.default(view_4075, [2, 3, 4, 0, 5, 1]);  view_4075 = None
        view_4076 = torch.ops.aten.view.default(permute_2142, [4, 1, 16, 512, 24]);  permute_2142 = None
        unbind_int_183 = torch.ops.aten.unbind.int(view_4076);  view_4076 = None
        getitem_3972 = unbind_int_183[0]
        getitem_3973 = unbind_int_183[1]
        getitem_3974 = unbind_int_183[2]
        getitem_3975 = unbind_int_183[3];  unbind_int_183 = None
        view_4077 = torch.ops.aten.view.default(arg1389_1, [1, 16, 1, 24]);  arg1389_1 = None
        add_436 = torch.ops.aten.add.Tensor(getitem_3972, view_4077);  getitem_3972 = view_4077 = None
        _to_copy_2293 = torch.ops.aten._to_copy.default(add_436, dtype = torch.bfloat16);  add_436 = None
        expand_266 = torch.ops.aten.expand.default(masked_fill_249, [1, 16, 512, 512]);  masked_fill_249 = None
        _scaled_dot_product_efficient_attention_default_155 = torch.ops.aten._scaled_dot_product_efficient_attention.default(_to_copy_2293, getitem_3973, getitem_3974, expand_266, False);  _to_copy_2293 = getitem_3973 = getitem_3974 = expand_266 = None
        getitem_3976 = _scaled_dot_product_efficient_attention_default_155[0]
        add_437 = torch.ops.aten.add.Tensor(getitem_3975, 1);  getitem_3975 = None
        sigmoid_329 = torch.ops.aten.sigmoid.default(add_437);  add_437 = None
        mul_544 = torch.ops.aten.mul.Tensor(getitem_3976, sigmoid_329);  getitem_3976 = sigmoid_329 = None
        _to_copy_2294 = torch.ops.aten._to_copy.default(arg1391_1, dtype = torch.bfloat16);  arg1391_1 = None
        unsqueeze_1286 = torch.ops.aten.unsqueeze.default(mul_544, 4);  mul_544 = None
        permute_2143 = torch.ops.aten.permute.default(unsqueeze_1286, [0, 2, 4, 3, 1]);  unsqueeze_1286 = None
        unsqueeze_1287 = torch.ops.aten.unsqueeze.default(_to_copy_2294, 3);  _to_copy_2294 = None
        unsqueeze_1288 = torch.ops.aten.unsqueeze.default(unsqueeze_1287, 4);  unsqueeze_1287 = None
        permute_2144 = torch.ops.aten.permute.default(unsqueeze_1288, [3, 4, 2, 1, 0]);  unsqueeze_1288 = None
        permute_2145 = torch.ops.aten.permute.default(permute_2143, [1, 3, 4, 0, 2]);  permute_2143 = None
        clone_333 = torch.ops.aten.clone.default(permute_2145, memory_format = torch.contiguous_format);  permute_2145 = None
        _unsafe_view_278 = torch.ops.aten._unsafe_view.default(clone_333, [1, 512, 384]);  clone_333 = None
        permute_2146 = torch.ops.aten.permute.default(permute_2144, [3, 4, 0, 2, 1]);  permute_2144 = None
        clone_334 = torch.ops.aten.clone.default(permute_2146, memory_format = torch.contiguous_format);  permute_2146 = None
        _unsafe_view_279 = torch.ops.aten._unsafe_view.default(clone_334, [1, 384, 384]);  clone_334 = None
        bmm_317 = torch.ops.aten.bmm.default(_unsafe_view_278, _unsafe_view_279);  _unsafe_view_278 = _unsafe_view_279 = None
        view_4078 = torch.ops.aten.view.default(bmm_317, [512, 1, 1, 1, 384]);  bmm_317 = None
        permute_2147 = torch.ops.aten.permute.default(view_4078, [3, 0, 4, 1, 2]);  view_4078 = None
        view_4079 = torch.ops.aten.view.default(permute_2147, [1, 512, 384]);  permute_2147 = None
        unsqueeze_1289 = torch.ops.aten.unsqueeze.default(arg1406_1, -1);  arg1406_1 = None
        mul_545 = torch.ops.aten.mul.Tensor(view_4079, unsqueeze_1289);  view_4079 = unsqueeze_1289 = None
        add_438 = torch.ops.aten.add.Tensor(add_431, mul_545);  mul_545 = None
        split_tensor_433 = torch.ops.aten.split.Tensor(add_431, 512, dim = -2);  add_431 = None
        getitem_3980 = split_tensor_433[0];  split_tensor_433 = None
        _to_copy_2295 = torch.ops.aten._to_copy.default(getitem_3980, dtype = torch.float32);  getitem_3980 = None
        native_layer_norm_default_473 = torch.ops.aten.native_layer_norm.default(_to_copy_2295, [384], arg1380_1, arg1381_1, 1e-05);  _to_copy_2295 = arg1380_1 = arg1381_1 = None
        getitem_3981 = native_layer_norm_default_473[0]
        _to_copy_2296 = torch.ops.aten._to_copy.default(arg1382_1, dtype = torch.bfloat16);  arg1382_1 = None
        _to_copy_2297 = torch.ops.aten._to_copy.default(getitem_3981, dtype = torch.bfloat16);  getitem_3981 = None
        t_859 = torch.ops.aten.t.default(_to_copy_2296);  _to_copy_2296 = None
        view_4080 = torch.ops.aten.view.default(_to_copy_2297, [512, 384]);  _to_copy_2297 = None
        mm_801 = torch.ops.aten.mm.default(view_4080, t_859);  view_4080 = t_859 = None
        view_4081 = torch.ops.aten.view.default(mm_801, [1, 512, 1536]);  mm_801 = None
        split_tensor_434 = torch.ops.aten.split.Tensor(view_4081, 768, dim = -1);  view_4081 = None
        getitem_3984 = split_tensor_434[0]
        getitem_3985 = split_tensor_434[1];  split_tensor_434 = None
        silu_113 = torch.ops.aten.silu.default(getitem_3984);  getitem_3984 = None
        mul_546 = torch.ops.aten.mul.Tensor(silu_113, getitem_3985);  silu_113 = getitem_3985 = None
        _to_copy_2298 = torch.ops.aten._to_copy.default(arg1383_1, dtype = torch.bfloat16);  arg1383_1 = None
        t_860 = torch.ops.aten.t.default(_to_copy_2298);  _to_copy_2298 = None
        view_4083 = torch.ops.aten.view.default(mul_546, [512, 768]);  mul_546 = None
        mm_802 = torch.ops.aten.mm.default(view_4083, t_860);  view_4083 = t_860 = None
        view_4084 = torch.ops.aten.view.default(mm_802, [1, 512, 384]);  mm_802 = None
        add_439 = torch.ops.aten.add.Tensor(add_438, view_4084);  add_438 = view_4084 = None
        return (add_439, add_435)
        
    # To see more debug info, please use `graph_module.print_readable()`